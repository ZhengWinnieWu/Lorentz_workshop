{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from netCDF4 import Dataset as ncread\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import math\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "from cftime import DatetimeNoLeap\n",
    "import xarray as xr\n",
    "from eofs.standard import Eof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams #For changing text properties\n",
    "import cmocean #A package with beautiful colormaps\n",
    "import matplotlib.path as mpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime \n",
    "from datetime import timedelta\n",
    "from datetime import date\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in true_divide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-09 14:57:38.637266: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-09 14:57:38.637304: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-09 14:57:39.725632: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-09-09 14:57:39.725663: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-09-09 14:57:39.725691: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (jupyternoteboo): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf    \n",
    "#tf.compat.v1.disable_v2_behavior() # <-- HERE !\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import regularizers\n",
    "import keras.backend as K\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.layers import (Dropout, Activation, Reshape, Flatten, \n",
    "                                     Conv2D, LSTM, Dense, Dropout, Embedding, Bidirectional, GRU,\n",
    "                                     TimeDistributed, Concatenate)\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import initializers, regularizers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import constraints\n",
    "from tensorflow.keras.layers import Layer, InputSpec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "no files to open",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m root_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mera5_hoa_dry_mask_2deg.nc\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m#0.25\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m mask\u001b[38;5;241m=\u001b[39m\u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_mfdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_data\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcombine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mby_coords\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mparallel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m mask_nan\u001b[38;5;241m=\u001b[39mmask\u001b[38;5;241m.\u001b[39mwhere(mask\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#keep the values==1 and mask the rest\u001b[39;00m\n\u001b[1;32m      9\u001b[0m mask_nan\u001b[38;5;241m.\u001b[39mtp\u001b[38;5;241m.\u001b[39mplot()\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/xarray/backends/api.py:878\u001b[0m, in \u001b[0;36mopen_mfdataset\u001b[0;34m(paths, chunks, concat_dim, compat, preprocess, engine, lock, data_vars, coords, combine, autoclose, parallel, join, attrs_file, **kwargs)\u001b[0m\n\u001b[1;32m    875\u001b[0m     paths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(p) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(p, Path) \u001b[38;5;28;01melse\u001b[39;00m p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m paths]\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m paths:\n\u001b[0;32m--> 878\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno files to open\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    880\u001b[0m \u001b[38;5;66;03m# If combine='by_coords' then this is unnecessary, but quick.\u001b[39;00m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;66;03m# If combine='nested' then this creates a flat list which is easier to\u001b[39;00m\n\u001b[1;32m    882\u001b[0m \u001b[38;5;66;03m# iterate over, while saving the originally-supplied structure as \"ids\"\u001b[39;00m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m combine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnested\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOSError\u001b[0m: no files to open"
     ]
    }
   ],
   "source": [
    "# Import regional mask\n",
    "root_data = './data/'\n",
    "root_results = './results/'\n",
    "\n",
    "file = 'era5_hoa_dry_mask_2deg.nc' #0.25\n",
    "\n",
    "mask=xr.open_mfdataset(root_data+file,combine='by_coords',parallel=True)\n",
    "mask_nan=mask.where(mask==1) #keep the values==1 and mask the rest\n",
    "mask_nan.tp.plot()\n",
    "mask_nan.sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare target \n",
    "#### define drought events when 31-day running mean rainfall in OND from 1980-2021 is below the 33rd percentile, the tercile is calculated based on the 2000-2020 period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the spatial mean of the tp file after applying the spatial mask\n",
    "\n",
    "file=xr.open_mfdataset(root_data+f'/era5_tp_1959-2021_1_12_daily_2.0deg.nc',\n",
    "                          combine='by_coords',parallel=True)\n",
    "tp_dim=file.sel(longitude=slice(10,70),latitude=slice(24,-30))\n",
    "tp_series=np.multiply(mask_nan,tp_dim).mean(dim='latitude',skipna=True).mean(dim='longitude',skipna=True)\n",
    "\n",
    "# Calculate 33 percentile\n",
    "# Create daily values equal to a 31 day rolling. Select OND 2000-2020 data to decide the quantile threshold\n",
    "tp_rol=tp_series.rolling(time=31, center=True).mean().sel(time=tp_series.time.dt.year.isin([np.arange(2000,2021)]))\n",
    "tp_quantile=tp_rol.sel(time=tp_rol.time.dt.month.isin([10,11,12])).quantile(0.33)\n",
    "print('value of the 33 percentile',tp_quantile.tp.values)\n",
    "\n",
    "# Create index time series\n",
    "# Replace the values bellow the 33 percentile with 1 and the rest with zeros\n",
    "SYY = 1980   # start year, could be changed\n",
    "EYY = 2021   # end year, could be changed\n",
    "tp_rol = tp_series.rolling(time=31, center=True).mean().sel(time=tp_series.time.dt.month.isin([10,11,12]))\n",
    "tp_rol_sel = tp_rol.sel(time = slice(str(SYY),str(EYY)))\n",
    "tp_index = tp_rol_sel < tp_quantile\n",
    "tp_index = tp_index.astype(int)\n",
    "print('tp index',tp_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select from the index time series the period for the target values (predictant) \n",
    "# Oct 16 to Dec 16 for the period 1980-2020 (each day corresponds to a 31-day rolling mean)\n",
    "# The chosen time period could be changed to any time period you want\n",
    "for iyr in range(SYY,EYY+1):\n",
    "    if iyr == SYY:\n",
    "        tp_target = tp_index.sel(time = slice(str(iyr)+'-10-16',str(iyr)+'-12-16'))\n",
    "    else:\n",
    "        tp_target = xr.concat([tp_target,tp_index.sel(time = slice(str(iyr)+'-10-16',str(iyr)+'-12-16'))], dim='time')\n",
    "print('number of 0 and 1: ',np.unique(tp_target['tp'],return_counts=True))\n",
    "print(tp_target)\n",
    "plt.plot(tp_target.tp)\n",
    "\n",
    "# Make it into a numpy array\n",
    "target = tp_target['tp'].values\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare predictors: 30 days time series with the last day two weeks before the target day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor data preprocessing\n",
    "# can select the values and region you want by changing the parameters\n",
    "\n",
    "file_vars = ['ERA5_t2m', 'era5_t_850hpa', 'era5_z_200hpa', 'era5_z_500hpa', 'sst', 'era5_olr']\n",
    "#file_vars = ['sst']\n",
    "header_vars = ['t2m', 't', 'z', 'z', 'sst', 'olr-mean']\n",
    "#header_vars = ['sst']\n",
    "\n",
    "# select regions for the individual predictor\n",
    "lon_slices = [[-16,54],[-30,90],[-30,90],[-30,90],[-180,180],[40,180]]\n",
    "lat_slices = [[16,0],[30,-20],[-20,30],[-20,30],[40,-20],[-20,20]]\n",
    "#lon_slices = [[-180,178]]\n",
    "#lat_slices = [[40,-20]]\n",
    "\n",
    "for file_var, header_var, lon_slice, lat_slice in zip (file_vars, header_vars, lon_slices, lat_slices):\n",
    "    if file_var=='era5_olr':\n",
    "        file = xr.open_mfdataset(root_data+file_var+'_1950_2021_daily_1deg_tropics.nc',\n",
    "                                 combine='by_coords',parallel=True)\n",
    "        print('olr')\n",
    "        var_dim = file.sel(lon=slice(lon_slice[0],lon_slice[1]),lat=slice(lat_slice[0],lat_slice[1]))\n",
    "        lon = var_dim.coords['lon'].values\n",
    "        lat = var_dim.coords['lat'].values\n",
    "    else:\n",
    "        file = xr.open_mfdataset(root_data+file_var+'_1959-2021_1_12_daily_2.0deg.nc',\n",
    "                                 combine='by_coords',parallel=True)\n",
    "        print(header_var)\n",
    "        var_dim = file.sel(longitude=slice(lon_slice[0],lon_slice[1]),latitude=slice(lat_slice[0],lat_slice[1]))\n",
    "        lon = var_dim.coords['longitude'].values\n",
    "        lat = var_dim.coords['latitude'].values\n",
    "\n",
    "    nlon = len(lon)\n",
    "    nlat = len(lat)\n",
    "    \n",
    "    var_series = var_dim.sel(time=var_dim.time.dt.year.isin([np.arange(SYY,EYY+1)])).rolling(time=7, center=False).mean(skipna=True)\n",
    "    #print(var_series)\n",
    "    \n",
    "    var_anom_series = var_series.groupby(\"time.dayofyear\") - var_series.groupby(\"time.dayofyear\").mean(\"time\",skipna=True)\n",
    "    var_std = var_anom_series / (var_anom_series.std(dim='time', ddof=1))\n",
    "    \n",
    "    # save the data for future use\n",
    "    np.save(root_results+'series_var_'+file_var+'.npy',var_std.to_array())\n",
    "    var_std.to_netcdf(root_results+'series_var_'+file_var+'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the original data \n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cf\n",
    "color = 'RdBu_r' \n",
    "colorbarMin = -3\n",
    "colorbarMax = 3\n",
    "colorspace = 0.5\n",
    "level = np.arange(colorbarMin,colorbarMax+colorspace,colorspace)\n",
    "ax = plt.axes(projection=ccrs.cartopy.crs.PlateCarree(central_longitude=180))\n",
    "h = ax.contourf(lon, lat, var_anom_series['sst'][122,:,:], level, transform=ccrs.PlateCarree(), cmap=color,extend='both')\n",
    "cbar = plt.colorbar(h, orientation='horizontal', shrink=1,fraction=0.1,pad=0.1,aspect=40)\n",
    "cbar.ax.tick_params(labelsize=10) \n",
    "colorLabel='SST [K]'\n",
    "cbar.set_label(label=colorLabel,fontsize=10)\n",
    "#Add in the coordinate system:\n",
    "long = np.arange(-180, 180, 45)#spacing of 45 degrees\n",
    "latg = np.arange(-20, 40, 10)#spacing of 15 degrees\n",
    "ax.set_xticks(long, crs=ccrs.PlateCarree());\n",
    "ax.set_yticks(latg, crs=ccrs.PlateCarree());\n",
    "ax.set_xticklabels(long,fontsize=8)\n",
    "ax.set_yticklabels(latg,fontsize=8)\n",
    "ax.set_ylabel('lat',fontsize=10);\n",
    "ax.set_xlabel('lon',fontsize=10);\n",
    "\n",
    "#Add in the continents\n",
    "#define the coastlines, the color (#000000) and the resolution (110m) \n",
    "feature1 = cf.NaturalEarthFeature(\n",
    "    name='coastline', category='physical',\n",
    "    scale='110m',\n",
    "    edgecolor='#000000', facecolor='none')\n",
    "#define the land, the color (#AAAAAA) and the resolution (110m), mask the land, use for SST\n",
    "feature2 = cf.NaturalEarthFeature(\n",
    "    name='land', category='physical',\n",
    "    scale='110m',\n",
    "    facecolor='#AAAAAA')\n",
    "\n",
    "ax.add_feature(feature2)\n",
    "\n",
    "#Set a title for your map:\n",
    "title = 'SST daily anomaly'\n",
    "plt.title(title,fontsize=10, y=1.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "SMM = 10\n",
    "SDD = 16\n",
    "EMM = 12\n",
    "EDD = 16\n",
    "ntimestep = 60\n",
    "rw = 31 # running mean window\n",
    "half_rw = int(rw/2)\n",
    "lead_time = 14\n",
    "rw_1 = 7 # running mean window for predictor\n",
    "# 31-day running mean so 15 days before the central day + 2 weeks in advance + 7-day running mean + 30-day time steps\n",
    "\n",
    "# Import predictors if the predictors have been saved before\n",
    "predictor = {}\n",
    "for file_var, header_var in zip(file_vars, header_vars):\n",
    "    x = xr.open_mfdataset(root_results+'series_var_'+file_var+'.nc')\n",
    "    date_target = datetime.strftime(datetime(year=SYY,month=SMM,day=SDD), \"%Y.%m.%d\")\n",
    "    predictor[file_var] = np.ndarray((len(tp_target['tp']),ntimestep)+x[header_var].shape[1:]+(1,))\n",
    "    \n",
    "    it = 0\n",
    "    ii = 0\n",
    "    YYY = SYY\n",
    "    while YYY < EYY+1:\n",
    "        date_start = datetime.strftime(datetime.strptime(date_target, \"%Y.%m.%d\")-timedelta(days=half_rw+lead_time+rw_1+ntimestep-1),\"%Y.%m.%d\")\n",
    "        date_end = datetime.strftime(datetime.strptime(date_target, \"%Y.%m.%d\")-timedelta(days=half_rw+lead_time+rw_1),\"%Y.%m.%d\")\n",
    "        predictor[file_var][ii,:,:,:,:] = np.expand_dims(x[header_var].sel(time = slice(date_start,date_end)),axis=-1)\n",
    "        if date_target == datetime.strftime(datetime(year=YYY,month=EMM,day=EDD),\"%Y.%m.%d\"):\n",
    "            YYY = YYY+1\n",
    "            date_target = datetime.strftime(datetime(year=YYY,month=SMM,day=SDD), \"%Y.%m.%d\")\n",
    "            it = 0\n",
    "            print(YYY)\n",
    "        else:\n",
    "            it = 1\n",
    "        ii = ii+1\n",
    "        date_target = datetime.strftime(datetime.strptime(date_target, \"%Y.%m.%d\")+timedelta(days=it),\"%Y.%m.%d\") \n",
    "    end = time.time()\n",
    "    duration = end-start\n",
    "    print(\"Time took {:3.1f} min\".format(duration/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_val(file_vars, data_predictor, data_target, test_frac, val_frac):\n",
    "    \"\"\"Splits data across periods into train, test, and validation\"\"\"\n",
    "    # assign the last int(-test_frac*len(tp_predictor)) rows to test data\n",
    "    test_predictor = {}\n",
    "    for file_var in file_vars:\n",
    "        test_predictor[file_var] = data_predictor[file_var][int(-test_frac*len(data_target)):]\n",
    "    test_target = data_target[int(-test_frac*len(data_target)):]\n",
    "    \n",
    "    # assign the last int(-test_frac*len(tp_predictor)) from the remaining rows to validation data\n",
    "    remain_predictor = {}\n",
    "    val_predictor = {}\n",
    "    for file_var in file_vars:\n",
    "        remain_predictor[file_var] = data_predictor[file_var][0:int(-test_frac*len(data_target))]\n",
    "        val_predictor[file_var] = remain_predictor[file_var][int(-val_frac*len(remain_predictor[file_var])):]\n",
    "    remain_target = data_target[0:int(-test_frac*len(data_target))]\n",
    "    val_target = remain_target[int(-val_frac*len(remain_predictor[file_var])):]\n",
    "    \n",
    "    # the remaining rows are assigned to train data\n",
    "    train_predictor = {}\n",
    "    for file_var in file_vars:\n",
    "        train_predictor[file_var] = remain_predictor[file_var][:int(-val_frac*len(remain_predictor[file_var]))]\n",
    "    train_target = remain_target[:int(-val_frac*len(remain_predictor[file_var]))]\n",
    "    return train_predictor, train_target, test_predictor, test_target, val_predictor, val_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input and output data for LSTM\n",
    "y_all = keras.utils.to_categorical(target)\n",
    "X_all = predictor\n",
    "for file_var in file_vars:\n",
    "    print(X_all[file_var].shape)\n",
    "print(y_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y, test_X, test_y, val_X, val_y = get_train_test_val(file_vars, X_all, y_all, test_frac=0.2, val_frac=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_var in file_vars:\n",
    "    print(train_X[file_var].shape)\n",
    "print(train_y.shape)\n",
    "\n",
    "for file_var in file_vars:\n",
    "    print(test_X[file_var].shape)\n",
    "print(test_y.shape)\n",
    "\n",
    "for file_var in file_vars:\n",
    "    print(val_X[file_var].shape)\n",
    "print(val_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = [0, 1, 2]\n",
    "names = [\"train\", \"val\", \"test\"]\n",
    "width = 0.75\n",
    "event_cnts = [np.unique(train_y[:,1],return_counts=True)[1][1],np.unique(val_y[:,1],return_counts=True)[1][1],np.unique(test_y[:,1],return_counts=True)[1][1]]\n",
    "nonevent_cnts = [np.unique(train_y[:,1],return_counts=True)[1][0],np.unique(val_y[:,1],return_counts=True)[1][0],np.unique(test_y[:,1],return_counts=True)[1][0]]\n",
    "\n",
    "p1 = plt.barh(ind, event_cnts, width)\n",
    "p2 = plt.barh(ind, nonevent_cnts, width, left=event_cnts)\n",
    "\n",
    "plt.yticks(ind, names)\n",
    "plt.ylabel(\"data set\")\n",
    "plt.xlabel(\"samples\")\n",
    "plt.title(\"Train/Validation/Test Splits\", fontsize=16)\n",
    "plt.legend([\"Event\", \"Non-event\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM with attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a class weight dictionary to help if the classes are unbalanced\n",
    "def class_weight_creator(Y):\n",
    "    class_dict = {}\n",
    "    weights = np.max(np.sum(Y, axis=0)) / np.sum(Y, axis=0)\n",
    "    for i in range( Y.shape[-1] ):\n",
    "        class_dict[i] = weights[i]\n",
    "        \n",
    "    return class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = class_weight_creator(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1 #4 #32\n",
    "epochs = 30\n",
    "shuffle = True \n",
    "verbose = 2 #Set whether the model will output information when trained (0 = no output; 2 = output accuracy every epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_path = '/home/zwu/Lorentz_workshop/test/checkpoint_test'\n",
    "callbacks_list = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=callbacks_path,\n",
    "        monitor='val_acc',   # tf.keras.metrics.AUC(from_logits=True)\n",
    "        save_best_only=True,\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM with attention layer\n",
    "ntimestep = 60    # number of time step used in the predictors\n",
    "\n",
    "# input_tensors = [] \n",
    "# CNN_branches = []\n",
    "# for file_var in file_vars:\n",
    "#     input_tensors.append(Input(shape=train_X[file_var].shape[1:]))\n",
    "#     # print(input_tensors[-1].shape)\n",
    "#     CNN_branches.append(build_CNN(input_tensors[-1]))\n",
    "# CNN_outputs = layers.Concatenate(axis=-1)(CNN_branches)\n",
    "\n",
    "# # print(CNN_outputs.shape)\n",
    "                        \n",
    "# layer1 = layers.LSTM(100, return_sequences=True, kernel_regularizer=regularizers.l2(1))(CNN_outputs)\n",
    "# layer1 = layers.LSTM(20, return_sequences=True, kernel_regularizer=regularizers.l2(0.01))(layer1)\n",
    "# layer1, alfa = AttentionWithContext()(layer1)\n",
    "# layer1 = Addition()(layer1)\n",
    "# layer1 = layers.Dense(5, activation=\"relu\")(layer1)\n",
    "# output_tensor = layers.Dense(2,activation='softmax')(layer1)\n",
    "\n",
    "# model = Model(input_tensors, output_tensor)\n",
    "import models\n",
    "from models import assemble_network\n",
    "hyperparams = {'regval':[1,0.1], 'neurons': [20,20],'layers': 2}\n",
    "model = assemble_network(train_X,file_var,ntimestep, nfeature, **hyperparams)\n",
    "opt = optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['acc'])\n",
    "\n",
    "model.summary()\n",
    "plot_model(model, to_file='model.png', show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "history = model.fit(train_X, train_y, epochs=epochs, batch_size=batch_size, validation_data=(val_X, val_y), shuffle = shuffle, verbose=verbose, class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curve\n",
    "train_acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=plt.figaspect(0.25))\n",
    "ax1.plot(train_acc, label='Training Accuracy')\n",
    "ax1.plot(val_acc, label='Validation Accuracy')\n",
    "ax1.set_title('Accuracy')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_ylim(0,1.1)\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(train_loss, label='Training loss')\n",
    "ax2.plot(val_loss, label='Validation loss')\n",
    "ax2.set_title('Loss')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curve\n",
    "train_acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=plt.figaspect(0.25))\n",
    "ax1.plot(train_acc, label='Training Accuracy')\n",
    "ax1.plot(val_acc, label='Validation Accuracy')\n",
    "ax1.set_title('Accuracy')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_ylim(0,1.1)\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(train_loss, label='Training loss')\n",
    "ax2.plot(val_loss, label='Validation loss')\n",
    "ax2.set_title('Loss')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.calibration import calibration_curve\n",
    "test_predict = model.predict(test_X)\n",
    "y_pred = np.argmax(model.predict(test_X),axis=1)\n",
    "print('Recall: '+str(round(recall_score(test_y[:,1],y_pred),2)))\n",
    "print('Precision: '+str(round(precision_score(test_y[:,1],y_pred),2)))\n",
    "print('F1-score: '+str(round(f1_score(test_y[:,1],y_pred),2)))\n",
    "print('Accuracy: '+str(round(accuracy_score(test_y[:,1],y_pred),2)))\n",
    "print('Brier score:' +str(brier_score_loss(test_y[0:-20,1], test_predict[0:-20,1])))\n",
    "\n",
    "calib_y, calib_x = calibration_curve(test_y[:,1],test_predict[:,1],n_bins=10)\n",
    "plt.plot(calib_y, calib_x, marker='o', color=\"darkorange\", label='LSTMatt')\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", linestyle=\"--\", label='Best score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "yprob = model.predict(test_X)\n",
    "yprob = yprob[0:-20,1]\n",
    "testy = test_y[0:-20,1]\n",
    "lr_auc = roc_auc_score(testy, yprob)\n",
    "print(lr_auc)\n",
    "lr_fpr, lr_tpr, thredhs = roc_curve(testy, yprob)\n",
    "print(testy.shape,lr_fpr.shape, lr_tpr.shape,thredhs.shape)\n",
    "# plot the roc curve for the model\n",
    "plt.plot(lr_fpr, lr_tpr, marker='.', color=\"darkorange\", label='LSTMatt')\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", linestyle=\"--\", label='No Skill')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()\n",
    "\n",
    "Y_predict = model.predict(test_X)\n",
    "yhat = np.argmax(Y_predict,axis=1)\n",
    "yhat = yhat[0:-20]\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(testy, yprob)\n",
    "lr_f1, lr_auc = f1_score(testy, yhat), auc(lr_recall, lr_precision)\n",
    "print(testy.shape,lr_recall.shape, lr_precision.shape)\n",
    "# summarize scores\n",
    "print('f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n",
    "# plot the precision-recall curves\n",
    "no_skill = len(testy[testy==1]) / len(testy)\n",
    "plt.plot(lr_recall, lr_precision, marker='.', color=\"darkorange\", label='LSTMatt')\n",
    "plt.plot([0, 1], [no_skill, no_skill], color=\"navy\", linestyle='--', label='No Skill')\n",
    "# axis labels\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get weightings of each time step and each sample\n",
    "intermediate_layer_model2 = Model(inputs=model.input,\n",
    "                                 outputs=model.layers[3].output)\n",
    "\n",
    "intermediate_layer_model1 = Model(inputs=model.input,\n",
    "                                 outputs=model.layers[2].output)\n",
    "\n",
    "intermediate_layer_model3 = Model(inputs=model.input,\n",
    "                                 outputs=model.layers[4].output)\n",
    "\n",
    "intermediate_output2, alfa_output = intermediate_layer_model2.predict(test_X, verbose=0)\n",
    "intermediate_output1 = intermediate_layer_model1.predict(test_X, verbose=0)\n",
    "intermediate_output3 = intermediate_layer_model3.predict(test_X, verbose=0)\n",
    "\n",
    "weights = intermediate_output2 / intermediate_output1\n",
    "print(np.shape(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the weights\n",
    "val_weights = np.ndarray((len(test_X),ntimestep))+np.nan\n",
    "for ii in range(len(test_X)):\n",
    "    for j in range(ntimestep):\n",
    "        val_weights[ii,j] = weights[ii][j][0]\n",
    "print(np.shape(val_weights))\n",
    "\n",
    "fig, axs = plt.subplots(1, figsize=plt.figaspect(0.15))\n",
    "for ii in range(len(test_X)):\n",
    "    plt.plot(val_weights[ii,:])\n",
    "\n",
    "fig, axs = plt.subplots(1, figsize=plt.figaspect(0.15))\n",
    "plt.plot(np.nanmean(val_weights,axis=0),'k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SST predictor\n",
    "root_data = '/s2s_nobackup/zhengwu/workshop/'\n",
    "file = xr.open_mfdataset(root_data+f'/era5_sst_1959-2021_1_12_daily_2.0deg.nc',\n",
    "                          combine='by_coords',parallel=True)\n",
    "#sst_dim = file.sel(longitude=slice(50,180),latitude=slice(40,-20))\n",
    "sst_dim = file.sel(latitude=slice(40,-20))\n",
    "lon = sst_dim.coords['longitude'].values\n",
    "lat = sst_dim.coords['latitude'].values\n",
    "nlon = len(lon)\n",
    "nlat = len(lat)\n",
    "\n",
    "sst_series = sst_dim.sel(time=sst_dim.time.dt.year.isin([np.arange(SYY,EYY+1)])).rolling(time=7, center=False).mean(skipna=True)\n",
    "print(sst_series)\n",
    "\n",
    "sst_anom_series = sst_series.groupby(\"time.dayofyear\") - sst_series.groupby(\"time.dayofyear\").mean(\"time\",skipna=True)\n",
    "\n",
    "# EOF focus on July to Sep\n",
    "sst_anom_sel = sst_anom_series.sel(time=sst_anom_series.time.dt.month.isin([7,8,9,10,11]))['sst'].values\n",
    "# first, make a grid out of the longitude and latitude vectors so they have the same dimensions \n",
    "lon2d,lat2d = np.meshgrid(lon,lat)\n",
    "wgts = np.cos(lat2d/180*np.pi)**0.5\n",
    "print(wgts.shape)\n",
    "solver = Eof(sst_anom_sel,weights=wgts)\n",
    "\n",
    "# EOFs are multiplied by the square-root of their eigenvalues (then the EOF patterns will carry the units)\n",
    "nmode = 4\n",
    "EOF = solver.eofs(neofs=nmode,eofscaling=2) # get the first four eofs\n",
    "print(np.shape(EOF))\n",
    "eigenv = solver.eigenvalues(neigs=nmode)\n",
    "print(eigenv)\n",
    "VarEx = solver.varianceFraction(neigs=nmode)*100\n",
    "print(sum(VarEx))\n",
    "PC = solver.pcs(npcs=nmode,pcscaling=1)\n",
    "print(np.shape(PC))\n",
    "\n",
    "mode = np.arange(nmode)\n",
    "sst_anom_dim = sst_anom_series.sel(time=sst_anom_series.time.dt.month.isin([7,8,9,10,11]))\n",
    "time_dim = sst_anom_dim.coords['time']\n",
    "print(sst_anom_dim.coords['time'])\n",
    "pc_xr = xr.DataArray(PC, coords={'time': time_dim, 'mode': mode}, dims=[\"time\",\"mode\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
