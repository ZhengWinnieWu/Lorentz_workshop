{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from netCDF4 import Dataset as ncread\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import math\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "from cftime import DatetimeNoLeap\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 17:10:46.232293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-09 17:10:48.081846: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-07-09 17:10:48.081902: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-07-09 17:10:55.913552: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-09 17:10:55.913739: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-09 17:10:55.913753: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from preprocess import get_principle_components_and_EOFs\n",
    "from L_functions import sel_train_data_lead, climat_probab\n",
    "from train import get_train_test_val, cross_valid_one_out\n",
    "from post import score, rps, rpss\n",
    "import os\n",
    "import visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams #For changing text properties\n",
    "import cmocean #A package with beautiful colormaps\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cf\n",
    "import matplotlib.path as mpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime \n",
    "from datetime import timedelta\n",
    "from datetime import date\n",
    "import time\n",
    "from eofs.xarray import Eof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in true_divide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf    \n",
    "#tf.compat.v1.disable_v2_behavior() # <-- HERE !\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dropout, Activation, Reshape, Flatten, LSTM, Dense, Dropout, Embedding, Bidirectional, GRU\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import initializers, regularizers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import constraints\n",
    "from tensorflow.keras.layers import Layer, InputSpec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjust paths for your own machine\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "print('Adjust paths for your own machine')\n",
    "root_data = '/s2s/mpyrina/Lorentz_Workshop/Data/'\n",
    "\n",
    "root_results = '/s2s_nobackup/zhengwu/workshop/Results/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### target data and climatological/ presistence probabilities\n",
    "see https://github.com/AI4S2S/Lorentz_s2spy_workshop/blob/main/preprocess_target.ipynb\n",
    "\n",
    "-target should be center alinged like in the notebook of the link above\n",
    "\n",
    "-no direct import of file chrips_1981-2021_target_new_left.nc, its corrupted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYY = 1981   # start year of target data\n",
    "EYY = 2010   # DUE TO OLR missing values in 2021 !!!\n",
    "NYRS = EYY-SYY+1\n",
    "\n",
    "CSYY = SYY # start year for the calculation of climatology\n",
    "CEYY = EYY\n",
    "\n",
    "#CSYY = 2000 # start year for the calculation of climatology\n",
    "#CEYY = 2020 # end year for the calculation of climatology\n",
    "\n",
    "all_years = list(np.arange(SYY,EYY+1))\n",
    "#take_OND_years = list(np.arange(SYY,EYY+1))\n",
    "#take_OND_years = [y for y in take_OND_years if y not in drop_OND_years]\n",
    "# If model used to predict MAM then:\n",
    "#drop_MAM_years = [2009,2001,2002,2005,2020]\n",
    "#drop_OND_years = [2004,2005,2006,2007,2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target array <xarray.Dataset>\n",
      "Dimensions:  (time: 1920)\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 1981-10-08 1981-10-09 ... 2010-12-10\n",
      "Data variables:\n",
      "    precip   (time) int64 0 0 0 0 0 0 0 0 0 0 0 0 0 ... 1 1 1 1 1 1 1 1 1 1 1 1 counts (array([0, 1]), array([1329,  591]))\n",
      "target array <xarray.Dataset>\n",
      "Dimensions:  (time: 1920)\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 1981-10-08 1981-10-09 ... 2010-12-10\n",
      "Data variables:\n",
      "    precip   (time) int64 0 0 0 0 0 0 0 0 0 2 2 2 2 ... 0 0 0 0 0 0 0 0 0 0 0 0 counts (array([0, 2]), array([1264,  656]))\n",
      "target array <xarray.Dataset>\n",
      "Dimensions:  (time: 1920)\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 1981-10-08 1981-10-09 ... 2010-12-10\n",
      "Data variables:\n",
      "    precip   (time) int64 3 3 3 3 3 3 3 3 3 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 0 0 counts (array([0, 3]), array([1247,  673]))\n",
      "target array <xarray.Dataset>\n",
      "Dimensions:  (time: 1920)\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 1981-10-08 1981-10-09 ... 2010-12-10\n",
      "Data variables:\n",
      "    precip   (time) int64 3 3 3 3 3 3 3 3 3 2 2 2 2 ... 1 1 1 1 1 1 1 1 1 1 1 1 counts (array([1, 2, 3]), array([591, 656, 673]))\n"
     ]
    }
   ],
   "source": [
    "# Calculate the spatial mean of the tp file after applying the spatial mask\n",
    "file=xr.open_mfdataset(root_data+f'/chrips_precip_1981-2021.nc',combine='by_coords').compute()\n",
    "tp_series=file.sel(longitude=slice(38,53),latitude=slice(-5,8)).mean(dim=('latitude', 'longitude'))\n",
    "\n",
    "# Apply rolling mean #time=28\n",
    "tp_rol = tp_series.rolling(time=15, min_periods=1, center=True).mean().sel(time = slice(str(SYY),str(EYY)))\n",
    "\n",
    "# Select climatology years + calculate tercile\n",
    "tp_rol_cl = tp_rol.sel(time = slice(str(CSYY),str(CEYY)))\n",
    "tp_rol_q1 = tp_rol_cl.groupby(tp_rol_cl.time.dt.dayofyear).quantile(q=0.333,dim='time')\n",
    "tp_rol_q3 = tp_rol_cl.groupby(tp_rol_cl.time.dt.dayofyear).quantile(q=0.666,dim='time')\n",
    "\n",
    "# Smoothed percentile threshold\n",
    "tp_rol_q_sm1 = tp_rol_q1.rolling(dayofyear=31, min_periods=1, center=True).mean()\n",
    "tp_rol_q_sm3 = tp_rol_q3.rolling(dayofyear=31, min_periods=1, center=True).mean()\n",
    "#print('values of tercile', tp_rol_q_sm1)\n",
    "\n",
    "# Tercile index (like Sem, but according to smoothed tercile)\n",
    "tp1_terc = (tp_rol.groupby(tp_rol.time.dt.dayofyear) <= tp_rol_q_sm1).astype(int).drop('dayofyear').drop('quantile')\n",
    "\n",
    "tp3_terc_a = (tp_rol.groupby(tp_rol.time.dt.dayofyear) < tp_rol_q_sm3).astype(int).drop('dayofyear').drop('quantile')\n",
    "tp3_terc_b = (tp_rol.groupby(tp_rol.time.dt.dayofyear) > tp_rol_q_sm1).astype(int).drop('dayofyear').drop('quantile')\n",
    "tp3_terc = tp3_terc_a * tp3_terc_b\n",
    "\n",
    "tp4_terc = (tp_rol.groupby(tp_rol.time.dt.dayofyear) >= tp_rol_q_sm3).astype(int).drop('dayofyear').drop('quantile')\n",
    "#print('target array', tp1_terc, 'counts',np.unique(tp1_terc['precip'], return_counts=True))\n",
    "\n",
    "# Tercile index (like Sem, but according to smoothed tercile)\n",
    "tp11_terc = tp1_terc\n",
    "tp33_terc = xr.where(tp3_terc==1,2,0)\n",
    "tp44_terc = xr.where(tp4_terc==1,3,0)\n",
    "\n",
    "# Select daily values to be predicted\n",
    "sm_sd = 8\n",
    "em_ed = 10\n",
    "t1 = xr.concat([tp11_terc.sel(time=slice(f\"{yyyy}-10-{sm_sd}\",f\"{yyyy}-12-{em_ed}\")) for yyyy in tp11_terc.time.dt.year.to_index().unique()],\"time\")\n",
    "print('target array', t1, 'counts',np.unique(t1['precip'], return_counts=True))\n",
    "t3 = xr.concat([tp33_terc.sel(time=slice(f\"{yyyy}-10-{sm_sd}\",f\"{yyyy}-12-{em_ed}\")) for yyyy in tp33_terc.time.dt.year.to_index().unique()],\"time\")\n",
    "print('target array', t3, 'counts',np.unique(t3['precip'], return_counts=True))\n",
    "t4 = xr.concat([tp44_terc.sel(time=slice(f\"{yyyy}-10-{sm_sd}\",f\"{yyyy}-12-{em_ed}\")) for yyyy in tp44_terc.time.dt.year.to_index().unique()],\"time\")\n",
    "print('target array', t4, 'counts',np.unique(t4['precip'], return_counts=True))\n",
    "\n",
    "# Create and save tp target\n",
    "tp_target = t1 + t3 + t4\n",
    "print('target array', tp_target, 'counts',np.unique(tp_target['precip'], return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2 {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt;\n",
       "Dimensions:    (time: 1920)\n",
       "Coordinates:\n",
       "  * time       (time) datetime64[ns] 1981-10-08 1981-10-09 ... 2010-12-10\n",
       "    dayofyear  (time) int64 281 282 283 284 285 286 ... 339 340 341 342 343 344\n",
       "Data variables:\n",
       "    precip     (time) float32 0.1622 0.1063 0.09337 ... -0.6619 -0.6312 -0.6131</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-77c1e340-3215-4ec5-a4a0-2d7cba85cabb' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-77c1e340-3215-4ec5-a4a0-2d7cba85cabb' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span class='xr-has-index'>time</span>: 1920</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-27ba806d-6a28-4031-8e9d-48d703611e67' class='xr-section-summary-in' type='checkbox'  checked><label for='section-27ba806d-6a28-4031-8e9d-48d703611e67' class='xr-section-summary' >Coordinates: <span>(2)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>time</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>1981-10-08 ... 2010-12-10</div><input id='attrs-8a2b622d-8961-471f-b688-10a163cefb07' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-8a2b622d-8961-471f-b688-10a163cefb07' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-051497b2-eb20-44ea-ba92-04eb9d1546ce' class='xr-var-data-in' type='checkbox'><label for='data-051497b2-eb20-44ea-ba92-04eb9d1546ce' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>standard_name :</span></dt><dd>time</dd><dt><span>axis :</span></dt><dd>T</dd></dl></div><div class='xr-var-data'><pre>array([&#x27;1981-10-08T00:00:00.000000000&#x27;, &#x27;1981-10-09T00:00:00.000000000&#x27;,\n",
       "       &#x27;1981-10-10T00:00:00.000000000&#x27;, ..., &#x27;2010-12-08T00:00:00.000000000&#x27;,\n",
       "       &#x27;2010-12-09T00:00:00.000000000&#x27;, &#x27;2010-12-10T00:00:00.000000000&#x27;],\n",
       "      dtype=&#x27;datetime64[ns]&#x27;)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>dayofyear</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>281 282 283 284 ... 341 342 343 344</div><input id='attrs-98517380-8ce4-4158-9b09-9b4b2f48266a' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-98517380-8ce4-4158-9b09-9b4b2f48266a' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-884572bc-316c-4422-aa47-31af9bd45adb' class='xr-var-data-in' type='checkbox'><label for='data-884572bc-316c-4422-aa47-31af9bd45adb' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([281, 282, 283, ..., 342, 343, 344])</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-dc0f9d55-f5e1-4367-9270-2335801b0557' class='xr-section-summary-in' type='checkbox'  checked><label for='section-dc0f9d55-f5e1-4367-9270-2335801b0557' class='xr-section-summary' >Data variables: <span>(1)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>precip</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>0.1622 0.1063 ... -0.6312 -0.6131</div><input id='attrs-14786a5f-c642-419b-9d9f-13e54e5bd69d' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-14786a5f-c642-419b-9d9f-13e54e5bd69d' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-03a44d1a-b5d4-4525-9d05-b1b97d302bf9' class='xr-var-data-in' type='checkbox'><label for='data-03a44d1a-b5d4-4525-9d05-b1b97d302bf9' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([ 0.16218817,  0.10634816,  0.09337091, ..., -0.6618509 ,\n",
       "       -0.6312262 , -0.61313325], dtype=float32)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-c9397d93-c4ad-4fa8-8e88-23c77489c1a1' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-c9397d93-c4ad-4fa8-8e88-23c77489c1a1' class='xr-section-summary'  title='Expand/collapse section'>Attributes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:    (time: 1920)\n",
       "Coordinates:\n",
       "  * time       (time) datetime64[ns] 1981-10-08 1981-10-09 ... 2010-12-10\n",
       "    dayofyear  (time) int64 281 282 283 284 285 286 ... 339 340 341 342 343 344\n",
       "Data variables:\n",
       "    precip     (time) float32 0.1622 0.1063 0.09337 ... -0.6619 -0.6312 -0.6131"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp_rol_cl_anom = tp_rol_cl.groupby(\"time.dayofyear\") - tp_rol_cl.groupby(\"time.dayofyear\").mean(\"time\",skipna=True)\n",
    "tp_anom = xr.concat([tp_rol_cl_anom.sel(time=slice(f\"{yyyy}-10-{sm_sd}\",f\"{yyyy}-12-{em_ed}\")) for yyyy in tp11_terc.time.dt.year.to_index().unique()],\"time\")\n",
    "tp_anom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4f3b23a130>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x172.8 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, figsize=plt.figaspect(0.15))\n",
    "plt.plot(tp_target.precip[0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'time' (time: 30)>\n",
      "array(['1981-10-28T00:00:00.000000000', '1982-10-28T00:00:00.000000000',\n",
      "       '1983-10-28T00:00:00.000000000', '1984-10-27T00:00:00.000000000',\n",
      "       '1985-10-28T00:00:00.000000000', '1986-10-28T00:00:00.000000000',\n",
      "       '1987-10-28T00:00:00.000000000', '1988-10-27T00:00:00.000000000',\n",
      "       '1989-10-28T00:00:00.000000000', '1990-10-28T00:00:00.000000000',\n",
      "       '1991-10-28T00:00:00.000000000', '1992-10-27T00:00:00.000000000',\n",
      "       '1993-10-28T00:00:00.000000000', '1994-10-28T00:00:00.000000000',\n",
      "       '1995-10-28T00:00:00.000000000', '1996-10-27T00:00:00.000000000',\n",
      "       '1997-10-28T00:00:00.000000000', '1998-10-28T00:00:00.000000000',\n",
      "       '1999-10-28T00:00:00.000000000', '2000-10-27T00:00:00.000000000',\n",
      "       '2001-10-28T00:00:00.000000000', '2002-10-28T00:00:00.000000000',\n",
      "       '2003-10-28T00:00:00.000000000', '2004-10-27T00:00:00.000000000',\n",
      "       '2005-10-28T00:00:00.000000000', '2006-10-28T00:00:00.000000000',\n",
      "       '2007-10-28T00:00:00.000000000', '2008-10-27T00:00:00.000000000',\n",
      "       '2009-10-28T00:00:00.000000000', '2010-10-28T00:00:00.000000000'],\n",
      "      dtype='datetime64[ns]')\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 1981-10-28 1982-10-28 ... 2010-10-28\n",
      "Attributes:\n",
      "    standard_name:  time\n",
      "    axis:           T\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1., 4., 4., 7., 3., 0., 0., 3., 1., 1., 2., 1., 1., 0., 0., 0., 0.,\n",
       "        1., 0., 1.]),\n",
       " array([0.2765902, 0.680053 , 1.0835158, 1.4869787, 1.8904414, 2.2939043,\n",
       "        2.697367 , 3.1008298, 3.5042925, 3.9077554, 4.3112183, 4.714681 ,\n",
       "        5.1181436, 5.5216064, 5.9250693, 6.328532 , 6.731995 , 7.135458 ,\n",
       "        7.5389204, 7.9423833, 8.345846 ], dtype=float32),\n",
       " <BarContainer object of 20 artists>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dummy = tp_rol_cl.precip[np.where(tp_rol_cl['time.dayofyear'].values == 301)]\n",
    "print(dummy.time)\n",
    "plt.plot(dummy)\n",
    "fig, axs = plt.subplots(1, 1, sharey=True, tight_layout=True)\n",
    "# We can set the number of bins with the *bins* keyword argument.\n",
    "axs.hist(dummy, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff75b4e6550>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(tp_rol_q1['precip'])\n",
    "plt.plot(tp_rol_q_sm1['precip'])\n",
    "plt.plot(tp_rol_q3['precip'])\n",
    "plt.plot(tp_rol_q_sm3['precip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of years to calculate probability from 30\n",
      "number of days per year (64,)\n",
      "(64,) [0.33333333 0.33333333 0.2        0.23333333 0.26666667 0.26666667\n",
      " 0.3        0.23333333 0.23333333 0.26666667 0.3        0.3\n",
      " 0.3        0.26666667 0.26666667 0.23333333 0.23333333 0.3\n",
      " 0.3        0.3        0.33333333 0.33333333 0.3        0.33333333\n",
      " 0.3        0.26666667 0.23333333 0.23333333 0.26666667 0.3\n",
      " 0.33333333 0.4        0.36666667 0.36666667 0.33333333 0.33333333\n",
      " 0.3        0.33333333 0.33333333 0.4        0.33333333 0.23333333\n",
      " 0.23333333 0.3        0.26666667 0.3        0.4        0.46666667\n",
      " 0.4        0.36666667 0.4        0.33333333 0.36666667 0.36666667\n",
      " 0.4        0.33333333 0.3        0.3        0.3        0.3\n",
      " 0.3        0.3        0.26666667 0.26666667]\n",
      "number of years to calculate probability from 30\n",
      "number of days per year (64,)\n",
      "(64,) [0.33333333 0.26666667 0.43333333 0.36666667 0.33333333 0.36666667\n",
      " 0.3        0.4        0.43333333 0.4        0.36666667 0.4\n",
      " 0.36666667 0.4        0.36666667 0.4        0.4        0.33333333\n",
      " 0.33333333 0.36666667 0.3        0.3        0.3        0.26666667\n",
      " 0.3        0.33333333 0.3        0.4        0.4        0.4\n",
      " 0.36666667 0.3        0.3        0.33333333 0.36666667 0.36666667\n",
      " 0.46666667 0.43333333 0.36666667 0.3        0.33333333 0.46666667\n",
      " 0.43333333 0.36666667 0.43333333 0.36666667 0.26666667 0.2\n",
      " 0.26666667 0.26666667 0.23333333 0.3        0.26666667 0.26666667\n",
      " 0.26666667 0.3        0.3        0.26666667 0.26666667 0.3\n",
      " 0.3        0.33333333 0.4        0.4       ]\n",
      "number of years to calculate probability from 30\n",
      "number of days per year (64,)\n",
      "(64,) [0.33333333 0.4        0.36666667 0.4        0.4        0.36666667\n",
      " 0.4        0.36666667 0.33333333 0.33333333 0.33333333 0.3\n",
      " 0.33333333 0.33333333 0.36666667 0.36666667 0.36666667 0.36666667\n",
      " 0.36666667 0.33333333 0.36666667 0.36666667 0.4        0.4\n",
      " 0.4        0.4        0.46666667 0.36666667 0.33333333 0.3\n",
      " 0.3        0.3        0.33333333 0.3        0.3        0.3\n",
      " 0.23333333 0.23333333 0.3        0.3        0.33333333 0.3\n",
      " 0.33333333 0.33333333 0.3        0.33333333 0.33333333 0.33333333\n",
      " 0.33333333 0.36666667 0.36666667 0.36666667 0.36666667 0.36666667\n",
      " 0.33333333 0.36666667 0.4        0.43333333 0.43333333 0.4\n",
      " 0.4        0.36666667 0.33333333 0.33333333]\n"
     ]
    }
   ],
   "source": [
    "# calculate daily climatological probability of getting a category\n",
    "\n",
    "# Category 1\n",
    "cat_1 = xr.concat([tp1_terc.sel(time=slice(f\"{yyyy}-10-{sm_sd}\",f\"{yyyy}-12-{em_ed}\")) for yyyy in tp1_terc.time.dt.year.to_index().unique()],\"time\")\n",
    "index_file,my_var,month,s_days_list,e_days_list = cat_1,'precip',10,[sm_sd,1,1],[31,30,em_ed]\n",
    "n_years=len(index_file.time.dt.year.to_index().unique())\n",
    "print('number of years to calculate probability from',n_years)\n",
    "\n",
    "# Probability excluding test years\n",
    "clim_pr_cat1, clim_pr_y_cat1 = climat_probab(index_file,my_var,month,s_days_list,e_days_list)\n",
    "print(clim_pr_cat1.shape,clim_pr_cat1)\n",
    "\n",
    "# Category 3\n",
    "cat_3 = xr.concat([tp3_terc.sel(time=slice(f\"{yyyy}-10-{sm_sd}\",f\"{yyyy}-12-{em_ed}\")) for yyyy in tp3_terc.time.dt.year.to_index().unique()],\"time\")\n",
    "index_file,my_var,month,s_days_list,e_days_list = cat_3,'precip',10,[sm_sd,1,1],[31,30,em_ed]\n",
    "n_years=len(index_file.time.dt.year.to_index().unique())\n",
    "print('number of years to calculate probability from',n_years)\n",
    "\n",
    "# Probability excluding test years\n",
    "clim_pr_cat3, clim_pr_y_cat3 = climat_probab(index_file,my_var,month,s_days_list,e_days_list)\n",
    "print(clim_pr_cat3.shape,clim_pr_cat3)\n",
    "\n",
    "# Category 4\n",
    "cat_4 = xr.concat([tp4_terc.sel(time=slice(f\"{yyyy}-10-{sm_sd}\",f\"{yyyy}-12-{em_ed}\")) for yyyy in tp4_terc.time.dt.year.to_index().unique()],\"time\")\n",
    "index_file,my_var,month,s_days_list,e_days_list = cat_4,'precip',10,[sm_sd,1,1],[31,30,em_ed]\n",
    "n_years=len(index_file.time.dt.year.to_index().unique())\n",
    "print('number of years to calculate probability from',n_years)\n",
    "\n",
    "# Probability excluding test years\n",
    "clim_pr_cat4, clim_pr_y_cat4 = climat_probab(index_file,my_var,month,s_days_list,e_days_list)\n",
    "print(clim_pr_cat4.shape,clim_pr_cat4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2 {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt;\n",
       "Dimensions:    (time: 10957)\n",
       "Coordinates:\n",
       "  * time       (time) datetime64[ns] 1981-01-01 1981-01-02 ... 2010-12-31\n",
       "Data variables:\n",
       "    amplitude  (time) float32 nan nan nan 0.5651 0.4607 ... 0.7636 nan nan nan\n",
       "    phase      (time) float32 nan nan nan 3.286 3.571 ... 5.571 nan nan nan\n",
       "    RMM1       (time) float32 nan nan nan 0.1393 0.2246 ... 0.4809 nan nan nan\n",
       "    RMM2       (time) float32 nan nan nan -0.3833 -0.2881 ... 0.5447 nan nan nan</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-b9f1625b-0a55-4d30-a745-92b1b5b11401' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-b9f1625b-0a55-4d30-a745-92b1b5b11401' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span class='xr-has-index'>time</span>: 10957</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-45eb84c0-a4d6-405e-9f70-51340c049758' class='xr-section-summary-in' type='checkbox'  checked><label for='section-45eb84c0-a4d6-405e-9f70-51340c049758' class='xr-section-summary' >Coordinates: <span>(1)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>time</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>1981-01-01 ... 2010-12-31</div><input id='attrs-37fdc462-8ec1-4e87-bcca-6499c899edbc' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-37fdc462-8ec1-4e87-bcca-6499c899edbc' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-222721a4-1f14-42f6-8c85-e4a1974ecc9c' class='xr-var-data-in' type='checkbox'><label for='data-222721a4-1f14-42f6-8c85-e4a1974ecc9c' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([&#x27;1981-01-01T00:00:00.000000000&#x27;, &#x27;1981-01-02T00:00:00.000000000&#x27;,\n",
       "       &#x27;1981-01-03T00:00:00.000000000&#x27;, ..., &#x27;2010-12-29T00:00:00.000000000&#x27;,\n",
       "       &#x27;2010-12-30T00:00:00.000000000&#x27;, &#x27;2010-12-31T00:00:00.000000000&#x27;],\n",
       "      dtype=&#x27;datetime64[ns]&#x27;)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-500e0e7d-7671-4545-99df-04ddcf4e40ea' class='xr-section-summary-in' type='checkbox'  checked><label for='section-500e0e7d-7671-4545-99df-04ddcf4e40ea' class='xr-section-summary' >Data variables: <span>(4)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>amplitude</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>nan nan nan 0.5651 ... nan nan nan</div><input id='attrs-13cc6edd-edda-4554-a9e1-fd96685283ee' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-13cc6edd-edda-4554-a9e1-fd96685283ee' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-456f458e-ac59-4945-8878-c482accf7796' class='xr-var-data-in' type='checkbox'><label for='data-456f458e-ac59-4945-8878-c482accf7796' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([nan, nan, nan, ..., nan, nan, nan], dtype=float32)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>phase</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>nan nan nan 3.286 ... nan nan nan</div><input id='attrs-a6426dae-c766-4ee0-95d4-7ad84d665202' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-a6426dae-c766-4ee0-95d4-7ad84d665202' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-e47c56fb-065f-48f3-8dbf-93b6102f6bb7' class='xr-var-data-in' type='checkbox'><label for='data-e47c56fb-065f-48f3-8dbf-93b6102f6bb7' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([nan, nan, nan, ..., nan, nan, nan], dtype=float32)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>RMM1</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>nan nan nan 0.1393 ... nan nan nan</div><input id='attrs-8d321db9-7cd0-4eb5-bed8-cad2a94e9741' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-8d321db9-7cd0-4eb5-bed8-cad2a94e9741' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-32bf8afb-cd1a-475d-89d3-220bdaf4dab4' class='xr-var-data-in' type='checkbox'><label for='data-32bf8afb-cd1a-475d-89d3-220bdaf4dab4' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([nan, nan, nan, ..., nan, nan, nan], dtype=float32)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>RMM2</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>nan nan nan -0.3833 ... nan nan nan</div><input id='attrs-7db74107-8b9e-40d6-8322-53a8ab6eb460' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-7db74107-8b9e-40d6-8322-53a8ab6eb460' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-fa399639-d68d-425e-9418-af74bd3a6b4f' class='xr-var-data-in' type='checkbox'><label for='data-fa399639-d68d-425e-9418-af74bd3a6b4f' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([nan, nan, nan, ..., nan, nan, nan], dtype=float32)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-b36b343a-2277-4072-9fc3-e2733a411bbd' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-b36b343a-2277-4072-9fc3-e2733a411bbd' class='xr-section-summary'  title='Expand/collapse section'>Attributes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:    (time: 10957)\n",
       "Coordinates:\n",
       "  * time       (time) datetime64[ns] 1981-01-01 1981-01-02 ... 2010-12-31\n",
       "Data variables:\n",
       "    amplitude  (time) float32 nan nan nan 0.5651 0.4607 ... 0.7636 nan nan nan\n",
       "    phase      (time) float32 nan nan nan 3.286 3.571 ... 5.571 nan nan nan\n",
       "    RMM1       (time) float32 nan nan nan 0.1393 0.2246 ... 0.4809 nan nan nan\n",
       "    RMM2       (time) float32 nan nan nan -0.3833 -0.2881 ... 0.5447 nan nan nan"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read RMM index\n",
    "# data is from Cheng Zhang, ERA-interim daily data from 1981.1.1-2019.8.31 \n",
    "DIR = '/net/cfc/s2s/zhengwu/Datasets/MJO/'\n",
    "datafn = DIR+'rmm_ERA-Interim.nc'\n",
    "rmm = xr.open_mfdataset(datafn,combine='by_coords').compute()\n",
    "\n",
    "date_start = datetime.strftime(datetime(year=1960,month=1,day=1), \"%Y.%m.%d\")\n",
    "time_date = []\n",
    "for ii in range(len(rmm.time)):\n",
    "    time_date.append(datetime.strftime(datetime.strptime(date_start, \"%Y.%m.%d\") + timedelta(days=int(rmm.time.values[ii])),\"%Y.%m.%d\"))\n",
    "    \n",
    "rmm = rmm.assign_coords(time=time_date)\n",
    "rmm = rmm.assign_coords(time=pd.DatetimeIndex(rmm.time))\n",
    "rmm = rmm.sel(time = slice(str(SYY),str(EYY))).rolling(time=7, center=True).mean(skipna=True)\n",
    "rmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:    (time: 10957, lon: 26, lat: 7)\n",
      "Coordinates:\n",
      "  * time       (time) datetime64[ns] 1981-01-01 1981-01-02 ... 2010-12-31\n",
      "  * lon        (lon) float32 -170.0 -168.0 -166.0 ... -124.0 -122.0 -120.0\n",
      "  * lat        (lat) float32 6.0 4.0 2.0 0.0 -2.0 -4.0 -6.0\n",
      "    dayofyear  (time) int64 1 2 3 4 5 6 7 8 ... 358 359 360 361 362 363 364 365\n",
      "Data variables:\n",
      "    sst        (time, lat, lon) float32 nan nan nan nan nan ... nan nan nan nan\n",
      "enso <xarray.Dataset>\n",
      "Dimensions:    (time: 10957)\n",
      "Coordinates:\n",
      "  * time       (time) datetime64[ns] 1981-01-01 1981-01-02 ... 2010-12-31\n",
      "    dayofyear  (time) int64 1 2 3 4 5 6 7 8 ... 358 359 360 361 362 363 364 365\n",
      "Data variables:\n",
      "    sst        (time) float32 nan nan nan nan nan nan ... nan nan nan nan nan\n",
      "<xarray.Dataset>\n",
      "Dimensions:    (time: 10957, lon: 31, lat: 11)\n",
      "Coordinates:\n",
      "  * time       (time) datetime64[ns] 1981-01-01 1981-01-02 ... 2010-12-31\n",
      "  * lon        (lon) float32 50.0 52.0 54.0 56.0 ... 104.0 106.0 108.0 110.0\n",
      "  * lat        (lat) float32 10.0 8.0 6.0 4.0 2.0 0.0 -2.0 -4.0 -6.0 -8.0 -10.0\n",
      "    dayofyear  (time) int64 1 2 3 4 5 6 7 8 ... 358 359 360 361 362 363 364 365\n",
      "Data variables:\n",
      "    sst        (time, lat, lon) float32 nan nan nan nan nan ... nan nan nan nan\n",
      "iod <xarray.Dataset>\n",
      "Dimensions:    (time: 10957)\n",
      "Coordinates:\n",
      "  * time       (time) datetime64[ns] 1981-01-01 1981-01-02 ... 2010-12-31\n",
      "    dayofyear  (time) int64 1 2 3 4 5 6 7 8 ... 358 359 360 361 362 363 364 365\n",
      "Data variables:\n",
      "    sst        (time) float32 nan nan nan nan nan nan ... nan nan nan nan nan\n",
      "<xarray.Dataset>\n",
      "Dimensions:    (time: 10957, lon: 9, lat: 11)\n",
      "Coordinates:\n",
      "  * time       (time) datetime64[ns] 1981-01-01 1981-01-02 ... 2010-12-31\n",
      "  * lon        (lon) float32 38.0 40.0 42.0 44.0 46.0 48.0 50.0 52.0 54.0\n",
      "  * lat        (lat) float32 10.0 8.0 6.0 4.0 2.0 0.0 -2.0 -4.0 -6.0 -8.0 -10.0\n",
      "    dayofyear  (time) int64 1 2 3 4 5 6 7 8 ... 358 359 360 361 362 363 364 365\n",
      "Data variables:\n",
      "    t2m        (time, lat, lon) float32 nan nan nan nan nan ... nan nan nan nan\n",
      "t2m <xarray.Dataset>\n",
      "Dimensions:    (time: 10957)\n",
      "Coordinates:\n",
      "  * time       (time) datetime64[ns] 1981-01-01 1981-01-02 ... 2010-12-31\n",
      "    dayofyear  (time) int64 1 2 3 4 5 6 7 8 ... 358 359 360 361 362 363 364 365\n",
      "Data variables:\n",
      "    t2m        (time) float32 nan nan nan nan nan nan ... nan nan nan nan nan\n",
      "<xarray.Dataset>\n",
      "Dimensions:    (time: 10957, lon: 17, lat: 20)\n",
      "Coordinates:\n",
      "  * time       (time) datetime64[ns] 1981-01-01 1981-01-02 ... 2010-12-31\n",
      "  * lon        (lon) float64 38.5 39.5 40.5 41.5 42.5 ... 51.5 52.5 53.5 54.5\n",
      "  * lat        (lat) float64 -9.5 -8.5 -7.5 -6.5 -5.5 ... 5.5 6.5 7.5 8.5 9.5\n",
      "    dayofyear  (time) int64 1 2 3 4 5 6 7 8 ... 358 359 360 361 362 363 364 365\n",
      "Data variables:\n",
      "    E          (time, lat, lon) float32 nan nan nan nan nan ... nan nan nan nan\n",
      "evap <xarray.Dataset>\n",
      "Dimensions:    (time: 10957)\n",
      "Coordinates:\n",
      "  * time       (time) datetime64[ns] 1981-01-01 1981-01-02 ... 2010-12-31\n",
      "    dayofyear  (time) int64 1 2 3 4 5 6 7 8 ... 358 359 360 361 362 363 364 365\n",
      "Data variables:\n",
      "    E          (time) float32 nan nan nan nan nan nan ... nan nan nan nan nan\n"
     ]
    }
   ],
   "source": [
    "root_data = '/s2s_nobackup/zhengwu/workshop/'\n",
    "file_vars = ['sst', 'sst', 'ERA5_t2m', 'gleam_E']\n",
    "header_vars = ['sst', 'sst', 't2m', 'E']\n",
    "\n",
    "# select regions for the individual predictor\n",
    "lon_slices = [[-170,-120],[50,110],[38,55],[38,55]]\n",
    "lat_slices = [[6,-6],[10,-10],[10,-10],[-10,10]]\n",
    "for file_var, header_var, lon_slice, lat_slice in zip (file_vars, header_vars, lon_slices, lat_slices):\n",
    "\n",
    "    if file_var == 'gleam_E':\n",
    "        file = xr.open_dataset(root_data+file_var+'_1980-2021_daily_1.0deg_65S-65N.nc')\n",
    "    else:\n",
    "        file = xr.open_dataset(root_data+file_var+'_1959-2021_1_12_daily_2.0deg.nc')\n",
    "\n",
    "    if \"longitude\" in file.coords:\n",
    "        file = file.rename({\"longitude\": \"lon\",\"latitude\": \"lat\"})\n",
    "\n",
    "    assert \"lat\" in file.coords\n",
    "    assert \"lon\" in file.coords\n",
    "    #lon = file.coords['lon'].values\n",
    "    #lat = file.coords['lat'].values\n",
    "\n",
    "    # select region\n",
    "    var_dim = file.sel(lon=slice(lon_slice[0],lon_slice[1]),lat=slice(lat_slice[0],lat_slice[1]))\n",
    "    lon = var_dim.lon.values\n",
    "    lat = var_dim.lat.values\n",
    "    \n",
    "    var_series = var_dim.sel(time=var_dim.time.dt.year.isin([np.arange(SYY,EYY+1)])).rolling(time=15, center=True).mean(skipna=True)\n",
    "    var_series = var_series.sel(time=var_series.time.dt.year.isin(all_years))\n",
    "\n",
    "    # remove climatology\n",
    "    var_anom_series = var_series.groupby(\"time.dayofyear\") - var_series.groupby(\"time.dayofyear\").mean(\"time\",skipna=True)\n",
    "    print(var_anom_series)\n",
    "    \n",
    "    # ENSO indices\n",
    "    if file_var == 'ERA5_t2m':\n",
    "        t2m = var_anom_series.mean(dim=('lat', 'lon'),skipna=True)\n",
    "        print('t2m',t2m)\n",
    "    elif file_var == 'gleam_E':\n",
    "        evap = var_anom_series.mean(dim=('lat', 'lon'),skipna=True)\n",
    "        print('evap',evap)\n",
    "    else:\n",
    "        if lat_slice == [6, -6]:\n",
    "            enso = var_anom_series.mean(dim=('lat', 'lon'),skipna=True)\n",
    "            print('enso',enso)\n",
    "        else:\n",
    "            iod_1 = var_anom_series.sel(lon=slice(50,70),lat=slice(10,-10)).mean(dim=('lat', 'lon'),skipna=True)\n",
    "            iod_2 = var_anom_series.sel(lon=slice(90,110),lat=slice(0,-10)).mean(dim=('lat', 'lon'),skipna=True)\n",
    "            iod = iod_1-iod_2\n",
    "            print('iod',iod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_dim = rmm.coords['time']\n",
    "enso = xr.Dataset({\"Index\": ((\"time\"), enso.sst.values),}, \n",
    "                  coords={'time': time_dim})\n",
    "enso = enso.assign_coords(feature=np.arange(1))\n",
    "iod = xr.Dataset({\"Index\": ((\"time\"), iod.sst.values),}, \n",
    "                  coords={'time': time_dim})\n",
    "iod = iod.assign_coords(feature=(np.arange(1)+1))\n",
    "rmm1 = xr.Dataset({\"Index\": ((\"time\"), rmm.RMM1.values),}, \n",
    "                  coords={'time': time_dim})\n",
    "rmm1 = rmm1.assign_coords(feature=(np.arange(1)+2))\n",
    "rmm2 = xr.Dataset({\"Index\": ((\"time\"), rmm.RMM2.values),}, \n",
    "                  coords={'time': time_dim})\n",
    "rmm2 = rmm2.assign_coords(feature=(np.arange(1)+3))\n",
    "t2m = xr.Dataset({\"Index\": ((\"time\"), t2m.t2m.values),}, \n",
    "                  coords={'time': time_dim})\n",
    "t2m = t2m.assign_coords(feature=(np.arange(1)+4))\n",
    "evap = xr.Dataset({\"Index\": ((\"time\"), evap.E.values),}, \n",
    "                  coords={'time': time_dim})\n",
    "evap = evap.assign_coords(feature=(np.arange(1)+5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = root_results+'index_ENSO.nc'\n",
    "enso.to_netcdf(path)\n",
    "path = root_results+'index_IOD.nc'\n",
    "iod.to_netcdf(path)\n",
    "path = root_results+'index_RMM1.nc'\n",
    "rmm1.to_netcdf(path)\n",
    "path = root_results+'index_RMM2.nc'\n",
    "rmm2.to_netcdf(path)\n",
    "path = root_results+'index_T2M.nc'\n",
    "rmm2.to_netcdf(path)\n",
    "path = root_results+'index_E.nc'\n",
    "rmm2.to_netcdf(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open(root_data+'rMII_index_latest.txt', 'r')\n",
    "# Lines = file.readlines()\n",
    "\n",
    "# count = 0\n",
    "# dates = []\n",
    "# years = []\n",
    "# months = []\n",
    "# days = []\n",
    "# rmm1 = []\n",
    "# rmm2 = []\n",
    "# phase = []\n",
    "# # Strips the newline character\n",
    "# for line in Lines:\n",
    "#     count += 1\n",
    "#     #print(\"Line{}: {}\".format(count, line.strip()))\n",
    "#     tmp = line.split()\n",
    "#     years.append(tmp[0])\n",
    "#     months.append(tmp[1])\n",
    "#     days.append(tmp[2])\n",
    "#     rmm1.append(float(tmp[3]))\n",
    "#     rmm2.append(float(tmp[4]))\n",
    "#     phase.append(tmp[5])\n",
    "#     dates.append(datetime.strftime(datetime(year=int(tmp[0]),month=int(tmp[1]),day=int(tmp[2])),\"%Y-%m-%d\"))\n",
    "# file = pd.DataFrame(list(zip(rmm1, rmm2)), columns =['RMM1', 'RMM2'])\n",
    "# nrmm = np.arange(2)\n",
    "# mjo = xr.DataArray(file, coords={'time': dates, 'feature': nrmm}, dims=[\"time\",\"feature\"])\n",
    "# print(mjo)\n",
    "\n",
    "# mjo_rol = mjo.rolling(time=7, center=True, min_periods=1).mean(skipna=True)\n",
    "# mjo_sel = xr.concat([mjo_rol.sel(time=slice(f\"{yyyy}-01-01\",f\"{yyyy}-12-31\")) for yyyy in all_years],\"time\")\n",
    "# print(mjo_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = []\n",
    "# tmp1 = enso.assign_coords(feature=range(1))\n",
    "# tmp2 = iod.assign_coords(feature=range(1))\n",
    "# tmp.append(tmp1)\n",
    "# tmp.append(tmp2)\n",
    "# tmp = xr.concat(tmp,\"feature\")\n",
    "# tmp3 = tmp.transpose()\n",
    "# del tmp\n",
    "# tmp = tmp3.sst.values\n",
    "# print(tmp.shape)\n",
    "# del tmp1\n",
    "# tmp1 = np.concatenate((tmp,rmm.RMM1),axis=1)\n",
    "# tmp1 = np.concatenate((tmp,rmm.RMM1),axis=1)\n",
    "# print(tmp1.shape)\n",
    "# time_dim = enso.coords['time']\n",
    "# features = np.arange(4)\n",
    "# predictors = xr.DataArray(tmp1, coords={'time': time_dim, 'feature': features}, dims=[\"time\",\"feature\"])\n",
    "# print(predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "start target date 8 10 1981\n",
      "end target 10 12 2010\n",
      "1981.10.08 1981.07.13 1981.09.10 0\n",
      "1981.10.09 1981.07.14 1981.09.11 1\n",
      "1981.10.10 1981.07.15 1981.09.12 1\n",
      "1981.10.11 1981.07.16 1981.09.13 1\n",
      "1981.10.12 1981.07.17 1981.09.14 1\n",
      "1981.10.13 1981.07.18 1981.09.15 1\n",
      "1981.10.14 1981.07.19 1981.09.16 1\n",
      "1981.10.15 1981.07.20 1981.09.17 1\n",
      "1981.10.16 1981.07.21 1981.09.18 1\n",
      "1981.10.17 1981.07.22 1981.09.19 1\n",
      "1981.10.18 1981.07.23 1981.09.20 1\n",
      "1981.10.19 1981.07.24 1981.09.21 1\n",
      "1981.10.20 1981.07.25 1981.09.22 1\n",
      "1981.10.21 1981.07.26 1981.09.23 1\n",
      "1981.10.22 1981.07.27 1981.09.24 1\n",
      "1981.10.23 1981.07.28 1981.09.25 1\n",
      "1981.10.24 1981.07.29 1981.09.26 1\n",
      "1981.10.25 1981.07.30 1981.09.27 1\n",
      "1981.10.26 1981.07.31 1981.09.28 1\n",
      "1981.10.27 1981.08.01 1981.09.29 1\n",
      "1981.10.28 1981.08.02 1981.09.30 1\n",
      "1981.10.29 1981.08.03 1981.10.01 1\n",
      "1981.10.30 1981.08.04 1981.10.02 1\n",
      "1981.10.31 1981.08.05 1981.10.03 1\n",
      "1981.11.01 1981.08.06 1981.10.04 1\n",
      "1981.11.02 1981.08.07 1981.10.05 1\n",
      "1981.11.03 1981.08.08 1981.10.06 1\n",
      "1981.11.04 1981.08.09 1981.10.07 1\n",
      "1981.11.05 1981.08.10 1981.10.08 1\n",
      "1981.11.06 1981.08.11 1981.10.09 1\n",
      "1981.11.07 1981.08.12 1981.10.10 1\n",
      "1981.11.08 1981.08.13 1981.10.11 1\n",
      "1981.11.09 1981.08.14 1981.10.12 1\n",
      "1981.11.10 1981.08.15 1981.10.13 1\n",
      "1981.11.11 1981.08.16 1981.10.14 1\n",
      "1981.11.12 1981.08.17 1981.10.15 1\n",
      "1981.11.13 1981.08.18 1981.10.16 1\n",
      "1981.11.14 1981.08.19 1981.10.17 1\n",
      "1981.11.15 1981.08.20 1981.10.18 1\n",
      "1981.11.16 1981.08.21 1981.10.19 1\n",
      "1981.11.17 1981.08.22 1981.10.20 1\n",
      "1981.11.18 1981.08.23 1981.10.21 1\n",
      "1981.11.19 1981.08.24 1981.10.22 1\n",
      "1981.11.20 1981.08.25 1981.10.23 1\n",
      "1981.11.21 1981.08.26 1981.10.24 1\n",
      "1981.11.22 1981.08.27 1981.10.25 1\n",
      "1981.11.23 1981.08.28 1981.10.26 1\n",
      "1981.11.24 1981.08.29 1981.10.27 1\n",
      "1981.11.25 1981.08.30 1981.10.28 1\n",
      "1981.11.26 1981.08.31 1981.10.29 1\n",
      "1981.11.27 1981.09.01 1981.10.30 1\n",
      "1981.11.28 1981.09.02 1981.10.31 1\n",
      "1981.11.29 1981.09.03 1981.11.01 1\n",
      "1981.11.30 1981.09.04 1981.11.02 1\n",
      "1981.12.01 1981.09.05 1981.11.03 1\n",
      "1981.12.02 1981.09.06 1981.11.04 1\n",
      "1981.12.03 1981.09.07 1981.11.05 1\n",
      "1981.12.04 1981.09.08 1981.11.06 1\n",
      "1981.12.05 1981.09.09 1981.11.07 1\n",
      "1981.12.06 1981.09.10 1981.11.08 1\n",
      "1981.12.07 1981.09.11 1981.11.09 1\n",
      "1981.12.08 1981.09.12 1981.11.10 1\n",
      "1981.12.09 1981.09.13 1981.11.11 1\n",
      "1981.12.10 1981.09.14 1981.11.12 1\n",
      "1982.10.08 1982.07.13 1982.09.10 0\n",
      "1982.10.09 1982.07.14 1982.09.11 1\n",
      "1982.10.10 1982.07.15 1982.09.12 1\n",
      "1982.10.11 1982.07.16 1982.09.13 1\n",
      "1982.10.12 1982.07.17 1982.09.14 1\n",
      "1982.10.13 1982.07.18 1982.09.15 1\n",
      "1982.10.14 1982.07.19 1982.09.16 1\n",
      "1982.10.15 1982.07.20 1982.09.17 1\n",
      "1982.10.16 1982.07.21 1982.09.18 1\n",
      "1982.10.17 1982.07.22 1982.09.19 1\n",
      "1982.10.18 1982.07.23 1982.09.20 1\n",
      "1982.10.19 1982.07.24 1982.09.21 1\n",
      "1982.10.20 1982.07.25 1982.09.22 1\n",
      "1982.10.21 1982.07.26 1982.09.23 1\n",
      "1982.10.22 1982.07.27 1982.09.24 1\n",
      "1982.10.23 1982.07.28 1982.09.25 1\n",
      "1982.10.24 1982.07.29 1982.09.26 1\n",
      "1982.10.25 1982.07.30 1982.09.27 1\n",
      "1982.10.26 1982.07.31 1982.09.28 1\n",
      "1982.10.27 1982.08.01 1982.09.29 1\n",
      "1982.10.28 1982.08.02 1982.09.30 1\n",
      "1982.10.29 1982.08.03 1982.10.01 1\n",
      "1982.10.30 1982.08.04 1982.10.02 1\n",
      "1982.10.31 1982.08.05 1982.10.03 1\n",
      "1982.11.01 1982.08.06 1982.10.04 1\n",
      "1982.11.02 1982.08.07 1982.10.05 1\n",
      "1982.11.03 1982.08.08 1982.10.06 1\n",
      "1982.11.04 1982.08.09 1982.10.07 1\n",
      "1982.11.05 1982.08.10 1982.10.08 1\n",
      "1982.11.06 1982.08.11 1982.10.09 1\n",
      "1982.11.07 1982.08.12 1982.10.10 1\n",
      "1982.11.08 1982.08.13 1982.10.11 1\n",
      "1982.11.09 1982.08.14 1982.10.12 1\n",
      "1982.11.10 1982.08.15 1982.10.13 1\n",
      "1982.11.11 1982.08.16 1982.10.14 1\n",
      "1982.11.12 1982.08.17 1982.10.15 1\n",
      "1982.11.13 1982.08.18 1982.10.16 1\n",
      "1982.11.14 1982.08.19 1982.10.17 1\n",
      "1982.11.15 1982.08.20 1982.10.18 1\n",
      "1982.11.16 1982.08.21 1982.10.19 1\n",
      "1982.11.17 1982.08.22 1982.10.20 1\n",
      "1982.11.18 1982.08.23 1982.10.21 1\n",
      "1982.11.19 1982.08.24 1982.10.22 1\n",
      "1982.11.20 1982.08.25 1982.10.23 1\n",
      "1982.11.21 1982.08.26 1982.10.24 1\n",
      "1982.11.22 1982.08.27 1982.10.25 1\n",
      "1982.11.23 1982.08.28 1982.10.26 1\n",
      "1982.11.24 1982.08.29 1982.10.27 1\n",
      "1982.11.25 1982.08.30 1982.10.28 1\n",
      "1982.11.26 1982.08.31 1982.10.29 1\n",
      "1982.11.27 1982.09.01 1982.10.30 1\n",
      "1982.11.28 1982.09.02 1982.10.31 1\n",
      "1982.11.29 1982.09.03 1982.11.01 1\n",
      "1982.11.30 1982.09.04 1982.11.02 1\n",
      "1982.12.01 1982.09.05 1982.11.03 1\n",
      "1982.12.02 1982.09.06 1982.11.04 1\n",
      "1982.12.03 1982.09.07 1982.11.05 1\n",
      "1982.12.04 1982.09.08 1982.11.06 1\n",
      "1982.12.05 1982.09.09 1982.11.07 1\n",
      "1982.12.06 1982.09.10 1982.11.08 1\n",
      "1982.12.07 1982.09.11 1982.11.09 1\n",
      "1982.12.08 1982.09.12 1982.11.10 1\n",
      "1982.12.09 1982.09.13 1982.11.11 1\n",
      "1982.12.10 1982.09.14 1982.11.12 1\n",
      "1983.10.08 1983.07.13 1983.09.10 0\n",
      "1983.10.09 1983.07.14 1983.09.11 1\n",
      "1983.10.10 1983.07.15 1983.09.12 1\n",
      "1983.10.11 1983.07.16 1983.09.13 1\n",
      "1983.10.12 1983.07.17 1983.09.14 1\n",
      "1983.10.13 1983.07.18 1983.09.15 1\n",
      "1983.10.14 1983.07.19 1983.09.16 1\n",
      "1983.10.15 1983.07.20 1983.09.17 1\n",
      "1983.10.16 1983.07.21 1983.09.18 1\n",
      "1983.10.17 1983.07.22 1983.09.19 1\n",
      "1983.10.18 1983.07.23 1983.09.20 1\n",
      "1983.10.19 1983.07.24 1983.09.21 1\n",
      "1983.10.20 1983.07.25 1983.09.22 1\n",
      "1983.10.21 1983.07.26 1983.09.23 1\n",
      "1983.10.22 1983.07.27 1983.09.24 1\n",
      "1983.10.23 1983.07.28 1983.09.25 1\n",
      "1983.10.24 1983.07.29 1983.09.26 1\n",
      "1983.10.25 1983.07.30 1983.09.27 1\n",
      "1983.10.26 1983.07.31 1983.09.28 1\n",
      "1983.10.27 1983.08.01 1983.09.29 1\n",
      "1983.10.28 1983.08.02 1983.09.30 1\n",
      "1983.10.29 1983.08.03 1983.10.01 1\n",
      "1983.10.30 1983.08.04 1983.10.02 1\n",
      "1983.10.31 1983.08.05 1983.10.03 1\n",
      "1983.11.01 1983.08.06 1983.10.04 1\n",
      "1983.11.02 1983.08.07 1983.10.05 1\n",
      "1983.11.03 1983.08.08 1983.10.06 1\n",
      "1983.11.04 1983.08.09 1983.10.07 1\n",
      "1983.11.05 1983.08.10 1983.10.08 1\n",
      "1983.11.06 1983.08.11 1983.10.09 1\n",
      "1983.11.07 1983.08.12 1983.10.10 1\n",
      "1983.11.08 1983.08.13 1983.10.11 1\n",
      "1983.11.09 1983.08.14 1983.10.12 1\n",
      "1983.11.10 1983.08.15 1983.10.13 1\n",
      "1983.11.11 1983.08.16 1983.10.14 1\n",
      "1983.11.12 1983.08.17 1983.10.15 1\n",
      "1983.11.13 1983.08.18 1983.10.16 1\n",
      "1983.11.14 1983.08.19 1983.10.17 1\n",
      "1983.11.15 1983.08.20 1983.10.18 1\n",
      "1983.11.16 1983.08.21 1983.10.19 1\n",
      "1983.11.17 1983.08.22 1983.10.20 1\n",
      "1983.11.18 1983.08.23 1983.10.21 1\n",
      "1983.11.19 1983.08.24 1983.10.22 1\n",
      "1983.11.20 1983.08.25 1983.10.23 1\n",
      "1983.11.21 1983.08.26 1983.10.24 1\n",
      "1983.11.22 1983.08.27 1983.10.25 1\n",
      "1983.11.23 1983.08.28 1983.10.26 1\n",
      "1983.11.24 1983.08.29 1983.10.27 1\n",
      "1983.11.25 1983.08.30 1983.10.28 1\n",
      "1983.11.26 1983.08.31 1983.10.29 1\n",
      "1983.11.27 1983.09.01 1983.10.30 1\n",
      "1983.11.28 1983.09.02 1983.10.31 1\n",
      "1983.11.29 1983.09.03 1983.11.01 1\n",
      "1983.11.30 1983.09.04 1983.11.02 1\n",
      "1983.12.01 1983.09.05 1983.11.03 1\n",
      "1983.12.02 1983.09.06 1983.11.04 1\n",
      "1983.12.03 1983.09.07 1983.11.05 1\n",
      "1983.12.04 1983.09.08 1983.11.06 1\n",
      "1983.12.05 1983.09.09 1983.11.07 1\n",
      "1983.12.06 1983.09.10 1983.11.08 1\n",
      "1983.12.07 1983.09.11 1983.11.09 1\n",
      "1983.12.08 1983.09.12 1983.11.10 1\n",
      "1983.12.09 1983.09.13 1983.11.11 1\n",
      "1983.12.10 1983.09.14 1983.11.12 1\n",
      "1984.10.08 1984.07.13 1984.09.10 0\n",
      "1984.10.09 1984.07.14 1984.09.11 1\n",
      "1984.10.10 1984.07.15 1984.09.12 1\n",
      "1984.10.11 1984.07.16 1984.09.13 1\n",
      "1984.10.12 1984.07.17 1984.09.14 1\n",
      "1984.10.13 1984.07.18 1984.09.15 1\n",
      "1984.10.14 1984.07.19 1984.09.16 1\n",
      "1984.10.15 1984.07.20 1984.09.17 1\n",
      "1984.10.16 1984.07.21 1984.09.18 1\n",
      "1984.10.17 1984.07.22 1984.09.19 1\n",
      "1984.10.18 1984.07.23 1984.09.20 1\n",
      "1984.10.19 1984.07.24 1984.09.21 1\n",
      "1984.10.20 1984.07.25 1984.09.22 1\n",
      "1984.10.21 1984.07.26 1984.09.23 1\n",
      "1984.10.22 1984.07.27 1984.09.24 1\n",
      "1984.10.23 1984.07.28 1984.09.25 1\n",
      "1984.10.24 1984.07.29 1984.09.26 1\n",
      "1984.10.25 1984.07.30 1984.09.27 1\n",
      "1984.10.26 1984.07.31 1984.09.28 1\n",
      "1984.10.27 1984.08.01 1984.09.29 1\n",
      "1984.10.28 1984.08.02 1984.09.30 1\n",
      "1984.10.29 1984.08.03 1984.10.01 1\n",
      "1984.10.30 1984.08.04 1984.10.02 1\n",
      "1984.10.31 1984.08.05 1984.10.03 1\n",
      "1984.11.01 1984.08.06 1984.10.04 1\n",
      "1984.11.02 1984.08.07 1984.10.05 1\n",
      "1984.11.03 1984.08.08 1984.10.06 1\n",
      "1984.11.04 1984.08.09 1984.10.07 1\n",
      "1984.11.05 1984.08.10 1984.10.08 1\n",
      "1984.11.06 1984.08.11 1984.10.09 1\n",
      "1984.11.07 1984.08.12 1984.10.10 1\n",
      "1984.11.08 1984.08.13 1984.10.11 1\n",
      "1984.11.09 1984.08.14 1984.10.12 1\n",
      "1984.11.10 1984.08.15 1984.10.13 1\n",
      "1984.11.11 1984.08.16 1984.10.14 1\n",
      "1984.11.12 1984.08.17 1984.10.15 1\n",
      "1984.11.13 1984.08.18 1984.10.16 1\n",
      "1984.11.14 1984.08.19 1984.10.17 1\n",
      "1984.11.15 1984.08.20 1984.10.18 1\n",
      "1984.11.16 1984.08.21 1984.10.19 1\n",
      "1984.11.17 1984.08.22 1984.10.20 1\n",
      "1984.11.18 1984.08.23 1984.10.21 1\n",
      "1984.11.19 1984.08.24 1984.10.22 1\n",
      "1984.11.20 1984.08.25 1984.10.23 1\n",
      "1984.11.21 1984.08.26 1984.10.24 1\n",
      "1984.11.22 1984.08.27 1984.10.25 1\n",
      "1984.11.23 1984.08.28 1984.10.26 1\n",
      "1984.11.24 1984.08.29 1984.10.27 1\n",
      "1984.11.25 1984.08.30 1984.10.28 1\n",
      "1984.11.26 1984.08.31 1984.10.29 1\n",
      "1984.11.27 1984.09.01 1984.10.30 1\n",
      "1984.11.28 1984.09.02 1984.10.31 1\n",
      "1984.11.29 1984.09.03 1984.11.01 1\n",
      "1984.11.30 1984.09.04 1984.11.02 1\n",
      "1984.12.01 1984.09.05 1984.11.03 1\n",
      "1984.12.02 1984.09.06 1984.11.04 1\n",
      "1984.12.03 1984.09.07 1984.11.05 1\n",
      "1984.12.04 1984.09.08 1984.11.06 1\n",
      "1984.12.05 1984.09.09 1984.11.07 1\n",
      "1984.12.06 1984.09.10 1984.11.08 1\n",
      "1984.12.07 1984.09.11 1984.11.09 1\n",
      "1984.12.08 1984.09.12 1984.11.10 1\n",
      "1984.12.09 1984.09.13 1984.11.11 1\n",
      "1984.12.10 1984.09.14 1984.11.12 1\n",
      "1985.10.08 1985.07.13 1985.09.10 0\n",
      "1985.10.09 1985.07.14 1985.09.11 1\n",
      "1985.10.10 1985.07.15 1985.09.12 1\n",
      "1985.10.11 1985.07.16 1985.09.13 1\n",
      "1985.10.12 1985.07.17 1985.09.14 1\n",
      "1985.10.13 1985.07.18 1985.09.15 1\n",
      "1985.10.14 1985.07.19 1985.09.16 1\n",
      "1985.10.15 1985.07.20 1985.09.17 1\n",
      "1985.10.16 1985.07.21 1985.09.18 1\n",
      "1985.10.17 1985.07.22 1985.09.19 1\n",
      "1985.10.18 1985.07.23 1985.09.20 1\n",
      "1985.10.19 1985.07.24 1985.09.21 1\n",
      "1985.10.20 1985.07.25 1985.09.22 1\n",
      "1985.10.21 1985.07.26 1985.09.23 1\n",
      "1985.10.22 1985.07.27 1985.09.24 1\n",
      "1985.10.23 1985.07.28 1985.09.25 1\n",
      "1985.10.24 1985.07.29 1985.09.26 1\n",
      "1985.10.25 1985.07.30 1985.09.27 1\n",
      "1985.10.26 1985.07.31 1985.09.28 1\n",
      "1985.10.27 1985.08.01 1985.09.29 1\n",
      "1985.10.28 1985.08.02 1985.09.30 1\n",
      "1985.10.29 1985.08.03 1985.10.01 1\n",
      "1985.10.30 1985.08.04 1985.10.02 1\n",
      "1985.10.31 1985.08.05 1985.10.03 1\n",
      "1985.11.01 1985.08.06 1985.10.04 1\n",
      "1985.11.02 1985.08.07 1985.10.05 1\n",
      "1985.11.03 1985.08.08 1985.10.06 1\n",
      "1985.11.04 1985.08.09 1985.10.07 1\n",
      "1985.11.05 1985.08.10 1985.10.08 1\n",
      "1985.11.06 1985.08.11 1985.10.09 1\n",
      "1985.11.07 1985.08.12 1985.10.10 1\n",
      "1985.11.08 1985.08.13 1985.10.11 1\n",
      "1985.11.09 1985.08.14 1985.10.12 1\n",
      "1985.11.10 1985.08.15 1985.10.13 1\n",
      "1985.11.11 1985.08.16 1985.10.14 1\n",
      "1985.11.12 1985.08.17 1985.10.15 1\n",
      "1985.11.13 1985.08.18 1985.10.16 1\n",
      "1985.11.14 1985.08.19 1985.10.17 1\n",
      "1985.11.15 1985.08.20 1985.10.18 1\n",
      "1985.11.16 1985.08.21 1985.10.19 1\n",
      "1985.11.17 1985.08.22 1985.10.20 1\n",
      "1985.11.18 1985.08.23 1985.10.21 1\n",
      "1985.11.19 1985.08.24 1985.10.22 1\n",
      "1985.11.20 1985.08.25 1985.10.23 1\n",
      "1985.11.21 1985.08.26 1985.10.24 1\n",
      "1985.11.22 1985.08.27 1985.10.25 1\n",
      "1985.11.23 1985.08.28 1985.10.26 1\n",
      "1985.11.24 1985.08.29 1985.10.27 1\n",
      "1985.11.25 1985.08.30 1985.10.28 1\n",
      "1985.11.26 1985.08.31 1985.10.29 1\n",
      "1985.11.27 1985.09.01 1985.10.30 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1985.11.28 1985.09.02 1985.10.31 1\n",
      "1985.11.29 1985.09.03 1985.11.01 1\n",
      "1985.11.30 1985.09.04 1985.11.02 1\n",
      "1985.12.01 1985.09.05 1985.11.03 1\n",
      "1985.12.02 1985.09.06 1985.11.04 1\n",
      "1985.12.03 1985.09.07 1985.11.05 1\n",
      "1985.12.04 1985.09.08 1985.11.06 1\n",
      "1985.12.05 1985.09.09 1985.11.07 1\n",
      "1985.12.06 1985.09.10 1985.11.08 1\n",
      "1985.12.07 1985.09.11 1985.11.09 1\n",
      "1985.12.08 1985.09.12 1985.11.10 1\n",
      "1985.12.09 1985.09.13 1985.11.11 1\n",
      "1985.12.10 1985.09.14 1985.11.12 1\n",
      "1986.10.08 1986.07.13 1986.09.10 0\n",
      "1986.10.09 1986.07.14 1986.09.11 1\n",
      "1986.10.10 1986.07.15 1986.09.12 1\n",
      "1986.10.11 1986.07.16 1986.09.13 1\n",
      "1986.10.12 1986.07.17 1986.09.14 1\n",
      "1986.10.13 1986.07.18 1986.09.15 1\n",
      "1986.10.14 1986.07.19 1986.09.16 1\n",
      "1986.10.15 1986.07.20 1986.09.17 1\n",
      "1986.10.16 1986.07.21 1986.09.18 1\n",
      "1986.10.17 1986.07.22 1986.09.19 1\n",
      "1986.10.18 1986.07.23 1986.09.20 1\n",
      "1986.10.19 1986.07.24 1986.09.21 1\n",
      "1986.10.20 1986.07.25 1986.09.22 1\n",
      "1986.10.21 1986.07.26 1986.09.23 1\n",
      "1986.10.22 1986.07.27 1986.09.24 1\n",
      "1986.10.23 1986.07.28 1986.09.25 1\n",
      "1986.10.24 1986.07.29 1986.09.26 1\n",
      "1986.10.25 1986.07.30 1986.09.27 1\n",
      "1986.10.26 1986.07.31 1986.09.28 1\n",
      "1986.10.27 1986.08.01 1986.09.29 1\n",
      "1986.10.28 1986.08.02 1986.09.30 1\n",
      "1986.10.29 1986.08.03 1986.10.01 1\n",
      "1986.10.30 1986.08.04 1986.10.02 1\n",
      "1986.10.31 1986.08.05 1986.10.03 1\n",
      "1986.11.01 1986.08.06 1986.10.04 1\n",
      "1986.11.02 1986.08.07 1986.10.05 1\n",
      "1986.11.03 1986.08.08 1986.10.06 1\n",
      "1986.11.04 1986.08.09 1986.10.07 1\n",
      "1986.11.05 1986.08.10 1986.10.08 1\n",
      "1986.11.06 1986.08.11 1986.10.09 1\n",
      "1986.11.07 1986.08.12 1986.10.10 1\n",
      "1986.11.08 1986.08.13 1986.10.11 1\n",
      "1986.11.09 1986.08.14 1986.10.12 1\n",
      "1986.11.10 1986.08.15 1986.10.13 1\n",
      "1986.11.11 1986.08.16 1986.10.14 1\n",
      "1986.11.12 1986.08.17 1986.10.15 1\n",
      "1986.11.13 1986.08.18 1986.10.16 1\n",
      "1986.11.14 1986.08.19 1986.10.17 1\n",
      "1986.11.15 1986.08.20 1986.10.18 1\n",
      "1986.11.16 1986.08.21 1986.10.19 1\n",
      "1986.11.17 1986.08.22 1986.10.20 1\n",
      "1986.11.18 1986.08.23 1986.10.21 1\n",
      "1986.11.19 1986.08.24 1986.10.22 1\n",
      "1986.11.20 1986.08.25 1986.10.23 1\n",
      "1986.11.21 1986.08.26 1986.10.24 1\n",
      "1986.11.22 1986.08.27 1986.10.25 1\n",
      "1986.11.23 1986.08.28 1986.10.26 1\n",
      "1986.11.24 1986.08.29 1986.10.27 1\n",
      "1986.11.25 1986.08.30 1986.10.28 1\n",
      "1986.11.26 1986.08.31 1986.10.29 1\n",
      "1986.11.27 1986.09.01 1986.10.30 1\n",
      "1986.11.28 1986.09.02 1986.10.31 1\n",
      "1986.11.29 1986.09.03 1986.11.01 1\n",
      "1986.11.30 1986.09.04 1986.11.02 1\n",
      "1986.12.01 1986.09.05 1986.11.03 1\n",
      "1986.12.02 1986.09.06 1986.11.04 1\n",
      "1986.12.03 1986.09.07 1986.11.05 1\n",
      "1986.12.04 1986.09.08 1986.11.06 1\n",
      "1986.12.05 1986.09.09 1986.11.07 1\n",
      "1986.12.06 1986.09.10 1986.11.08 1\n",
      "1986.12.07 1986.09.11 1986.11.09 1\n",
      "1986.12.08 1986.09.12 1986.11.10 1\n",
      "1986.12.09 1986.09.13 1986.11.11 1\n",
      "1986.12.10 1986.09.14 1986.11.12 1\n",
      "1987.10.08 1987.07.13 1987.09.10 0\n",
      "1987.10.09 1987.07.14 1987.09.11 1\n",
      "1987.10.10 1987.07.15 1987.09.12 1\n",
      "1987.10.11 1987.07.16 1987.09.13 1\n",
      "1987.10.12 1987.07.17 1987.09.14 1\n",
      "1987.10.13 1987.07.18 1987.09.15 1\n",
      "1987.10.14 1987.07.19 1987.09.16 1\n",
      "1987.10.15 1987.07.20 1987.09.17 1\n",
      "1987.10.16 1987.07.21 1987.09.18 1\n",
      "1987.10.17 1987.07.22 1987.09.19 1\n",
      "1987.10.18 1987.07.23 1987.09.20 1\n",
      "1987.10.19 1987.07.24 1987.09.21 1\n",
      "1987.10.20 1987.07.25 1987.09.22 1\n",
      "1987.10.21 1987.07.26 1987.09.23 1\n",
      "1987.10.22 1987.07.27 1987.09.24 1\n",
      "1987.10.23 1987.07.28 1987.09.25 1\n",
      "1987.10.24 1987.07.29 1987.09.26 1\n",
      "1987.10.25 1987.07.30 1987.09.27 1\n",
      "1987.10.26 1987.07.31 1987.09.28 1\n",
      "1987.10.27 1987.08.01 1987.09.29 1\n",
      "1987.10.28 1987.08.02 1987.09.30 1\n",
      "1987.10.29 1987.08.03 1987.10.01 1\n",
      "1987.10.30 1987.08.04 1987.10.02 1\n",
      "1987.10.31 1987.08.05 1987.10.03 1\n",
      "1987.11.01 1987.08.06 1987.10.04 1\n",
      "1987.11.02 1987.08.07 1987.10.05 1\n",
      "1987.11.03 1987.08.08 1987.10.06 1\n",
      "1987.11.04 1987.08.09 1987.10.07 1\n",
      "1987.11.05 1987.08.10 1987.10.08 1\n",
      "1987.11.06 1987.08.11 1987.10.09 1\n",
      "1987.11.07 1987.08.12 1987.10.10 1\n",
      "1987.11.08 1987.08.13 1987.10.11 1\n",
      "1987.11.09 1987.08.14 1987.10.12 1\n",
      "1987.11.10 1987.08.15 1987.10.13 1\n",
      "1987.11.11 1987.08.16 1987.10.14 1\n",
      "1987.11.12 1987.08.17 1987.10.15 1\n",
      "1987.11.13 1987.08.18 1987.10.16 1\n",
      "1987.11.14 1987.08.19 1987.10.17 1\n",
      "1987.11.15 1987.08.20 1987.10.18 1\n",
      "1987.11.16 1987.08.21 1987.10.19 1\n",
      "1987.11.17 1987.08.22 1987.10.20 1\n",
      "1987.11.18 1987.08.23 1987.10.21 1\n",
      "1987.11.19 1987.08.24 1987.10.22 1\n",
      "1987.11.20 1987.08.25 1987.10.23 1\n",
      "1987.11.21 1987.08.26 1987.10.24 1\n",
      "1987.11.22 1987.08.27 1987.10.25 1\n",
      "1987.11.23 1987.08.28 1987.10.26 1\n",
      "1987.11.24 1987.08.29 1987.10.27 1\n",
      "1987.11.25 1987.08.30 1987.10.28 1\n",
      "1987.11.26 1987.08.31 1987.10.29 1\n",
      "1987.11.27 1987.09.01 1987.10.30 1\n",
      "1987.11.28 1987.09.02 1987.10.31 1\n",
      "1987.11.29 1987.09.03 1987.11.01 1\n",
      "1987.11.30 1987.09.04 1987.11.02 1\n",
      "1987.12.01 1987.09.05 1987.11.03 1\n",
      "1987.12.02 1987.09.06 1987.11.04 1\n",
      "1987.12.03 1987.09.07 1987.11.05 1\n",
      "1987.12.04 1987.09.08 1987.11.06 1\n",
      "1987.12.05 1987.09.09 1987.11.07 1\n",
      "1987.12.06 1987.09.10 1987.11.08 1\n",
      "1987.12.07 1987.09.11 1987.11.09 1\n",
      "1987.12.08 1987.09.12 1987.11.10 1\n",
      "1987.12.09 1987.09.13 1987.11.11 1\n",
      "1987.12.10 1987.09.14 1987.11.12 1\n",
      "1988.10.08 1988.07.13 1988.09.10 0\n",
      "1988.10.09 1988.07.14 1988.09.11 1\n",
      "1988.10.10 1988.07.15 1988.09.12 1\n",
      "1988.10.11 1988.07.16 1988.09.13 1\n",
      "1988.10.12 1988.07.17 1988.09.14 1\n",
      "1988.10.13 1988.07.18 1988.09.15 1\n",
      "1988.10.14 1988.07.19 1988.09.16 1\n",
      "1988.10.15 1988.07.20 1988.09.17 1\n",
      "1988.10.16 1988.07.21 1988.09.18 1\n",
      "1988.10.17 1988.07.22 1988.09.19 1\n",
      "1988.10.18 1988.07.23 1988.09.20 1\n",
      "1988.10.19 1988.07.24 1988.09.21 1\n",
      "1988.10.20 1988.07.25 1988.09.22 1\n",
      "1988.10.21 1988.07.26 1988.09.23 1\n",
      "1988.10.22 1988.07.27 1988.09.24 1\n",
      "1988.10.23 1988.07.28 1988.09.25 1\n",
      "1988.10.24 1988.07.29 1988.09.26 1\n",
      "1988.10.25 1988.07.30 1988.09.27 1\n",
      "1988.10.26 1988.07.31 1988.09.28 1\n",
      "1988.10.27 1988.08.01 1988.09.29 1\n",
      "1988.10.28 1988.08.02 1988.09.30 1\n",
      "1988.10.29 1988.08.03 1988.10.01 1\n",
      "1988.10.30 1988.08.04 1988.10.02 1\n",
      "1988.10.31 1988.08.05 1988.10.03 1\n",
      "1988.11.01 1988.08.06 1988.10.04 1\n",
      "1988.11.02 1988.08.07 1988.10.05 1\n",
      "1988.11.03 1988.08.08 1988.10.06 1\n",
      "1988.11.04 1988.08.09 1988.10.07 1\n",
      "1988.11.05 1988.08.10 1988.10.08 1\n",
      "1988.11.06 1988.08.11 1988.10.09 1\n",
      "1988.11.07 1988.08.12 1988.10.10 1\n",
      "1988.11.08 1988.08.13 1988.10.11 1\n",
      "1988.11.09 1988.08.14 1988.10.12 1\n",
      "1988.11.10 1988.08.15 1988.10.13 1\n",
      "1988.11.11 1988.08.16 1988.10.14 1\n",
      "1988.11.12 1988.08.17 1988.10.15 1\n",
      "1988.11.13 1988.08.18 1988.10.16 1\n",
      "1988.11.14 1988.08.19 1988.10.17 1\n",
      "1988.11.15 1988.08.20 1988.10.18 1\n",
      "1988.11.16 1988.08.21 1988.10.19 1\n",
      "1988.11.17 1988.08.22 1988.10.20 1\n",
      "1988.11.18 1988.08.23 1988.10.21 1\n",
      "1988.11.19 1988.08.24 1988.10.22 1\n",
      "1988.11.20 1988.08.25 1988.10.23 1\n",
      "1988.11.21 1988.08.26 1988.10.24 1\n",
      "1988.11.22 1988.08.27 1988.10.25 1\n",
      "1988.11.23 1988.08.28 1988.10.26 1\n",
      "1988.11.24 1988.08.29 1988.10.27 1\n",
      "1988.11.25 1988.08.30 1988.10.28 1\n",
      "1988.11.26 1988.08.31 1988.10.29 1\n",
      "1988.11.27 1988.09.01 1988.10.30 1\n",
      "1988.11.28 1988.09.02 1988.10.31 1\n",
      "1988.11.29 1988.09.03 1988.11.01 1\n",
      "1988.11.30 1988.09.04 1988.11.02 1\n",
      "1988.12.01 1988.09.05 1988.11.03 1\n",
      "1988.12.02 1988.09.06 1988.11.04 1\n",
      "1988.12.03 1988.09.07 1988.11.05 1\n",
      "1988.12.04 1988.09.08 1988.11.06 1\n",
      "1988.12.05 1988.09.09 1988.11.07 1\n",
      "1988.12.06 1988.09.10 1988.11.08 1\n",
      "1988.12.07 1988.09.11 1988.11.09 1\n",
      "1988.12.08 1988.09.12 1988.11.10 1\n",
      "1988.12.09 1988.09.13 1988.11.11 1\n",
      "1988.12.10 1988.09.14 1988.11.12 1\n",
      "1989.10.08 1989.07.13 1989.09.10 0\n",
      "1989.10.09 1989.07.14 1989.09.11 1\n",
      "1989.10.10 1989.07.15 1989.09.12 1\n",
      "1989.10.11 1989.07.16 1989.09.13 1\n",
      "1989.10.12 1989.07.17 1989.09.14 1\n",
      "1989.10.13 1989.07.18 1989.09.15 1\n",
      "1989.10.14 1989.07.19 1989.09.16 1\n",
      "1989.10.15 1989.07.20 1989.09.17 1\n",
      "1989.10.16 1989.07.21 1989.09.18 1\n",
      "1989.10.17 1989.07.22 1989.09.19 1\n",
      "1989.10.18 1989.07.23 1989.09.20 1\n",
      "1989.10.19 1989.07.24 1989.09.21 1\n",
      "1989.10.20 1989.07.25 1989.09.22 1\n",
      "1989.10.21 1989.07.26 1989.09.23 1\n",
      "1989.10.22 1989.07.27 1989.09.24 1\n",
      "1989.10.23 1989.07.28 1989.09.25 1\n",
      "1989.10.24 1989.07.29 1989.09.26 1\n",
      "1989.10.25 1989.07.30 1989.09.27 1\n",
      "1989.10.26 1989.07.31 1989.09.28 1\n",
      "1989.10.27 1989.08.01 1989.09.29 1\n",
      "1989.10.28 1989.08.02 1989.09.30 1\n",
      "1989.10.29 1989.08.03 1989.10.01 1\n",
      "1989.10.30 1989.08.04 1989.10.02 1\n",
      "1989.10.31 1989.08.05 1989.10.03 1\n",
      "1989.11.01 1989.08.06 1989.10.04 1\n",
      "1989.11.02 1989.08.07 1989.10.05 1\n",
      "1989.11.03 1989.08.08 1989.10.06 1\n",
      "1989.11.04 1989.08.09 1989.10.07 1\n",
      "1989.11.05 1989.08.10 1989.10.08 1\n",
      "1989.11.06 1989.08.11 1989.10.09 1\n",
      "1989.11.07 1989.08.12 1989.10.10 1\n",
      "1989.11.08 1989.08.13 1989.10.11 1\n",
      "1989.11.09 1989.08.14 1989.10.12 1\n",
      "1989.11.10 1989.08.15 1989.10.13 1\n",
      "1989.11.11 1989.08.16 1989.10.14 1\n",
      "1989.11.12 1989.08.17 1989.10.15 1\n",
      "1989.11.13 1989.08.18 1989.10.16 1\n",
      "1989.11.14 1989.08.19 1989.10.17 1\n",
      "1989.11.15 1989.08.20 1989.10.18 1\n",
      "1989.11.16 1989.08.21 1989.10.19 1\n",
      "1989.11.17 1989.08.22 1989.10.20 1\n",
      "1989.11.18 1989.08.23 1989.10.21 1\n",
      "1989.11.19 1989.08.24 1989.10.22 1\n",
      "1989.11.20 1989.08.25 1989.10.23 1\n",
      "1989.11.21 1989.08.26 1989.10.24 1\n",
      "1989.11.22 1989.08.27 1989.10.25 1\n",
      "1989.11.23 1989.08.28 1989.10.26 1\n",
      "1989.11.24 1989.08.29 1989.10.27 1\n",
      "1989.11.25 1989.08.30 1989.10.28 1\n",
      "1989.11.26 1989.08.31 1989.10.29 1\n",
      "1989.11.27 1989.09.01 1989.10.30 1\n",
      "1989.11.28 1989.09.02 1989.10.31 1\n",
      "1989.11.29 1989.09.03 1989.11.01 1\n",
      "1989.11.30 1989.09.04 1989.11.02 1\n",
      "1989.12.01 1989.09.05 1989.11.03 1\n",
      "1989.12.02 1989.09.06 1989.11.04 1\n",
      "1989.12.03 1989.09.07 1989.11.05 1\n",
      "1989.12.04 1989.09.08 1989.11.06 1\n",
      "1989.12.05 1989.09.09 1989.11.07 1\n",
      "1989.12.06 1989.09.10 1989.11.08 1\n",
      "1989.12.07 1989.09.11 1989.11.09 1\n",
      "1989.12.08 1989.09.12 1989.11.10 1\n",
      "1989.12.09 1989.09.13 1989.11.11 1\n",
      "1989.12.10 1989.09.14 1989.11.12 1\n",
      "1990.10.08 1990.07.13 1990.09.10 0\n",
      "1990.10.09 1990.07.14 1990.09.11 1\n",
      "1990.10.10 1990.07.15 1990.09.12 1\n",
      "1990.10.11 1990.07.16 1990.09.13 1\n",
      "1990.10.12 1990.07.17 1990.09.14 1\n",
      "1990.10.13 1990.07.18 1990.09.15 1\n",
      "1990.10.14 1990.07.19 1990.09.16 1\n",
      "1990.10.15 1990.07.20 1990.09.17 1\n",
      "1990.10.16 1990.07.21 1990.09.18 1\n",
      "1990.10.17 1990.07.22 1990.09.19 1\n",
      "1990.10.18 1990.07.23 1990.09.20 1\n",
      "1990.10.19 1990.07.24 1990.09.21 1\n",
      "1990.10.20 1990.07.25 1990.09.22 1\n",
      "1990.10.21 1990.07.26 1990.09.23 1\n",
      "1990.10.22 1990.07.27 1990.09.24 1\n",
      "1990.10.23 1990.07.28 1990.09.25 1\n",
      "1990.10.24 1990.07.29 1990.09.26 1\n",
      "1990.10.25 1990.07.30 1990.09.27 1\n",
      "1990.10.26 1990.07.31 1990.09.28 1\n",
      "1990.10.27 1990.08.01 1990.09.29 1\n",
      "1990.10.28 1990.08.02 1990.09.30 1\n",
      "1990.10.29 1990.08.03 1990.10.01 1\n",
      "1990.10.30 1990.08.04 1990.10.02 1\n",
      "1990.10.31 1990.08.05 1990.10.03 1\n",
      "1990.11.01 1990.08.06 1990.10.04 1\n",
      "1990.11.02 1990.08.07 1990.10.05 1\n",
      "1990.11.03 1990.08.08 1990.10.06 1\n",
      "1990.11.04 1990.08.09 1990.10.07 1\n",
      "1990.11.05 1990.08.10 1990.10.08 1\n",
      "1990.11.06 1990.08.11 1990.10.09 1\n",
      "1990.11.07 1990.08.12 1990.10.10 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990.11.08 1990.08.13 1990.10.11 1\n",
      "1990.11.09 1990.08.14 1990.10.12 1\n",
      "1990.11.10 1990.08.15 1990.10.13 1\n",
      "1990.11.11 1990.08.16 1990.10.14 1\n",
      "1990.11.12 1990.08.17 1990.10.15 1\n",
      "1990.11.13 1990.08.18 1990.10.16 1\n",
      "1990.11.14 1990.08.19 1990.10.17 1\n",
      "1990.11.15 1990.08.20 1990.10.18 1\n",
      "1990.11.16 1990.08.21 1990.10.19 1\n",
      "1990.11.17 1990.08.22 1990.10.20 1\n",
      "1990.11.18 1990.08.23 1990.10.21 1\n",
      "1990.11.19 1990.08.24 1990.10.22 1\n",
      "1990.11.20 1990.08.25 1990.10.23 1\n",
      "1990.11.21 1990.08.26 1990.10.24 1\n",
      "1990.11.22 1990.08.27 1990.10.25 1\n",
      "1990.11.23 1990.08.28 1990.10.26 1\n",
      "1990.11.24 1990.08.29 1990.10.27 1\n",
      "1990.11.25 1990.08.30 1990.10.28 1\n",
      "1990.11.26 1990.08.31 1990.10.29 1\n",
      "1990.11.27 1990.09.01 1990.10.30 1\n",
      "1990.11.28 1990.09.02 1990.10.31 1\n",
      "1990.11.29 1990.09.03 1990.11.01 1\n",
      "1990.11.30 1990.09.04 1990.11.02 1\n",
      "1990.12.01 1990.09.05 1990.11.03 1\n",
      "1990.12.02 1990.09.06 1990.11.04 1\n",
      "1990.12.03 1990.09.07 1990.11.05 1\n",
      "1990.12.04 1990.09.08 1990.11.06 1\n",
      "1990.12.05 1990.09.09 1990.11.07 1\n",
      "1990.12.06 1990.09.10 1990.11.08 1\n",
      "1990.12.07 1990.09.11 1990.11.09 1\n",
      "1990.12.08 1990.09.12 1990.11.10 1\n",
      "1990.12.09 1990.09.13 1990.11.11 1\n",
      "1990.12.10 1990.09.14 1990.11.12 1\n",
      "1991.10.08 1991.07.13 1991.09.10 0\n",
      "1991.10.09 1991.07.14 1991.09.11 1\n",
      "1991.10.10 1991.07.15 1991.09.12 1\n",
      "1991.10.11 1991.07.16 1991.09.13 1\n",
      "1991.10.12 1991.07.17 1991.09.14 1\n",
      "1991.10.13 1991.07.18 1991.09.15 1\n",
      "1991.10.14 1991.07.19 1991.09.16 1\n",
      "1991.10.15 1991.07.20 1991.09.17 1\n",
      "1991.10.16 1991.07.21 1991.09.18 1\n",
      "1991.10.17 1991.07.22 1991.09.19 1\n",
      "1991.10.18 1991.07.23 1991.09.20 1\n",
      "1991.10.19 1991.07.24 1991.09.21 1\n",
      "1991.10.20 1991.07.25 1991.09.22 1\n",
      "1991.10.21 1991.07.26 1991.09.23 1\n",
      "1991.10.22 1991.07.27 1991.09.24 1\n",
      "1991.10.23 1991.07.28 1991.09.25 1\n",
      "1991.10.24 1991.07.29 1991.09.26 1\n",
      "1991.10.25 1991.07.30 1991.09.27 1\n",
      "1991.10.26 1991.07.31 1991.09.28 1\n",
      "1991.10.27 1991.08.01 1991.09.29 1\n",
      "1991.10.28 1991.08.02 1991.09.30 1\n",
      "1991.10.29 1991.08.03 1991.10.01 1\n",
      "1991.10.30 1991.08.04 1991.10.02 1\n",
      "1991.10.31 1991.08.05 1991.10.03 1\n",
      "1991.11.01 1991.08.06 1991.10.04 1\n",
      "1991.11.02 1991.08.07 1991.10.05 1\n",
      "1991.11.03 1991.08.08 1991.10.06 1\n",
      "1991.11.04 1991.08.09 1991.10.07 1\n",
      "1991.11.05 1991.08.10 1991.10.08 1\n",
      "1991.11.06 1991.08.11 1991.10.09 1\n",
      "1991.11.07 1991.08.12 1991.10.10 1\n",
      "1991.11.08 1991.08.13 1991.10.11 1\n",
      "1991.11.09 1991.08.14 1991.10.12 1\n",
      "1991.11.10 1991.08.15 1991.10.13 1\n",
      "1991.11.11 1991.08.16 1991.10.14 1\n",
      "1991.11.12 1991.08.17 1991.10.15 1\n",
      "1991.11.13 1991.08.18 1991.10.16 1\n",
      "1991.11.14 1991.08.19 1991.10.17 1\n",
      "1991.11.15 1991.08.20 1991.10.18 1\n",
      "1991.11.16 1991.08.21 1991.10.19 1\n",
      "1991.11.17 1991.08.22 1991.10.20 1\n",
      "1991.11.18 1991.08.23 1991.10.21 1\n",
      "1991.11.19 1991.08.24 1991.10.22 1\n",
      "1991.11.20 1991.08.25 1991.10.23 1\n",
      "1991.11.21 1991.08.26 1991.10.24 1\n",
      "1991.11.22 1991.08.27 1991.10.25 1\n",
      "1991.11.23 1991.08.28 1991.10.26 1\n",
      "1991.11.24 1991.08.29 1991.10.27 1\n",
      "1991.11.25 1991.08.30 1991.10.28 1\n",
      "1991.11.26 1991.08.31 1991.10.29 1\n",
      "1991.11.27 1991.09.01 1991.10.30 1\n",
      "1991.11.28 1991.09.02 1991.10.31 1\n",
      "1991.11.29 1991.09.03 1991.11.01 1\n",
      "1991.11.30 1991.09.04 1991.11.02 1\n",
      "1991.12.01 1991.09.05 1991.11.03 1\n",
      "1991.12.02 1991.09.06 1991.11.04 1\n",
      "1991.12.03 1991.09.07 1991.11.05 1\n",
      "1991.12.04 1991.09.08 1991.11.06 1\n",
      "1991.12.05 1991.09.09 1991.11.07 1\n",
      "1991.12.06 1991.09.10 1991.11.08 1\n",
      "1991.12.07 1991.09.11 1991.11.09 1\n",
      "1991.12.08 1991.09.12 1991.11.10 1\n",
      "1991.12.09 1991.09.13 1991.11.11 1\n",
      "1991.12.10 1991.09.14 1991.11.12 1\n",
      "1992.10.08 1992.07.13 1992.09.10 0\n",
      "1992.10.09 1992.07.14 1992.09.11 1\n",
      "1992.10.10 1992.07.15 1992.09.12 1\n",
      "1992.10.11 1992.07.16 1992.09.13 1\n",
      "1992.10.12 1992.07.17 1992.09.14 1\n",
      "1992.10.13 1992.07.18 1992.09.15 1\n",
      "1992.10.14 1992.07.19 1992.09.16 1\n",
      "1992.10.15 1992.07.20 1992.09.17 1\n",
      "1992.10.16 1992.07.21 1992.09.18 1\n",
      "1992.10.17 1992.07.22 1992.09.19 1\n",
      "1992.10.18 1992.07.23 1992.09.20 1\n",
      "1992.10.19 1992.07.24 1992.09.21 1\n",
      "1992.10.20 1992.07.25 1992.09.22 1\n",
      "1992.10.21 1992.07.26 1992.09.23 1\n",
      "1992.10.22 1992.07.27 1992.09.24 1\n",
      "1992.10.23 1992.07.28 1992.09.25 1\n",
      "1992.10.24 1992.07.29 1992.09.26 1\n",
      "1992.10.25 1992.07.30 1992.09.27 1\n",
      "1992.10.26 1992.07.31 1992.09.28 1\n",
      "1992.10.27 1992.08.01 1992.09.29 1\n",
      "1992.10.28 1992.08.02 1992.09.30 1\n",
      "1992.10.29 1992.08.03 1992.10.01 1\n",
      "1992.10.30 1992.08.04 1992.10.02 1\n",
      "1992.10.31 1992.08.05 1992.10.03 1\n",
      "1992.11.01 1992.08.06 1992.10.04 1\n",
      "1992.11.02 1992.08.07 1992.10.05 1\n",
      "1992.11.03 1992.08.08 1992.10.06 1\n",
      "1992.11.04 1992.08.09 1992.10.07 1\n",
      "1992.11.05 1992.08.10 1992.10.08 1\n",
      "1992.11.06 1992.08.11 1992.10.09 1\n",
      "1992.11.07 1992.08.12 1992.10.10 1\n",
      "1992.11.08 1992.08.13 1992.10.11 1\n",
      "1992.11.09 1992.08.14 1992.10.12 1\n",
      "1992.11.10 1992.08.15 1992.10.13 1\n",
      "1992.11.11 1992.08.16 1992.10.14 1\n",
      "1992.11.12 1992.08.17 1992.10.15 1\n",
      "1992.11.13 1992.08.18 1992.10.16 1\n",
      "1992.11.14 1992.08.19 1992.10.17 1\n",
      "1992.11.15 1992.08.20 1992.10.18 1\n",
      "1992.11.16 1992.08.21 1992.10.19 1\n",
      "1992.11.17 1992.08.22 1992.10.20 1\n",
      "1992.11.18 1992.08.23 1992.10.21 1\n",
      "1992.11.19 1992.08.24 1992.10.22 1\n",
      "1992.11.20 1992.08.25 1992.10.23 1\n",
      "1992.11.21 1992.08.26 1992.10.24 1\n",
      "1992.11.22 1992.08.27 1992.10.25 1\n",
      "1992.11.23 1992.08.28 1992.10.26 1\n",
      "1992.11.24 1992.08.29 1992.10.27 1\n",
      "1992.11.25 1992.08.30 1992.10.28 1\n",
      "1992.11.26 1992.08.31 1992.10.29 1\n",
      "1992.11.27 1992.09.01 1992.10.30 1\n",
      "1992.11.28 1992.09.02 1992.10.31 1\n",
      "1992.11.29 1992.09.03 1992.11.01 1\n",
      "1992.11.30 1992.09.04 1992.11.02 1\n",
      "1992.12.01 1992.09.05 1992.11.03 1\n",
      "1992.12.02 1992.09.06 1992.11.04 1\n",
      "1992.12.03 1992.09.07 1992.11.05 1\n",
      "1992.12.04 1992.09.08 1992.11.06 1\n",
      "1992.12.05 1992.09.09 1992.11.07 1\n",
      "1992.12.06 1992.09.10 1992.11.08 1\n",
      "1992.12.07 1992.09.11 1992.11.09 1\n",
      "1992.12.08 1992.09.12 1992.11.10 1\n",
      "1992.12.09 1992.09.13 1992.11.11 1\n",
      "1992.12.10 1992.09.14 1992.11.12 1\n",
      "1993.10.08 1993.07.13 1993.09.10 0\n",
      "1993.10.09 1993.07.14 1993.09.11 1\n",
      "1993.10.10 1993.07.15 1993.09.12 1\n",
      "1993.10.11 1993.07.16 1993.09.13 1\n",
      "1993.10.12 1993.07.17 1993.09.14 1\n",
      "1993.10.13 1993.07.18 1993.09.15 1\n",
      "1993.10.14 1993.07.19 1993.09.16 1\n",
      "1993.10.15 1993.07.20 1993.09.17 1\n",
      "1993.10.16 1993.07.21 1993.09.18 1\n",
      "1993.10.17 1993.07.22 1993.09.19 1\n",
      "1993.10.18 1993.07.23 1993.09.20 1\n",
      "1993.10.19 1993.07.24 1993.09.21 1\n",
      "1993.10.20 1993.07.25 1993.09.22 1\n",
      "1993.10.21 1993.07.26 1993.09.23 1\n",
      "1993.10.22 1993.07.27 1993.09.24 1\n",
      "1993.10.23 1993.07.28 1993.09.25 1\n",
      "1993.10.24 1993.07.29 1993.09.26 1\n",
      "1993.10.25 1993.07.30 1993.09.27 1\n",
      "1993.10.26 1993.07.31 1993.09.28 1\n",
      "1993.10.27 1993.08.01 1993.09.29 1\n",
      "1993.10.28 1993.08.02 1993.09.30 1\n",
      "1993.10.29 1993.08.03 1993.10.01 1\n",
      "1993.10.30 1993.08.04 1993.10.02 1\n",
      "1993.10.31 1993.08.05 1993.10.03 1\n",
      "1993.11.01 1993.08.06 1993.10.04 1\n",
      "1993.11.02 1993.08.07 1993.10.05 1\n",
      "1993.11.03 1993.08.08 1993.10.06 1\n",
      "1993.11.04 1993.08.09 1993.10.07 1\n",
      "1993.11.05 1993.08.10 1993.10.08 1\n",
      "1993.11.06 1993.08.11 1993.10.09 1\n",
      "1993.11.07 1993.08.12 1993.10.10 1\n",
      "1993.11.08 1993.08.13 1993.10.11 1\n",
      "1993.11.09 1993.08.14 1993.10.12 1\n",
      "1993.11.10 1993.08.15 1993.10.13 1\n",
      "1993.11.11 1993.08.16 1993.10.14 1\n",
      "1993.11.12 1993.08.17 1993.10.15 1\n",
      "1993.11.13 1993.08.18 1993.10.16 1\n",
      "1993.11.14 1993.08.19 1993.10.17 1\n",
      "1993.11.15 1993.08.20 1993.10.18 1\n",
      "1993.11.16 1993.08.21 1993.10.19 1\n",
      "1993.11.17 1993.08.22 1993.10.20 1\n",
      "1993.11.18 1993.08.23 1993.10.21 1\n",
      "1993.11.19 1993.08.24 1993.10.22 1\n",
      "1993.11.20 1993.08.25 1993.10.23 1\n",
      "1993.11.21 1993.08.26 1993.10.24 1\n",
      "1993.11.22 1993.08.27 1993.10.25 1\n",
      "1993.11.23 1993.08.28 1993.10.26 1\n",
      "1993.11.24 1993.08.29 1993.10.27 1\n",
      "1993.11.25 1993.08.30 1993.10.28 1\n",
      "1993.11.26 1993.08.31 1993.10.29 1\n",
      "1993.11.27 1993.09.01 1993.10.30 1\n",
      "1993.11.28 1993.09.02 1993.10.31 1\n",
      "1993.11.29 1993.09.03 1993.11.01 1\n",
      "1993.11.30 1993.09.04 1993.11.02 1\n",
      "1993.12.01 1993.09.05 1993.11.03 1\n",
      "1993.12.02 1993.09.06 1993.11.04 1\n",
      "1993.12.03 1993.09.07 1993.11.05 1\n",
      "1993.12.04 1993.09.08 1993.11.06 1\n",
      "1993.12.05 1993.09.09 1993.11.07 1\n",
      "1993.12.06 1993.09.10 1993.11.08 1\n",
      "1993.12.07 1993.09.11 1993.11.09 1\n",
      "1993.12.08 1993.09.12 1993.11.10 1\n",
      "1993.12.09 1993.09.13 1993.11.11 1\n",
      "1993.12.10 1993.09.14 1993.11.12 1\n",
      "1994.10.08 1994.07.13 1994.09.10 0\n",
      "1994.10.09 1994.07.14 1994.09.11 1\n",
      "1994.10.10 1994.07.15 1994.09.12 1\n",
      "1994.10.11 1994.07.16 1994.09.13 1\n",
      "1994.10.12 1994.07.17 1994.09.14 1\n",
      "1994.10.13 1994.07.18 1994.09.15 1\n",
      "1994.10.14 1994.07.19 1994.09.16 1\n",
      "1994.10.15 1994.07.20 1994.09.17 1\n",
      "1994.10.16 1994.07.21 1994.09.18 1\n",
      "1994.10.17 1994.07.22 1994.09.19 1\n",
      "1994.10.18 1994.07.23 1994.09.20 1\n",
      "1994.10.19 1994.07.24 1994.09.21 1\n",
      "1994.10.20 1994.07.25 1994.09.22 1\n",
      "1994.10.21 1994.07.26 1994.09.23 1\n",
      "1994.10.22 1994.07.27 1994.09.24 1\n",
      "1994.10.23 1994.07.28 1994.09.25 1\n",
      "1994.10.24 1994.07.29 1994.09.26 1\n",
      "1994.10.25 1994.07.30 1994.09.27 1\n",
      "1994.10.26 1994.07.31 1994.09.28 1\n",
      "1994.10.27 1994.08.01 1994.09.29 1\n",
      "1994.10.28 1994.08.02 1994.09.30 1\n",
      "1994.10.29 1994.08.03 1994.10.01 1\n",
      "1994.10.30 1994.08.04 1994.10.02 1\n",
      "1994.10.31 1994.08.05 1994.10.03 1\n",
      "1994.11.01 1994.08.06 1994.10.04 1\n",
      "1994.11.02 1994.08.07 1994.10.05 1\n",
      "1994.11.03 1994.08.08 1994.10.06 1\n",
      "1994.11.04 1994.08.09 1994.10.07 1\n",
      "1994.11.05 1994.08.10 1994.10.08 1\n",
      "1994.11.06 1994.08.11 1994.10.09 1\n",
      "1994.11.07 1994.08.12 1994.10.10 1\n",
      "1994.11.08 1994.08.13 1994.10.11 1\n",
      "1994.11.09 1994.08.14 1994.10.12 1\n",
      "1994.11.10 1994.08.15 1994.10.13 1\n",
      "1994.11.11 1994.08.16 1994.10.14 1\n",
      "1994.11.12 1994.08.17 1994.10.15 1\n",
      "1994.11.13 1994.08.18 1994.10.16 1\n",
      "1994.11.14 1994.08.19 1994.10.17 1\n",
      "1994.11.15 1994.08.20 1994.10.18 1\n",
      "1994.11.16 1994.08.21 1994.10.19 1\n",
      "1994.11.17 1994.08.22 1994.10.20 1\n",
      "1994.11.18 1994.08.23 1994.10.21 1\n",
      "1994.11.19 1994.08.24 1994.10.22 1\n",
      "1994.11.20 1994.08.25 1994.10.23 1\n",
      "1994.11.21 1994.08.26 1994.10.24 1\n",
      "1994.11.22 1994.08.27 1994.10.25 1\n",
      "1994.11.23 1994.08.28 1994.10.26 1\n",
      "1994.11.24 1994.08.29 1994.10.27 1\n",
      "1994.11.25 1994.08.30 1994.10.28 1\n",
      "1994.11.26 1994.08.31 1994.10.29 1\n",
      "1994.11.27 1994.09.01 1994.10.30 1\n",
      "1994.11.28 1994.09.02 1994.10.31 1\n",
      "1994.11.29 1994.09.03 1994.11.01 1\n",
      "1994.11.30 1994.09.04 1994.11.02 1\n",
      "1994.12.01 1994.09.05 1994.11.03 1\n",
      "1994.12.02 1994.09.06 1994.11.04 1\n",
      "1994.12.03 1994.09.07 1994.11.05 1\n",
      "1994.12.04 1994.09.08 1994.11.06 1\n",
      "1994.12.05 1994.09.09 1994.11.07 1\n",
      "1994.12.06 1994.09.10 1994.11.08 1\n",
      "1994.12.07 1994.09.11 1994.11.09 1\n",
      "1994.12.08 1994.09.12 1994.11.10 1\n",
      "1994.12.09 1994.09.13 1994.11.11 1\n",
      "1994.12.10 1994.09.14 1994.11.12 1\n",
      "1995.10.08 1995.07.13 1995.09.10 0\n",
      "1995.10.09 1995.07.14 1995.09.11 1\n",
      "1995.10.10 1995.07.15 1995.09.12 1\n",
      "1995.10.11 1995.07.16 1995.09.13 1\n",
      "1995.10.12 1995.07.17 1995.09.14 1\n",
      "1995.10.13 1995.07.18 1995.09.15 1\n",
      "1995.10.14 1995.07.19 1995.09.16 1\n",
      "1995.10.15 1995.07.20 1995.09.17 1\n",
      "1995.10.16 1995.07.21 1995.09.18 1\n",
      "1995.10.17 1995.07.22 1995.09.19 1\n",
      "1995.10.18 1995.07.23 1995.09.20 1\n",
      "1995.10.19 1995.07.24 1995.09.21 1\n",
      "1995.10.20 1995.07.25 1995.09.22 1\n",
      "1995.10.21 1995.07.26 1995.09.23 1\n",
      "1995.10.22 1995.07.27 1995.09.24 1\n",
      "1995.10.23 1995.07.28 1995.09.25 1\n",
      "1995.10.24 1995.07.29 1995.09.26 1\n",
      "1995.10.25 1995.07.30 1995.09.27 1\n",
      "1995.10.26 1995.07.31 1995.09.28 1\n",
      "1995.10.27 1995.08.01 1995.09.29 1\n",
      "1995.10.28 1995.08.02 1995.09.30 1\n",
      "1995.10.29 1995.08.03 1995.10.01 1\n",
      "1995.10.30 1995.08.04 1995.10.02 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1995.10.31 1995.08.05 1995.10.03 1\n",
      "1995.11.01 1995.08.06 1995.10.04 1\n",
      "1995.11.02 1995.08.07 1995.10.05 1\n",
      "1995.11.03 1995.08.08 1995.10.06 1\n",
      "1995.11.04 1995.08.09 1995.10.07 1\n",
      "1995.11.05 1995.08.10 1995.10.08 1\n",
      "1995.11.06 1995.08.11 1995.10.09 1\n",
      "1995.11.07 1995.08.12 1995.10.10 1\n",
      "1995.11.08 1995.08.13 1995.10.11 1\n",
      "1995.11.09 1995.08.14 1995.10.12 1\n",
      "1995.11.10 1995.08.15 1995.10.13 1\n",
      "1995.11.11 1995.08.16 1995.10.14 1\n",
      "1995.11.12 1995.08.17 1995.10.15 1\n",
      "1995.11.13 1995.08.18 1995.10.16 1\n",
      "1995.11.14 1995.08.19 1995.10.17 1\n",
      "1995.11.15 1995.08.20 1995.10.18 1\n",
      "1995.11.16 1995.08.21 1995.10.19 1\n",
      "1995.11.17 1995.08.22 1995.10.20 1\n",
      "1995.11.18 1995.08.23 1995.10.21 1\n",
      "1995.11.19 1995.08.24 1995.10.22 1\n",
      "1995.11.20 1995.08.25 1995.10.23 1\n",
      "1995.11.21 1995.08.26 1995.10.24 1\n",
      "1995.11.22 1995.08.27 1995.10.25 1\n",
      "1995.11.23 1995.08.28 1995.10.26 1\n",
      "1995.11.24 1995.08.29 1995.10.27 1\n",
      "1995.11.25 1995.08.30 1995.10.28 1\n",
      "1995.11.26 1995.08.31 1995.10.29 1\n",
      "1995.11.27 1995.09.01 1995.10.30 1\n",
      "1995.11.28 1995.09.02 1995.10.31 1\n",
      "1995.11.29 1995.09.03 1995.11.01 1\n",
      "1995.11.30 1995.09.04 1995.11.02 1\n",
      "1995.12.01 1995.09.05 1995.11.03 1\n",
      "1995.12.02 1995.09.06 1995.11.04 1\n",
      "1995.12.03 1995.09.07 1995.11.05 1\n",
      "1995.12.04 1995.09.08 1995.11.06 1\n",
      "1995.12.05 1995.09.09 1995.11.07 1\n",
      "1995.12.06 1995.09.10 1995.11.08 1\n",
      "1995.12.07 1995.09.11 1995.11.09 1\n",
      "1995.12.08 1995.09.12 1995.11.10 1\n",
      "1995.12.09 1995.09.13 1995.11.11 1\n",
      "1995.12.10 1995.09.14 1995.11.12 1\n",
      "1996.10.08 1996.07.13 1996.09.10 0\n",
      "1996.10.09 1996.07.14 1996.09.11 1\n",
      "1996.10.10 1996.07.15 1996.09.12 1\n",
      "1996.10.11 1996.07.16 1996.09.13 1\n",
      "1996.10.12 1996.07.17 1996.09.14 1\n",
      "1996.10.13 1996.07.18 1996.09.15 1\n",
      "1996.10.14 1996.07.19 1996.09.16 1\n",
      "1996.10.15 1996.07.20 1996.09.17 1\n",
      "1996.10.16 1996.07.21 1996.09.18 1\n",
      "1996.10.17 1996.07.22 1996.09.19 1\n",
      "1996.10.18 1996.07.23 1996.09.20 1\n",
      "1996.10.19 1996.07.24 1996.09.21 1\n",
      "1996.10.20 1996.07.25 1996.09.22 1\n",
      "1996.10.21 1996.07.26 1996.09.23 1\n",
      "1996.10.22 1996.07.27 1996.09.24 1\n",
      "1996.10.23 1996.07.28 1996.09.25 1\n",
      "1996.10.24 1996.07.29 1996.09.26 1\n",
      "1996.10.25 1996.07.30 1996.09.27 1\n",
      "1996.10.26 1996.07.31 1996.09.28 1\n",
      "1996.10.27 1996.08.01 1996.09.29 1\n",
      "1996.10.28 1996.08.02 1996.09.30 1\n",
      "1996.10.29 1996.08.03 1996.10.01 1\n",
      "1996.10.30 1996.08.04 1996.10.02 1\n",
      "1996.10.31 1996.08.05 1996.10.03 1\n",
      "1996.11.01 1996.08.06 1996.10.04 1\n",
      "1996.11.02 1996.08.07 1996.10.05 1\n",
      "1996.11.03 1996.08.08 1996.10.06 1\n",
      "1996.11.04 1996.08.09 1996.10.07 1\n",
      "1996.11.05 1996.08.10 1996.10.08 1\n",
      "1996.11.06 1996.08.11 1996.10.09 1\n",
      "1996.11.07 1996.08.12 1996.10.10 1\n",
      "1996.11.08 1996.08.13 1996.10.11 1\n",
      "1996.11.09 1996.08.14 1996.10.12 1\n",
      "1996.11.10 1996.08.15 1996.10.13 1\n",
      "1996.11.11 1996.08.16 1996.10.14 1\n",
      "1996.11.12 1996.08.17 1996.10.15 1\n",
      "1996.11.13 1996.08.18 1996.10.16 1\n",
      "1996.11.14 1996.08.19 1996.10.17 1\n",
      "1996.11.15 1996.08.20 1996.10.18 1\n",
      "1996.11.16 1996.08.21 1996.10.19 1\n",
      "1996.11.17 1996.08.22 1996.10.20 1\n",
      "1996.11.18 1996.08.23 1996.10.21 1\n",
      "1996.11.19 1996.08.24 1996.10.22 1\n",
      "1996.11.20 1996.08.25 1996.10.23 1\n",
      "1996.11.21 1996.08.26 1996.10.24 1\n",
      "1996.11.22 1996.08.27 1996.10.25 1\n",
      "1996.11.23 1996.08.28 1996.10.26 1\n",
      "1996.11.24 1996.08.29 1996.10.27 1\n",
      "1996.11.25 1996.08.30 1996.10.28 1\n",
      "1996.11.26 1996.08.31 1996.10.29 1\n",
      "1996.11.27 1996.09.01 1996.10.30 1\n",
      "1996.11.28 1996.09.02 1996.10.31 1\n",
      "1996.11.29 1996.09.03 1996.11.01 1\n",
      "1996.11.30 1996.09.04 1996.11.02 1\n",
      "1996.12.01 1996.09.05 1996.11.03 1\n",
      "1996.12.02 1996.09.06 1996.11.04 1\n",
      "1996.12.03 1996.09.07 1996.11.05 1\n",
      "1996.12.04 1996.09.08 1996.11.06 1\n",
      "1996.12.05 1996.09.09 1996.11.07 1\n",
      "1996.12.06 1996.09.10 1996.11.08 1\n",
      "1996.12.07 1996.09.11 1996.11.09 1\n",
      "1996.12.08 1996.09.12 1996.11.10 1\n",
      "1996.12.09 1996.09.13 1996.11.11 1\n",
      "1996.12.10 1996.09.14 1996.11.12 1\n",
      "1997.10.08 1997.07.13 1997.09.10 0\n",
      "1997.10.09 1997.07.14 1997.09.11 1\n",
      "1997.10.10 1997.07.15 1997.09.12 1\n",
      "1997.10.11 1997.07.16 1997.09.13 1\n",
      "1997.10.12 1997.07.17 1997.09.14 1\n",
      "1997.10.13 1997.07.18 1997.09.15 1\n",
      "1997.10.14 1997.07.19 1997.09.16 1\n",
      "1997.10.15 1997.07.20 1997.09.17 1\n",
      "1997.10.16 1997.07.21 1997.09.18 1\n",
      "1997.10.17 1997.07.22 1997.09.19 1\n",
      "1997.10.18 1997.07.23 1997.09.20 1\n",
      "1997.10.19 1997.07.24 1997.09.21 1\n",
      "1997.10.20 1997.07.25 1997.09.22 1\n",
      "1997.10.21 1997.07.26 1997.09.23 1\n",
      "1997.10.22 1997.07.27 1997.09.24 1\n",
      "1997.10.23 1997.07.28 1997.09.25 1\n",
      "1997.10.24 1997.07.29 1997.09.26 1\n",
      "1997.10.25 1997.07.30 1997.09.27 1\n",
      "1997.10.26 1997.07.31 1997.09.28 1\n",
      "1997.10.27 1997.08.01 1997.09.29 1\n",
      "1997.10.28 1997.08.02 1997.09.30 1\n",
      "1997.10.29 1997.08.03 1997.10.01 1\n",
      "1997.10.30 1997.08.04 1997.10.02 1\n",
      "1997.10.31 1997.08.05 1997.10.03 1\n",
      "1997.11.01 1997.08.06 1997.10.04 1\n",
      "1997.11.02 1997.08.07 1997.10.05 1\n",
      "1997.11.03 1997.08.08 1997.10.06 1\n",
      "1997.11.04 1997.08.09 1997.10.07 1\n",
      "1997.11.05 1997.08.10 1997.10.08 1\n",
      "1997.11.06 1997.08.11 1997.10.09 1\n",
      "1997.11.07 1997.08.12 1997.10.10 1\n",
      "1997.11.08 1997.08.13 1997.10.11 1\n",
      "1997.11.09 1997.08.14 1997.10.12 1\n",
      "1997.11.10 1997.08.15 1997.10.13 1\n",
      "1997.11.11 1997.08.16 1997.10.14 1\n",
      "1997.11.12 1997.08.17 1997.10.15 1\n",
      "1997.11.13 1997.08.18 1997.10.16 1\n",
      "1997.11.14 1997.08.19 1997.10.17 1\n",
      "1997.11.15 1997.08.20 1997.10.18 1\n",
      "1997.11.16 1997.08.21 1997.10.19 1\n",
      "1997.11.17 1997.08.22 1997.10.20 1\n",
      "1997.11.18 1997.08.23 1997.10.21 1\n",
      "1997.11.19 1997.08.24 1997.10.22 1\n",
      "1997.11.20 1997.08.25 1997.10.23 1\n",
      "1997.11.21 1997.08.26 1997.10.24 1\n",
      "1997.11.22 1997.08.27 1997.10.25 1\n",
      "1997.11.23 1997.08.28 1997.10.26 1\n",
      "1997.11.24 1997.08.29 1997.10.27 1\n",
      "1997.11.25 1997.08.30 1997.10.28 1\n",
      "1997.11.26 1997.08.31 1997.10.29 1\n",
      "1997.11.27 1997.09.01 1997.10.30 1\n",
      "1997.11.28 1997.09.02 1997.10.31 1\n",
      "1997.11.29 1997.09.03 1997.11.01 1\n",
      "1997.11.30 1997.09.04 1997.11.02 1\n",
      "1997.12.01 1997.09.05 1997.11.03 1\n",
      "1997.12.02 1997.09.06 1997.11.04 1\n",
      "1997.12.03 1997.09.07 1997.11.05 1\n",
      "1997.12.04 1997.09.08 1997.11.06 1\n",
      "1997.12.05 1997.09.09 1997.11.07 1\n",
      "1997.12.06 1997.09.10 1997.11.08 1\n",
      "1997.12.07 1997.09.11 1997.11.09 1\n",
      "1997.12.08 1997.09.12 1997.11.10 1\n",
      "1997.12.09 1997.09.13 1997.11.11 1\n",
      "1997.12.10 1997.09.14 1997.11.12 1\n",
      "1998.10.08 1998.07.13 1998.09.10 0\n",
      "1998.10.09 1998.07.14 1998.09.11 1\n",
      "1998.10.10 1998.07.15 1998.09.12 1\n",
      "1998.10.11 1998.07.16 1998.09.13 1\n",
      "1998.10.12 1998.07.17 1998.09.14 1\n",
      "1998.10.13 1998.07.18 1998.09.15 1\n",
      "1998.10.14 1998.07.19 1998.09.16 1\n",
      "1998.10.15 1998.07.20 1998.09.17 1\n",
      "1998.10.16 1998.07.21 1998.09.18 1\n",
      "1998.10.17 1998.07.22 1998.09.19 1\n",
      "1998.10.18 1998.07.23 1998.09.20 1\n",
      "1998.10.19 1998.07.24 1998.09.21 1\n",
      "1998.10.20 1998.07.25 1998.09.22 1\n",
      "1998.10.21 1998.07.26 1998.09.23 1\n",
      "1998.10.22 1998.07.27 1998.09.24 1\n",
      "1998.10.23 1998.07.28 1998.09.25 1\n",
      "1998.10.24 1998.07.29 1998.09.26 1\n",
      "1998.10.25 1998.07.30 1998.09.27 1\n",
      "1998.10.26 1998.07.31 1998.09.28 1\n",
      "1998.10.27 1998.08.01 1998.09.29 1\n",
      "1998.10.28 1998.08.02 1998.09.30 1\n",
      "1998.10.29 1998.08.03 1998.10.01 1\n",
      "1998.10.30 1998.08.04 1998.10.02 1\n",
      "1998.10.31 1998.08.05 1998.10.03 1\n",
      "1998.11.01 1998.08.06 1998.10.04 1\n",
      "1998.11.02 1998.08.07 1998.10.05 1\n",
      "1998.11.03 1998.08.08 1998.10.06 1\n",
      "1998.11.04 1998.08.09 1998.10.07 1\n",
      "1998.11.05 1998.08.10 1998.10.08 1\n",
      "1998.11.06 1998.08.11 1998.10.09 1\n",
      "1998.11.07 1998.08.12 1998.10.10 1\n",
      "1998.11.08 1998.08.13 1998.10.11 1\n",
      "1998.11.09 1998.08.14 1998.10.12 1\n",
      "1998.11.10 1998.08.15 1998.10.13 1\n",
      "1998.11.11 1998.08.16 1998.10.14 1\n",
      "1998.11.12 1998.08.17 1998.10.15 1\n",
      "1998.11.13 1998.08.18 1998.10.16 1\n",
      "1998.11.14 1998.08.19 1998.10.17 1\n",
      "1998.11.15 1998.08.20 1998.10.18 1\n",
      "1998.11.16 1998.08.21 1998.10.19 1\n",
      "1998.11.17 1998.08.22 1998.10.20 1\n",
      "1998.11.18 1998.08.23 1998.10.21 1\n",
      "1998.11.19 1998.08.24 1998.10.22 1\n",
      "1998.11.20 1998.08.25 1998.10.23 1\n",
      "1998.11.21 1998.08.26 1998.10.24 1\n",
      "1998.11.22 1998.08.27 1998.10.25 1\n",
      "1998.11.23 1998.08.28 1998.10.26 1\n",
      "1998.11.24 1998.08.29 1998.10.27 1\n",
      "1998.11.25 1998.08.30 1998.10.28 1\n",
      "1998.11.26 1998.08.31 1998.10.29 1\n",
      "1998.11.27 1998.09.01 1998.10.30 1\n",
      "1998.11.28 1998.09.02 1998.10.31 1\n",
      "1998.11.29 1998.09.03 1998.11.01 1\n",
      "1998.11.30 1998.09.04 1998.11.02 1\n",
      "1998.12.01 1998.09.05 1998.11.03 1\n",
      "1998.12.02 1998.09.06 1998.11.04 1\n",
      "1998.12.03 1998.09.07 1998.11.05 1\n",
      "1998.12.04 1998.09.08 1998.11.06 1\n",
      "1998.12.05 1998.09.09 1998.11.07 1\n",
      "1998.12.06 1998.09.10 1998.11.08 1\n",
      "1998.12.07 1998.09.11 1998.11.09 1\n",
      "1998.12.08 1998.09.12 1998.11.10 1\n",
      "1998.12.09 1998.09.13 1998.11.11 1\n",
      "1998.12.10 1998.09.14 1998.11.12 1\n",
      "1999.10.08 1999.07.13 1999.09.10 0\n",
      "1999.10.09 1999.07.14 1999.09.11 1\n",
      "1999.10.10 1999.07.15 1999.09.12 1\n",
      "1999.10.11 1999.07.16 1999.09.13 1\n",
      "1999.10.12 1999.07.17 1999.09.14 1\n",
      "1999.10.13 1999.07.18 1999.09.15 1\n",
      "1999.10.14 1999.07.19 1999.09.16 1\n",
      "1999.10.15 1999.07.20 1999.09.17 1\n",
      "1999.10.16 1999.07.21 1999.09.18 1\n",
      "1999.10.17 1999.07.22 1999.09.19 1\n",
      "1999.10.18 1999.07.23 1999.09.20 1\n",
      "1999.10.19 1999.07.24 1999.09.21 1\n",
      "1999.10.20 1999.07.25 1999.09.22 1\n",
      "1999.10.21 1999.07.26 1999.09.23 1\n",
      "1999.10.22 1999.07.27 1999.09.24 1\n",
      "1999.10.23 1999.07.28 1999.09.25 1\n",
      "1999.10.24 1999.07.29 1999.09.26 1\n",
      "1999.10.25 1999.07.30 1999.09.27 1\n",
      "1999.10.26 1999.07.31 1999.09.28 1\n",
      "1999.10.27 1999.08.01 1999.09.29 1\n",
      "1999.10.28 1999.08.02 1999.09.30 1\n",
      "1999.10.29 1999.08.03 1999.10.01 1\n",
      "1999.10.30 1999.08.04 1999.10.02 1\n",
      "1999.10.31 1999.08.05 1999.10.03 1\n",
      "1999.11.01 1999.08.06 1999.10.04 1\n",
      "1999.11.02 1999.08.07 1999.10.05 1\n",
      "1999.11.03 1999.08.08 1999.10.06 1\n",
      "1999.11.04 1999.08.09 1999.10.07 1\n",
      "1999.11.05 1999.08.10 1999.10.08 1\n",
      "1999.11.06 1999.08.11 1999.10.09 1\n",
      "1999.11.07 1999.08.12 1999.10.10 1\n",
      "1999.11.08 1999.08.13 1999.10.11 1\n",
      "1999.11.09 1999.08.14 1999.10.12 1\n",
      "1999.11.10 1999.08.15 1999.10.13 1\n",
      "1999.11.11 1999.08.16 1999.10.14 1\n",
      "1999.11.12 1999.08.17 1999.10.15 1\n",
      "1999.11.13 1999.08.18 1999.10.16 1\n",
      "1999.11.14 1999.08.19 1999.10.17 1\n",
      "1999.11.15 1999.08.20 1999.10.18 1\n",
      "1999.11.16 1999.08.21 1999.10.19 1\n",
      "1999.11.17 1999.08.22 1999.10.20 1\n",
      "1999.11.18 1999.08.23 1999.10.21 1\n",
      "1999.11.19 1999.08.24 1999.10.22 1\n",
      "1999.11.20 1999.08.25 1999.10.23 1\n",
      "1999.11.21 1999.08.26 1999.10.24 1\n",
      "1999.11.22 1999.08.27 1999.10.25 1\n",
      "1999.11.23 1999.08.28 1999.10.26 1\n",
      "1999.11.24 1999.08.29 1999.10.27 1\n",
      "1999.11.25 1999.08.30 1999.10.28 1\n",
      "1999.11.26 1999.08.31 1999.10.29 1\n",
      "1999.11.27 1999.09.01 1999.10.30 1\n",
      "1999.11.28 1999.09.02 1999.10.31 1\n",
      "1999.11.29 1999.09.03 1999.11.01 1\n",
      "1999.11.30 1999.09.04 1999.11.02 1\n",
      "1999.12.01 1999.09.05 1999.11.03 1\n",
      "1999.12.02 1999.09.06 1999.11.04 1\n",
      "1999.12.03 1999.09.07 1999.11.05 1\n",
      "1999.12.04 1999.09.08 1999.11.06 1\n",
      "1999.12.05 1999.09.09 1999.11.07 1\n",
      "1999.12.06 1999.09.10 1999.11.08 1\n",
      "1999.12.07 1999.09.11 1999.11.09 1\n",
      "1999.12.08 1999.09.12 1999.11.10 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999.12.09 1999.09.13 1999.11.11 1\n",
      "1999.12.10 1999.09.14 1999.11.12 1\n",
      "2000.10.08 2000.07.13 2000.09.10 0\n",
      "2000.10.09 2000.07.14 2000.09.11 1\n",
      "2000.10.10 2000.07.15 2000.09.12 1\n",
      "2000.10.11 2000.07.16 2000.09.13 1\n",
      "2000.10.12 2000.07.17 2000.09.14 1\n",
      "2000.10.13 2000.07.18 2000.09.15 1\n",
      "2000.10.14 2000.07.19 2000.09.16 1\n",
      "2000.10.15 2000.07.20 2000.09.17 1\n",
      "2000.10.16 2000.07.21 2000.09.18 1\n",
      "2000.10.17 2000.07.22 2000.09.19 1\n",
      "2000.10.18 2000.07.23 2000.09.20 1\n",
      "2000.10.19 2000.07.24 2000.09.21 1\n",
      "2000.10.20 2000.07.25 2000.09.22 1\n",
      "2000.10.21 2000.07.26 2000.09.23 1\n",
      "2000.10.22 2000.07.27 2000.09.24 1\n",
      "2000.10.23 2000.07.28 2000.09.25 1\n",
      "2000.10.24 2000.07.29 2000.09.26 1\n",
      "2000.10.25 2000.07.30 2000.09.27 1\n",
      "2000.10.26 2000.07.31 2000.09.28 1\n",
      "2000.10.27 2000.08.01 2000.09.29 1\n",
      "2000.10.28 2000.08.02 2000.09.30 1\n",
      "2000.10.29 2000.08.03 2000.10.01 1\n",
      "2000.10.30 2000.08.04 2000.10.02 1\n",
      "2000.10.31 2000.08.05 2000.10.03 1\n",
      "2000.11.01 2000.08.06 2000.10.04 1\n",
      "2000.11.02 2000.08.07 2000.10.05 1\n",
      "2000.11.03 2000.08.08 2000.10.06 1\n",
      "2000.11.04 2000.08.09 2000.10.07 1\n",
      "2000.11.05 2000.08.10 2000.10.08 1\n",
      "2000.11.06 2000.08.11 2000.10.09 1\n",
      "2000.11.07 2000.08.12 2000.10.10 1\n",
      "2000.11.08 2000.08.13 2000.10.11 1\n",
      "2000.11.09 2000.08.14 2000.10.12 1\n",
      "2000.11.10 2000.08.15 2000.10.13 1\n",
      "2000.11.11 2000.08.16 2000.10.14 1\n",
      "2000.11.12 2000.08.17 2000.10.15 1\n",
      "2000.11.13 2000.08.18 2000.10.16 1\n",
      "2000.11.14 2000.08.19 2000.10.17 1\n",
      "2000.11.15 2000.08.20 2000.10.18 1\n",
      "2000.11.16 2000.08.21 2000.10.19 1\n",
      "2000.11.17 2000.08.22 2000.10.20 1\n",
      "2000.11.18 2000.08.23 2000.10.21 1\n",
      "2000.11.19 2000.08.24 2000.10.22 1\n",
      "2000.11.20 2000.08.25 2000.10.23 1\n",
      "2000.11.21 2000.08.26 2000.10.24 1\n",
      "2000.11.22 2000.08.27 2000.10.25 1\n",
      "2000.11.23 2000.08.28 2000.10.26 1\n",
      "2000.11.24 2000.08.29 2000.10.27 1\n",
      "2000.11.25 2000.08.30 2000.10.28 1\n",
      "2000.11.26 2000.08.31 2000.10.29 1\n",
      "2000.11.27 2000.09.01 2000.10.30 1\n",
      "2000.11.28 2000.09.02 2000.10.31 1\n",
      "2000.11.29 2000.09.03 2000.11.01 1\n",
      "2000.11.30 2000.09.04 2000.11.02 1\n",
      "2000.12.01 2000.09.05 2000.11.03 1\n",
      "2000.12.02 2000.09.06 2000.11.04 1\n",
      "2000.12.03 2000.09.07 2000.11.05 1\n",
      "2000.12.04 2000.09.08 2000.11.06 1\n",
      "2000.12.05 2000.09.09 2000.11.07 1\n",
      "2000.12.06 2000.09.10 2000.11.08 1\n",
      "2000.12.07 2000.09.11 2000.11.09 1\n",
      "2000.12.08 2000.09.12 2000.11.10 1\n",
      "2000.12.09 2000.09.13 2000.11.11 1\n",
      "2000.12.10 2000.09.14 2000.11.12 1\n",
      "2001.10.08 2001.07.13 2001.09.10 0\n",
      "2001.10.09 2001.07.14 2001.09.11 1\n",
      "2001.10.10 2001.07.15 2001.09.12 1\n",
      "2001.10.11 2001.07.16 2001.09.13 1\n",
      "2001.10.12 2001.07.17 2001.09.14 1\n",
      "2001.10.13 2001.07.18 2001.09.15 1\n",
      "2001.10.14 2001.07.19 2001.09.16 1\n",
      "2001.10.15 2001.07.20 2001.09.17 1\n",
      "2001.10.16 2001.07.21 2001.09.18 1\n",
      "2001.10.17 2001.07.22 2001.09.19 1\n",
      "2001.10.18 2001.07.23 2001.09.20 1\n",
      "2001.10.19 2001.07.24 2001.09.21 1\n",
      "2001.10.20 2001.07.25 2001.09.22 1\n",
      "2001.10.21 2001.07.26 2001.09.23 1\n",
      "2001.10.22 2001.07.27 2001.09.24 1\n",
      "2001.10.23 2001.07.28 2001.09.25 1\n",
      "2001.10.24 2001.07.29 2001.09.26 1\n",
      "2001.10.25 2001.07.30 2001.09.27 1\n",
      "2001.10.26 2001.07.31 2001.09.28 1\n",
      "2001.10.27 2001.08.01 2001.09.29 1\n",
      "2001.10.28 2001.08.02 2001.09.30 1\n",
      "2001.10.29 2001.08.03 2001.10.01 1\n",
      "2001.10.30 2001.08.04 2001.10.02 1\n",
      "2001.10.31 2001.08.05 2001.10.03 1\n",
      "2001.11.01 2001.08.06 2001.10.04 1\n",
      "2001.11.02 2001.08.07 2001.10.05 1\n",
      "2001.11.03 2001.08.08 2001.10.06 1\n",
      "2001.11.04 2001.08.09 2001.10.07 1\n",
      "2001.11.05 2001.08.10 2001.10.08 1\n",
      "2001.11.06 2001.08.11 2001.10.09 1\n",
      "2001.11.07 2001.08.12 2001.10.10 1\n",
      "2001.11.08 2001.08.13 2001.10.11 1\n",
      "2001.11.09 2001.08.14 2001.10.12 1\n",
      "2001.11.10 2001.08.15 2001.10.13 1\n",
      "2001.11.11 2001.08.16 2001.10.14 1\n",
      "2001.11.12 2001.08.17 2001.10.15 1\n",
      "2001.11.13 2001.08.18 2001.10.16 1\n",
      "2001.11.14 2001.08.19 2001.10.17 1\n",
      "2001.11.15 2001.08.20 2001.10.18 1\n",
      "2001.11.16 2001.08.21 2001.10.19 1\n",
      "2001.11.17 2001.08.22 2001.10.20 1\n",
      "2001.11.18 2001.08.23 2001.10.21 1\n",
      "2001.11.19 2001.08.24 2001.10.22 1\n",
      "2001.11.20 2001.08.25 2001.10.23 1\n",
      "2001.11.21 2001.08.26 2001.10.24 1\n",
      "2001.11.22 2001.08.27 2001.10.25 1\n",
      "2001.11.23 2001.08.28 2001.10.26 1\n",
      "2001.11.24 2001.08.29 2001.10.27 1\n",
      "2001.11.25 2001.08.30 2001.10.28 1\n",
      "2001.11.26 2001.08.31 2001.10.29 1\n",
      "2001.11.27 2001.09.01 2001.10.30 1\n",
      "2001.11.28 2001.09.02 2001.10.31 1\n",
      "2001.11.29 2001.09.03 2001.11.01 1\n",
      "2001.11.30 2001.09.04 2001.11.02 1\n",
      "2001.12.01 2001.09.05 2001.11.03 1\n",
      "2001.12.02 2001.09.06 2001.11.04 1\n",
      "2001.12.03 2001.09.07 2001.11.05 1\n",
      "2001.12.04 2001.09.08 2001.11.06 1\n",
      "2001.12.05 2001.09.09 2001.11.07 1\n",
      "2001.12.06 2001.09.10 2001.11.08 1\n",
      "2001.12.07 2001.09.11 2001.11.09 1\n",
      "2001.12.08 2001.09.12 2001.11.10 1\n",
      "2001.12.09 2001.09.13 2001.11.11 1\n",
      "2001.12.10 2001.09.14 2001.11.12 1\n",
      "2002.10.08 2002.07.13 2002.09.10 0\n",
      "2002.10.09 2002.07.14 2002.09.11 1\n",
      "2002.10.10 2002.07.15 2002.09.12 1\n",
      "2002.10.11 2002.07.16 2002.09.13 1\n",
      "2002.10.12 2002.07.17 2002.09.14 1\n",
      "2002.10.13 2002.07.18 2002.09.15 1\n",
      "2002.10.14 2002.07.19 2002.09.16 1\n",
      "2002.10.15 2002.07.20 2002.09.17 1\n",
      "2002.10.16 2002.07.21 2002.09.18 1\n",
      "2002.10.17 2002.07.22 2002.09.19 1\n",
      "2002.10.18 2002.07.23 2002.09.20 1\n",
      "2002.10.19 2002.07.24 2002.09.21 1\n",
      "2002.10.20 2002.07.25 2002.09.22 1\n",
      "2002.10.21 2002.07.26 2002.09.23 1\n",
      "2002.10.22 2002.07.27 2002.09.24 1\n",
      "2002.10.23 2002.07.28 2002.09.25 1\n",
      "2002.10.24 2002.07.29 2002.09.26 1\n",
      "2002.10.25 2002.07.30 2002.09.27 1\n",
      "2002.10.26 2002.07.31 2002.09.28 1\n",
      "2002.10.27 2002.08.01 2002.09.29 1\n",
      "2002.10.28 2002.08.02 2002.09.30 1\n",
      "2002.10.29 2002.08.03 2002.10.01 1\n",
      "2002.10.30 2002.08.04 2002.10.02 1\n",
      "2002.10.31 2002.08.05 2002.10.03 1\n",
      "2002.11.01 2002.08.06 2002.10.04 1\n",
      "2002.11.02 2002.08.07 2002.10.05 1\n",
      "2002.11.03 2002.08.08 2002.10.06 1\n",
      "2002.11.04 2002.08.09 2002.10.07 1\n",
      "2002.11.05 2002.08.10 2002.10.08 1\n",
      "2002.11.06 2002.08.11 2002.10.09 1\n",
      "2002.11.07 2002.08.12 2002.10.10 1\n",
      "2002.11.08 2002.08.13 2002.10.11 1\n",
      "2002.11.09 2002.08.14 2002.10.12 1\n",
      "2002.11.10 2002.08.15 2002.10.13 1\n",
      "2002.11.11 2002.08.16 2002.10.14 1\n",
      "2002.11.12 2002.08.17 2002.10.15 1\n",
      "2002.11.13 2002.08.18 2002.10.16 1\n",
      "2002.11.14 2002.08.19 2002.10.17 1\n",
      "2002.11.15 2002.08.20 2002.10.18 1\n",
      "2002.11.16 2002.08.21 2002.10.19 1\n",
      "2002.11.17 2002.08.22 2002.10.20 1\n",
      "2002.11.18 2002.08.23 2002.10.21 1\n",
      "2002.11.19 2002.08.24 2002.10.22 1\n",
      "2002.11.20 2002.08.25 2002.10.23 1\n",
      "2002.11.21 2002.08.26 2002.10.24 1\n",
      "2002.11.22 2002.08.27 2002.10.25 1\n",
      "2002.11.23 2002.08.28 2002.10.26 1\n",
      "2002.11.24 2002.08.29 2002.10.27 1\n",
      "2002.11.25 2002.08.30 2002.10.28 1\n",
      "2002.11.26 2002.08.31 2002.10.29 1\n",
      "2002.11.27 2002.09.01 2002.10.30 1\n",
      "2002.11.28 2002.09.02 2002.10.31 1\n",
      "2002.11.29 2002.09.03 2002.11.01 1\n",
      "2002.11.30 2002.09.04 2002.11.02 1\n",
      "2002.12.01 2002.09.05 2002.11.03 1\n",
      "2002.12.02 2002.09.06 2002.11.04 1\n",
      "2002.12.03 2002.09.07 2002.11.05 1\n",
      "2002.12.04 2002.09.08 2002.11.06 1\n",
      "2002.12.05 2002.09.09 2002.11.07 1\n",
      "2002.12.06 2002.09.10 2002.11.08 1\n",
      "2002.12.07 2002.09.11 2002.11.09 1\n",
      "2002.12.08 2002.09.12 2002.11.10 1\n",
      "2002.12.09 2002.09.13 2002.11.11 1\n",
      "2002.12.10 2002.09.14 2002.11.12 1\n",
      "2003.10.08 2003.07.13 2003.09.10 0\n",
      "2003.10.09 2003.07.14 2003.09.11 1\n",
      "2003.10.10 2003.07.15 2003.09.12 1\n",
      "2003.10.11 2003.07.16 2003.09.13 1\n",
      "2003.10.12 2003.07.17 2003.09.14 1\n",
      "2003.10.13 2003.07.18 2003.09.15 1\n",
      "2003.10.14 2003.07.19 2003.09.16 1\n",
      "2003.10.15 2003.07.20 2003.09.17 1\n",
      "2003.10.16 2003.07.21 2003.09.18 1\n",
      "2003.10.17 2003.07.22 2003.09.19 1\n",
      "2003.10.18 2003.07.23 2003.09.20 1\n",
      "2003.10.19 2003.07.24 2003.09.21 1\n",
      "2003.10.20 2003.07.25 2003.09.22 1\n",
      "2003.10.21 2003.07.26 2003.09.23 1\n",
      "2003.10.22 2003.07.27 2003.09.24 1\n",
      "2003.10.23 2003.07.28 2003.09.25 1\n",
      "2003.10.24 2003.07.29 2003.09.26 1\n",
      "2003.10.25 2003.07.30 2003.09.27 1\n",
      "2003.10.26 2003.07.31 2003.09.28 1\n",
      "2003.10.27 2003.08.01 2003.09.29 1\n",
      "2003.10.28 2003.08.02 2003.09.30 1\n",
      "2003.10.29 2003.08.03 2003.10.01 1\n",
      "2003.10.30 2003.08.04 2003.10.02 1\n",
      "2003.10.31 2003.08.05 2003.10.03 1\n",
      "2003.11.01 2003.08.06 2003.10.04 1\n",
      "2003.11.02 2003.08.07 2003.10.05 1\n",
      "2003.11.03 2003.08.08 2003.10.06 1\n",
      "2003.11.04 2003.08.09 2003.10.07 1\n",
      "2003.11.05 2003.08.10 2003.10.08 1\n",
      "2003.11.06 2003.08.11 2003.10.09 1\n",
      "2003.11.07 2003.08.12 2003.10.10 1\n",
      "2003.11.08 2003.08.13 2003.10.11 1\n",
      "2003.11.09 2003.08.14 2003.10.12 1\n",
      "2003.11.10 2003.08.15 2003.10.13 1\n",
      "2003.11.11 2003.08.16 2003.10.14 1\n",
      "2003.11.12 2003.08.17 2003.10.15 1\n",
      "2003.11.13 2003.08.18 2003.10.16 1\n",
      "2003.11.14 2003.08.19 2003.10.17 1\n",
      "2003.11.15 2003.08.20 2003.10.18 1\n",
      "2003.11.16 2003.08.21 2003.10.19 1\n",
      "2003.11.17 2003.08.22 2003.10.20 1\n",
      "2003.11.18 2003.08.23 2003.10.21 1\n",
      "2003.11.19 2003.08.24 2003.10.22 1\n",
      "2003.11.20 2003.08.25 2003.10.23 1\n",
      "2003.11.21 2003.08.26 2003.10.24 1\n",
      "2003.11.22 2003.08.27 2003.10.25 1\n",
      "2003.11.23 2003.08.28 2003.10.26 1\n",
      "2003.11.24 2003.08.29 2003.10.27 1\n",
      "2003.11.25 2003.08.30 2003.10.28 1\n",
      "2003.11.26 2003.08.31 2003.10.29 1\n",
      "2003.11.27 2003.09.01 2003.10.30 1\n",
      "2003.11.28 2003.09.02 2003.10.31 1\n",
      "2003.11.29 2003.09.03 2003.11.01 1\n",
      "2003.11.30 2003.09.04 2003.11.02 1\n",
      "2003.12.01 2003.09.05 2003.11.03 1\n",
      "2003.12.02 2003.09.06 2003.11.04 1\n",
      "2003.12.03 2003.09.07 2003.11.05 1\n",
      "2003.12.04 2003.09.08 2003.11.06 1\n",
      "2003.12.05 2003.09.09 2003.11.07 1\n",
      "2003.12.06 2003.09.10 2003.11.08 1\n",
      "2003.12.07 2003.09.11 2003.11.09 1\n",
      "2003.12.08 2003.09.12 2003.11.10 1\n",
      "2003.12.09 2003.09.13 2003.11.11 1\n",
      "2003.12.10 2003.09.14 2003.11.12 1\n",
      "2004.10.08 2004.07.13 2004.09.10 0\n",
      "2004.10.09 2004.07.14 2004.09.11 1\n",
      "2004.10.10 2004.07.15 2004.09.12 1\n",
      "2004.10.11 2004.07.16 2004.09.13 1\n",
      "2004.10.12 2004.07.17 2004.09.14 1\n",
      "2004.10.13 2004.07.18 2004.09.15 1\n",
      "2004.10.14 2004.07.19 2004.09.16 1\n",
      "2004.10.15 2004.07.20 2004.09.17 1\n",
      "2004.10.16 2004.07.21 2004.09.18 1\n",
      "2004.10.17 2004.07.22 2004.09.19 1\n",
      "2004.10.18 2004.07.23 2004.09.20 1\n",
      "2004.10.19 2004.07.24 2004.09.21 1\n",
      "2004.10.20 2004.07.25 2004.09.22 1\n",
      "2004.10.21 2004.07.26 2004.09.23 1\n",
      "2004.10.22 2004.07.27 2004.09.24 1\n",
      "2004.10.23 2004.07.28 2004.09.25 1\n",
      "2004.10.24 2004.07.29 2004.09.26 1\n",
      "2004.10.25 2004.07.30 2004.09.27 1\n",
      "2004.10.26 2004.07.31 2004.09.28 1\n",
      "2004.10.27 2004.08.01 2004.09.29 1\n",
      "2004.10.28 2004.08.02 2004.09.30 1\n",
      "2004.10.29 2004.08.03 2004.10.01 1\n",
      "2004.10.30 2004.08.04 2004.10.02 1\n",
      "2004.10.31 2004.08.05 2004.10.03 1\n",
      "2004.11.01 2004.08.06 2004.10.04 1\n",
      "2004.11.02 2004.08.07 2004.10.05 1\n",
      "2004.11.03 2004.08.08 2004.10.06 1\n",
      "2004.11.04 2004.08.09 2004.10.07 1\n",
      "2004.11.05 2004.08.10 2004.10.08 1\n",
      "2004.11.06 2004.08.11 2004.10.09 1\n",
      "2004.11.07 2004.08.12 2004.10.10 1\n",
      "2004.11.08 2004.08.13 2004.10.11 1\n",
      "2004.11.09 2004.08.14 2004.10.12 1\n",
      "2004.11.10 2004.08.15 2004.10.13 1\n",
      "2004.11.11 2004.08.16 2004.10.14 1\n",
      "2004.11.12 2004.08.17 2004.10.15 1\n",
      "2004.11.13 2004.08.18 2004.10.16 1\n",
      "2004.11.14 2004.08.19 2004.10.17 1\n",
      "2004.11.15 2004.08.20 2004.10.18 1\n",
      "2004.11.16 2004.08.21 2004.10.19 1\n",
      "2004.11.17 2004.08.22 2004.10.20 1\n",
      "2004.11.18 2004.08.23 2004.10.21 1\n",
      "2004.11.19 2004.08.24 2004.10.22 1\n",
      "2004.11.20 2004.08.25 2004.10.23 1\n",
      "2004.11.21 2004.08.26 2004.10.24 1\n",
      "2004.11.22 2004.08.27 2004.10.25 1\n",
      "2004.11.23 2004.08.28 2004.10.26 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2004.11.24 2004.08.29 2004.10.27 1\n",
      "2004.11.25 2004.08.30 2004.10.28 1\n",
      "2004.11.26 2004.08.31 2004.10.29 1\n",
      "2004.11.27 2004.09.01 2004.10.30 1\n",
      "2004.11.28 2004.09.02 2004.10.31 1\n",
      "2004.11.29 2004.09.03 2004.11.01 1\n",
      "2004.11.30 2004.09.04 2004.11.02 1\n",
      "2004.12.01 2004.09.05 2004.11.03 1\n",
      "2004.12.02 2004.09.06 2004.11.04 1\n",
      "2004.12.03 2004.09.07 2004.11.05 1\n",
      "2004.12.04 2004.09.08 2004.11.06 1\n",
      "2004.12.05 2004.09.09 2004.11.07 1\n",
      "2004.12.06 2004.09.10 2004.11.08 1\n",
      "2004.12.07 2004.09.11 2004.11.09 1\n",
      "2004.12.08 2004.09.12 2004.11.10 1\n",
      "2004.12.09 2004.09.13 2004.11.11 1\n",
      "2004.12.10 2004.09.14 2004.11.12 1\n",
      "2005.10.08 2005.07.13 2005.09.10 0\n",
      "2005.10.09 2005.07.14 2005.09.11 1\n",
      "2005.10.10 2005.07.15 2005.09.12 1\n",
      "2005.10.11 2005.07.16 2005.09.13 1\n",
      "2005.10.12 2005.07.17 2005.09.14 1\n",
      "2005.10.13 2005.07.18 2005.09.15 1\n",
      "2005.10.14 2005.07.19 2005.09.16 1\n",
      "2005.10.15 2005.07.20 2005.09.17 1\n",
      "2005.10.16 2005.07.21 2005.09.18 1\n",
      "2005.10.17 2005.07.22 2005.09.19 1\n",
      "2005.10.18 2005.07.23 2005.09.20 1\n",
      "2005.10.19 2005.07.24 2005.09.21 1\n",
      "2005.10.20 2005.07.25 2005.09.22 1\n",
      "2005.10.21 2005.07.26 2005.09.23 1\n",
      "2005.10.22 2005.07.27 2005.09.24 1\n",
      "2005.10.23 2005.07.28 2005.09.25 1\n",
      "2005.10.24 2005.07.29 2005.09.26 1\n",
      "2005.10.25 2005.07.30 2005.09.27 1\n",
      "2005.10.26 2005.07.31 2005.09.28 1\n",
      "2005.10.27 2005.08.01 2005.09.29 1\n",
      "2005.10.28 2005.08.02 2005.09.30 1\n",
      "2005.10.29 2005.08.03 2005.10.01 1\n",
      "2005.10.30 2005.08.04 2005.10.02 1\n",
      "2005.10.31 2005.08.05 2005.10.03 1\n",
      "2005.11.01 2005.08.06 2005.10.04 1\n",
      "2005.11.02 2005.08.07 2005.10.05 1\n",
      "2005.11.03 2005.08.08 2005.10.06 1\n",
      "2005.11.04 2005.08.09 2005.10.07 1\n",
      "2005.11.05 2005.08.10 2005.10.08 1\n",
      "2005.11.06 2005.08.11 2005.10.09 1\n",
      "2005.11.07 2005.08.12 2005.10.10 1\n",
      "2005.11.08 2005.08.13 2005.10.11 1\n",
      "2005.11.09 2005.08.14 2005.10.12 1\n",
      "2005.11.10 2005.08.15 2005.10.13 1\n",
      "2005.11.11 2005.08.16 2005.10.14 1\n",
      "2005.11.12 2005.08.17 2005.10.15 1\n",
      "2005.11.13 2005.08.18 2005.10.16 1\n",
      "2005.11.14 2005.08.19 2005.10.17 1\n",
      "2005.11.15 2005.08.20 2005.10.18 1\n",
      "2005.11.16 2005.08.21 2005.10.19 1\n",
      "2005.11.17 2005.08.22 2005.10.20 1\n",
      "2005.11.18 2005.08.23 2005.10.21 1\n",
      "2005.11.19 2005.08.24 2005.10.22 1\n",
      "2005.11.20 2005.08.25 2005.10.23 1\n",
      "2005.11.21 2005.08.26 2005.10.24 1\n",
      "2005.11.22 2005.08.27 2005.10.25 1\n",
      "2005.11.23 2005.08.28 2005.10.26 1\n",
      "2005.11.24 2005.08.29 2005.10.27 1\n",
      "2005.11.25 2005.08.30 2005.10.28 1\n",
      "2005.11.26 2005.08.31 2005.10.29 1\n",
      "2005.11.27 2005.09.01 2005.10.30 1\n",
      "2005.11.28 2005.09.02 2005.10.31 1\n",
      "2005.11.29 2005.09.03 2005.11.01 1\n",
      "2005.11.30 2005.09.04 2005.11.02 1\n",
      "2005.12.01 2005.09.05 2005.11.03 1\n",
      "2005.12.02 2005.09.06 2005.11.04 1\n",
      "2005.12.03 2005.09.07 2005.11.05 1\n",
      "2005.12.04 2005.09.08 2005.11.06 1\n",
      "2005.12.05 2005.09.09 2005.11.07 1\n",
      "2005.12.06 2005.09.10 2005.11.08 1\n",
      "2005.12.07 2005.09.11 2005.11.09 1\n",
      "2005.12.08 2005.09.12 2005.11.10 1\n",
      "2005.12.09 2005.09.13 2005.11.11 1\n",
      "2005.12.10 2005.09.14 2005.11.12 1\n",
      "2006.10.08 2006.07.13 2006.09.10 0\n",
      "2006.10.09 2006.07.14 2006.09.11 1\n",
      "2006.10.10 2006.07.15 2006.09.12 1\n",
      "2006.10.11 2006.07.16 2006.09.13 1\n",
      "2006.10.12 2006.07.17 2006.09.14 1\n",
      "2006.10.13 2006.07.18 2006.09.15 1\n",
      "2006.10.14 2006.07.19 2006.09.16 1\n",
      "2006.10.15 2006.07.20 2006.09.17 1\n",
      "2006.10.16 2006.07.21 2006.09.18 1\n",
      "2006.10.17 2006.07.22 2006.09.19 1\n",
      "2006.10.18 2006.07.23 2006.09.20 1\n",
      "2006.10.19 2006.07.24 2006.09.21 1\n",
      "2006.10.20 2006.07.25 2006.09.22 1\n",
      "2006.10.21 2006.07.26 2006.09.23 1\n",
      "2006.10.22 2006.07.27 2006.09.24 1\n",
      "2006.10.23 2006.07.28 2006.09.25 1\n",
      "2006.10.24 2006.07.29 2006.09.26 1\n",
      "2006.10.25 2006.07.30 2006.09.27 1\n",
      "2006.10.26 2006.07.31 2006.09.28 1\n",
      "2006.10.27 2006.08.01 2006.09.29 1\n",
      "2006.10.28 2006.08.02 2006.09.30 1\n",
      "2006.10.29 2006.08.03 2006.10.01 1\n",
      "2006.10.30 2006.08.04 2006.10.02 1\n",
      "2006.10.31 2006.08.05 2006.10.03 1\n",
      "2006.11.01 2006.08.06 2006.10.04 1\n",
      "2006.11.02 2006.08.07 2006.10.05 1\n",
      "2006.11.03 2006.08.08 2006.10.06 1\n",
      "2006.11.04 2006.08.09 2006.10.07 1\n",
      "2006.11.05 2006.08.10 2006.10.08 1\n",
      "2006.11.06 2006.08.11 2006.10.09 1\n",
      "2006.11.07 2006.08.12 2006.10.10 1\n",
      "2006.11.08 2006.08.13 2006.10.11 1\n",
      "2006.11.09 2006.08.14 2006.10.12 1\n",
      "2006.11.10 2006.08.15 2006.10.13 1\n",
      "2006.11.11 2006.08.16 2006.10.14 1\n",
      "2006.11.12 2006.08.17 2006.10.15 1\n",
      "2006.11.13 2006.08.18 2006.10.16 1\n",
      "2006.11.14 2006.08.19 2006.10.17 1\n",
      "2006.11.15 2006.08.20 2006.10.18 1\n",
      "2006.11.16 2006.08.21 2006.10.19 1\n",
      "2006.11.17 2006.08.22 2006.10.20 1\n",
      "2006.11.18 2006.08.23 2006.10.21 1\n",
      "2006.11.19 2006.08.24 2006.10.22 1\n",
      "2006.11.20 2006.08.25 2006.10.23 1\n",
      "2006.11.21 2006.08.26 2006.10.24 1\n",
      "2006.11.22 2006.08.27 2006.10.25 1\n",
      "2006.11.23 2006.08.28 2006.10.26 1\n",
      "2006.11.24 2006.08.29 2006.10.27 1\n",
      "2006.11.25 2006.08.30 2006.10.28 1\n",
      "2006.11.26 2006.08.31 2006.10.29 1\n",
      "2006.11.27 2006.09.01 2006.10.30 1\n",
      "2006.11.28 2006.09.02 2006.10.31 1\n",
      "2006.11.29 2006.09.03 2006.11.01 1\n",
      "2006.11.30 2006.09.04 2006.11.02 1\n",
      "2006.12.01 2006.09.05 2006.11.03 1\n",
      "2006.12.02 2006.09.06 2006.11.04 1\n",
      "2006.12.03 2006.09.07 2006.11.05 1\n",
      "2006.12.04 2006.09.08 2006.11.06 1\n",
      "2006.12.05 2006.09.09 2006.11.07 1\n",
      "2006.12.06 2006.09.10 2006.11.08 1\n",
      "2006.12.07 2006.09.11 2006.11.09 1\n",
      "2006.12.08 2006.09.12 2006.11.10 1\n",
      "2006.12.09 2006.09.13 2006.11.11 1\n",
      "2006.12.10 2006.09.14 2006.11.12 1\n",
      "2007.10.08 2007.07.13 2007.09.10 0\n",
      "2007.10.09 2007.07.14 2007.09.11 1\n",
      "2007.10.10 2007.07.15 2007.09.12 1\n",
      "2007.10.11 2007.07.16 2007.09.13 1\n",
      "2007.10.12 2007.07.17 2007.09.14 1\n",
      "2007.10.13 2007.07.18 2007.09.15 1\n",
      "2007.10.14 2007.07.19 2007.09.16 1\n",
      "2007.10.15 2007.07.20 2007.09.17 1\n",
      "2007.10.16 2007.07.21 2007.09.18 1\n",
      "2007.10.17 2007.07.22 2007.09.19 1\n",
      "2007.10.18 2007.07.23 2007.09.20 1\n",
      "2007.10.19 2007.07.24 2007.09.21 1\n",
      "2007.10.20 2007.07.25 2007.09.22 1\n",
      "2007.10.21 2007.07.26 2007.09.23 1\n",
      "2007.10.22 2007.07.27 2007.09.24 1\n",
      "2007.10.23 2007.07.28 2007.09.25 1\n",
      "2007.10.24 2007.07.29 2007.09.26 1\n",
      "2007.10.25 2007.07.30 2007.09.27 1\n",
      "2007.10.26 2007.07.31 2007.09.28 1\n",
      "2007.10.27 2007.08.01 2007.09.29 1\n",
      "2007.10.28 2007.08.02 2007.09.30 1\n",
      "2007.10.29 2007.08.03 2007.10.01 1\n",
      "2007.10.30 2007.08.04 2007.10.02 1\n",
      "2007.10.31 2007.08.05 2007.10.03 1\n",
      "2007.11.01 2007.08.06 2007.10.04 1\n",
      "2007.11.02 2007.08.07 2007.10.05 1\n",
      "2007.11.03 2007.08.08 2007.10.06 1\n",
      "2007.11.04 2007.08.09 2007.10.07 1\n",
      "2007.11.05 2007.08.10 2007.10.08 1\n",
      "2007.11.06 2007.08.11 2007.10.09 1\n",
      "2007.11.07 2007.08.12 2007.10.10 1\n",
      "2007.11.08 2007.08.13 2007.10.11 1\n",
      "2007.11.09 2007.08.14 2007.10.12 1\n",
      "2007.11.10 2007.08.15 2007.10.13 1\n",
      "2007.11.11 2007.08.16 2007.10.14 1\n",
      "2007.11.12 2007.08.17 2007.10.15 1\n",
      "2007.11.13 2007.08.18 2007.10.16 1\n",
      "2007.11.14 2007.08.19 2007.10.17 1\n",
      "2007.11.15 2007.08.20 2007.10.18 1\n",
      "2007.11.16 2007.08.21 2007.10.19 1\n",
      "2007.11.17 2007.08.22 2007.10.20 1\n",
      "2007.11.18 2007.08.23 2007.10.21 1\n",
      "2007.11.19 2007.08.24 2007.10.22 1\n",
      "2007.11.20 2007.08.25 2007.10.23 1\n",
      "2007.11.21 2007.08.26 2007.10.24 1\n",
      "2007.11.22 2007.08.27 2007.10.25 1\n",
      "2007.11.23 2007.08.28 2007.10.26 1\n",
      "2007.11.24 2007.08.29 2007.10.27 1\n",
      "2007.11.25 2007.08.30 2007.10.28 1\n",
      "2007.11.26 2007.08.31 2007.10.29 1\n",
      "2007.11.27 2007.09.01 2007.10.30 1\n",
      "2007.11.28 2007.09.02 2007.10.31 1\n",
      "2007.11.29 2007.09.03 2007.11.01 1\n",
      "2007.11.30 2007.09.04 2007.11.02 1\n",
      "2007.12.01 2007.09.05 2007.11.03 1\n",
      "2007.12.02 2007.09.06 2007.11.04 1\n",
      "2007.12.03 2007.09.07 2007.11.05 1\n",
      "2007.12.04 2007.09.08 2007.11.06 1\n",
      "2007.12.05 2007.09.09 2007.11.07 1\n",
      "2007.12.06 2007.09.10 2007.11.08 1\n",
      "2007.12.07 2007.09.11 2007.11.09 1\n",
      "2007.12.08 2007.09.12 2007.11.10 1\n",
      "2007.12.09 2007.09.13 2007.11.11 1\n",
      "2007.12.10 2007.09.14 2007.11.12 1\n",
      "2008.10.08 2008.07.13 2008.09.10 0\n",
      "2008.10.09 2008.07.14 2008.09.11 1\n",
      "2008.10.10 2008.07.15 2008.09.12 1\n",
      "2008.10.11 2008.07.16 2008.09.13 1\n",
      "2008.10.12 2008.07.17 2008.09.14 1\n",
      "2008.10.13 2008.07.18 2008.09.15 1\n",
      "2008.10.14 2008.07.19 2008.09.16 1\n",
      "2008.10.15 2008.07.20 2008.09.17 1\n",
      "2008.10.16 2008.07.21 2008.09.18 1\n",
      "2008.10.17 2008.07.22 2008.09.19 1\n",
      "2008.10.18 2008.07.23 2008.09.20 1\n",
      "2008.10.19 2008.07.24 2008.09.21 1\n",
      "2008.10.20 2008.07.25 2008.09.22 1\n",
      "2008.10.21 2008.07.26 2008.09.23 1\n",
      "2008.10.22 2008.07.27 2008.09.24 1\n",
      "2008.10.23 2008.07.28 2008.09.25 1\n",
      "2008.10.24 2008.07.29 2008.09.26 1\n",
      "2008.10.25 2008.07.30 2008.09.27 1\n",
      "2008.10.26 2008.07.31 2008.09.28 1\n",
      "2008.10.27 2008.08.01 2008.09.29 1\n",
      "2008.10.28 2008.08.02 2008.09.30 1\n",
      "2008.10.29 2008.08.03 2008.10.01 1\n",
      "2008.10.30 2008.08.04 2008.10.02 1\n",
      "2008.10.31 2008.08.05 2008.10.03 1\n",
      "2008.11.01 2008.08.06 2008.10.04 1\n",
      "2008.11.02 2008.08.07 2008.10.05 1\n",
      "2008.11.03 2008.08.08 2008.10.06 1\n",
      "2008.11.04 2008.08.09 2008.10.07 1\n",
      "2008.11.05 2008.08.10 2008.10.08 1\n",
      "2008.11.06 2008.08.11 2008.10.09 1\n",
      "2008.11.07 2008.08.12 2008.10.10 1\n",
      "2008.11.08 2008.08.13 2008.10.11 1\n",
      "2008.11.09 2008.08.14 2008.10.12 1\n",
      "2008.11.10 2008.08.15 2008.10.13 1\n",
      "2008.11.11 2008.08.16 2008.10.14 1\n",
      "2008.11.12 2008.08.17 2008.10.15 1\n",
      "2008.11.13 2008.08.18 2008.10.16 1\n",
      "2008.11.14 2008.08.19 2008.10.17 1\n",
      "2008.11.15 2008.08.20 2008.10.18 1\n",
      "2008.11.16 2008.08.21 2008.10.19 1\n",
      "2008.11.17 2008.08.22 2008.10.20 1\n",
      "2008.11.18 2008.08.23 2008.10.21 1\n",
      "2008.11.19 2008.08.24 2008.10.22 1\n",
      "2008.11.20 2008.08.25 2008.10.23 1\n",
      "2008.11.21 2008.08.26 2008.10.24 1\n",
      "2008.11.22 2008.08.27 2008.10.25 1\n",
      "2008.11.23 2008.08.28 2008.10.26 1\n",
      "2008.11.24 2008.08.29 2008.10.27 1\n",
      "2008.11.25 2008.08.30 2008.10.28 1\n",
      "2008.11.26 2008.08.31 2008.10.29 1\n",
      "2008.11.27 2008.09.01 2008.10.30 1\n",
      "2008.11.28 2008.09.02 2008.10.31 1\n",
      "2008.11.29 2008.09.03 2008.11.01 1\n",
      "2008.11.30 2008.09.04 2008.11.02 1\n",
      "2008.12.01 2008.09.05 2008.11.03 1\n",
      "2008.12.02 2008.09.06 2008.11.04 1\n",
      "2008.12.03 2008.09.07 2008.11.05 1\n",
      "2008.12.04 2008.09.08 2008.11.06 1\n",
      "2008.12.05 2008.09.09 2008.11.07 1\n",
      "2008.12.06 2008.09.10 2008.11.08 1\n",
      "2008.12.07 2008.09.11 2008.11.09 1\n",
      "2008.12.08 2008.09.12 2008.11.10 1\n",
      "2008.12.09 2008.09.13 2008.11.11 1\n",
      "2008.12.10 2008.09.14 2008.11.12 1\n",
      "2009.10.08 2009.07.13 2009.09.10 0\n",
      "2009.10.09 2009.07.14 2009.09.11 1\n",
      "2009.10.10 2009.07.15 2009.09.12 1\n",
      "2009.10.11 2009.07.16 2009.09.13 1\n",
      "2009.10.12 2009.07.17 2009.09.14 1\n",
      "2009.10.13 2009.07.18 2009.09.15 1\n",
      "2009.10.14 2009.07.19 2009.09.16 1\n",
      "2009.10.15 2009.07.20 2009.09.17 1\n",
      "2009.10.16 2009.07.21 2009.09.18 1\n",
      "2009.10.17 2009.07.22 2009.09.19 1\n",
      "2009.10.18 2009.07.23 2009.09.20 1\n",
      "2009.10.19 2009.07.24 2009.09.21 1\n",
      "2009.10.20 2009.07.25 2009.09.22 1\n",
      "2009.10.21 2009.07.26 2009.09.23 1\n",
      "2009.10.22 2009.07.27 2009.09.24 1\n",
      "2009.10.23 2009.07.28 2009.09.25 1\n",
      "2009.10.24 2009.07.29 2009.09.26 1\n",
      "2009.10.25 2009.07.30 2009.09.27 1\n",
      "2009.10.26 2009.07.31 2009.09.28 1\n",
      "2009.10.27 2009.08.01 2009.09.29 1\n",
      "2009.10.28 2009.08.02 2009.09.30 1\n",
      "2009.10.29 2009.08.03 2009.10.01 1\n",
      "2009.10.30 2009.08.04 2009.10.02 1\n",
      "2009.10.31 2009.08.05 2009.10.03 1\n",
      "2009.11.01 2009.08.06 2009.10.04 1\n",
      "2009.11.02 2009.08.07 2009.10.05 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009.11.03 2009.08.08 2009.10.06 1\n",
      "2009.11.04 2009.08.09 2009.10.07 1\n",
      "2009.11.05 2009.08.10 2009.10.08 1\n",
      "2009.11.06 2009.08.11 2009.10.09 1\n",
      "2009.11.07 2009.08.12 2009.10.10 1\n",
      "2009.11.08 2009.08.13 2009.10.11 1\n",
      "2009.11.09 2009.08.14 2009.10.12 1\n",
      "2009.11.10 2009.08.15 2009.10.13 1\n",
      "2009.11.11 2009.08.16 2009.10.14 1\n",
      "2009.11.12 2009.08.17 2009.10.15 1\n",
      "2009.11.13 2009.08.18 2009.10.16 1\n",
      "2009.11.14 2009.08.19 2009.10.17 1\n",
      "2009.11.15 2009.08.20 2009.10.18 1\n",
      "2009.11.16 2009.08.21 2009.10.19 1\n",
      "2009.11.17 2009.08.22 2009.10.20 1\n",
      "2009.11.18 2009.08.23 2009.10.21 1\n",
      "2009.11.19 2009.08.24 2009.10.22 1\n",
      "2009.11.20 2009.08.25 2009.10.23 1\n",
      "2009.11.21 2009.08.26 2009.10.24 1\n",
      "2009.11.22 2009.08.27 2009.10.25 1\n",
      "2009.11.23 2009.08.28 2009.10.26 1\n",
      "2009.11.24 2009.08.29 2009.10.27 1\n",
      "2009.11.25 2009.08.30 2009.10.28 1\n",
      "2009.11.26 2009.08.31 2009.10.29 1\n",
      "2009.11.27 2009.09.01 2009.10.30 1\n",
      "2009.11.28 2009.09.02 2009.10.31 1\n",
      "2009.11.29 2009.09.03 2009.11.01 1\n",
      "2009.11.30 2009.09.04 2009.11.02 1\n",
      "2009.12.01 2009.09.05 2009.11.03 1\n",
      "2009.12.02 2009.09.06 2009.11.04 1\n",
      "2009.12.03 2009.09.07 2009.11.05 1\n",
      "2009.12.04 2009.09.08 2009.11.06 1\n",
      "2009.12.05 2009.09.09 2009.11.07 1\n",
      "2009.12.06 2009.09.10 2009.11.08 1\n",
      "2009.12.07 2009.09.11 2009.11.09 1\n",
      "2009.12.08 2009.09.12 2009.11.10 1\n",
      "2009.12.09 2009.09.13 2009.11.11 1\n",
      "2009.12.10 2009.09.14 2009.11.12 1\n",
      "2010.10.08 2010.07.13 2010.09.10 0\n",
      "2010.10.09 2010.07.14 2010.09.11 1\n",
      "2010.10.10 2010.07.15 2010.09.12 1\n",
      "2010.10.11 2010.07.16 2010.09.13 1\n",
      "2010.10.12 2010.07.17 2010.09.14 1\n",
      "2010.10.13 2010.07.18 2010.09.15 1\n",
      "2010.10.14 2010.07.19 2010.09.16 1\n",
      "2010.10.15 2010.07.20 2010.09.17 1\n",
      "2010.10.16 2010.07.21 2010.09.18 1\n",
      "2010.10.17 2010.07.22 2010.09.19 1\n",
      "2010.10.18 2010.07.23 2010.09.20 1\n",
      "2010.10.19 2010.07.24 2010.09.21 1\n",
      "2010.10.20 2010.07.25 2010.09.22 1\n",
      "2010.10.21 2010.07.26 2010.09.23 1\n",
      "2010.10.22 2010.07.27 2010.09.24 1\n",
      "2010.10.23 2010.07.28 2010.09.25 1\n",
      "2010.10.24 2010.07.29 2010.09.26 1\n",
      "2010.10.25 2010.07.30 2010.09.27 1\n",
      "2010.10.26 2010.07.31 2010.09.28 1\n",
      "2010.10.27 2010.08.01 2010.09.29 1\n",
      "2010.10.28 2010.08.02 2010.09.30 1\n",
      "2010.10.29 2010.08.03 2010.10.01 1\n",
      "2010.10.30 2010.08.04 2010.10.02 1\n",
      "2010.10.31 2010.08.05 2010.10.03 1\n",
      "2010.11.01 2010.08.06 2010.10.04 1\n",
      "2010.11.02 2010.08.07 2010.10.05 1\n",
      "2010.11.03 2010.08.08 2010.10.06 1\n",
      "2010.11.04 2010.08.09 2010.10.07 1\n",
      "2010.11.05 2010.08.10 2010.10.08 1\n",
      "2010.11.06 2010.08.11 2010.10.09 1\n",
      "2010.11.07 2010.08.12 2010.10.10 1\n",
      "2010.11.08 2010.08.13 2010.10.11 1\n",
      "2010.11.09 2010.08.14 2010.10.12 1\n",
      "2010.11.10 2010.08.15 2010.10.13 1\n",
      "2010.11.11 2010.08.16 2010.10.14 1\n",
      "2010.11.12 2010.08.17 2010.10.15 1\n",
      "2010.11.13 2010.08.18 2010.10.16 1\n",
      "2010.11.14 2010.08.19 2010.10.17 1\n",
      "2010.11.15 2010.08.20 2010.10.18 1\n",
      "2010.11.16 2010.08.21 2010.10.19 1\n",
      "2010.11.17 2010.08.22 2010.10.20 1\n",
      "2010.11.18 2010.08.23 2010.10.21 1\n",
      "2010.11.19 2010.08.24 2010.10.22 1\n",
      "2010.11.20 2010.08.25 2010.10.23 1\n",
      "2010.11.21 2010.08.26 2010.10.24 1\n",
      "2010.11.22 2010.08.27 2010.10.25 1\n",
      "2010.11.23 2010.08.28 2010.10.26 1\n",
      "2010.11.24 2010.08.29 2010.10.27 1\n",
      "2010.11.25 2010.08.30 2010.10.28 1\n",
      "2010.11.26 2010.08.31 2010.10.29 1\n",
      "2010.11.27 2010.09.01 2010.10.30 1\n",
      "2010.11.28 2010.09.02 2010.10.31 1\n",
      "2010.11.29 2010.09.03 2010.11.01 1\n",
      "2010.11.30 2010.09.04 2010.11.02 1\n",
      "2010.12.01 2010.09.05 2010.11.03 1\n",
      "2010.12.02 2010.09.06 2010.11.04 1\n",
      "2010.12.03 2010.09.07 2010.11.05 1\n",
      "2010.12.04 2010.09.08 2010.11.06 1\n",
      "2010.12.05 2010.09.09 2010.11.07 1\n",
      "2010.12.06 2010.09.10 2010.11.08 1\n",
      "2010.12.07 2010.09.11 2010.11.09 1\n",
      "2010.12.08 2010.09.12 2010.11.10 1\n",
      "2010.12.09 2010.09.13 2010.11.11 1\n",
      "2010.12.10 2010.09.14 2010.11.12 1\n"
     ]
    }
   ],
   "source": [
    "pc_xr = xr.concat([enso,iod,rmm1,rmm2,t2m,evap],\"feature\")\n",
    "\n",
    "# Run the function\n",
    "s_target_date='08-10-1981'\n",
    "e_target_date='10-12-2010' # end of year - approximatelly 14 days\n",
    "\n",
    "# if eof is 15 days centered then rw_1=7 # If eof is 15 days NOT-centered then rw_1=15\n",
    "rw_1 = 7 # half eof smoothing\n",
    "lead_time = 14 # days until valid_time starts\n",
    "rw = 15 # data full smoothing value (gets half in function), because the data are centered, otherwise set to 0\n",
    "\n",
    "# LSTM timestep\n",
    "ntimestep = 60 # lags to consider for the lstm\n",
    "target_len = len(tp_target) # len(tp_target)\n",
    "\n",
    "drop_OND_years = [1970, 1971, 1972, 1973, 1974] # fake years to drop because we dont want to do any drop currently\n",
    "predictor_array=sel_train_data_lead(pc_xr, target_len, s_target_date, e_target_date,\n",
    "                rw_1, lead_time, rw, ntimestep, drop_OND_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2 {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt;\n",
       "Dimensions:  (time: 1920, feature: 6, lag: 60)\n",
       "Coordinates:\n",
       "  * lag      (lag) int64 0 1 2 3 4 5 6 7 8 9 ... 50 51 52 53 54 55 56 57 58 59\n",
       "  * feature  (feature) int64 0 1 2 3 4 5\n",
       "  * time     (time) datetime64[ns] 1981-10-08 1981-10-09 ... 2010-12-10\n",
       "Data variables:\n",
       "    Index    (time, feature, lag) float32 -0.3146 -0.3216 ... -0.4997 -0.5018</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-f510321d-b3ff-48bd-9e01-f90e3785c664' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-f510321d-b3ff-48bd-9e01-f90e3785c664' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span class='xr-has-index'>time</span>: 1920</li><li><span class='xr-has-index'>feature</span>: 6</li><li><span class='xr-has-index'>lag</span>: 60</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-3bd169f9-6f47-4a59-8509-d2d6dea2afb3' class='xr-section-summary-in' type='checkbox'  checked><label for='section-3bd169f9-6f47-4a59-8509-d2d6dea2afb3' class='xr-section-summary' >Coordinates: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>lag</span></div><div class='xr-var-dims'>(lag)</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>0 1 2 3 4 5 6 ... 54 55 56 57 58 59</div><input id='attrs-63d16db6-939a-43e9-b010-b32bd300a45b' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-63d16db6-939a-43e9-b010-b32bd300a45b' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-36bc8183-0f88-4298-87e9-2e223c8c8ed8' class='xr-var-data-in' type='checkbox'><label for='data-36bc8183-0f88-4298-87e9-2e223c8c8ed8' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
       "       54, 55, 56, 57, 58, 59])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>feature</span></div><div class='xr-var-dims'>(feature)</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>0 1 2 3 4 5</div><input id='attrs-9956d73a-2e4c-4d4b-a31d-dfccbfd7768b' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-9956d73a-2e4c-4d4b-a31d-dfccbfd7768b' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-3fbfd350-8727-4893-b952-969eb686918c' class='xr-var-data-in' type='checkbox'><label for='data-3fbfd350-8727-4893-b952-969eb686918c' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([0, 1, 2, 3, 4, 5])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>time</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>1981-10-08 ... 2010-12-10</div><input id='attrs-23bd56b9-7b21-4d18-9ef6-f94417e0d3da' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-23bd56b9-7b21-4d18-9ef6-f94417e0d3da' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-28e43f0f-394c-4d7b-adf3-fd8450b3bc7e' class='xr-var-data-in' type='checkbox'><label for='data-28e43f0f-394c-4d7b-adf3-fd8450b3bc7e' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([&#x27;1981-10-08T00:00:00.000000000&#x27;, &#x27;1981-10-09T00:00:00.000000000&#x27;,\n",
       "       &#x27;1981-10-10T00:00:00.000000000&#x27;, ..., &#x27;2010-12-08T00:00:00.000000000&#x27;,\n",
       "       &#x27;2010-12-09T00:00:00.000000000&#x27;, &#x27;2010-12-10T00:00:00.000000000&#x27;],\n",
       "      dtype=&#x27;datetime64[ns]&#x27;)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-b08ed87d-938c-4a21-86d0-086e6c9f11a9' class='xr-section-summary-in' type='checkbox'  checked><label for='section-b08ed87d-938c-4a21-86d0-086e6c9f11a9' class='xr-section-summary' >Data variables: <span>(1)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>Index</span></div><div class='xr-var-dims'>(time, feature, lag)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>-0.3146 -0.3216 ... -0.4997 -0.5018</div><input id='attrs-1361398c-7b56-43a0-ad65-21b9e6e72fcd' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-1361398c-7b56-43a0-ad65-21b9e6e72fcd' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-a9caec88-79ef-4c7f-bac1-96ea4d562b99' class='xr-var-data-in' type='checkbox'><label for='data-a9caec88-79ef-4c7f-bac1-96ea4d562b99' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([[[-0.31456128, -0.32162827, -0.33145428, ..., -0.30425572,\n",
       "         -0.30746108, -0.31270927],\n",
       "        [-0.48985755, -0.52880895, -0.56364214, ..., -0.33921638,\n",
       "         -0.36307105, -0.38509777],\n",
       "        [ 0.62628627,  0.50971484,  0.3737148 , ..., -0.3258567 ,\n",
       "         -0.3772853 , -0.4147139 ],\n",
       "        [-1.0228574 , -0.99000025, -0.93114305, ..., -1.3359998 ,\n",
       "         -1.1784284 , -1.0577141 ],\n",
       "        [-0.23331736, -0.24289434, -0.27533266, ..., -0.53332275,\n",
       "         -0.53702676, -0.5404398 ],\n",
       "        [ 0.05604304,  0.06632186,  0.05964773, ...,  0.13190036,\n",
       "          0.1308144 ,  0.12520641]],\n",
       "\n",
       "       [[-0.32162827, -0.33145428, -0.3395357 , ..., -0.30746108,\n",
       "         -0.31270927, -0.3193014 ],\n",
       "        [-0.52880895, -0.56364214, -0.5858073 , ..., -0.36307105,\n",
       "         -0.38509777, -0.4031227 ],\n",
       "        [ 0.50971484,  0.3737148 ,  0.2800005 , ..., -0.3772853 ,\n",
       "         -0.4147139 , -0.3694282 ],\n",
       "        [-0.99000025, -0.93114305, -0.9265716 , ..., -1.1784284 ,\n",
       "...\n",
       "          0.19829014, -0.09213845],\n",
       "        [-0.3845752 , -0.3355752 , -0.31971803, ...,  1.3787106 ,\n",
       "          1.4135678 ,  1.3977106 ],\n",
       "        [ 0.22163653,  0.18694174,  0.1460636 , ...,  0.4595007 ,\n",
       "          0.4631289 ,  0.4590784 ],\n",
       "        [ 0.11654925,  0.09926004,  0.09070601, ..., -0.48886877,\n",
       "         -0.49664137, -0.49967936]],\n",
       "\n",
       "       [[-1.3704528 , -1.3694731 , -1.3715869 , ..., -1.5419267 ,\n",
       "         -1.5404159 , -1.5405803 ],\n",
       "        [-1.0200422 , -1.0762665 , -1.1215518 , ..., -0.34045446,\n",
       "         -0.3225106 , -0.3150346 ],\n",
       "        [ 0.27986124,  0.15728982,  0.0460041 , ...,  0.19829014,\n",
       "         -0.09213845, -0.3392813 ],\n",
       "        [-0.3355752 , -0.31971803, -0.30657518, ...,  1.4135678 ,\n",
       "          1.3977106 ,  1.326282  ],\n",
       "        [ 0.18694174,  0.1460636 ,  0.09009745, ...,  0.4631289 ,\n",
       "          0.4590784 ,  0.451195  ],\n",
       "        [ 0.09926004,  0.09070601,  0.07303006, ..., -0.49664137,\n",
       "         -0.49967936, -0.5017516 ]]], dtype=float32)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-4c0b39ac-c42b-4028-8731-22e4dbf2c9b2' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-4c0b39ac-c42b-4028-8731-22e4dbf2c9b2' class='xr-section-summary'  title='Expand/collapse section'>Attributes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:  (time: 1920, feature: 6, lag: 60)\n",
       "Coordinates:\n",
       "  * lag      (lag) int64 0 1 2 3 4 5 6 7 8 9 ... 50 51 52 53 54 55 56 57 58 59\n",
       "  * feature  (feature) int64 0 1 2 3 4 5\n",
       "  * time     (time) datetime64[ns] 1981-10-08 1981-10-09 ... 2010-12-10\n",
       "Data variables:\n",
       "    Index    (time, feature, lag) float32 -0.3146 -0.3216 ... -0.4997 -0.5018"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f40bcc53ac0>]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(predictor_array.Index[100,5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2 {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt;\n",
       "Dimensions:  (feature: 6, time: 1920)\n",
       "Coordinates:\n",
       "  * time     (time) datetime64[ns] 1981-10-08 1981-10-09 ... 2010-12-10\n",
       "  * feature  (feature) int64 0 1 2 3 4 5\n",
       "Data variables:\n",
       "    Index    (feature, time) float32 -0.4799 -0.4852 -0.4902 ... -0.6017 -0.596</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-a7ee7f28-13a8-4191-bccd-db2140c4a948' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-a7ee7f28-13a8-4191-bccd-db2140c4a948' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span class='xr-has-index'>feature</span>: 6</li><li><span class='xr-has-index'>time</span>: 1920</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-cb04728f-b681-405b-a62b-228d4e411609' class='xr-section-summary-in' type='checkbox'  checked><label for='section-cb04728f-b681-405b-a62b-228d4e411609' class='xr-section-summary' >Coordinates: <span>(2)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>time</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>1981-10-08 ... 2010-12-10</div><input id='attrs-aa93a6a4-2275-4111-aad9-38152433e64b' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-aa93a6a4-2275-4111-aad9-38152433e64b' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-d799f339-e603-46b8-a63c-458378f064a5' class='xr-var-data-in' type='checkbox'><label for='data-d799f339-e603-46b8-a63c-458378f064a5' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([&#x27;1981-10-08T00:00:00.000000000&#x27;, &#x27;1981-10-09T00:00:00.000000000&#x27;,\n",
       "       &#x27;1981-10-10T00:00:00.000000000&#x27;, ..., &#x27;2010-12-08T00:00:00.000000000&#x27;,\n",
       "       &#x27;2010-12-09T00:00:00.000000000&#x27;, &#x27;2010-12-10T00:00:00.000000000&#x27;],\n",
       "      dtype=&#x27;datetime64[ns]&#x27;)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>feature</span></div><div class='xr-var-dims'>(feature)</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>0 1 2 3 4 5</div><input id='attrs-885f9138-8787-44d7-865d-d6649d093d4f' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-885f9138-8787-44d7-865d-d6649d093d4f' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-6343e124-be3f-4180-92f7-421aeb772025' class='xr-var-data-in' type='checkbox'><label for='data-6343e124-be3f-4180-92f7-421aeb772025' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([0, 1, 2, 3, 4, 5])</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-2c81f0b2-047f-4067-ac3d-3a657bb28ae6' class='xr-section-summary-in' type='checkbox'  checked><label for='section-2c81f0b2-047f-4067-ac3d-3a657bb28ae6' class='xr-section-summary' >Data variables: <span>(1)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>Index</span></div><div class='xr-var-dims'>(feature, time)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>-0.4799 -0.4852 ... -0.6017 -0.596</div><input id='attrs-f6168a4d-e1a1-42d6-929c-29c98df84808' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-f6168a4d-e1a1-42d6-929c-29c98df84808' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-8bc2aaa9-6c62-4105-b766-9529a53a2e2a' class='xr-var-data-in' type='checkbox'><label for='data-8bc2aaa9-6c62-4105-b766-9529a53a2e2a' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([[-0.479934  , -0.48521137, -0.49020872, ..., -1.615551  ,\n",
       "        -1.6127369 , -1.6066345 ],\n",
       "       [-0.23925146, -0.16631234, -0.09374923, ..., -0.51085377,\n",
       "        -0.4843791 , -0.46045852],\n",
       "       [-0.7818566 , -0.6735709 , -0.5288566 , ...,  1.2778615 ,\n",
       "         1.2368616 ,  1.1975758 ],\n",
       "       [-0.13099954, -0.34699953, -0.517571  , ...,  0.34942478,\n",
       "         0.53971046,  0.6784247 ],\n",
       "       [-0.0628693 , -0.05568287, -0.02374021, ...,  0.24959987,\n",
       "         0.24879286,  0.24186383],\n",
       "       [ 0.05672307,  0.0496854 ,  0.05478047, ..., -0.605598  ,\n",
       "        -0.60171914, -0.5960305 ]], dtype=float32)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-654048c6-229b-4cfa-a921-cb676cfc17d0' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-654048c6-229b-4cfa-a921-cb676cfc17d0' class='xr-section-summary'  title='Expand/collapse section'>Attributes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:  (feature: 6, time: 1920)\n",
       "Coordinates:\n",
       "  * time     (time) datetime64[ns] 1981-10-08 1981-10-09 ... 2010-12-10\n",
       "  * feature  (feature) int64 0 1 2 3 4 5\n",
       "Data variables:\n",
       "    Index    (feature, time) float32 -0.4799 -0.4852 -0.4902 ... -0.6017 -0.596"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc_xr_sel = xr.concat([pc_xr.sel(time=slice(f\"{yyyy}-10-{sm_sd}\",f\"{yyyy}-12-{em_ed}\")) for yyyy in tp11_terc.time.dt.year.to_index().unique()],\"time\")\n",
    "pc_xr_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9999999999999998, 0.0)\n",
      "(0.6591583315208596, 1.0755464972513812e-239)\n",
      "(-0.4258415548249008, 1.9769545941038392e-85)\n",
      "(-0.4816426065504341, 4.461297424701924e-112)\n",
      "(0.06949372703437452, 0.0023131975310272355)\n",
      "(0.5358031023854866, 3.9275291968165692e-143)\n",
      "(0.47023528130885794, 3.213350703863899e-106)\n",
      "(0.6591583315208596, 1.0755464972513812e-239)\n",
      "(1.0, 0.0)\n",
      "(-0.42477728756848, 5.71981707807475e-85)\n",
      "(-0.32397134271364936, 3.6825148986473964e-48)\n",
      "(0.07628638345400632, 0.0008216377818101097)\n",
      "(0.661691133719448, 3.6206396242101785e-242)\n",
      "(0.6156986772718859, 9.74199111435493e-201)\n",
      "(-0.4258415548249008, 1.9769545941038392e-85)\n",
      "(-0.42477728756848, 5.71981707807475e-85)\n",
      "(0.9999999999999997, 0.0)\n",
      "(0.18566745185301448, 2.371061833487173e-16)\n",
      "(0.1366212221982893, 1.846703188210949e-09)\n",
      "(-0.3238110067314348, 4.117986241569516e-48)\n",
      "(-0.4241906566017373, 1.0256347611501523e-84)\n",
      "(-0.4816426065504341, 4.461297424701924e-112)\n",
      "(-0.32397134271364936, 3.6825148986473964e-48)\n",
      "(0.18566745185301448, 2.371061833487173e-16)\n",
      "(1.0, 0.0)\n",
      "(-0.002712955094696285, 0.9054349665262654)\n",
      "(-0.37925271887516016, 1.0091677887567968e-66)\n",
      "(-0.3233744880903274, 5.580693793063648e-48)\n",
      "(0.06949372703437452, 0.0023131975310272355)\n",
      "(0.07628638345400632, 0.0008216377818101097)\n",
      "(0.1366212221982893, 1.846703188210949e-09)\n",
      "(-0.002712955094696285, 0.9054349665262654)\n",
      "(1.0, 0.0)\n",
      "(-0.1900891914878873, 4.443089247279337e-17)\n",
      "(-0.21734150622089843, 5.824071726125643e-22)\n",
      "(0.5358031023854866, 3.9275291968165692e-143)\n",
      "(0.661691133719448, 3.6206396242101785e-242)\n",
      "(-0.3238110067314348, 4.117986241569516e-48)\n",
      "(-0.37925271887516016, 1.0091677887567968e-66)\n",
      "(-0.1900891914878873, 4.443089247279337e-17)\n",
      "(1.0, 0.0)\n",
      "(0.7711987830641682, 0.0)\n",
      "(0.47023528130885794, 3.213350703863899e-106)\n",
      "(0.6156986772718859, 9.74199111435493e-201)\n",
      "(-0.4241906566017373, 1.0256347611501523e-84)\n",
      "(-0.3233744880903274, 5.580693793063648e-48)\n",
      "(-0.21734150622089843, 5.824071726125643e-22)\n",
      "(0.7711987830641682, 0.0)\n",
      "(1.0, 0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19293/3821366655.py:1: DeprecationWarning: Please use `pearsonr` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n",
      "  from scipy.stats.stats import pearsonr\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "pc_xr_corr = np.ndarray((7,7))\n",
    "pc_xr_pval = np.ndarray((6,6))\n",
    "for ii in range(7):\n",
    "    if ii < 6:\n",
    "        dat1 = pc_xr_sel.Index[ii,:]\n",
    "    if ii == 6:\n",
    "        dat1 = tp_anom.precip[:]\n",
    "    for jj in range(7):\n",
    "        if jj < 6:\n",
    "            dat2 = pc_xr_sel.Index[jj,:]\n",
    "        if jj == 6:\n",
    "            dat2 = tp_anom.precip[:]\n",
    "        #pc_xr_corr[ii,jj] = np.corrcoef(dat1,dat2)[0,1]\n",
    "        pc_xr_corr[ii,jj] = round(pearsonr(dat1, dat2)[0],2)\n",
    "        print(pearsonr(dat1, dat2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = [2,4,6,8,10,15,20,25,30,35,40]\n",
    "pc_xr_lagcorr = np.ndarray((6,len(lags)))\n",
    "for ii in range(6):\n",
    "    dummy = pc_xr_sel.Index[ii,:]\n",
    "    kk = 0\n",
    "    for jj in list(lags):\n",
    "        dat1 = dummy[0:len(dummy)-jj]\n",
    "        dat2 = dummy[jj:]\n",
    "        pc_xr_lagcorr[ii,kk] = round(pearsonr(dat1, dat2)[0],2)\n",
    "        kk = kk+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(pc_xr_corr,cmap='RdBu_r')\n",
    "im.set_clim(-1, 1)\n",
    "ax.grid(False)\n",
    "ax.xaxis.set(ticks=(0, 1, 2, 3, 4, 5, 6), ticklabels=('ENSO', 'IOD', 'RMM1', 'RMM2', 'T2m', 'E', 'Precip'))\n",
    "ax.yaxis.set(ticks=(0, 1, 2, 3, 4, 5, 6), ticklabels=('ENSO', 'IOD', 'RMM1', 'RMM2', 'T2m', 'E', 'Precip'))\n",
    "ax.set_ylim(6.5, -0.5)\n",
    "for i in range(7):\n",
    "    for j in range(7):\n",
    "        ax.text(j, i, pc_xr_corr[i, j], ha='center', va='center',\n",
    "                color='k')\n",
    "cbar = ax.figure.colorbar(im, ax=ax, format='% .2f')\n",
    "plt.show()\n",
    "fig.savefig('indices_precip_corr.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(pc_xr_lagcorr,cmap='RdBu_r')\n",
    "im.set_clim(-1, 1)\n",
    "ax.grid(False)\n",
    "ax.xaxis.set(ticks=np.arange(len(lags)), ticklabels=['2','4','6','8','10','15','20','25','30','35','40'])\n",
    "ax.yaxis.set(ticks=(0, 1, 2, 3, 4, 5), ticklabels=('ENSO', 'IOD', 'RMM1', 'RMM2', 'T2m', 'E'))\n",
    "ax.set_ylim(5.5, -0.5)\n",
    "ax.set_xlabel('lag [day]')\n",
    "# for i in range(6):\n",
    "#     for j in range(len(lags)):\n",
    "#         ax.text(j, i, pc_xr_lagcorr[i, j], ha='center', va='center',\n",
    "#                 color='r')\n",
    "cbar = ax.figure.colorbar(im, ax=ax, format='% .2f')\n",
    "plt.show()\n",
    "fig.savefig('indices_lagcorr.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression with cross-entropy loss\n",
    "def build_logistic(nfeature, **kwargs):\n",
    "    \n",
    "    out_neurons = kwargs.get('out_neurons', 2)\n",
    "    regval = kwargs.get('regval', 0.2)\n",
    "    \n",
    "    initializer = tf.keras.initializers.HeNormal()\n",
    "    \n",
    "    input_tensor = Input(shape=(nfeature,))\n",
    "    output_tensor = layers.Dense(out_neurons, kernel_regularizer=regularizers.l1(regval), kernel_initializer=initializer, activation='softmax')(input_tensor)\n",
    "    \n",
    "    model = Model(input_tensor, output_tensor)\n",
    "    model.summary()                        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_X, train_y, val_X, val_y, lr, callbacks_path, epochs, batch_size, class_weight):\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    decay_rate = lr / epochs\n",
    "    momentum = 0.9\n",
    "    #opt = tf.keras.optimizers.legacy.SGD(learning_rate=lr, decay=decay_rate, momentum=momentum, nesterov=True)\n",
    "    #opt = tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
    "    model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    callbacks_list = [\n",
    "        #keras.callbacks.EarlyStopping(\n",
    "        #    monitor='val_loss',\n",
    "        #    patience=20,\n",
    "        #),\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath=callbacks_path,\n",
    "            monitor='val_accuracy',   #save_weights_only=True, save_best_only=True,\n",
    "        )\n",
    "    ]\n",
    "    history = model.fit(\n",
    "        train_X, train_y,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=(val_X, val_y),\n",
    "        #callbacks=callbacks_list,\n",
    "        verbose=True,\n",
    "        shuffle=True,\n",
    "        class_weight=class_weight\n",
    "    )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_valid_n_out(X_all, y_all, all_year, **kwargs):\n",
    "    val_year = kwargs.get('val_year', [[2004,2005,2006,2007,2018]])\n",
    "    val_X = X_all.sel(time = X_all.time.dt.year.isin([val_year]))   # if test year has not been defined in the main module\n",
    "    val_y = y_all.sel(time = y_all.time.dt.year.isin([val_year]))\n",
    "    print('validation years',val_year)\n",
    "    train_year = set(all_year) - set(val_year)\n",
    "    print('train years',train_year)\n",
    "    train_X = X_all.sel(time = X_all.time.dt.year.isin([list(train_year)]))\n",
    "    train_y = y_all.sel(time = y_all.time.dt.year.isin([list(train_year)]))\n",
    "    return train_X, train_y, val_X, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a class weight dictionary to help if the classes are unbalanced\n",
    "def class_weight_creator(Y):\n",
    "    class_dict = {}\n",
    "    weights = np.max(np.sum(Y, axis=0)) / np.sum(Y, axis=0)\n",
    "    for i in range( Y.shape[-1] ):\n",
    "        class_dict[i] = weights[i]\n",
    "        \n",
    "    return class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cat=3\n",
    "dummy = predictor_array.Index\n",
    "#X_all = dummy.sel(lag=slice(40,59)).mean(dim='lag',skipna=True)\n",
    "X_all = dummy.sel(lag=slice(20,59))\n",
    "y_all = tp_target['precip']\n",
    "time_dim = tp_target.coords['time']\n",
    "nclass = np.arange(n_cat)\n",
    "dummy = keras.utils.to_categorical(tp_target['precip']-1)\n",
    "y_all_2d = xr.DataArray(dummy, coords={'time': time_dim, 'category': nclass}, dims=[\"time\",\"category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlen = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_new = xr.DataArray(X_all.values.reshape(1920,6*nlen), coords={'time': time_dim, 'feature': np.arange(6*nlen)}, dims=[\"time\",\"feature\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2 {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.DataArray (time: 1920, feature: 240)&gt;\n",
       "array([[-0.3212728 , -0.366523  , -0.41395837, ...,  0.13190036,\n",
       "         0.1308144 ,  0.12520641],\n",
       "       [-0.366523  , -0.41395837, -0.46152252, ...,  0.1308144 ,\n",
       "         0.12520641,  0.11758745],\n",
       "       [-0.41395837, -0.46152252, -0.50647944, ...,  0.12520641,\n",
       "         0.11758745,  0.11415125],\n",
       "       ...,\n",
       "       [-1.4852294 , -1.5044475 , -1.5249382 , ..., -0.4805436 ,\n",
       "        -0.48886877, -0.49664137],\n",
       "       [-1.5044475 , -1.5249382 , -1.5429955 , ..., -0.48886877,\n",
       "        -0.49664137, -0.49967936],\n",
       "       [-1.5249382 , -1.5429955 , -1.5607444 , ..., -0.49664137,\n",
       "        -0.49967936, -0.5017516 ]], dtype=float32)\n",
       "Coordinates:\n",
       "  * time     (time) datetime64[ns] 1981-10-08 1981-10-09 ... 2010-12-10\n",
       "  * feature  (feature) int64 0 1 2 3 4 5 6 7 ... 232 233 234 235 236 237 238 239</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.DataArray</div><div class='xr-array-name'></div><ul class='xr-dim-list'><li><span class='xr-has-index'>time</span>: 1920</li><li><span class='xr-has-index'>feature</span>: 240</li></ul></div><ul class='xr-sections'><li class='xr-section-item'><div class='xr-array-wrap'><input id='section-1adf432c-f8f1-4baf-a739-46e1afaca5e7' class='xr-array-in' type='checkbox' checked><label for='section-1adf432c-f8f1-4baf-a739-46e1afaca5e7' title='Show/hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-array-preview xr-preview'><span>-0.3213 -0.3665 -0.414 -0.4615 ... -0.4889 -0.4966 -0.4997 -0.5018</span></div><div class='xr-array-data'><pre>array([[-0.3212728 , -0.366523  , -0.41395837, ...,  0.13190036,\n",
       "         0.1308144 ,  0.12520641],\n",
       "       [-0.366523  , -0.41395837, -0.46152252, ...,  0.1308144 ,\n",
       "         0.12520641,  0.11758745],\n",
       "       [-0.41395837, -0.46152252, -0.50647944, ...,  0.12520641,\n",
       "         0.11758745,  0.11415125],\n",
       "       ...,\n",
       "       [-1.4852294 , -1.5044475 , -1.5249382 , ..., -0.4805436 ,\n",
       "        -0.48886877, -0.49664137],\n",
       "       [-1.5044475 , -1.5249382 , -1.5429955 , ..., -0.48886877,\n",
       "        -0.49664137, -0.49967936],\n",
       "       [-1.5249382 , -1.5429955 , -1.5607444 , ..., -0.49664137,\n",
       "        -0.49967936, -0.5017516 ]], dtype=float32)</pre></div></div></li><li class='xr-section-item'><input id='section-ba87b06b-5560-4b38-9832-649a4c310354' class='xr-section-summary-in' type='checkbox'  checked><label for='section-ba87b06b-5560-4b38-9832-649a4c310354' class='xr-section-summary' >Coordinates: <span>(2)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>time</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>1981-10-08 ... 2010-12-10</div><input id='attrs-445796b5-5820-4cca-ad85-49d5f50a43ea' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-445796b5-5820-4cca-ad85-49d5f50a43ea' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-8bcc1cc0-390d-421e-bdbd-db675fc70912' class='xr-var-data-in' type='checkbox'><label for='data-8bcc1cc0-390d-421e-bdbd-db675fc70912' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>standard_name :</span></dt><dd>time</dd><dt><span>axis :</span></dt><dd>T</dd></dl></div><div class='xr-var-data'><pre>array([&#x27;1981-10-08T00:00:00.000000000&#x27;, &#x27;1981-10-09T00:00:00.000000000&#x27;,\n",
       "       &#x27;1981-10-10T00:00:00.000000000&#x27;, ..., &#x27;2010-12-08T00:00:00.000000000&#x27;,\n",
       "       &#x27;2010-12-09T00:00:00.000000000&#x27;, &#x27;2010-12-10T00:00:00.000000000&#x27;],\n",
       "      dtype=&#x27;datetime64[ns]&#x27;)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>feature</span></div><div class='xr-var-dims'>(feature)</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>0 1 2 3 4 5 ... 235 236 237 238 239</div><input id='attrs-985e7c73-37d3-468d-8782-db3bb41c6630' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-985e7c73-37d3-468d-8782-db3bb41c6630' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-16c56b7b-b51a-47a9-90a3-b4bf839fedb8' class='xr-var-data-in' type='checkbox'><label for='data-16c56b7b-b51a-47a9-90a3-b4bf839fedb8' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([  0,   1,   2, ..., 237, 238, 239])</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-31266413-f931-40ea-a329-515a08f9cf33' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-31266413-f931-40ea-a329-515a08f9cf33' class='xr-section-summary'  title='Expand/collapse section'>Attributes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.DataArray (time: 1920, feature: 240)>\n",
       "array([[-0.3212728 , -0.366523  , -0.41395837, ...,  0.13190036,\n",
       "         0.1308144 ,  0.12520641],\n",
       "       [-0.366523  , -0.41395837, -0.46152252, ...,  0.1308144 ,\n",
       "         0.12520641,  0.11758745],\n",
       "       [-0.41395837, -0.46152252, -0.50647944, ...,  0.12520641,\n",
       "         0.11758745,  0.11415125],\n",
       "       ...,\n",
       "       [-1.4852294 , -1.5044475 , -1.5249382 , ..., -0.4805436 ,\n",
       "        -0.48886877, -0.49664137],\n",
       "       [-1.5044475 , -1.5249382 , -1.5429955 , ..., -0.48886877,\n",
       "        -0.49664137, -0.49967936],\n",
       "       [-1.5249382 , -1.5429955 , -1.5607444 , ..., -0.49664137,\n",
       "        -0.49967936, -0.5017516 ]], dtype=float32)\n",
       "Coordinates:\n",
       "  * time     (time) datetime64[ns] 1981-10-08 1981-10-09 ... 2010-12-10\n",
       "  * feature  (feature) int64 0 1 2 3 4 5 6 7 ... 232 233 234 235 236 237 238 239"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f40b1f379a0>]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA1m0lEQVR4nO3deXxU1dnA8d8zM9l3kpAQIBAkrLIICChLUXHDFXfr1laluPRt69tWa9/2rfa11WoXq1arVbFWxa0K7qDWgsouIGtYA4Q1Ycm+zcx5/8gEI2YlM/femTzfzyefTO7cuefJzeTJyXPPPUeMMSillIp8LrsDUEopZQ1N+Eop1UVowldKqS5CE75SSnURmvCVUqqL8NgdQGsyMjJM37597Q5DKaXCxooVK0qMMZnNPefohN+3b1+WL19udxhKKRU2RGRHS89pSUcppboITfhKKdVFaMJXSqkuQhO+Ukp1EZrwlVKqi9CEr5RSXYQmfKWU6iI04St1nErLa5n1/BqOlNXaHYpS7aIJX6njUFVdz6O/WUTlZ8U88cdldoejVLtowleqg/w+P4/ct5jUMj9Hklwk7atj9msb7Q5LqTZpwleqg3xeQ2lZLSX58fz0vokciRf2fLSb3fsq7A5NqVZpwleqg6Ji3Lyd6oMBSURHezjvxqFEGXjh6S/tDk2pVmnCV+o4+DC4RAAYObQ7NX3jidtVzbJV+2yOTKmWacJ3oJrqSpY+/0s2P32T3aGoFviMwe2So19ff+Nw6gTmv7bJxqiUap2jp0fuag4X76Vg/tPkbv4HY81+FsjJ5PvqwR1ld2jqGH4D8lW+JyszAW/POOJ2V1Nb5yUmWn+1lPPou9JiJft2UrxzE5WH91FTWoy3vJjYip1klq0jz7ud8WLY5BnAA93u4KmiXDa7PEjbh1UWM+arkk6jE4ZlsL+oiIWLdjP1W31sikyplmnCt8DmlQs49Nmz9ClZSDbFZBzz/CGS2Rudx5KeN5M97jIGDB1H+sJteHduoKzaS0q89vCdxuc3uI9J+FMm9+bF93axZsV+TfjKkYKS8EXkGeB84IAx5sRmnp8CzAG2Bzb9yxhzbzDadrLNS96j/uPfMqT2S2pMFOsTx1OYcz3xOYNJ7NaD1PTupKZn0S0uhW7HvDYjMQaAg5W1mvAdyG/Adcy/XhlpcZQnupGdOjxTOVOwevizgEeBf7Syz0JjzPlBas/Rtqz+lKoP7mV41RKKSWVR/k8Yet6tjEpNb/cx0hOjAThYWUe/ZlenVHYxxgAg8s1iW2peEqwpZdeecnrnJFkdmlKtCkrCN8YsEJG+wTiWUxm/n9Ufv0zd2rlklK3HJx6qotKoTjkBUnrjio7Hd3gnafsXM8i7gTITz2d5P2DEZXdySmLHf/HTEwI9/Aqdp8VpfP6GhO8+tosPjBqXwxdrSln4aRHfvmKw1aEp1Sora/iniMhqYA/wE2PMuuZ2EpEZwAyA3NxcC8Nr2f6irex7YSYjq5dSSgKFsUNAhMS6EgbuW0Xs/noAvMbFTncuiwf8hMHn3sKEtGOr9e2XEejhl1TUBeV7UMETyPffKOkAjBmZxRJZz+6tRyyNSan2sCrhfwH0McZUiMg04E0gv7kdjTFPAk8CjBkzxlgUX4v2FBYgs84j35SxeOBPGHP5nYyIij76vM/rpbT0INWVZaR170m/2Hj6BaHdtIRASUcTvuP4WynpRHlcVMa7objG6rCUapMlCd8YU9bk8bsi8lcRyTDGlFjR/vEq3lOIzDqPeKrYc8m/GD9i4jf2cXs8pKRnkZKeFdS2o9wuUuKiOFipJR2naUz4zZV0AGK7x+LZXqnj8ZXjWHKnrYhkS6A7JCJjA+0etKLt4+Wtr6N41rWkmDIOXPwy/ZtJ9qGWnhitPXwHaq2kA5DTLwUPwsq1xdYFpVQ7BCXhi8hLwCJgoIgUiciNIjJTRGYGdrkMWBuo4f8FuMo0DnVwqGWzfsaQujWsH30P+SMn2RJDRkIMJXrR1nEae/jH3njVaPjw7gBsXOfof2BVFxSsUTpXt/H8ozQM2wwLm774hHFFs1iaNo2xF95iWxzpidFsPqBjup3G72894Q/OT+MDMVTsKLcyLKXapJOnHcNbV4vnnR9SImkM/u5jtsbSUNLRHr7TtFXScblcVCe68ZXohVvlLJrwj7Fi9m/o5yuk6NTfkJRy7P2v1kpPiOFwVT1en9/WONTXHS3ptJTxgfjseBJrDBWVeg1GOYcm/Cb2bN/AiK1P8EX8RE468xq7wzk6Fv9QlSYNJ2mrhg/Qd2AaboRFy/ZaFZZSbdKEH2D8fkpevh0vHnK+/UizY6ytlt44n46O1HEUf+AfrtYS/oTxPTEYNnypI3WUc2jCD1jx9t8YXrOcNQP/i+xewbh1qvPS9eYrR/qqh9/yPt0z4imLEcp1IjXlIJrwabibduCKe9gQNZSxV/zM7nCOSm8yY6ZyjvbU8AGic+JJrPBRVV1vRVhKtanLJ/y62hpKX/gOACnXPIvb45w7Ixtq+F6Ky3W0h5O0p6QDkDe4Gx6t4ysH6dIJ3/j9rP7rdQyuX0/ByfeS03eg3SF9nauahL5P8O+9r9sdCV6fj43FRfx72xrqvF67w7FVe0o6ABNPaajjr199wIKolGqbc7qzFvP7fCx98lbGl85jUZ+ZnHL+DLtD+oaUmGQy4zNZXfUib204jQsGn2x5DH6/n4c+fY1/bvkLxl3asNGXSH78ZB4//26yElMsj8lu7RmlAw3r3JbFCGaH1vGVM3TJHn5F2WFW//FCxu+fzZLMyxh/w+/sDqlZIsIz5z+Iy5/ILz+/i8NV1iYOn9/HBbN/xPPbf4PHJHF65ve5IOdHZHgGs6nmPc56eTpvbVhmaUxO0N4aPkBcbiLJFT72HagMdVhKtalLJXzj97Pi3Weo+uMohld8xuIBP2XsLU8hLueehn7dsvjBsF/i8xzg5x/9zbJ2/X4/V7z6c3bW/5sh8Rfx+fVzeHja7fz2zBv59/XPcOeIP2OknrsX3c7C7esti8sJ2rrTtqlxk3vhQvhg/va2d1YqxJyb6YJs56ZVrHvgNEYv/TFl7jQ2X/A647/9P45O9o1uPvlskvxD+az4Nct6+TfNvZ9NNe/RP2YaL116L7FN1gAAuO6k03ly6t8BF7d/fCtbD+6zJC4naG9JB2D86GwqPFC0xtGTw6ouwvnZLgiWvfkYmS+cSW7tJpYOuou8ny9l0Jgz7A6rQ24bNRPcFfzPx38PeVt3znuSZaUvke2axGuX/w5XC38Ux+cO5Nfj/oDPdYRb34v4NemP8rUxeVpTLpcLT248SUe8lByuDnVoSrUqohO+8ftZ/PhMTl51N9tiBlE3cwljr/q5o4Zettc1I6aQ4B/IggOvUlkbunH5d37wDO/seZRkM4w3r/wTbnfrb5FLh57KsMQL2eNbyOwvF4QsLicxHSjpAIyZ1As3wvsfaFlH2StiE77f52Ppozcwfv9LLMm4lIE//YiMbGeskXu8rht8A7jL+NPnwR+maYzh+28+yjt7/0wKg3j7iidJiI5p12sfPe+niC+V3y+/H6/PF/TYnKYjJR2AiSfnUB4FO5fsx+/XifCUfSIy4Ru/n2WP38y4Q3NZlHMDY2/9O55jatDh6PsnT8Ply2DO9leDetw6r5fps3/B56V/I9MzjA+unkVafGK7X58en8SlfWdQ797FXxbNCWpsTtRY0mlpicNjuT0uMkdnkFJt+GjBrlCGplSrIjLhL551F+NKXmdx1tWMv+nPYXFhtj08bjeTul9IjXsLc9cvD8oxS2uqOOuFGWyte4v8uDP54OpnSYyJ7/Bx7px0JS5vOi9tei7ie7GNo3Q6Mr/elZcPotplWPp+YUhiUqo9IiMTNnGkZB8Dds5mWco5jJ3xWMQk+0Y/n3w9xh/FX794rtPHKq4o46yXruMgy5ic/l1ev/wPRLuP7z+h2KhoTsu5nBr3Nl5aE9m1fNPBkg5AYkI00YNSSD3i4+OFO0MVmlKtiqxsCKRmZFP/vY8Yeds/cLnddocTdD2T08mLnUBR/ecUHjr+oX419XVMf20mlbKZK/rcyWPn39HpKaH/91s3gC+eJ1Y93anjOF1HSzqNrrl+KBUeWD57s96IpWwRcQkfIDs3n6h2XnAMR7ePuQ5x1XH/wheO+xiXv3YnpbKGc3vcxi+nXBuUuNLiExmWfC6HzWqWF20JyjGd6HhKOgDpqXFMvH4gsT545uEVwQ9MqTZEZMKPdGfnjyXe9OHzA29TW9/xUTF/+PR1Cus+ZHDchTx49veDGtvPTr0BgIcW/SOox3WS4ynpNJowtidmcDIpB72s3ag3YylracIPUxf2uxQTvZe/Lf2oQ68rKN7DrE0PEu3rzazpvwp6XCNz8ugmI1lXPp/y2si80chnjq+k0+iiSwbixzDv7cj9L0g5kyb8MPXDcVcg/nhe3Ph8h153y3u/wkgtD37rfuKjQlP2umbIVeCu4I+fvRaS49utI3PpNKdv72TKu0Xh31ZBdU3XnmpaWSsoCV9EnhGRAyKytoXnRUT+IiJbRORLERkVjHa7ssSYBMamn0+FezXvF3zZrtfM+uIjis0STkq+hNNPGB6y2G4cdTYubyZvF9o/j38oNN541ZmL3MO+1ZM4vzDnHe3lK+sEq4c/CzinlefPBfIDHzOAx4PUbpf2y8kzwLj587Kn2ty3pr6Oh1c+iHi78ci0O0Ial8ftZmLWhdS4t/JOQXDuF3CSxhq+uxMJ/5wz8qh0G7Yt2x+ssJRqU1ASvjFmAXColV0uAv5hGiwGUkWkRzDa7sr6pGbRL3YKRfUL2xwVc+e8J/F6dnPtgNtJjUsIeWx3Tbyu4X6BFR0rOYUDXzuXOGxNlMeFKzeBhCNeDh6JzGsdynmsquH3BJreU14U2PYNIjJDRJaLyPLi4mJLggtn9075IeDivz/6TYv7bDm4j4/3/4NE/2B+MuFSS+LqnZpO7+gJ7KhdSFFpa32B8PNVSadzxxl1ag4ehHnzCzsflFLtYFXCb+5XwzS3ozHmSWPMGGPMmMzMzBCHFf5G9ujLuLQrOMQXPLnsvWb3+cH792Fctfz2W79qcarjULh9zA2Iq57f/OcZy9q0gunkKJ1G3zqlF1Uuw/bV2rFR1rDqt78I6N3k617AHovajnh/PveHuLyZPLrmPlbs3vq15/7nw2cp8n7CkITzOa3fiZbGdd7AMST4B7GoZA5V9aGb0tlqwSjpQMOkaqZnHPEl9ZRX1AUhMqVaZ1XCnwtcHxitMx4oNcbstajtiJcUE8cDkx7CUMv33r+Zf29bQ53Xy+/+8zJvFj1Mon8wsy4O/pj79rhm0LUY9xEe/PQVW9oPha+mR+78sUackkMUwhtzN3f+YEq1IVjDMl8CFgEDRaRIRG4UkZkiMjOwy7vANmAL8BRwazDaVV85Z8Ao7h79ED7XEf5r4bcZ/Y9TebHw/4jydeeV6Y+HbMx9W24Zez5ubxZvFc4+WgoJdx1ZxLwtZ07JpTQW9n6+n9o6HZOvQisoSz8ZY65u43kD3BaMtlTLrh7xLU7KmctfFr9KwZH1TM2dyn+Nv5iEGPvmFfK43YxMupJVe3bgMz48En6rjR2rowugtMblcjH07FyK5uzkxZc38N3rhnX6mEq1JPx/+9TXDMrsxV8v+LHdYXzNwKSJfF7SC48rMt5u/qM1/OAc74Kz+/G7ebuoXFyM/xq/pRfWVdei7ywVcsmxUdR5/dQcx0RvThTMHj409PJ7jc4kwQdrdEI1FUKa8FXIJcdFAVBWU29zJMERzBp+oxEndQdg9Sq981aFjiZ8FXLJsQ2lnLLqyLgo2dnJ05ozbGAGNWLYu7U0eAdV6hia8FXINfbwyyOsh9+ZuXSO5fa4qEnx4C+OnPsVlPNowlchd7SHHyFTAfv9nZ8tszmpvRNJroMDJVVBPa5SjTThq5BLjg3U8KsjpYff8DmYJR2A/CEZACxervckqtDQhK9CLlIv2nZ2Lp1jjRuTjR/Dtg2RNdmcco7IGBitHO2rHn5klHR8ISrppCTFUB4rmL2VQT2uUo20h69CLjbKRZRbIuairQlRSQfAnRZDTIUvYqahUM6iCV+FnIiQFBsVcSWdYN141VS3ngnE+YWde8qDfmylNOErSyTHeiKnpBOiGj5AXn4aAF+u0TnyVfBpwleWSI6LnB5+Y7UlBB18RgxrWPRn5za9AUsFn160VZZIjo2KnGGZ/tCVdDK7xVPpAa9euFUhoD18ZYnkOA/lkXLjVaCHH8w7bZvyJXmQ0sj446icRRO+skRSTOSUdHxBWsS8JYlZ8STWGUp12UMVZJrwlSWS4yLnoq0xBpHgj8Nv1DMvGRfCqrV64VYFlyZ8ZYnk2Ciq633Uef12h9JpfmNCVs4BGDq0YYqFLZv0jlsVXJrwlSUiacZMnz80F2wb5eelUieGkiIdi6+CSxO+skRyXMOAsEi4cNtY0gkVl9tFdbyb+hKdKlkFlyZ8ZYmkmMiZQM1vTEhuumoqOiOWhGo/3ggogSnn0ISvLHF0xswIuHAb6pIOQFZuEtFGWLdZ6/hdzYI3C/jgkWXUhKBzpAlfWaKxpBMpPfwQ53sGDOoGwIYNJaFtSDmK3+8netl+kvfXEB3tDvrxg5LwReQcESkQkS0iclczz08RkVIRWRX4+FUw2lXhI5IWQTEWlHSGD8nEh2FvYVlI21HOsvLTXfTyCbUj03G5gt8f7/TUCiLiBh4DzgSKgGUiMtcYs/6YXRcaY87vbHsqPH01SicCSjrGhLykExfnoTJG8O+vDmk7ylkOLygiBsO4afkhOX4w/oSMBbYYY7YZY+qA2cBFQTiuiiDxLsOU3SuRjevsDqXT/CY0c+EfS9JiiC4P/z+Qqn12bDnEgAofu/smEBcfFZI2gpHwewK7mnxdFNh2rFNEZLWIvCciQ1s6mIjMEJHlIrK8uFjvNIwYfj8z18wl+81/2h1JpxkLevgAqTnxxPuFXTo3fpew8Y1N1AMjLwxN7x6Ck/Cbe+cfu1zPF0AfY8wI4BHgzZYOZox50hgzxhgzJjMzMwjhKSdwxcRQMvVCTtj2JRs/X2V3OJ3i81uT8HPzUgBYX3Aw5G0pe23dWMKgg3VszokjKyc5ZO0EI+EXAb2bfN0L2NN0B2NMmTGmIvD4XSBKRDKC0LYKI6f8eAY17mgKHnnc7lA6xaqSzpBBDb8iO7fr3PiRbusbm6gDxlw5OKTtBCPhLwPyRSRPRKKBq4C5TXcQkWwJzDQlImMD7Wq3pYtJ79mdneNOJ2/VZ+zeVGh3OMfNbwwuCzJ+bs9EasVwWOfGj2g7thxiUKmXbbnxZGQlhrStTid8Y4wXuB34ANgAvGKMWSciM0VkZmC3y4C1IrIa+AtwldFVmrukkf99Gwis+N/77Q7luPktKum4XC6q41zUH9JpkiPZxve34QdGXDQg5G0FZcWrQJnm3WO2PdHk8aPAo8FoS4W3PkP78+bkCxj4yZts+GQJg6eMszukDrOqpAPg6RaDZ3c1fr8/JOOylb2qq+roXVTF5mQPZ/YMXe2+kb6DlOW+de/PKI1JZNdvfovfH35zxVhV0gFIy44n1ghFeyssaU9Za8m7W0hG6DapuYGNwacJX1kurXsaJVd8h967N7H4+TfsDqfDjAn9XDqNevcNjNTZqJe8Ik1dnZf4lSXsdhtOmtC77RcEgSZ8ZYsz//tm9qZkUffXR6ivDa8adcOwTGvaGjIoHdCROpFo4SsbyPEJZnJPy8p1mvCVLWJio3HPvJ2s0v18/Ken7Q6nQ/wW3XgF0DsnkRoxHNlXZUl7yhqHD1aRs/Ywm+OEsWfmWdauJnxlm0k3XEJRZi68/7bdoXSI38KSjsvloibeRf0hXQwlkiz7+2rigJ6X5lt6MV4TvrKNy+Wi+qRx9NxfSGnJYbvDabeGi7bWtde4GEp9iBdD8fv9FGzZwbqNW0PaTle3YE4BQw57KeiXxIATsyxtOyjDMpU6Xj1Om4R73quseecTJt4w3e5w2iXUi5gfK6tPMuU7qllXUMLIod1D0sb78z9lxayHifU2zM75avcBXHDjTYweOSQk7XVV2wpK6L5oP9ujhdO/M9zy9rWHr2w17OyJ1LqjOLjwM7tDaTe/AbEw4Q8Z2nDhdv260IzUmf/R56x6+iG8nniSTr8K15hziS7Zwfz7f8Hrb3wQkja7ogN7Kzj83Hq8An2/eyJRIVjgpC3aw1e2io2PY3evASSuX2l3KO3mt3CUDsDwwRl8hmFvCEbq1FZVsfzpP1IXk8x37/s9ub0aSgyFOy/h2Xt+ybbZj/L3koPcdPO32zyWz+vj3XkLKC8rx+PxkJqWwpjRw+mWmhT0uMPN4YNVbP7rSrL8UHNZf3rnpdkShyZ8ZTsz6mRy3pjFvu1FZOf1sjucNlmxiHlT0dEeKuJcUFwT/GPHxfFB9zOZNvmko8keoG9uD+7405/5093/g+vDF3mspppbbvtuixcY/X4/v7/390QXfPWf2i5gSVQSp992JxNOGRn02MNFyf4KNj/yBT28UHxmL8aNybEtFi3pKNv1mToZgA3v/8fmSNqnYU1bC7v4QFRmLPGVPnxBvnBbXe+jMLYXyenp33guJTmROx96gIoeQ6j59A3umXkbr7/xAdsKd3/9GDW1/OG3fya64DPqBp7KaT9/kFN/ch99rrwNgE8f/l/mfRQ+Jbtgqq6qY8NjK+nuhYPn5DLujH62xqM9fGW7AaeexCZxUb722FUxncnvx9JROgBZfZKo2FnN2k0HGTEkeOtElAbWGG5cc/hYcbEx3P3Qb5n17GxqPplD4exHKJwNZYnZZI+aQFV5OZXrlpBQV0pV31Hc/as7cXsCtemTR7Br/GievfPHfP78U0yZNJbo6NCs5OREfr+fhX9ZwZA62DE5hwlT+todkvbwlf1i4mI5kJaNqzA8hgNaXdIBGDy4YW789etKgnrcsuqGJRRT4lpOxFEeDzfffC23P/4sJ978c2JOvQiXt46qBa/jXzkfX3Q8/a/7Mb/43a+/SvYBvXO6M/jia0mqLuHZZ14MauxO9+mcTQw54mX9gCQmTOtvdziA9vCVQ1Tm9KXb9g12h9EuVt5p22jEiRl8LoZdBcG9X+FoDz+u7VSQkpzI2VMncPbUCfj9N7Jx8w5ye2WTmBDX6usunX4298x/h9r/zGX7tDPJy7Wvhm2Vqoo6kpceYEeUcMb11g+/bIn28JUjuE7oT3rlYY4UH7I7lDb5LB6WCRAT7aEqLQr21gR1htGyNko6LXG5XAwZmNdmsm/c9+Jbf4DL+Jh13/9RV1d/XLGGk09fWkeGERLO7Yvb45w065xIVJeWOnQQANuWfmlzJG1rWMTc+nZ7DE4jwQcr1xQH7ZhlNQ3Jt7WSTjCcNHwgPaZdS/KRnTx07wNU1zRMFRGO02O3ZefWw/TbWs6GZDcjT7VmFsz20pKOcoTcMcOpBA6sXg/nTbE7nFZZfadto8mTe/P+Z8Us+Xw3o0cE55b8r0o6ob+YesP1l3H/1i3EbPyUB2Z+H+OOIqFiP1UJGSQNGMktP5xBfFxsyOMIJb/fz9Z/riMHGHrdULvD+Qbt4StH6DUoj6qoWGo3bbI7lDb5/NaXdABO6JNKaQwc3loWtGM2XrRNjrWm73fXPXfR56ofIKZhqJNv0CmY6HhYOY/f33Yra9ZvsSSOUFnwr43kVxuKRnQjp3eK3eF8g/bwlSO4XC6KM3oRvWu73aG0ya6SDkBcn0Q8m8rZX1xJVmZCp49XVlNPQrQbj9u6vt9l08/msulnf23b6298wMZXn2LOfXfjvfM3nDR8YNDaM8ZQU1ZL0a4yxCWkpseFZLHwbQUl9FxewpZYYcrlzpyDSBO+coza3n3JWfmZ49dvtWNYZqOJp+eyZNM6/vnsGv77Z+M7fbzS6npLyjltuXT62XyRl8t7D/6Kdx74JQm/eYgB/XI7fJzS/RXs3l1O+cEqyneW49lfRY9yLwlGaLy8XAMs9kB5j3iyRmfR64Q0ElNiiI4+/nRYXVXH/ufXkwIMvGmEoy7UNqUJXzlGzMCBJC79kN2bCuk9yN47ElvTsOKVPQn/5JHZfNxzK/HbKlmxej+eKBf5eanEH2fSLquuD/kF2/YaNXIw5qf3MP/+u3nx4b/w64cf6vAxdv/5C5KN0Lgc+G6XYWOyB7rHkdo9HpcRqkuqiNpZQf6uSjy7tlPFdsow7IgVarITiO8eT1pOIj36ppLWPb7Nzkd1VR2LH1rKCV7YO7UXQ3qFfjHy46UJXzlG1qjh8DxsX/SFoxO+MWBTvgfg+hkjefHXi/n88bW4EN6JFW655xTSUjp+wbOspr7DQzJDafTIIXx68tlELX2Hue98zIXnnd6h1//ZVcPgXimcMaonufnpjOvW8rDRI4eqWbeoiOpD1fhK60jeV01eYQWuwkqgmGrgIIaSKKEi3o1kx9NrVDY5fVKIjY2isryWjSv24l20lxNqDdvGZDBlqnPft6AJXzlI/ikjKUQ4snotcJnd4bTIzpIOQI+sBE44rw8FK/eTkBZL0toj/PX/FvPjeyd0uKdfWu2lZ6qzRsbcfOt3eXD156x85VnOmjqR2Jjodr2uzutnnq+e4QO7MXxc25PwpXaLY8J5+V/bVl1dz56dpRzYVUb5vkr8JdVEl9XRrdxLdmk5FJRTDpQH9s8FjmAoHNedKdMHdewbtUFQEr6InAM8DLiBvxtj7j/meQk8Pw2oAr5jjPkiGG2ryJGYmsyB1CxcW5w9UsfKJQ5bcvH5/eH8htv1n39pHa7/7OfV1wu44doTO3Scsup6Bvdw1vTF8XGxDL/sOra98Beee/Ylvj/zhna9rjxwT0FSJ/5jiYuL4oSBGZwwMOMbz+3bXcampXuoK6vFX+fHFeMmuVcSJ03MtWVu++PR6YQvIm7gMeBMoAhYJiJzjTFNZ8I6F8gPfIwDHg98VuprKnqfQPrWdXaH0Sqf39ha0jnWNVcO5oHP93NkZTFc27HXOq2k0+ii86dyz7tzqFnwNgevvJj0tLaHOJbVBIaYtmOaiOOR3TOZ7OnOrc+3RzAuJY8Fthhjthlj6oDZwEXH7HMR8A/TYDGQKiI9gtC2ijCegYPoVnWEAzv32h1Ki4xNN161xOVykTIkldRKw7qN7V8Vy+c3lNd4HTFK51gul4up199IrK+aWX+b1a7XNPbwnfgHzCmCkfB70rDWQaOiwLaO7gOAiMwQkeUisry4OHi3kKvwkDGqYaKprZ87t+LnhJLOsc497wT8GOa/t63dr6moaXumTDtNOnUU5dmDqf1yIaXllW3u33gTWWdKOpEuGAm/uXe+OY59GjYa86QxZowxZkxmZvDm/VbhIf/U0QAcXOXcOXV8xlg+H35b8nJTKEvxULeljKrq9k1O9tVc+M4duzHhwunE+Gp45eU5be7bOC9QqEo6kSAYb9sioOkMQb2APcexj1KkZadTnJiOv6DA7lBaZGyYHrk9Rp7RmwSf8NysNe3a36qJ0zrjjNPGU57Qnd2fzmtzorVgXLSNdMFI+MuAfBHJE5Fo4Cpg7jH7zAWulwbjgVJjjHOLtMpWR3L7k76jwLEzKTqxpANw3ln9OJLmpubLw+zY3fZ8O2UWTpx2vFwuF30nnU1SdQnvzW99mUSr5wUKR51O+MYYL3A78AGwAXjFGLNORGaKyMzAbu8C24AtwFPArZ1tV0Wu6NFj6FZ1hJ1rnTmRVsOdtnZH0bwLbxiKy8Dsv7fdy29reUOnuPzKC6hxx7H07Tdb3a+8ph4RSOjEFAmRLiiVSGPMu8aYAcaYE4wx9wW2PWGMeSLw2Bhjbgs8P8wYszwY7arIlHfGRAA2z3PmouZ+Y3A5NOMPG5SBr38iiXtr+Wzp7lb3PVrSiXd2wk+Ijyd22ESSDhSwZn3Ly2CW1XhJivE49mfjBA679KQU5I8dTllMAlXLl9kdSrOMQ0s6ja6/aThVLsMnsze1WhYLpxLIpddegR8Xb89+ucV9ymrqtX7fBk34ynFcLhcH8obQbbMzb8ByckkHICM1ju6nZpFaZZj1/NoW9yutrscVJiWQvr17UJM7HNm0jJJDR5rdp6zamfcUOIkmfOVInlGjyag8xI51zqvjO7mk0+jbVw3hSLKLikXFfPLZrmb3qd1ezhRvNMbf7Ahpxzl9+iVEmXpeefmtZp8vr6knKQz+W7GTJnzlSH2nTgJg07wFNkfyTU4v6QBEeVzc9NOTqfIIy17YxO59FV97vq7eh3tdGfleDy63s7+XRhNOOYnKhO7sXfpvfM38kSqr8Tr+ArTdNOErRxowdgSVUbFUrVpldyjf4LNxxauOyMpM4IybhhDjh+ceW/m1515/cxNJ9ZA/pactyzUeDxGh38SppFYd4O1/f/P6TnlNvd501QZN+MqR3B43B3rkEb/NeTNn2rWI+fE4eWQ2rhNTSSmu59U3G25m83n9FC7cS1k0TD+vv80Rdsz0y87HKx4WvfP2N54rq3bmRHBOoglfOZa3/yCySoqoqaq2O5SjjDGBBVDCI+ED3DxjBKWxwu73i5jz3hb++OASkuug76Qejl2KryUJycnEDBhFyp4v+XLrVzfr+/2G8lpvWIw4slN4/bRVl5Jy0giijI+Cz5wzkVpj6djpNfymYqI9XPvT0VTFCEVzdhK/o5qqvnFcMT14C4Vb6cJrribaeHnjxdeObqus82KMTqvQFk34yrHyJo4BYO8SJyX8hozvDrPfnNyeydz8v6dQ0TOGjLNy+Oldp4Rd775Rv4H51GXn49qwkOIjDbNohnou/EgRnj9x1SX0GphHaWwStWtbHktutcaEH04lnUaZ3eK485cTuPIS5y/F15bJl15OvK+aF198A9CJ09pLE75yLJfLRUnPfiTt2Gx3KEc13rgaTiWdSHTqpFOoSsri0OJ51NV7m9w1rAm/NZrwlaP5Bwwh6/A+yg+V2h0KEL4lnUgjIgw+80KSaw/x2pz5TXr4WtJpjb5tlaOljx6BC0PBQmfMq9OY8LWHb7+Lpk+jKjqJjfPmNln8RHv4rdGErxwtf/JYAA4sW2VvIAGNo3TCsYYfaaKio8gYdyYppbtYuGAJEB4TwdlJE75ytO65PTiYkIZ3gzMmUvMHMn6YzEYQ8a654SqqopKIWv4WYvx60bYNmvCV4x3q3Z/Unc6YRO1oSScc5lboApKT4uk37Uq615UwrHoT0WE61NQqenaU47kGDyGzvISDuw/YHYqWdBzo6qsupCy5F+MOLqG+rtbucBxNE75yvMwxJwGwyQEXbr+6aGtzIOool8vFtXf8mNNm3kFUdIzd4TiaJnzleAMmNdxxe3DFKnsDocmwTO3hO0r+4IGcOmm83WE4niZ85Xhp3dPZn9IdU7De7lDCci4dpRppwldhoSy3P92KttkdxtFROprvVTjShK/Cgqtff7pVHaG05LCtcXx1p61mfBV+NOGrsJA8KB+AwpX2lnW0pKPCWacSvoh0E5H5IrI58Dmthf0KRWSNiKwSkeWdaVN1TT1HDAHgwJqNtsbh05KOCmOd7eHfBXxkjMkHPgp83ZLTjDEjjTFjOtmm6oJyh/an3uWmerO9N2AZLemoMNbZhH8R8Fzg8XPAxZ08nlLNioqJpjg1G9fO7bbGoSUdFc46m/CzjDF7AQKfu7ewnwHmicgKEZnR2gFFZIaILBeR5cXFxZ0MT0WSyh69ST6w29YYGks62sFX4ajNqeVE5EMgu5mnftGBdiYYY/aISHdgvohsNMYsaG5HY8yTwJMAY8aMMR1oQ0U4V99+pK9bQkVpOYkpSbbEoNMjq3DWZg/fGDPVGHNiMx9zgP0i0gMg8LnZyU6MMXsCnw8AbwBjg/ctqK4icWA+LgyFKzfYFoPRko4KY50t6cwFbgg8vgGYc+wOIpIgIkmNj4GzAOcsUqrCRs7whrVY96+xL+H7js6WaVsISh23zr5t7wfOFJHNwJmBrxGRHBF5N7BPFvCpiKwGlgLvGGPe72S7qgvqM2IQXnFRucm+NW61pKPCWaeWhzHGHATOaGb7HmBa4PE2YERn2lEKICYuloPJmbBjh20xGE34KozpP6YqrFRk9yJpf5Ft7fv8DZ814atwpAlfhZfcvmSUHaCmusaW5v1aw1dhTN+2KqwkDMjHY/xs/3KTLe1rDV+FM034KqxkD2sYqbP3S3vm1NFhmSqcacJXYaXPyMEAlBXYM1Kn8U5bt/7mqDCkb1sVVuJTkzmckIbZYc+cOo0lHV3EXIUjTfgq7JR270n83l22tK0lHRXONOGrsOPv3YfMw/uoq/da3vbRko4mfBWGNOGrsBPf/wTifHXs2FhoedtflXQsb1qpTtOEr8JO5okNI3WKVlm/3KHOh6/CmSZ8FXb6jjoRgLINBZa3rYuYq3CmCV+FncTsTMpik/Bt22Z521/deGV500p1miZ8FZYOd+9F/B7rJ1FrLOnosEwVjjThq7Dkze1L5qE9eBtnM7OI368lHRW+NOGrsBSbn0+8t5adG6y9AUtLOiqcacJXYSkjMFJn96p1lraro3RUONOEr8JS3phhABxZb+0kao0lHZd28VUY0oSvwlJKVgZH4pLxb9tqabta0lHhTBO+CluHu/cmzuKROlrSUeFME74KW/W5eWQe3IvP67OsTZ8ugKLCmCZ8FbbiBuQT66ujaP0Wy9o0WtJRYUwTvgpbmcOHArBrxRrL2jx60VZ7+CoMacJXYavf2BPxI5RaOFLHpzV8FcY6lfBF5HIRWScifhEZ08p+54hIgYhsEZG7OtOmUo1S0lIoTkrHbLWhpKNdJRWGOvu2XQtcAixoaQcRcQOPAecCQ4CrRWRIJ9tVCoDS7FwSLRyp49eLtiqMdSrhG2M2GGPamqN2LLDFGLPNGFMHzAYu6ky7SjXy9+1HRukB6qqqrWlPSzoqjFnxj2lPoOkCpEWBbc0SkRkislxElhcXF4c8OBXeEgYNwm387FhpzWIoPr+WdFT4avNtKyIfisjaZj7a20tvritkWtrZGPOkMWaMMWZMZmZmO5tQXVX2yIaROvtWrrWkPaMlHRXGPG3tYIyZ2sk2ioDeTb7uBezp5DGVAuCEkwaxxeWhwqLVr7Sko8KZFf+YLgPyRSRPRKKBq4C5FrSruoCE+Fj2pWYjhdbMqXO0pKP5XoWhzg7LnC4iRcApwDsi8kFge46IvAtgjPECtwMfABuAV4wx1s5pqyJaZXZvkvYXWdKWMQYRXfFKhac2SzqtMca8AbzRzPY9wLQmX78LvNuZtpRqUV4/uq1fRE1pObEpSSFtym+0nKPCl441UGEvedAAAAotGKnjMwa3JnwVpjThq7DXc0TDfXz7vgx9wvcHSjpKhSNN+Crs5Q3Pp97lpqJgU8jbMlrSUWFME74Ke7GxMRxIzUZ2FIa8LZ/f4NYhOipMdeqirVJOUZndm267Nrd7/5qqaj76v0fw7ijElZHJBQ/f067XaUlHhTPt4auIIHn96FZ+iKrS8nbt/9EDj9PvX8+S8+Vi+n/wCms/Wdqu12lJR4UzTfgqIiQPHIALw7Yv2r5wW1NVTepbr7K91yD6z59PncvD1udebFc7WtJR4UwTvooIPUcOBmBvO+bU+c+j/6Bb1RG6zbiZtOx0dgwdS87yBVSVV7b5Wr8xepetClua8FVEyDtpCFVRsVStXNXqft66emJfe4Fd3fsw9rJzAMi+6goS66tZ+Nfn8fv9rb7eb/QuWxW+NOGriOCJ8rCvVz5JW1ov6Xz2zKt0Lysm5vrv4grMcTz6oqnsTc0m99mHWThuCive+rjF1/v9euOVCl+a8FXE8A8dRo/Dezi872Dzz/v9eP/5LHtTspjwncuObnd73Ax79UV2XP8DEPD/4qdsWdF8aSjucDG5h3eHJH6lQk0TvooYWaeMBWD9h581+/wXr75LTkkRtZddg8fj/tpzmb17cM7dt9L3mafxudzsnfF9ls/56Gv71FRVM/75h7jjw8fw19SE5ptQKoQ04auIMeSMU/CJi4NLlzX7/MGnn+ZgfCpTbruuxWP0OTGfhD8+AkDCnbcz58Y78Hl9AHxw28/pc6CQ2tvuwBUbG/xvQKkQ0xuvVMRITE1mT2ZvPOu/WY4p+M8ScndupOCS7xEX33qyHnb6OCo//oAP7/gVAz57j7e/Vw8GBiz7kE3fupCLbroyVN+CUiGlCV9FlOoBJ9Jn0TxqqqqJjY87un3LI38jyxPLpB/e2K7jJKQkcsFTD/HWTA8DFryFT1xsmnw+5z58b6hCVyrktKSjIkrWWacT46vn06dmH922e8M2+qxdzM4JZ5Oe1a3dx3K5XFzwxP3snvkzPE+/wEVPPkh0bEwowlbKEprwVUQZe9k57E3Nxrz60tEx9Sv/+FeMCKN//P0OH8/lcjH1R99l0KkjgxypUtbThK8iisvlov7iK+lVsouV73zCkQMH6bloPluHnUqfQXl2h6eUrTThq4gz8ZZrKY+O5+CDv2fBXfcR662j/+0d790rFWk04auIk5CSSOUtd5B+aB/5n7/HttwhnDh5jN1hKWU7TfgqIp12yzXkzJnLpskX0O/Xv7A7HKUcQYdlqojV44RcLnry93aHoZRjaA9fKaW6iE4lfBG5XETWiYhfRFoskopIoYisEZFVIrK8M20qpZQ6Pp0t6awFLgH+1o59TzPGlHSyPaWUUsepUwnfGLMBdEEIpZQKB1bV8A0wT0RWiMiM1nYUkRkislxElhcXF1sUnlJKRb42e/gi8iGQ3cxTvzDGzGlnOxOMMXtEpDswX0Q2GmMWNLejMeZJ4EmAMWPGmHYeXymlVBvaTPjGmKmdbcQYsyfw+YCIvAGMBZpN+EoppUIj5CUdEUkQkaTGx8BZNFzsVUopZSEx5virJiIyHXgEyASOAKuMMWeLSA7wd2PMNBHpB7wReIkHeNEYc187j18M7DjO8DKAcBgVFC5xQvjEGi5xQvjEGi5xQvjEGqo4+xhjMpt7olMJ38lEZLkxxvETqIRLnBA+sYZLnBA+sYZLnBA+sdoRp95pq5RSXYQmfKWU6iIiOeE/aXcA7RQucUL4xBoucUL4xBoucUL4xGp5nBFbw1dKKfV1kdzDV0op1YQmfKWU6iIiLuGLyDkiUiAiW0TkLrvjaUpEeovIv0VkQ2Ba6R8Gtv9aRHYHpo9eJSLTHBDrN6a0FpFuIjJfRDYHPqc5IM6BTc7bKhEpE5EfOeGcisgzInJARNY22dbiORSRnwfetwUicrYDYn1QRDaKyJci8oaIpAa29xWR6ibn9gmb42zxZ+3Ac/pykzgLRWRVYLs159QYEzEfgBvYCvQDooHVwBC742oSXw9gVOBxErAJGAL8GviJ3fEdE2shkHHMtt8DdwUe3wU8YHeczfz89wF9nHBOgcnAKGBtW+cw8D5YDcQAeYH3sdvmWM8CPIHHDzSJtW/T/RxwTpv9WTvxnB7z/B+AX1l5TiOthz8W2GKM2WaMqQNmAxfZHNNRxpi9xpgvAo/LgQ1AT3uj6pCLgOcCj58DLrYvlGadAWw1xhzv3dlBZRomCDx0zOaWzuFFwGxjTK0xZjuwhYb3syWai9UYM88Y4w18uRjoZVU8LWnhnLbEcee0kTTMKX8F8JJV8UDklXR6AruafF2EQxOqiPQFTgKWBDbdHvjX+RknlEpofkrrLGPMXmj44wV0ty265l3F13+BnHZOoeVz6PT37veA95p8nSciK0XkPyIyya6gmmjuZ+3kczoJ2G+M2dxkW8jPaaQl/OZWYnHcuFMRSQReB35kjCkDHgdOAEYCe2n4V89uE4wxo4BzgdtEZLLdAbVGRKKBC4FXA5uceE5b49j3roj8AvACLwQ27QVyjTEnAXcAL4pIsl3x0fLP2rHnFLiar3dOLDmnkZbwi4DeTb7uBeyxKZZmiUgUDcn+BWPMvwCMMfuNMT5jjB94Cgv/7WyJaTKlNQ2T340F9otID4DA5wP2RfgN5wJfGGP2gzPPaUBL59CR710RuQE4H7jGBIrNgRLJwcDjFTTUxgfYFWMrP2unnlMPDUvDvty4zapzGmkJfxmQLyJ5gR7fVcBcm2M6KlC3exrYYIz5Y5PtPZrsNh2bp4+Wlqe0ngvcENjtBqC9C+BY4Ws9Jqed0yZaOodzgatEJEZE8oB8YKkN8R0lIucAdwIXGmOqmmzPFBF34HE/GmLdZk+Urf6sHXdOA6YCG40xRY0bLDunVl2xtuoDmEbD6JetNKzKZXtMTWKbSMO/lF8CqwIf04DngTWB7XOBHjbH2Y+G0Q2rgXWN5xFIBz4CNgc+d7P7nAbiigcOAilNttl+Tmn4A7QXqKeht3lja+cQ+EXgfVsAnOuAWLfQUANvfK8+Edj30sD7YjXwBXCBzXG2+LN22jkNbJ8FzDxmX0vOqU6toJRSXUSklXSUUkq1QBO+Ukp1EZrwlVKqi9CEr5RSXYQmfKWU6iI04SulVBehCV8ppbqI/wcMwpdlcK8T9gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_all_new[100,:])\n",
    "plt.plot(np.arange(30),X_all[100,0,:])\n",
    "plt.plot(np.arange(30)+30,X_all[100,1,:])\n",
    "plt.plot(np.arange(30)+30*2,X_all[100,2,:])\n",
    "plt.plot(np.arange(30)+30*3,X_all[100,3,:])\n",
    "plt.plot(np.arange(30)+30*4,X_all[100,4,:])\n",
    "plt.plot(np.arange(30)+30*5,X_all[100,5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** 0\n",
      "validation years [1981, 1982]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1983}\n",
      "Recall: [0.64 0.55 0.73]\n",
      "Precision: [0.64 0.57 0.7 ]\n",
      "F1-score: [0.64 0.56 0.71]\n",
      "Accuracy: 0.64\n",
      "Brier score:0.14715774103484186\n",
      "Brier climat:0.21363214285714288\n",
      "Brier skill score:0.3111629220830915\n",
      "Recall: [0.64 0.55 0.73]\n",
      "Precision: [0.64 0.57 0.7 ]\n",
      "F1-score: [0.64 0.56 0.71]\n",
      "Accuracy: 0.64\n",
      "Brier score:0.19019389491360136\n",
      "Brier climat:0.22786205357142864\n",
      "Brier skill score:0.16531124014477172\n",
      "Recall: [0.64 0.55 0.73]\n",
      "Precision: [0.64 0.57 0.7 ]\n",
      "F1-score: [0.64 0.56 0.71]\n",
      "Accuracy: 0.64\n",
      "Brier score:0.14054870775692505\n",
      "Brier climat:0.2252058035714286\n",
      "Brier skill score:0.37590992093439923\n",
      "Recall: [0.72 0.38 0.62]\n",
      "Precision: [0.58 0.35 0.76]\n",
      "F1-score: [0.64 0.37 0.68]\n",
      "Accuracy: 0.59\n",
      "Brier score:0.18335401285175432\n",
      "Brier climat:0.2124937500000001\n",
      "Brier skill score:0.13713220811551285\n",
      "Recall: [0.72 0.38 0.62]\n",
      "Precision: [0.58 0.35 0.76]\n",
      "F1-score: [0.64 0.37 0.68]\n",
      "Accuracy: 0.59\n",
      "Brier score:0.16959601628361715\n",
      "Brier climat:0.18593125000000005\n",
      "Brier skill score:0.0878563109557049\n",
      "Recall: [0.72 0.38 0.62]\n",
      "Precision: [0.58 0.35 0.76]\n",
      "F1-score: [0.64 0.37 0.68]\n",
      "Accuracy: 0.59\n",
      "Brier score:0.23603916022499588\n",
      "Brier climat:0.26827500000000004\n",
      "Brier skill score:0.12015968604977789\n",
      "******** 1\n",
      "validation years [1982, 1983]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981}\n",
      "Recall: [0.64 0.54 0.7 ]\n",
      "Precision: [0.62 0.57 0.68]\n",
      "F1-score: [0.63 0.56 0.69]\n",
      "Accuracy: 0.62\n",
      "Brier score:0.1468538266974532\n",
      "Brier climat:0.2132526785714286\n",
      "Brier skill score:0.3113623346669253\n",
      "Recall: [0.64 0.54 0.7 ]\n",
      "Precision: [0.62 0.57 0.68]\n",
      "F1-score: [0.63 0.56 0.69]\n",
      "Accuracy: 0.62\n",
      "Brier score:0.18629624656613175\n",
      "Brier climat:0.22653392857142865\n",
      "Brier skill score:0.17762320310712099\n",
      "Recall: [0.64 0.54 0.7 ]\n",
      "Precision: [0.62 0.57 0.68]\n",
      "F1-score: [0.63 0.56 0.69]\n",
      "Accuracy: 0.62\n",
      "Brier score:0.14809106925922766\n",
      "Brier climat:0.2269133928571429\n",
      "Brier skill score:0.34736743655998814\n",
      "Recall: [0.66 0.06 0.73]\n",
      "Precision: [0.49 0.08 0.76]\n",
      "F1-score: [0.56 0.07 0.74]\n",
      "Accuracy: 0.52\n",
      "Brier score:0.19677916725575956\n",
      "Brier climat:0.21780625000000003\n",
      "Brier skill score:0.0965403093081143\n",
      "Recall: [0.66 0.06 0.73]\n",
      "Precision: [0.49 0.08 0.76]\n",
      "F1-score: [0.56 0.07 0.74]\n",
      "Accuracy: 0.52\n",
      "Brier score:0.24477145932999061\n",
      "Brier climat:0.20452500000000004\n",
      "Brier skill score:-0.1967801458500944\n",
      "Recall: [0.66 0.06 0.73]\n",
      "Precision: [0.49 0.08 0.76]\n",
      "F1-score: [0.56 0.07 0.74]\n",
      "Accuracy: 0.52\n",
      "Brier score:0.1355960649856306\n",
      "Brier climat:0.2443687500000001\n",
      "Brier skill score:0.44511700049359615\n",
      "******** 2\n",
      "validation years [1983, 1984]\n",
      "train years {1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982}\n",
      "Recall: [0.67 0.51 0.69]\n",
      "Precision: [0.63 0.55 0.68]\n",
      "F1-score: [0.65 0.53 0.69]\n",
      "Accuracy: 0.62\n",
      "Brier score:0.14451778276968816\n",
      "Brier climat:0.21211428571428576\n",
      "Brier skill score:0.3186796340329897\n",
      "Recall: [0.67 0.51 0.69]\n",
      "Precision: [0.63 0.55 0.68]\n",
      "F1-score: [0.65 0.53 0.69]\n",
      "Accuracy: 0.62\n",
      "Brier score:0.18650565316912365\n",
      "Brier climat:0.22198035714285716\n",
      "Brier skill score:0.15981010405755625\n",
      "Recall: [0.67 0.51 0.69]\n",
      "Precision: [0.63 0.55 0.68]\n",
      "F1-score: [0.65 0.53 0.69]\n",
      "Accuracy: 0.62\n",
      "Brier score:0.15279619903754796\n",
      "Brier climat:0.23260535714285718\n",
      "Brier skill score:0.3431097163264969\n",
      "Recall: [0.62 0.43 0.71]\n",
      "Precision: [0.48 0.55 0.75]\n",
      "F1-score: [0.54 0.49 0.73]\n",
      "Accuracy: 0.55\n",
      "Brier score:0.23748067213994278\n",
      "Brier climat:0.2337437500000001\n",
      "Brier skill score:-0.01598726015109575\n",
      "Recall: [0.62 0.43 0.71]\n",
      "Precision: [0.48 0.55 0.75]\n",
      "F1-score: [0.54 0.49 0.73]\n",
      "Accuracy: 0.55\n",
      "Brier score:0.2664236800651288\n",
      "Brier climat:0.26827500000000004\n",
      "Brier skill score:0.006900829130076391\n",
      "Recall: [0.62 0.43 0.71]\n",
      "Precision: [0.48 0.55 0.75]\n",
      "F1-score: [0.54 0.49 0.73]\n",
      "Accuracy: 0.55\n",
      "Brier score:0.07623635645704165\n",
      "Brier climat:0.16468125000000003\n",
      "Brier skill score:0.5370671739676397\n",
      "******** 3\n",
      "validation years [1984, 1985]\n",
      "train years {1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "Recall: [0.68 0.47 0.71]\n",
      "Precision: [0.64 0.54 0.67]\n",
      "F1-score: [0.66 0.5  0.69]\n",
      "Accuracy: 0.62\n",
      "Brier score:0.1463975267912096\n",
      "Brier climat:0.21609866071428577\n",
      "Brier skill score:0.3225431092108031\n",
      "Recall: [0.68 0.47 0.71]\n",
      "Precision: [0.64 0.54 0.67]\n",
      "F1-score: [0.66 0.5  0.69]\n",
      "Accuracy: 0.62\n",
      "Brier score:0.18594793254177594\n",
      "Brier climat:0.21799598214285718\n",
      "Brier skill score:0.14701211135203174\n",
      "Recall: [0.68 0.47 0.71]\n",
      "Precision: [0.64 0.54 0.67]\n",
      "F1-score: [0.66 0.5  0.69]\n",
      "Accuracy: 0.62\n",
      "Brier score:0.15177449817253158\n",
      "Brier climat:0.23260535714285718\n",
      "Brier skill score:0.34750213822754916\n",
      "Recall: [0.19 0.49 0.76]\n",
      "Precision: [0.14 0.63 0.55]\n",
      "F1-score: [0.16 0.56 0.64]\n",
      "Accuracy: 0.48\n",
      "Brier score:0.21792846006478567\n",
      "Brier climat:0.17796250000000002\n",
      "Brier skill score:-0.22457517771882074\n",
      "Recall: [0.19 0.49 0.76]\n",
      "Precision: [0.14 0.63 0.55]\n",
      "F1-score: [0.16 0.56 0.64]\n",
      "Accuracy: 0.48\n",
      "Brier score:0.3040735206060725\n",
      "Brier climat:0.32405625000000005\n",
      "Brier skill score:0.061664385099585384\n",
      "Recall: [0.19 0.49 0.76]\n",
      "Precision: [0.14 0.63 0.55]\n",
      "F1-score: [0.16 0.56 0.64]\n",
      "Accuracy: 0.48\n",
      "Brier score:0.09939622609061188\n",
      "Brier climat:0.16468125000000006\n",
      "Brier skill score:0.39643264736810147\n",
      "******** 4\n",
      "validation years [1985, 1986]\n",
      "train years {1984, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "Recall: [0.68 0.54 0.71]\n",
      "Precision: [0.66 0.58 0.67]\n",
      "F1-score: [0.67 0.56 0.69]\n",
      "Accuracy: 0.64\n",
      "Brier score:0.13608949385976093\n",
      "Brier climat:0.21078616071428577\n",
      "Brier skill score:0.3543717794441633\n",
      "Recall: [0.68 0.54 0.71]\n",
      "Precision: [0.66 0.58 0.67]\n",
      "F1-score: [0.67 0.56 0.69]\n",
      "Accuracy: 0.64\n",
      "Brier score:0.18511513983025646\n",
      "Brier climat:0.22368794642857148\n",
      "Brier skill score:0.17244025533862273\n",
      "Recall: [0.68 0.54 0.71]\n",
      "Precision: [0.66 0.58 0.67]\n",
      "F1-score: [0.67 0.56 0.69]\n",
      "Accuracy: 0.64\n",
      "Brier score:0.14601595088678945\n",
      "Brier climat:0.23222589285714287\n",
      "Brier skill score:0.3712331166421081\n",
      "Recall: [0.09 0.39 0.09]\n",
      "Precision: [0.15 0.33 0.06]\n",
      "F1-score: [0.11 0.36 0.07]\n",
      "Accuracy: 0.21\n",
      "Brier score:0.3369568582557555\n",
      "Brier climat:0.25233750000000005\n",
      "Brier skill score:-0.3353419854589803\n",
      "Recall: [0.09 0.39 0.09]\n",
      "Precision: [0.15 0.33 0.06]\n",
      "F1-score: [0.11 0.36 0.07]\n",
      "Accuracy: 0.21\n",
      "Brier score:0.2729813193516073\n",
      "Brier climat:0.2443687500000001\n",
      "Brier skill score:-0.11708767733847791\n",
      "Recall: [0.09 0.39 0.09]\n",
      "Precision: [0.15 0.33 0.06]\n",
      "F1-score: [0.11 0.36 0.07]\n",
      "Accuracy: 0.21\n",
      "Brier score:0.1931669639638256\n",
      "Brier climat:0.16999375\n",
      "Brier skill score:-0.13631803500908468\n",
      "******** 5\n",
      "validation years [1986, 1987]\n",
      "train years {1984, 1985, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "Recall: [0.66 0.59 0.74]\n",
      "Precision: [0.7 0.6 0.7]\n",
      "F1-score: [0.68 0.59 0.72]\n",
      "Accuracy: 0.67\n",
      "Brier score:0.13152549935164598\n",
      "Brier climat:0.20907857142857145\n",
      "Brier skill score:0.3709278839386958\n",
      "Recall: [0.66 0.59 0.74]\n",
      "Precision: [0.7 0.6 0.7]\n",
      "F1-score: [0.68 0.59 0.72]\n",
      "Accuracy: 0.67\n",
      "Brier score:0.18247184516488993\n",
      "Brier climat:0.22729285714285716\n",
      "Brier skill score:0.1971949868613624\n",
      "Recall: [0.66 0.59 0.74]\n",
      "Precision: [0.7 0.6 0.7]\n",
      "F1-score: [0.68 0.59 0.72]\n",
      "Accuracy: 0.67\n",
      "Brier score:0.13507091445122507\n",
      "Brier climat:0.23032857142857147\n",
      "Brier skill score:0.4135729075491067\n",
      "Recall: [0.08 0.03 0.58]\n",
      "Precision: [0.5  0.03 0.22]\n",
      "F1-score: [0.14 0.03 0.32]\n",
      "Accuracy: 0.2\n",
      "Brier score:0.383427409699503\n",
      "Brier climat:0.2762437500000001\n",
      "Brier skill score:-0.3880039266028745\n",
      "Recall: [0.08 0.03 0.58]\n",
      "Precision: [0.5  0.03 0.22]\n",
      "F1-score: [0.14 0.03 0.32]\n",
      "Accuracy: 0.2\n",
      "Brier score:0.2704136635200244\n",
      "Brier climat:0.19390000000000004\n",
      "Brier skill score:-0.3946037314080677\n",
      "Recall: [0.08 0.03 0.58]\n",
      "Precision: [0.5  0.03 0.22]\n",
      "F1-score: [0.14 0.03 0.32]\n",
      "Accuracy: 0.2\n",
      "Brier score:0.3839119995803413\n",
      "Brier climat:0.19655625000000007\n",
      "Brier skill score:-0.9531915142883585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** 6\n",
      "validation years [1987, 1988]\n",
      "train years {1984, 1985, 1986, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "Recall: [0.63 0.54 0.77]\n",
      "Precision: [0.66 0.58 0.7 ]\n",
      "F1-score: [0.64 0.56 0.73]\n",
      "Accuracy: 0.65\n",
      "Brier score:0.14308631257568147\n",
      "Brier climat:0.21192455357142861\n",
      "Brier skill score:0.3248242822063815\n",
      "Recall: [0.63 0.54 0.77]\n",
      "Precision: [0.66 0.58 0.7 ]\n",
      "F1-score: [0.64 0.56 0.73]\n",
      "Accuracy: 0.65\n",
      "Brier score:0.180278482320198\n",
      "Brier climat:0.22008303571428575\n",
      "Brier skill score:0.1808615246736348\n",
      "Recall: [0.63 0.54 0.77]\n",
      "Precision: [0.66 0.58 0.7 ]\n",
      "F1-score: [0.64 0.56 0.73]\n",
      "Accuracy: 0.65\n",
      "Brier score:0.13706282602284364\n",
      "Brier climat:0.23469241071428576\n",
      "Brier skill score:0.41598952601111694\n",
      "Recall: [0.44 0.04 1.  ]\n",
      "Precision: [0.78 1.   0.1 ]\n",
      "F1-score: [0.56 0.08 0.19]\n",
      "Accuracy: 0.27\n",
      "Brier score:0.22319753157885916\n",
      "Brier climat:0.23640000000000005\n",
      "Brier skill score:0.05584800516557065\n",
      "Recall: [0.44 0.04 1.  ]\n",
      "Precision: [0.78 1.   0.1 ]\n",
      "F1-score: [0.56 0.08 0.19]\n",
      "Accuracy: 0.27\n",
      "Brier score:0.3550648006087407\n",
      "Brier climat:0.2948375000000001\n",
      "Brier skill score:-0.20427286423450397\n",
      "Recall: [0.44 0.04 1.  ]\n",
      "Precision: [0.78 1.   0.1 ]\n",
      "F1-score: [0.56 0.08 0.19]\n",
      "Accuracy: 0.27\n",
      "Brier score:0.36214212508638133\n",
      "Brier climat:0.13546250000000004\n",
      "Brier skill score:-1.6733754735545352\n",
      "******** 7\n",
      "validation years [1988, 1989]\n",
      "train years {1984, 1985, 1986, 1987, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "Recall: [0.62 0.54 0.75]\n",
      "Precision: [0.64 0.56 0.7 ]\n",
      "F1-score: [0.63 0.55 0.73]\n",
      "Accuracy: 0.64\n",
      "Brier score:0.15245803792771792\n",
      "Brier climat:0.21515\n",
      "Brier skill score:0.29138722785164806\n",
      "Recall: [0.62 0.54 0.75]\n",
      "Precision: [0.64 0.56 0.7 ]\n",
      "F1-score: [0.63 0.55 0.73]\n",
      "Accuracy: 0.64\n",
      "Brier score:0.1829306655632284\n",
      "Brier climat:0.22273928571428575\n",
      "Brier skill score:0.17872294069453487\n",
      "Recall: [0.62 0.54 0.75]\n",
      "Precision: [0.64 0.56 0.7 ]\n",
      "F1-score: [0.63 0.55 0.73]\n",
      "Accuracy: 0.64\n",
      "Brier score:0.13814448136213148\n",
      "Brier climat:0.2288107142857143\n",
      "Brier skill score:0.39624994487962895\n",
      "Recall: [1.   0.14 0.1 ]\n",
      "Precision: [0.54 0.25 0.1 ]\n",
      "F1-score: [0.7  0.18 0.1 ]\n",
      "Accuracy: 0.34\n",
      "Brier score:0.10916723675683068\n",
      "Brier climat:0.19124375000000007\n",
      "Brier skill score:0.4291722644173698\n",
      "Recall: [1.   0.14 0.1 ]\n",
      "Precision: [0.54 0.25 0.1 ]\n",
      "F1-score: [0.7  0.18 0.1 ]\n",
      "Accuracy: 0.34\n",
      "Brier score:0.3355664162321158\n",
      "Brier climat:0.2576500000000001\n",
      "Brier skill score:-0.30241186195270986\n",
      "Recall: [1.   0.14 0.1 ]\n",
      "Precision: [0.54 0.25 0.1 ]\n",
      "F1-score: [0.7  0.18 0.1 ]\n",
      "Accuracy: 0.34\n",
      "Brier score:0.29659323190718156\n",
      "Brier climat:0.21780625\n",
      "Brier skill score:-0.3617296652744426\n",
      "******** 8\n",
      "validation years [1989, 1990]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "Recall: [0.65 0.56 0.74]\n",
      "Precision: [0.65 0.59 0.71]\n",
      "F1-score: [0.65 0.58 0.72]\n",
      "Accuracy: 0.65\n",
      "Brier score:0.1522911529011554\n",
      "Brier climat:0.21837544642857146\n",
      "Brier skill score:0.30261778330940514\n",
      "Recall: [0.65 0.56 0.74]\n",
      "Precision: [0.65 0.59 0.71]\n",
      "F1-score: [0.65 0.58 0.72]\n",
      "Accuracy: 0.65\n",
      "Brier score:0.18192431500536296\n",
      "Brier climat:0.22217008928571433\n",
      "Brier skill score:0.18114848137183148\n",
      "Recall: [0.65 0.56 0.74]\n",
      "Precision: [0.65 0.59 0.71]\n",
      "F1-score: [0.65 0.58 0.72]\n",
      "Accuracy: 0.65\n",
      "Brier score:0.13791438212186244\n",
      "Brier climat:0.2261544642857143\n",
      "Brier skill score:0.39017616761424156\n",
      "Recall: [1.   0.41 0.04]\n",
      "Precision: [0.25 0.36 0.5 ]\n",
      "F1-score: [0.39 0.38 0.07]\n",
      "Accuracy: 0.31\n",
      "Brier score:0.13386342193008977\n",
      "Brier climat:0.14608750000000004\n",
      "Brier skill score:0.08367641358713285\n",
      "Recall: [1.   0.41 0.04]\n",
      "Precision: [0.25 0.36 0.5 ]\n",
      "F1-score: [0.39 0.38 0.07]\n",
      "Accuracy: 0.31\n",
      "Brier score:0.3350121804148667\n",
      "Brier climat:0.26561875\n",
      "Brier skill score:-0.2612520027854459\n",
      "Recall: [1.   0.41 0.04]\n",
      "Precision: [0.25 0.36 0.5 ]\n",
      "F1-score: [0.39 0.38 0.07]\n",
      "Accuracy: 0.31\n",
      "Brier score:0.3006220734550412\n",
      "Brier climat:0.25499375\n",
      "Brier skill score:-0.17893898754397397\n",
      "******** 9\n",
      "validation years [1990, 1991]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "Recall: [0.66 0.56 0.73]\n",
      "Precision: [0.66 0.6  0.68]\n",
      "F1-score: [0.66 0.58 0.7 ]\n",
      "Accuracy: 0.65\n",
      "Brier score:0.14612956300306815\n",
      "Brier climat:0.21382187500000002\n",
      "Brier skill score:0.3165827256773044\n",
      "Recall: [0.66 0.56 0.73]\n",
      "Precision: [0.66 0.6  0.68]\n",
      "F1-score: [0.66 0.58 0.7 ]\n",
      "Accuracy: 0.65\n",
      "Brier score:0.17938298015319035\n",
      "Brier climat:0.2189446428571429\n",
      "Brier skill score:0.18069253573729027\n",
      "Recall: [0.66 0.56 0.73]\n",
      "Precision: [0.66 0.6  0.68]\n",
      "F1-score: [0.66 0.58 0.7 ]\n",
      "Accuracy: 0.65\n",
      "Brier score:0.14523624554908915\n",
      "Brier climat:0.23393348214285717\n",
      "Brier skill score:0.37915580010732663\n",
      "Recall: [0.37 0.33 0.  ]\n",
      "Precision: [0.33 0.62 0.  ]\n",
      "F1-score: [0.35 0.43 0.  ]\n",
      "Accuracy: 0.3\n",
      "Brier score:0.23849851129309668\n",
      "Brier climat:0.20983750000000007\n",
      "Brier skill score:-0.13658669824553105\n",
      "Recall: [0.37 0.33 0.  ]\n",
      "Precision: [0.33 0.62 0.  ]\n",
      "F1-score: [0.35 0.43 0.  ]\n",
      "Accuracy: 0.3\n",
      "Brier score:0.3426231955206597\n",
      "Brier climat:0.3107750000000001\n",
      "Brier skill score:-0.10247991479578333\n",
      "Recall: [0.37 0.33 0.  ]\n",
      "Precision: [0.33 0.62 0.  ]\n",
      "F1-score: [0.35 0.43 0.  ]\n",
      "Accuracy: 0.3\n",
      "Brier score:0.22927198297728882\n",
      "Brier climat:0.14608750000000004\n",
      "Brier skill score:-0.5694154734476855\n",
      "******** 10\n",
      "validation years [1991, 1992]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "Recall: [0.67 0.51 0.73]\n",
      "Precision: [0.62 0.59 0.69]\n",
      "F1-score: [0.65 0.54 0.71]\n",
      "Accuracy: 0.64\n",
      "Brier score:0.14409401738046737\n",
      "Brier climat:0.21382187500000002\n",
      "Brier skill score:0.3261025450250712\n",
      "Recall: [0.67 0.51 0.73]\n",
      "Precision: [0.62 0.59 0.69]\n",
      "F1-score: [0.65 0.54 0.71]\n",
      "Accuracy: 0.64\n",
      "Brier score:0.18162544719942658\n",
      "Brier climat:0.21932410714285716\n",
      "Brier skill score:0.17188561911653188\n",
      "Recall: [0.67 0.51 0.73]\n",
      "Precision: [0.62 0.59 0.69]\n",
      "F1-score: [0.65 0.54 0.71]\n",
      "Accuracy: 0.64\n",
      "Brier score:0.14197549291185466\n",
      "Brier climat:0.2335540178571429\n",
      "Brier skill score:0.39210853996656025\n",
      "Recall: [0.29 0.32 0.  ]\n",
      "Precision: [0.24 0.65 0.  ]\n",
      "F1-score: [0.26 0.43 0.  ]\n",
      "Accuracy: 0.27\n",
      "Brier score:0.2637226173785795\n",
      "Brier climat:0.20983750000000012\n",
      "Brier skill score:-0.2567945070760915\n",
      "Recall: [0.29 0.32 0.  ]\n",
      "Precision: [0.24 0.65 0.  ]\n",
      "F1-score: [0.26 0.43 0.  ]\n",
      "Accuracy: 0.27\n",
      "Brier score:0.30525838572528574\n",
      "Brier climat:0.3054625\n",
      "Brier skill score:0.0006682138551026284\n",
      "Recall: [0.29 0.32 0.  ]\n",
      "Precision: [0.24 0.65 0.  ]\n",
      "F1-score: [0.26 0.43 0.  ]\n",
      "Accuracy: 0.27\n",
      "Brier score:0.25118130372804887\n",
      "Brier climat:0.15140000000000003\n",
      "Brier skill score:-0.6590574882962272\n",
      "******** 11\n",
      "validation years [1992, 1993]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "Recall: [0.66 0.52 0.72]\n",
      "Precision: [0.64 0.54 0.71]\n",
      "F1-score: [0.65 0.53 0.72]\n",
      "Accuracy: 0.63\n",
      "Brier score:0.14666612885505798\n",
      "Brier climat:0.21571919642857146\n",
      "Brier skill score:0.32010627110034784\n",
      "Recall: [0.66 0.52 0.72]\n",
      "Precision: [0.64 0.54 0.71]\n",
      "F1-score: [0.65 0.53 0.72]\n",
      "Accuracy: 0.63\n",
      "Brier score:0.1861558026908263\n",
      "Brier climat:0.22198035714285716\n",
      "Brier skill score:0.1613861465633002\n",
      "Recall: [0.66 0.52 0.72]\n",
      "Precision: [0.64 0.54 0.71]\n",
      "F1-score: [0.65 0.53 0.72]\n",
      "Accuracy: 0.63\n",
      "Brier score:0.13990646500397258\n",
      "Brier climat:0.22900044642857148\n",
      "Brier skill score:0.3890559289909008\n",
      "Recall: [0.89 0.37 0.12]\n",
      "Precision: [0.33 0.51 0.56]\n",
      "F1-score: [0.48 0.43 0.2 ]\n",
      "Accuracy: 0.41\n",
      "Brier score:0.2186648082940783\n",
      "Brier climat:0.18327500000000002\n",
      "Brier skill score:-0.19309675784519587\n",
      "Recall: [0.89 0.37 0.12]\n",
      "Precision: [0.33 0.51 0.56]\n",
      "F1-score: [0.48 0.43 0.2 ]\n",
      "Accuracy: 0.41\n",
      "Brier score:0.263370900021887\n",
      "Brier climat:0.2682750000000001\n",
      "Brier skill score:0.01828012292652348\n",
      "Recall: [0.89 0.37 0.12]\n",
      "Precision: [0.33 0.51 0.56]\n",
      "F1-score: [0.48 0.43 0.2 ]\n",
      "Accuracy: 0.41\n",
      "Brier score:0.21868213375382067\n",
      "Brier climat:0.21515000000000006\n",
      "Brier skill score:-0.01641707531406289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** 12\n",
      "validation years [1993, 1994]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "Recall: [0.62 0.56 0.7 ]\n",
      "Precision: [0.63 0.57 0.68]\n",
      "F1-score: [0.62 0.57 0.69]\n",
      "Accuracy: 0.62\n",
      "Brier score:0.15310888682729057\n",
      "Brier climat:0.21647812500000002\n",
      "Brier skill score:0.29272813672378883\n",
      "Recall: [0.62 0.56 0.7 ]\n",
      "Precision: [0.63 0.57 0.68]\n",
      "F1-score: [0.62 0.57 0.69]\n",
      "Accuracy: 0.62\n",
      "Brier score:0.1868022953521836\n",
      "Brier climat:0.2282415178571429\n",
      "Brier skill score:0.18155865284289008\n",
      "Recall: [0.62 0.56 0.7 ]\n",
      "Precision: [0.63 0.57 0.68]\n",
      "F1-score: [0.62 0.57 0.69]\n",
      "Accuracy: 0.62\n",
      "Brier score:0.138706997155053\n",
      "Brier climat:0.22198035714285716\n",
      "Brier skill score:0.3751384179196223\n",
      "Recall: [0.88 0.15 0.73]\n",
      "Precision: [0.66 0.17 0.77]\n",
      "F1-score: [0.75 0.16 0.75]\n",
      "Accuracy: 0.63\n",
      "Brier score:0.08669744638131531\n",
      "Brier climat:0.17265000000000008\n",
      "Brier skill score:0.49784276639840563\n",
      "Recall: [0.88 0.15 0.73]\n",
      "Precision: [0.66 0.17 0.77]\n",
      "F1-score: [0.75 0.16 0.75]\n",
      "Accuracy: 0.63\n",
      "Brier score:0.21498872929141466\n",
      "Brier climat:0.18061875000000005\n",
      "Brier skill score:-0.19029020681083564\n",
      "Recall: [0.88 0.15 0.73]\n",
      "Precision: [0.66 0.17 0.77]\n",
      "F1-score: [0.75 0.16 0.75]\n",
      "Accuracy: 0.63\n",
      "Brier score:0.22741651329580745\n",
      "Brier climat:0.31343125000000005\n",
      "Brier skill score:0.27442935796667556\n",
      "******** 13\n",
      "validation years [1994, 1995]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "Recall: [0.62 0.53 0.71]\n",
      "Precision: [0.63 0.56 0.67]\n",
      "F1-score: [0.62 0.54 0.69]\n",
      "Accuracy: 0.62\n",
      "Brier score:0.15130997013895972\n",
      "Brier climat:0.21477053571428575\n",
      "Brier skill score:0.29548078075173734\n",
      "Recall: [0.62 0.53 0.71]\n",
      "Precision: [0.63 0.56 0.67]\n",
      "F1-score: [0.62 0.54 0.69]\n",
      "Accuracy: 0.62\n",
      "Brier score:0.1868708136683688\n",
      "Brier climat:0.2288107142857143\n",
      "Brier skill score:0.18329517806135365\n",
      "Recall: [0.62 0.53 0.71]\n",
      "Precision: [0.63 0.56 0.67]\n",
      "F1-score: [0.62 0.54 0.69]\n",
      "Accuracy: 0.62\n",
      "Brier score:0.13960215806847034\n",
      "Brier climat:0.22311875000000003\n",
      "Brier skill score:0.3743145384757206\n",
      "Recall: [0.67 0.21 0.75]\n",
      "Precision: [0.67 0.19 0.77]\n",
      "F1-score: [0.67 0.2  0.76]\n",
      "Accuracy: 0.62\n",
      "Brier score:0.1303999104524223\n",
      "Brier climat:0.19655625000000002\n",
      "Brier skill score:0.336577135286096\n",
      "Recall: [0.67 0.21 0.75]\n",
      "Precision: [0.67 0.19 0.77]\n",
      "F1-score: [0.67 0.2  0.76]\n",
      "Accuracy: 0.62\n",
      "Brier score:0.21310148394935047\n",
      "Brier climat:0.17265000000000003\n",
      "Brier skill score:-0.23429761916797243\n",
      "Recall: [0.67 0.21 0.75]\n",
      "Precision: [0.67 0.19 0.77]\n",
      "F1-score: [0.67 0.2  0.76]\n",
      "Accuracy: 0.62\n",
      "Brier score:0.21452052375154598\n",
      "Brier climat:0.2974937500000001\n",
      "Brier skill score:0.2789074602355649\n",
      "******** 14\n",
      "validation years [1995, 1996]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "Recall: [0.62 0.54 0.73]\n",
      "Precision: [0.64 0.55 0.7 ]\n",
      "F1-score: [0.63 0.54 0.71]\n",
      "Accuracy: 0.63\n",
      "Brier score:0.14184219759255642\n",
      "Brier climat:0.20812991071428574\n",
      "Brier skill score:0.31849200768037145\n",
      "Recall: [0.62 0.54 0.73]\n",
      "Precision: [0.64 0.55 0.7 ]\n",
      "F1-score: [0.63 0.54 0.71]\n",
      "Accuracy: 0.63\n",
      "Brier score:0.18697108189489903\n",
      "Brier climat:0.22805178571428572\n",
      "Brier skill score:0.18013761080939128\n",
      "Recall: [0.62 0.54 0.73]\n",
      "Precision: [0.64 0.55 0.7 ]\n",
      "F1-score: [0.63 0.54 0.71]\n",
      "Accuracy: 0.63\n",
      "Brier score:0.13838313974008368\n",
      "Brier climat:0.2305183035714286\n",
      "Brier skill score:0.3996869767124407\n",
      "Recall: [0.51 0.61 0.06]\n",
      "Precision: [0.57 0.3  0.18]\n",
      "F1-score: [0.54 0.4  0.09]\n",
      "Accuracy: 0.42\n",
      "Brier score:0.2914822301553481\n",
      "Brier climat:0.28952500000000003\n",
      "Brier skill score:-0.006760142147821613\n",
      "Recall: [0.51 0.61 0.06]\n",
      "Precision: [0.57 0.3  0.18]\n",
      "F1-score: [0.54 0.4  0.09]\n",
      "Accuracy: 0.42\n",
      "Brier score:0.2424292403643611\n",
      "Brier climat:0.18327500000000005\n",
      "Brier skill score:-0.3227621899569555\n",
      "Recall: [0.51 0.61 0.06]\n",
      "Precision: [0.57 0.3  0.18]\n",
      "F1-score: [0.54 0.4  0.09]\n",
      "Accuracy: 0.42\n",
      "Brier score:0.2315530583221984\n",
      "Brier climat:0.19390000000000002\n",
      "Brier skill score:-0.1941880264167013\n",
      "******** 15\n",
      "validation years [1996, 1997]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "Recall: [0.63 0.54 0.68]\n",
      "Precision: [0.64 0.55 0.66]\n",
      "F1-score: [0.64 0.55 0.67]\n",
      "Accuracy: 0.62\n",
      "Brier score:0.1495704494539886\n",
      "Brier climat:0.21439107142857144\n",
      "Brier skill score:0.3023475816537402\n",
      "Recall: [0.63 0.54 0.68]\n",
      "Precision: [0.64 0.55 0.66]\n",
      "F1-score: [0.64 0.55 0.67]\n",
      "Accuracy: 0.62\n",
      "Brier score:0.19491463304461787\n",
      "Brier climat:0.23051830357142863\n",
      "Brier skill score:0.15445051423336797\n",
      "Recall: [0.63 0.54 0.68]\n",
      "Precision: [0.64 0.55 0.66]\n",
      "F1-score: [0.64 0.55 0.67]\n",
      "Accuracy: 0.62\n",
      "Brier score:0.1459068851313066\n",
      "Brier climat:0.221790625\n",
      "Brier skill score:0.34214133202741737\n",
      "Recall: [0.51 0.87 0.82]\n",
      "Precision: [0.53 0.43 1.  ]\n",
      "F1-score: [0.52 0.58 0.9 ]\n",
      "Accuracy: 0.74\n",
      "Brier score:0.16089302171627956\n",
      "Brier climat:0.20186875000000012\n",
      "Brier skill score:0.2029820280936031\n",
      "Recall: [0.51 0.87 0.82]\n",
      "Precision: [0.53 0.43 1.  ]\n",
      "F1-score: [0.52 0.58 0.9 ]\n",
      "Accuracy: 0.74\n",
      "Brier score:0.11209354099179897\n",
      "Brier climat:0.14874375000000004\n",
      "Brier skill score:0.24639831258927558\n",
      "Recall: [0.51 0.87 0.82]\n",
      "Precision: [0.53 0.43 1.  ]\n",
      "F1-score: [0.52 0.58 0.9 ]\n",
      "Accuracy: 0.74\n",
      "Brier score:0.1203503673922183\n",
      "Brier climat:0.3160875\n",
      "Brier skill score:0.619249836225038\n",
      "******** 16\n",
      "validation years [1997, 1998]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "Recall: [0.62 0.54 0.66]\n",
      "Precision: [0.62 0.56 0.64]\n",
      "F1-score: [0.62 0.55 0.65]\n",
      "Accuracy: 0.61\n",
      "Brier score:0.1531477758480469\n",
      "Brier climat:0.21344241071428574\n",
      "Brier skill score:0.2824866654403998\n",
      "Recall: [0.62 0.54 0.66]\n",
      "Precision: [0.62 0.56 0.64]\n",
      "F1-score: [0.62 0.55 0.65]\n",
      "Accuracy: 0.61\n",
      "Brier score:0.19530491424482938\n",
      "Brier climat:0.2288107142857143\n",
      "Brier skill score:0.1464345764816174\n",
      "Recall: [0.62 0.54 0.66]\n",
      "Precision: [0.62 0.56 0.64]\n",
      "F1-score: [0.62 0.55 0.65]\n",
      "Accuracy: 0.61\n",
      "Brier score:0.15511635326738954\n",
      "Brier climat:0.22444687500000002\n",
      "Brier skill score:0.30889501906680805\n",
      "Recall: [0.68 0.46 1.  ]\n",
      "Precision: [0.68 0.46 1.  ]\n",
      "F1-score: [0.68 0.46 1.  ]\n",
      "Accuracy: 0.8\n",
      "Brier score:0.13541556901002116\n",
      "Brier climat:0.21515\n",
      "Brier skill score:0.37059926093413353\n",
      "Recall: [0.68 0.46 1.  ]\n",
      "Precision: [0.68 0.46 1.  ]\n",
      "F1-score: [0.68 0.46 1.  ]\n",
      "Accuracy: 0.8\n",
      "Brier score:0.13965363933300318\n",
      "Brier climat:0.17265000000000003\n",
      "Brier skill score:0.1911170614943345\n",
      "Recall: [0.68 0.46 1.  ]\n",
      "Precision: [0.68 0.46 1.  ]\n",
      "F1-score: [0.68 0.46 1.  ]\n",
      "Accuracy: 0.8\n",
      "Brier score:0.011351872365151245\n",
      "Brier climat:0.2789000000000001\n",
      "Brier skill score:0.9592976967904222\n",
      "******** 17\n",
      "validation years [1998, 1999]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "Recall: [0.6  0.55 0.73]\n",
      "Precision: [0.62 0.56 0.71]\n",
      "F1-score: [0.61 0.55 0.72]\n",
      "Accuracy: 0.63\n",
      "Brier score:0.14631868325242528\n",
      "Brier climat:0.20831964285714286\n",
      "Brier skill score:0.2976241642620101\n",
      "Recall: [0.6  0.55 0.73]\n",
      "Precision: [0.62 0.56 0.71]\n",
      "F1-score: [0.61 0.55 0.72]\n",
      "Accuracy: 0.63\n",
      "Brier score:0.18930465230069263\n",
      "Brier climat:0.2259647321428572\n",
      "Brier skill score:0.16223806031371169\n",
      "Recall: [0.6  0.55 0.73]\n",
      "Precision: [0.62 0.56 0.71]\n",
      "F1-score: [0.61 0.55 0.72]\n",
      "Accuracy: 0.63\n",
      "Brier score:0.1422840715023726\n",
      "Brier climat:0.23241562500000001\n",
      "Brier skill score:0.38780333076843443\n",
      "Recall: [0.49 0.44 0.  ]\n",
      "Precision: [0.61 0.3  0.  ]\n",
      "F1-score: [0.55 0.35 0.  ]\n",
      "Accuracy: 0.39\n",
      "Brier score:0.2909029568296807\n",
      "Brier climat:0.28686875000000006\n",
      "Brier skill score:-0.014062900994551208\n",
      "Recall: [0.49 0.44 0.  ]\n",
      "Precision: [0.61 0.3  0.  ]\n",
      "F1-score: [0.55 0.35 0.  ]\n",
      "Accuracy: 0.39\n",
      "Brier score:0.25383006513318357\n",
      "Brier climat:0.2124937500000001\n",
      "Brier skill score:-0.19452955737843336\n",
      "Recall: [0.49 0.44 0.  ]\n",
      "Precision: [0.61 0.3  0.  ]\n",
      "F1-score: [0.55 0.35 0.  ]\n",
      "Accuracy: 0.39\n",
      "Brier score:0.17438141421322367\n",
      "Brier climat:0.16733750000000003\n",
      "Brier skill score:-0.042094056701119875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** 18\n",
      "validation years [1999, 2000]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "Recall: [0.63 0.56 0.74]\n",
      "Precision: [0.64 0.58 0.7 ]\n",
      "F1-score: [0.64 0.57 0.72]\n",
      "Accuracy: 0.64\n",
      "Brier score:0.14634605794003938\n",
      "Brier climat:0.21268348214285715\n",
      "Brier skill score:0.31190679941124744\n",
      "Recall: [0.63 0.56 0.74]\n",
      "Precision: [0.64 0.58 0.7 ]\n",
      "F1-score: [0.64 0.57 0.72]\n",
      "Accuracy: 0.64\n",
      "Brier score:0.1853781056456987\n",
      "Brier climat:0.22577500000000006\n",
      "Brier skill score:0.1789254539001277\n",
      "Recall: [0.63 0.56 0.74]\n",
      "Precision: [0.64 0.58 0.7 ]\n",
      "F1-score: [0.64 0.57 0.72]\n",
      "Accuracy: 0.64\n",
      "Brier score:0.13634843237365843\n",
      "Brier climat:0.2282415178571429\n",
      "Brier skill score:0.40261336476477805\n",
      "Recall: [0.11 0.55 0.25]\n",
      "Precision: [0.71 0.3  0.23]\n",
      "F1-score: [0.2  0.39 0.24]\n",
      "Accuracy: 0.3\n",
      "Brier score:0.23893930902939478\n",
      "Brier climat:0.22577500000000006\n",
      "Brier skill score:-0.05830720420504809\n",
      "Recall: [0.11 0.55 0.25]\n",
      "Precision: [0.71 0.3  0.23]\n",
      "F1-score: [0.2  0.39 0.24]\n",
      "Accuracy: 0.3\n",
      "Brier score:0.2755578750780509\n",
      "Brier climat:0.21515000000000006\n",
      "Brier skill score:-0.28077097410202567\n",
      "Recall: [0.11 0.55 0.25]\n",
      "Precision: [0.71 0.3  0.23]\n",
      "F1-score: [0.2  0.39 0.24]\n",
      "Accuracy: 0.3\n",
      "Brier score:0.28178373467830853\n",
      "Brier climat:0.22577500000000006\n",
      "Brier skill score:-0.24807323520455515\n",
      "******** 19\n",
      "validation years [2000, 2001]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "Recall: [0.65 0.55 0.71]\n",
      "Precision: [0.64 0.58 0.68]\n",
      "F1-score: [0.64 0.56 0.7 ]\n",
      "Accuracy: 0.64\n",
      "Brier score:0.14857210920614008\n",
      "Brier climat:0.2142013392857143\n",
      "Brier skill score:0.30639038158409504\n",
      "Recall: [0.65 0.55 0.71]\n",
      "Precision: [0.64 0.58 0.68]\n",
      "F1-score: [0.64 0.56 0.7 ]\n",
      "Accuracy: 0.64\n",
      "Brier score:0.1855624705988362\n",
      "Brier climat:0.22406741071428574\n",
      "Brier skill score:0.17184533883219721\n",
      "Recall: [0.65 0.55 0.71]\n",
      "Precision: [0.64 0.58 0.68]\n",
      "F1-score: [0.64 0.56 0.7 ]\n",
      "Accuracy: 0.64\n",
      "Brier score:0.14297378254754595\n",
      "Brier climat:0.22843125000000003\n",
      "Brier skill score:0.3741058522091617\n",
      "Recall: [0.31 0.39 0.56]\n",
      "Precision: [0.69 0.38 0.39]\n",
      "F1-score: [0.42 0.38 0.46]\n",
      "Accuracy: 0.42\n",
      "Brier score:0.1689309820449451\n",
      "Brier climat:0.20452500000000007\n",
      "Brier skill score:0.17403260215159488\n",
      "Recall: [0.31 0.39 0.56]\n",
      "Precision: [0.69 0.38 0.39]\n",
      "F1-score: [0.42 0.38 0.46]\n",
      "Accuracy: 0.42\n",
      "Brier score:0.26439826364911595\n",
      "Brier climat:0.23905625000000005\n",
      "Brier skill score:-0.10600858019447679\n",
      "Recall: [0.31 0.39 0.56]\n",
      "Precision: [0.69 0.38 0.39]\n",
      "F1-score: [0.42 0.38 0.46]\n",
      "Accuracy: 0.42\n",
      "Brier score:0.22519721028561132\n",
      "Brier climat:0.22311875000000006\n",
      "Brier skill score:-0.00931548910887714\n",
      "******** 20\n",
      "validation years [2001, 2002]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "Recall: [0.65 0.55 0.7 ]\n",
      "Precision: [0.64 0.58 0.68]\n",
      "F1-score: [0.65 0.57 0.69]\n",
      "Accuracy: 0.64\n",
      "Brier score:0.14784252659358213\n",
      "Brier climat:0.21496026785714287\n",
      "Brier skill score:0.3122332416712724\n",
      "Recall: [0.65 0.55 0.7 ]\n",
      "Precision: [0.64 0.58 0.68]\n",
      "F1-score: [0.65 0.57 0.69]\n",
      "Accuracy: 0.64\n",
      "Brier score:0.18825205952924992\n",
      "Brier climat:0.2265339285714286\n",
      "Brier skill score:0.16898956056424896\n",
      "Recall: [0.65 0.55 0.7 ]\n",
      "Precision: [0.64 0.58 0.68]\n",
      "F1-score: [0.65 0.57 0.69]\n",
      "Accuracy: 0.64\n",
      "Brier score:0.1427631700229687\n",
      "Brier climat:0.22520580357142858\n",
      "Brier skill score:0.36607686054729727\n",
      "Recall: [0.34 0.25 0.78]\n",
      "Precision: [0.69 0.41 0.52]\n",
      "F1-score: [0.46 0.31 0.63]\n",
      "Accuracy: 0.52\n",
      "Brier score:0.1684847734941121\n",
      "Brier climat:0.19390000000000004\n",
      "Brier skill score:0.13107388605408943\n",
      "Recall: [0.34 0.25 0.78]\n",
      "Precision: [0.69 0.41 0.52]\n",
      "F1-score: [0.46 0.31 0.63]\n",
      "Accuracy: 0.52\n",
      "Brier score:0.2007905420040173\n",
      "Brier climat:0.20452500000000007\n",
      "Brier skill score:0.018259176120194454\n",
      "Recall: [0.34 0.25 0.78]\n",
      "Precision: [0.69 0.41 0.52]\n",
      "F1-score: [0.46 0.31 0.63]\n",
      "Accuracy: 0.52\n",
      "Brier score:0.23049051761517256\n",
      "Brier climat:0.26827500000000004\n",
      "Brier skill score:0.14084235349856478\n",
      "******** 21\n",
      "validation years [2002, 2003]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "Recall: [0.68 0.57 0.68]\n",
      "Precision: [0.66 0.58 0.69]\n",
      "F1-score: [0.67 0.58 0.69]\n",
      "Accuracy: 0.65\n",
      "Brier score:0.1465984097583169\n",
      "Brier climat:0.21742678571428573\n",
      "Brier skill score:0.3257573611424416\n",
      "Recall: [0.68 0.57 0.68]\n",
      "Precision: [0.66 0.58 0.69]\n",
      "F1-score: [0.67 0.58 0.69]\n",
      "Accuracy: 0.65\n",
      "Brier score:0.18399836214402865\n",
      "Brier climat:0.22368794642857148\n",
      "Brier skill score:0.17743282513980518\n",
      "Recall: [0.68 0.57 0.68]\n",
      "Precision: [0.66 0.58 0.69]\n",
      "F1-score: [0.67 0.58 0.69]\n",
      "Accuracy: 0.65\n",
      "Brier score:0.14559815546820182\n",
      "Brier climat:0.22558526785714286\n",
      "Brier skill score:0.3545759576799792\n",
      "Recall: [0.  0.1 0.9]\n",
      "Precision: [0.   0.36 0.55]\n",
      "F1-score: [0.   0.15 0.68]\n",
      "Accuracy: 0.45\n",
      "Brier score:0.16900703359553165\n",
      "Brier climat:0.15936875000000006\n",
      "Brier skill score:-0.060477876594574376\n",
      "Recall: [0.  0.1 0.9]\n",
      "Precision: [0.   0.36 0.55]\n",
      "F1-score: [0.   0.15 0.68]\n",
      "Accuracy: 0.45\n",
      "Brier score:0.2732044658563737\n",
      "Brier climat:0.24436875000000005\n",
      "Brier skill score:-0.11800083217012669\n",
      "Recall: [0.  0.1 0.9]\n",
      "Precision: [0.   0.36 0.55]\n",
      "F1-score: [0.   0.15 0.68]\n",
      "Accuracy: 0.45\n",
      "Brier score:0.17630360130662692\n",
      "Brier climat:0.2629625000000001\n",
      "Brier skill score:0.3295485047996317\n",
      "******** 22\n",
      "validation years [2003, 2004]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "Recall: [0.68 0.55 0.68]\n",
      "Precision: [0.65 0.58 0.69]\n",
      "F1-score: [0.66 0.56 0.69]\n",
      "Accuracy: 0.64\n",
      "Brier score:0.15322823352362439\n",
      "Brier climat:0.21989330357142858\n",
      "Brier skill score:0.30317007823819064\n",
      "Recall: [0.68 0.55 0.68]\n",
      "Precision: [0.65 0.58 0.69]\n",
      "F1-score: [0.66 0.56 0.69]\n",
      "Accuracy: 0.64\n",
      "Brier score:0.1839504031400933\n",
      "Brier climat:0.2225495535714286\n",
      "Brier skill score:0.17344070033798864\n",
      "Recall: [0.68 0.55 0.68]\n",
      "Precision: [0.65 0.58 0.69]\n",
      "F1-score: [0.66 0.56 0.69]\n",
      "Accuracy: 0.64\n",
      "Brier score:0.14486052875875963\n",
      "Brier climat:0.22425714285714288\n",
      "Brier skill score:0.35404274346329645\n",
      "Recall: [0.   0.07 0.91]\n",
      "Precision: [0.   0.31 0.6 ]\n",
      "F1-score: [0.   0.11 0.72]\n",
      "Accuracy: 0.49\n",
      "Brier score:0.08049916513051159\n",
      "Brier climat:0.12483750000000002\n",
      "Brier skill score:0.35516839787314247\n",
      "Recall: [0.   0.07 0.91]\n",
      "Precision: [0.   0.31 0.6 ]\n",
      "F1-score: [0.   0.11 0.72]\n",
      "Accuracy: 0.49\n",
      "Brier score:0.3032700359722697\n",
      "Brier climat:0.2603062500000001\n",
      "Brier skill score:-0.16505091972347818\n",
      "Recall: [0.   0.07 0.91]\n",
      "Precision: [0.   0.31 0.6 ]\n",
      "F1-score: [0.   0.11 0.72]\n",
      "Accuracy: 0.49\n",
      "Brier score:0.20753602253290315\n",
      "Brier climat:0.28155625000000006\n",
      "Brier skill score:0.26289676562710607\n",
      "******** 23\n",
      "validation years [2004, 2005]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "Recall: [0.65 0.56 0.68]\n",
      "Precision: [0.65 0.58 0.67]\n",
      "F1-score: [0.65 0.57 0.68]\n",
      "Accuracy: 0.63\n",
      "Brier score:0.14716776632308953\n",
      "Brier climat:0.2142013392857143\n",
      "Brier skill score:0.3129465632015095\n",
      "Recall: [0.65 0.56 0.68]\n",
      "Precision: [0.65 0.58 0.67]\n",
      "F1-score: [0.65 0.57 0.68]\n",
      "Accuracy: 0.63\n",
      "Brier score:0.1865317202842963\n",
      "Brier climat:0.22463660714285716\n",
      "Brier skill score:0.16962901702983857\n",
      "Recall: [0.65 0.56 0.68]\n",
      "Precision: [0.65 0.58 0.67]\n",
      "F1-score: [0.65 0.57 0.68]\n",
      "Accuracy: 0.63\n",
      "Brier score:0.14818740860522814\n",
      "Brier climat:0.2278620535714286\n",
      "Brier skill score:0.3496617524392872\n",
      "Recall: [0.36 0.37 1.  ]\n",
      "Precision: [0.54 0.61 0.61]\n",
      "F1-score: [0.43 0.46 0.75]\n",
      "Accuracy: 0.59\n",
      "Brier score:0.17229347534794398\n",
      "Brier climat:0.204525\n",
      "Brier skill score:0.15759210195358042\n",
      "Recall: [0.36 0.37 1.  ]\n",
      "Precision: [0.54 0.61 0.61]\n",
      "F1-score: [0.43 0.46 0.75]\n",
      "Accuracy: 0.59\n",
      "Brier score:0.23212859224947197\n",
      "Brier climat:0.23108750000000006\n",
      "Brier skill score:-0.004505186344877732\n",
      "Recall: [0.36 0.37 1.  ]\n",
      "Precision: [0.54 0.61 0.61]\n",
      "F1-score: [0.43 0.46 0.75]\n",
      "Accuracy: 0.59\n",
      "Brier score:0.1523091282609704\n",
      "Brier climat:0.2310875000000001\n",
      "Brier skill score:0.3409027824483353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** 24\n",
      "validation years [2005, 2006]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "Recall: [0.65 0.56 0.66]\n",
      "Precision: [0.64 0.57 0.66]\n",
      "F1-score: [0.65 0.56 0.66]\n",
      "Accuracy: 0.62\n",
      "Brier score:0.14562322965265012\n",
      "Brier climat:0.21382187500000002\n",
      "Brier skill score:0.3189507404111478\n",
      "Recall: [0.65 0.56 0.66]\n",
      "Precision: [0.64 0.57 0.66]\n",
      "F1-score: [0.65 0.56 0.66]\n",
      "Accuracy: 0.62\n",
      "Brier score:0.1902489244398648\n",
      "Brier climat:0.22748258928571433\n",
      "Brier skill score:0.16367698716091483\n",
      "Recall: [0.65 0.56 0.66]\n",
      "Precision: [0.64 0.57 0.66]\n",
      "F1-score: [0.65 0.56 0.66]\n",
      "Accuracy: 0.62\n",
      "Brier score:0.14907625002347166\n",
      "Brier climat:0.22539553571428575\n",
      "Brier skill score:0.33860158520423134\n",
      "Recall: [0.37 0.39 0.86]\n",
      "Precision: [0.42 0.55 0.7 ]\n",
      "F1-score: [0.39 0.45 0.77]\n",
      "Accuracy: 0.6\n",
      "Brier score:0.20709012873296412\n",
      "Brier climat:0.20983750000000007\n",
      "Brier skill score:0.013092851692552365\n",
      "Recall: [0.37 0.39 0.86]\n",
      "Precision: [0.42 0.55 0.7 ]\n",
      "F1-score: [0.39 0.45 0.77]\n",
      "Accuracy: 0.6\n",
      "Brier score:0.16761133903840003\n",
      "Brier climat:0.1912437500000001\n",
      "Brier skill score:0.12357220019791526\n",
      "Recall: [0.37 0.39 0.86]\n",
      "Precision: [0.42 0.55 0.7 ]\n",
      "F1-score: [0.39 0.45 0.77]\n",
      "Accuracy: 0.6\n",
      "Brier score:0.14901701096483835\n",
      "Brier climat:0.26561875\n",
      "Brier skill score:0.43898158181665137\n",
      "******** 25\n",
      "validation years [2006, 2007]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "Recall: [0.65 0.58 0.66]\n",
      "Precision: [0.63 0.58 0.68]\n",
      "F1-score: [0.64 0.58 0.67]\n",
      "Accuracy: 0.63\n",
      "Brier score:0.1517836066545617\n",
      "Brier climat:0.21875491071428574\n",
      "Brier skill score:0.3061476601418781\n",
      "Recall: [0.65 0.58 0.66]\n",
      "Precision: [0.63 0.58 0.68]\n",
      "F1-score: [0.64 0.58 0.67]\n",
      "Accuracy: 0.63\n",
      "Brier score:0.1877212892620772\n",
      "Brier climat:0.22767232142857147\n",
      "Brier skill score:0.17547601709252236\n",
      "Recall: [0.65 0.58 0.66]\n",
      "Precision: [0.63 0.58 0.68]\n",
      "F1-score: [0.64 0.58 0.67]\n",
      "Accuracy: 0.63\n",
      "Brier score:0.14165176694367293\n",
      "Brier climat:0.22027276785714286\n",
      "Brier skill score:0.35692565031215895\n",
      "Recall: [0.17 0.27 0.52]\n",
      "Precision: [0.06 0.19 0.9 ]\n",
      "F1-score: [0.09 0.22 0.66]\n",
      "Accuracy: 0.43\n",
      "Brier score:0.16450209157445797\n",
      "Brier climat:0.140775\n",
      "Brier skill score:-0.16854620191410374\n",
      "Recall: [0.17 0.27 0.52]\n",
      "Precision: [0.06 0.19 0.9 ]\n",
      "F1-score: [0.09 0.22 0.66]\n",
      "Accuracy: 0.43\n",
      "Brier score:0.2292124205701664\n",
      "Brier climat:0.18858750000000002\n",
      "Brier skill score:-0.21541682545325846\n",
      "Recall: [0.17 0.27 0.52]\n",
      "Precision: [0.06 0.19 0.9 ]\n",
      "F1-score: [0.09 0.22 0.66]\n",
      "Accuracy: 0.43\n",
      "Brier score:0.26883181747495916\n",
      "Brier climat:0.33733750000000007\n",
      "Brier skill score:0.20307757816738692\n",
      "******** 26\n",
      "validation years [2007, 2008]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2009, 2010, 1981, 1982, 1983}\n",
      "Recall: [0.64 0.58 0.72]\n",
      "Precision: [0.66 0.58 0.7 ]\n",
      "F1-score: [0.65 0.58 0.71]\n",
      "Accuracy: 0.65\n",
      "Brier score:0.14749984048329456\n",
      "Brier climat:0.21439107142857144\n",
      "Brier skill score:0.31200567495443954\n",
      "Recall: [0.64 0.58 0.72]\n",
      "Precision: [0.66 0.58 0.7 ]\n",
      "F1-score: [0.65 0.58 0.71]\n",
      "Accuracy: 0.65\n",
      "Brier score:0.1852275116866812\n",
      "Brier climat:0.2255852678571429\n",
      "Brier skill score:0.17890244586370407\n",
      "Recall: [0.64 0.58 0.72]\n",
      "Precision: [0.66 0.58 0.7 ]\n",
      "F1-score: [0.65 0.58 0.71]\n",
      "Accuracy: 0.65\n",
      "Brier score:0.1397373769507406\n",
      "Brier climat:0.22672366071428574\n",
      "Brier skill score:0.3836665458271872\n",
      "Recall: [0.46 0.27 0.02]\n",
      "Precision: [0.28 0.19 0.08]\n",
      "F1-score: [0.35 0.22 0.03]\n",
      "Accuracy: 0.22\n",
      "Brier score:0.21296817806925594\n",
      "Brier climat:0.20186875\n",
      "Brier skill score:-0.054983389302484476\n",
      "Recall: [0.46 0.27 0.02]\n",
      "Precision: [0.28 0.19 0.08]\n",
      "F1-score: [0.35 0.22 0.03]\n",
      "Accuracy: 0.22\n",
      "Brier score:0.2849208658168402\n",
      "Brier climat:0.21780625000000006\n",
      "Brier skill score:-0.3081390723032056\n",
      "Recall: [0.46 0.27 0.02]\n",
      "Precision: [0.28 0.19 0.08]\n",
      "F1-score: [0.35 0.22 0.03]\n",
      "Accuracy: 0.22\n",
      "Brier score:0.27545964479161655\n",
      "Brier climat:0.24702500000000005\n",
      "Brier skill score:-0.11510836875464636\n",
      "******** 27\n",
      "validation years [2008, 2009]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2010, 1981, 1982, 1983}\n",
      "Recall: [0.62 0.55 0.68]\n",
      "Precision: [0.63 0.56 0.67]\n",
      "F1-score: [0.62 0.55 0.67]\n",
      "Accuracy: 0.62\n",
      "Brier score:0.15105200243035588\n",
      "Brier climat:0.21268348214285718\n",
      "Brier skill score:0.28978028331840133\n",
      "Recall: [0.62 0.55 0.68]\n",
      "Precision: [0.63 0.56 0.67]\n",
      "F1-score: [0.62 0.55 0.67]\n",
      "Accuracy: 0.62\n",
      "Brier score:0.19263623558446127\n",
      "Brier climat:0.2288107142857143\n",
      "Brier skill score:0.15809783564628976\n",
      "Recall: [0.62 0.55 0.68]\n",
      "Precision: [0.63 0.56 0.67]\n",
      "F1-score: [0.62 0.55 0.67]\n",
      "Accuracy: 0.62\n",
      "Brier score:0.14423825782267943\n",
      "Brier climat:0.22520580357142858\n",
      "Brier skill score:0.35952690589995673\n",
      "Recall: [0.48 0.21 0.62]\n",
      "Precision: [0.52 0.14 0.7 ]\n",
      "F1-score: [0.5  0.17 0.65]\n",
      "Accuracy: 0.49\n",
      "Brier score:0.15979077408628595\n",
      "Brier climat:0.22577500000000006\n",
      "Brier skill score:0.29225656478225714\n",
      "Recall: [0.48 0.21 0.62]\n",
      "Precision: [0.52 0.14 0.7 ]\n",
      "F1-score: [0.5  0.17 0.65]\n",
      "Accuracy: 0.49\n",
      "Brier score:0.17635533025589084\n",
      "Brier climat:0.17265000000000003\n",
      "Brier skill score:-0.021461513211067595\n",
      "Recall: [0.48 0.21 0.62]\n",
      "Precision: [0.52 0.14 0.7 ]\n",
      "F1-score: [0.5  0.17 0.65]\n",
      "Accuracy: 0.49\n",
      "Brier score:0.20350014730575205\n",
      "Brier climat:0.26827500000000004\n",
      "Brier skill score:0.24144945557449626\n",
      "******** 28\n",
      "validation years [2009, 2010]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 1981, 1982, 1983}\n",
      "Recall: [0.61 0.54 0.68]\n",
      "Precision: [0.62 0.55 0.66]\n",
      "F1-score: [0.61 0.55 0.67]\n",
      "Accuracy: 0.61\n",
      "Brier score:0.15013018738205214\n",
      "Brier climat:0.2088888392857143\n",
      "Brier skill score:0.2812914854837848\n",
      "Recall: [0.61 0.54 0.68]\n",
      "Precision: [0.62 0.55 0.66]\n",
      "F1-score: [0.61 0.55 0.67]\n",
      "Accuracy: 0.61\n",
      "Brier score:0.19256970436608226\n",
      "Brier climat:0.2278620535714286\n",
      "Brier skill score:0.15488471490617528\n",
      "Recall: [0.61 0.54 0.68]\n",
      "Precision: [0.62 0.55 0.66]\n",
      "F1-score: [0.61 0.55 0.67]\n",
      "Accuracy: 0.61\n",
      "Brier score:0.15252240346672835\n",
      "Brier climat:0.22994910714285718\n",
      "Brier skill score:0.3367123475196929\n",
      "Recall: [0.66 0.76 1.  ]\n",
      "Precision: [0.88 0.5  0.97]\n",
      "F1-score: [0.75 0.6  0.99]\n",
      "Accuracy: 0.77\n",
      "Brier score:0.1717871426090106\n",
      "Brier climat:0.27890000000000004\n",
      "Brier skill score:0.3840547055969502\n",
      "Recall: [0.66 0.76 1.  ]\n",
      "Precision: [0.88 0.5  0.97]\n",
      "F1-score: [0.75 0.6  0.99]\n",
      "Accuracy: 0.77\n",
      "Brier score:0.1649832644781048\n",
      "Brier climat:0.18593125000000005\n",
      "Brier skill score:0.1126652218058839\n",
      "Recall: [0.66 0.76 1.  ]\n",
      "Precision: [0.88 0.5  0.97]\n",
      "F1-score: [0.75 0.6  0.99]\n",
      "Accuracy: 0.77\n",
      "Brier score:0.07581676765797782\n",
      "Brier climat:0.2018687500000001\n",
      "Brier skill score:0.6244254365374642\n"
     ]
    }
   ],
   "source": [
    "nbins = 10\n",
    "train_loss_lr, train_acc_lr = [],[]\n",
    "valid_loss_lr, valid_acc_lr = [],[]\n",
    "acc_tr_lr, f1_tr_lr, prec_tr_lr, rec_tr_lr = [],[],[],[]\n",
    "bss_tr1_lr, calib_y_tr1_lr, calib_x_tr1_lr = [],[],[]\n",
    "bss_tr2_lr, calib_y_tr2_lr, calib_x_tr2_lr = [],[],[]\n",
    "bss_tr3_lr, calib_y_tr3_lr, calib_x_tr3_lr = [],[],[]\n",
    "acc_va_lr, f1_va_lr, prec_va_lr, rec_va_lr = [],[],[],[]\n",
    "bss_va1_lr, calib_y_va1_lr, calib_x_va1_lr = [],[],[]\n",
    "bss_va2_lr, calib_y_va2_lr, calib_x_va2_lr = [],[],[]\n",
    "bss_va3_lr, calib_y_va3_lr, calib_x_va3_lr = [],[],[]\n",
    "acc_te_lr, f1_te_lr, prec_te_lr, rec_te_lr = [],[],[],[]\n",
    "bss_te1_lr, calib_y_te1_lr, calib_x_te1_lr = [],[],[]\n",
    "bss_te2_lr, calib_y_te2_lr, calib_x_te2_lr = [],[],[]\n",
    "bss_te3_lr, calib_y_te3_lr, calib_x_te3_lr = [],[],[]\n",
    "rpss_tr_lr, rpss_va_lr, rpss_te_lr = [],[],[]\n",
    "coef_lr = []\n",
    "per_imp_lr, per_imp_idx_lr = [],[]\n",
    "for ii in range(NYRS-1):\n",
    "    print(\"********\", ii)\n",
    "    val_year = [SYY+ii,SYY+ii+1]\n",
    "    X_train, Y_train, X_val, Y_val = cross_valid_n_out(X_all_new, y_all, all_years, val_year=val_year)\n",
    "    Y_train_2d = keras.utils.to_categorical(Y_train-1)\n",
    "    Y_val_2d = keras.utils.to_categorical(Y_val-1)\n",
    "    lr_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000, penalty='l2', C=0.2)\n",
    "    lr_model.fit(X_train,Y_train)\n",
    "    coef_lr.append(lr_model.coef_)\n",
    "#     sfs = SFS(lr_model, \n",
    "#               n_features_to_select=10, \n",
    "#               direction='forward', \n",
    "#               scoring='accuracy',\n",
    "#               cv=10,\n",
    "#               n_jobs=-1)\n",
    "#     sfs = sfs.fit(X_train_tmp, Y_train)\n",
    "#     X_train = sfs.transform(X_train_tmp)\n",
    "#     X_val = sfs.transform(X_val_tmp)\n",
    "#     lr_model.fit(X_train,Y_train)\n",
    "    \n",
    "    # evaluation training\n",
    "    yprob = lr_model.predict_proba(X_train)\n",
    "    ypred = lr_model.predict(X_train)\n",
    "    y_prob_ref = clim_pr_y_cat1[0:len(Y_train)]*0+0.33\n",
    "    y_prob_ref_1 = np.repeat(np.array([[0.33,0.33,0.33]]),Y_train_2d.shape[0],axis=0)\n",
    "    recall, precision, f1, acc, bs, bss, calib_y, calib_x = score(Y_train, Y_train_2d[:,0], ypred, \n",
    "                                                    yprob[:,0], nbins, y_prob_ref)\n",
    "    acc_tr_lr.append(acc)\n",
    "    f1_tr_lr.append(f1)\n",
    "    prec_tr_lr.append(precision)\n",
    "    rec_tr_lr.append(recall)\n",
    "    calib_y_tr1_lr.append(calib_y)\n",
    "    calib_x_tr1_lr.append(calib_x)\n",
    "    bss_tr1_lr.append(bss)\n",
    "    \n",
    "    rpssv = rpss(Y_train_2d, yprob, y_prob_ref_1)\n",
    "    rpss_tr_lr.append(rpssv)\n",
    "    \n",
    "    #y_prob_ref = clim_pr_y_cat3[0:len(Y_train)]*0+0.33\n",
    "    recall, precision, f1, acc, bs, bss, calib_y, calib_x = score(Y_train, Y_train_2d[:,1], ypred, \n",
    "                                                    yprob[:,1], nbins, y_prob_ref)\n",
    "    calib_y_tr2_lr.append(calib_y)\n",
    "    calib_x_tr2_lr.append(calib_x)\n",
    "    bss_tr2_lr.append(bss)\n",
    "    \n",
    "    #y_prob_ref = clim_pr_y_cat4[0:len(Y_train)]*0+0.33\n",
    "    recall, precision, f1, acc, bs, bss, calib_y, calib_x = score(Y_train, Y_train_2d[:,2], ypred, \n",
    "                                                    yprob[:,2], nbins, y_prob_ref)\n",
    "    calib_y_tr3_lr.append(calib_y)\n",
    "    calib_x_tr3_lr.append(calib_x)\n",
    "    bss_tr3_lr.append(bss)\n",
    "    \n",
    "    # evaluation validation\n",
    "    yprob = lr_model.predict_proba(X_val)\n",
    "    ypred = lr_model.predict(X_val)\n",
    "    y_prob_ref = clim_pr_y_cat1[0:len(Y_val)]*0+0.33\n",
    "    y_prob_ref_1 = np.repeat(np.array([[0.33,0.33,0.33]]),Y_val_2d.shape[0],axis=0)\n",
    "    recall, precision, f1, acc, bs, bss, calib_y, calib_x = score(Y_val, Y_val_2d[:,0], ypred, \n",
    "                                                    yprob[:,0], nbins, y_prob_ref)\n",
    "    acc_va_lr.append(acc)\n",
    "    f1_va_lr.append(f1)\n",
    "    prec_va_lr.append(precision)\n",
    "    rec_va_lr.append(recall)\n",
    "    calib_y_va1_lr.append(calib_y)\n",
    "    calib_x_va1_lr.append(calib_x)\n",
    "    bss_va1_lr.append(bss)\n",
    "    \n",
    "    rpssv = rpss(Y_val_2d, yprob, y_prob_ref_1)\n",
    "    rpss_va_lr.append(rpssv)\n",
    "    \n",
    "    #y_prob_ref = clim_pr_y_cat3[0:len(Y_val)]*0+0.33\n",
    "    recall, precision, f1, acc, bs, bss, calib_y, calib_x = score(Y_val, Y_val_2d[:,1], ypred, \n",
    "                                                    yprob[:,1], nbins, y_prob_ref)\n",
    "    calib_y_va2_lr.append(calib_y)\n",
    "    calib_x_va2_lr.append(calib_x)\n",
    "    bss_va2_lr.append(bss)\n",
    "    \n",
    "    #y_prob_ref = clim_pr_y_cat4[0:len(Y_val)]*0+0.33\n",
    "    recall, precision, f1, acc, bs, bss, calib_y, calib_x = score(Y_val, Y_val_2d[:,2], ypred, \n",
    "                                                    yprob[:,2], nbins, y_prob_ref)\n",
    "    calib_y_va3_lr.append(calib_y)\n",
    "    calib_x_va3_lr.append(calib_x)\n",
    "    bss_va3_lr.append(bss)\n",
    "    \n",
    "    result = permutation_importance(lr_model, X_train, Y_train, n_repeats=10, random_state=42)\n",
    "    # Get the sorted feature importance indices\n",
    "    sorted_idx = result.importances_mean.argsort()\n",
    "    per_imp_lr.append(result.importances_mean)\n",
    "    per_imp_idx_lr.append(sorted_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2d = np.ndarray((len(X_train.values),6,nlen))\n",
    "for ii in range(6):\n",
    "    X_train_2d[:,ii,:] = X_train.values[:,nlen*ii:(ii+1)*nlen]\n",
    "tmp = np.where(Y_train == 1)[0]\n",
    "tmp1 = np.where(Y_train == 3)[0]\n",
    "tmp2 = np.where(Y_train == 2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "colors = prop_cycle.by_key()['color']\n",
    "fig, axs = plt.subplots(1, figsize=plt.figaspect(0.25))\n",
    "fig_title = ['enso','iod','rmm1','rmm2','t2m','e']\n",
    "for ii in list([0,1,2]):\n",
    "    plt.plot(np.nanmean(X_train_2d[tmp,ii,:],axis=0), color=colors[ii], label=fig_title[ii]+' below')\n",
    "#     plt.plot(np.percentile(X_train_2d[tmp,ii,:],75,axis=0),color=colors[ii])\n",
    "#     plt.plot(np.percentile(X_train_2d[tmp,ii,:],25,axis=0),color=colors[ii])\n",
    "#     axs.fill_between(\n",
    "#         np.arange(nlen),np.percentile(X_train_2d[tmp,ii,:],25,axis=0), np.percentile(X_train_2d[tmp,ii,:],75,axis=0), color=colors[ii], alpha=0.25, \n",
    "#     )\n",
    "    plt.plot(np.nanmean(X_train_2d[tmp1,ii,:],axis=0), linestyle='dashed', color=colors[ii], label=fig_title[ii]+' above')\n",
    "#     plt.plot(np.percentile(X_train_2d[tmp1,ii,:],75,axis=0),linestyle='dashed',color=colors[ii])\n",
    "#     plt.plot(np.percentile(X_train_2d[tmp1,ii,:],25,axis=0),linestyle='dashed',color=colors[ii])\n",
    "#     axs.fill_between(\n",
    "#         np.arange(nlen),np.percentile(X_train_2d[tmp1,ii,:],25,axis=0), np.percentile(X_train_2d[tmp1,ii,:],75,axis=0), color=colors[ii], alpha=0.25, \n",
    "#     )\n",
    "    #plt.plot(np.nanmean(X_train_2d[tmp2,ii,:],axis=0), linestyle='dotted', color=colors[ii])\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.xlabel('lag [day]')\n",
    "    plt.ylabel('indices')\n",
    "plt.savefig('indices_drought_flood_lead14.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "coef_mean = np.nanmean(coef_lr,axis=0)\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=plt.figaspect(0.25))\n",
    "for ii in range(6):\n",
    "    ax1.plot(np.arange(nlen)+nlen*ii,coef_mean[0,nlen*ii:(ii+1)*nlen])\n",
    "    ax1.axhline(y = 0, color = 'k', linestyle = '-')\n",
    "    ax2.plot(np.arange(nlen)+nlen*ii,coef_mean[1,nlen*ii:(ii+1)*nlen])\n",
    "    ax2.axhline(y = 0, color = 'k', linestyle = '-')\n",
    "    ax3.plot(np.arange(nlen)+nlen*ii,coef_mean[2,nlen*ii:(ii+1)*nlen])\n",
    "    ax3.axhline(y = 0, color = 'k', linestyle = '-')\n",
    "#plt.savefig('coef_lead14_lr.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 73,  71,  70,  72,   6,  93,  95,   5,  88, 170, 171,   7,  74,\n",
       "       169,  94,  10,  87,  82,   8,   9, 215, 214, 216, 179,  41, 172,\n",
       "       205, 188,   4, 204,  11, 189,  58,  42, 180, 178, 203,  57, 213,\n",
       "        34, 184,  59, 206,  60, 127, 190, 137, 217,  61, 202,  62, 131,\n",
       "       200, 201, 191, 103,  33, 185, 187, 181,  69, 138, 183, 135, 102,\n",
       "       182, 142,  83,  12, 195, 212, 192, 218, 196, 207, 134,  56, 128,\n",
       "       168, 126, 158, 186,  86, 101, 139, 129,  84, 177, 193, 136,   3,\n",
       "       104, 163, 194, 113,  63, 132, 219, 140, 141, 208, 111,  48, 211,\n",
       "        92,  49, 157, 220, 147,  89,  50, 146,  47, 143, 209, 197, 223,\n",
       "       114, 156, 109, 225,  13,  55, 221, 210, 112, 222, 224,  85, 148,\n",
       "       110, 130, 133, 105, 226, 227, 162, 235,  52, 228,  51, 145, 149,\n",
       "        54,  75,  64, 121,  53, 173, 230, 229, 232, 231, 167, 118, 106,\n",
       "       236, 234,  46, 164, 176, 233, 100,  14,  43,  96, 198,  91, 108,\n",
       "       150,   2, 237,  90, 107, 166, 165,  99, 175, 117, 174,  15,  98,\n",
       "       125,  32,  45, 161, 144, 155,  65,  97, 238,  16, 115,  44,  17,\n",
       "        18,  19,  68,   1, 151,  20,  31, 239, 199,  81, 116,  66,  35,\n",
       "        21,  67,  76,  22,  30, 124, 160,  23, 122,  26,  27,  24,  29,\n",
       "        25,  28, 123,   0,  40, 152, 159, 154,  77, 153, 120,  36,  78,\n",
       "       119,  79,  80,  37,  38,  39])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_imp_mean = np.nanmean(per_imp_lr,axis=0)\n",
    "sorted_idx = per_imp_mean.argsort()\n",
    "sorted_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Permutation Importance')"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 960x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the feature importance\n",
    "idx_list = []\n",
    "for ii in range(len(sorted_idx)):\n",
    "    if sorted_idx[ii] < nlen:\n",
    "        idx_list.append('enso'+str(sorted_idx[ii]))\n",
    "    elif sorted_idx[ii] < nlen*2:\n",
    "        idx_list.append('iod'+str(sorted_idx[ii]-nlen))\n",
    "    elif sorted_idx[ii] < nlen*3:\n",
    "        idx_list.append('rmm1'+str(sorted_idx[ii]-nlen*2))\n",
    "    elif sorted_idx[ii] < nlen*4:\n",
    "        idx_list.append('rmm2'+str(sorted_idx[ii]-nlen*3))\n",
    "    elif sorted_idx[ii] < nlen*5:\n",
    "        idx_list.append('t2m'+str(sorted_idx[ii]-nlen*4))\n",
    "    else:\n",
    "        idx_list.append('e'+str(sorted_idx[ii]-nlen*5))\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=plt.figaspect(0.30))\n",
    "ax1.barh(np.arange(nlen*6), per_imp_mean[sorted_idx], height=0.8)\n",
    "ax1.set_xlabel('Permutation Importance')\n",
    "\n",
    "ax2.barh(np.arange(20), per_imp_mean[sorted_idx[-20:]], tick_label=idx_list[-20:], height=0.8)\n",
    "ax2.set_xlabel('Permutation Importance')\n",
    "#plt.savefig('permut_import_lead14_lr.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.62281843],\n",
       "       [-0.62281843,  1.        ]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(coef_mean[0,:],coef_mean[2,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** 0\n",
      "validation years [1981, 1982]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1983}\n",
      "{0: 2.0, 1: 1.0, 2: 1.0}\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 60)]              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 183       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183\n",
      "Trainable params: 183\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "Epoch 1/300\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 2.0658 - accuracy: 0.3113WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "28/28 [==============================] - 4s 106ms/step - loss: 2.0684 - accuracy: 0.3097 - val_loss: 1.5512 - val_accuracy: 0.1719\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.7852 - accuracy: 0.3661 - val_loss: 1.2579 - val_accuracy: 0.5781\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.6346 - accuracy: 0.4247 - val_loss: 1.1184 - val_accuracy: 0.6719\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.5465 - accuracy: 0.4660 - val_loss: 1.0475 - val_accuracy: 0.6953\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.4966 - accuracy: 0.4805 - val_loss: 1.0102 - val_accuracy: 0.6953\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4635 - accuracy: 0.4816 - val_loss: 0.9775 - val_accuracy: 0.7109\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.4422 - accuracy: 0.4844 - val_loss: 0.9614 - val_accuracy: 0.7188\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4271 - accuracy: 0.4883 - val_loss: 0.9447 - val_accuracy: 0.7109\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4142 - accuracy: 0.4888 - val_loss: 0.9353 - val_accuracy: 0.7109\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4032 - accuracy: 0.4855 - val_loss: 0.9252 - val_accuracy: 0.7109\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3939 - accuracy: 0.4844 - val_loss: 0.9180 - val_accuracy: 0.7109\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3851 - accuracy: 0.4827 - val_loss: 0.9101 - val_accuracy: 0.7109\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3776 - accuracy: 0.4810 - val_loss: 0.9047 - val_accuracy: 0.7109\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3690 - accuracy: 0.4777 - val_loss: 0.8991 - val_accuracy: 0.7109\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3618 - accuracy: 0.4760 - val_loss: 0.8941 - val_accuracy: 0.7109\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3555 - accuracy: 0.4749 - val_loss: 0.8922 - val_accuracy: 0.7109\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.3497 - accuracy: 0.4754 - val_loss: 0.8854 - val_accuracy: 0.7031\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3437 - accuracy: 0.4760 - val_loss: 0.8856 - val_accuracy: 0.7109\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3375 - accuracy: 0.4727 - val_loss: 0.8783 - val_accuracy: 0.6953\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3325 - accuracy: 0.4743 - val_loss: 0.8751 - val_accuracy: 0.6953\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3275 - accuracy: 0.4732 - val_loss: 0.8725 - val_accuracy: 0.6875\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3228 - accuracy: 0.4743 - val_loss: 0.8704 - val_accuracy: 0.6875\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3182 - accuracy: 0.4754 - val_loss: 0.8695 - val_accuracy: 0.6875\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3136 - accuracy: 0.4699 - val_loss: 0.8647 - val_accuracy: 0.6875\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3098 - accuracy: 0.4738 - val_loss: 0.8647 - val_accuracy: 0.6875\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3056 - accuracy: 0.4727 - val_loss: 0.8609 - val_accuracy: 0.6875\n",
      "Epoch 27/300\n",
      "28/28 [==============================] - 1s 54ms/step - loss: 1.3025 - accuracy: 0.4721 - val_loss: 0.8582 - val_accuracy: 0.6875\n",
      "Epoch 28/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2987 - accuracy: 0.4732 - val_loss: 0.8499 - val_accuracy: 0.6875\n",
      "Epoch 29/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2957 - accuracy: 0.4715 - val_loss: 0.8488 - val_accuracy: 0.6875\n",
      "Epoch 30/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2931 - accuracy: 0.4699 - val_loss: 0.8493 - val_accuracy: 0.6875\n",
      "Epoch 31/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2901 - accuracy: 0.4704 - val_loss: 0.8480 - val_accuracy: 0.6875\n",
      "Epoch 32/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2869 - accuracy: 0.4715 - val_loss: 0.8464 - val_accuracy: 0.6875\n",
      "Epoch 33/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2845 - accuracy: 0.4710 - val_loss: 0.8431 - val_accuracy: 0.6875\n",
      "Epoch 34/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2819 - accuracy: 0.4710 - val_loss: 0.8382 - val_accuracy: 0.6875\n",
      "Epoch 35/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2805 - accuracy: 0.4760 - val_loss: 0.8405 - val_accuracy: 0.6875\n",
      "Epoch 36/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2783 - accuracy: 0.4732 - val_loss: 0.8377 - val_accuracy: 0.6875\n",
      "Epoch 37/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2752 - accuracy: 0.4704 - val_loss: 0.8356 - val_accuracy: 0.6875\n",
      "Epoch 38/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2736 - accuracy: 0.4676 - val_loss: 0.8348 - val_accuracy: 0.6875\n",
      "Epoch 39/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2717 - accuracy: 0.4688 - val_loss: 0.8315 - val_accuracy: 0.6875\n",
      "Epoch 40/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2704 - accuracy: 0.4721 - val_loss: 0.8313 - val_accuracy: 0.6875\n",
      "Epoch 41/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2680 - accuracy: 0.4688 - val_loss: 0.8284 - val_accuracy: 0.6875\n",
      "Epoch 42/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2659 - accuracy: 0.4693 - val_loss: 0.8232 - val_accuracy: 0.6875\n",
      "Epoch 43/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2638 - accuracy: 0.4710 - val_loss: 0.8263 - val_accuracy: 0.6875\n",
      "Epoch 44/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2634 - accuracy: 0.4727 - val_loss: 0.8240 - val_accuracy: 0.6875\n",
      "Epoch 45/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2614 - accuracy: 0.4715 - val_loss: 0.8187 - val_accuracy: 0.6875\n",
      "Epoch 46/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2598 - accuracy: 0.4721 - val_loss: 0.8182 - val_accuracy: 0.6875\n",
      "Epoch 47/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2584 - accuracy: 0.4710 - val_loss: 0.8199 - val_accuracy: 0.6875\n",
      "Epoch 48/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2568 - accuracy: 0.4715 - val_loss: 0.8138 - val_accuracy: 0.6875\n",
      "Epoch 49/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2554 - accuracy: 0.4699 - val_loss: 0.8144 - val_accuracy: 0.6875\n",
      "Epoch 50/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2539 - accuracy: 0.4676 - val_loss: 0.8163 - val_accuracy: 0.6875\n",
      "Epoch 51/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2528 - accuracy: 0.4721 - val_loss: 0.8112 - val_accuracy: 0.6875\n",
      "Epoch 52/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2522 - accuracy: 0.4760 - val_loss: 0.8092 - val_accuracy: 0.6875\n",
      "Epoch 53/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2506 - accuracy: 0.4704 - val_loss: 0.8098 - val_accuracy: 0.6875\n",
      "Epoch 54/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2498 - accuracy: 0.4721 - val_loss: 0.8101 - val_accuracy: 0.6875\n",
      "Epoch 55/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2477 - accuracy: 0.4710 - val_loss: 0.8078 - val_accuracy: 0.6875\n",
      "Epoch 56/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2459 - accuracy: 0.4715 - val_loss: 0.8071 - val_accuracy: 0.6875\n",
      "Epoch 57/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2461 - accuracy: 0.4738 - val_loss: 0.8070 - val_accuracy: 0.6875\n",
      "Epoch 58/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2439 - accuracy: 0.4727 - val_loss: 0.8043 - val_accuracy: 0.6875\n",
      "Epoch 59/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2429 - accuracy: 0.4738 - val_loss: 0.8052 - val_accuracy: 0.6875\n",
      "Epoch 60/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2423 - accuracy: 0.4727 - val_loss: 0.8055 - val_accuracy: 0.6875\n",
      "Epoch 61/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2416 - accuracy: 0.4738 - val_loss: 0.8011 - val_accuracy: 0.6875\n",
      "Epoch 62/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2407 - accuracy: 0.4715 - val_loss: 0.8020 - val_accuracy: 0.6875\n",
      "Epoch 63/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2397 - accuracy: 0.4727 - val_loss: 0.8008 - val_accuracy: 0.6875\n",
      "Epoch 64/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2390 - accuracy: 0.4754 - val_loss: 0.7975 - val_accuracy: 0.6875\n",
      "Epoch 65/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2386 - accuracy: 0.4749 - val_loss: 0.7995 - val_accuracy: 0.6875\n",
      "Epoch 66/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2383 - accuracy: 0.4749 - val_loss: 0.7983 - val_accuracy: 0.6875\n",
      "Epoch 67/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2370 - accuracy: 0.4715 - val_loss: 0.7998 - val_accuracy: 0.6875\n",
      "Epoch 68/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2357 - accuracy: 0.4743 - val_loss: 0.7988 - val_accuracy: 0.6875\n",
      "Epoch 69/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2342 - accuracy: 0.4777 - val_loss: 0.7977 - val_accuracy: 0.6875\n",
      "Epoch 70/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2332 - accuracy: 0.4760 - val_loss: 0.7934 - val_accuracy: 0.6875\n",
      "Epoch 71/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2334 - accuracy: 0.4743 - val_loss: 0.7935 - val_accuracy: 0.6875\n",
      "Epoch 72/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2328 - accuracy: 0.4777 - val_loss: 0.7934 - val_accuracy: 0.6875\n",
      "Epoch 73/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2324 - accuracy: 0.4749 - val_loss: 0.7960 - val_accuracy: 0.6875\n",
      "Epoch 74/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2308 - accuracy: 0.4788 - val_loss: 0.7930 - val_accuracy: 0.6875\n",
      "Epoch 75/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2302 - accuracy: 0.4754 - val_loss: 0.7929 - val_accuracy: 0.6875\n",
      "Epoch 76/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2293 - accuracy: 0.4771 - val_loss: 0.7912 - val_accuracy: 0.6875\n",
      "Epoch 77/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2285 - accuracy: 0.4754 - val_loss: 0.7906 - val_accuracy: 0.6875\n",
      "Epoch 78/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2287 - accuracy: 0.4749 - val_loss: 0.7917 - val_accuracy: 0.6875\n",
      "Epoch 79/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2291 - accuracy: 0.4810 - val_loss: 0.7890 - val_accuracy: 0.6875\n",
      "Epoch 80/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2280 - accuracy: 0.4749 - val_loss: 0.7916 - val_accuracy: 0.6875\n",
      "Epoch 81/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2282 - accuracy: 0.4760 - val_loss: 0.7888 - val_accuracy: 0.6875\n",
      "Epoch 82/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2269 - accuracy: 0.4738 - val_loss: 0.7888 - val_accuracy: 0.6875\n",
      "Epoch 83/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2260 - accuracy: 0.4754 - val_loss: 0.7896 - val_accuracy: 0.6875\n",
      "Epoch 84/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2259 - accuracy: 0.4738 - val_loss: 0.7895 - val_accuracy: 0.6875\n",
      "Epoch 85/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2263 - accuracy: 0.4738 - val_loss: 0.7890 - val_accuracy: 0.6875\n",
      "Epoch 86/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2256 - accuracy: 0.4788 - val_loss: 0.7874 - val_accuracy: 0.6875\n",
      "Epoch 87/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2251 - accuracy: 0.4760 - val_loss: 0.7916 - val_accuracy: 0.6875\n",
      "Epoch 88/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2246 - accuracy: 0.4782 - val_loss: 0.7858 - val_accuracy: 0.6875\n",
      "Epoch 89/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2239 - accuracy: 0.4766 - val_loss: 0.7872 - val_accuracy: 0.6875\n",
      "Epoch 90/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2237 - accuracy: 0.4743 - val_loss: 0.7881 - val_accuracy: 0.6875\n",
      "Epoch 91/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2243 - accuracy: 0.4777 - val_loss: 0.7854 - val_accuracy: 0.6875\n",
      "Epoch 92/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2241 - accuracy: 0.4754 - val_loss: 0.7846 - val_accuracy: 0.6875\n",
      "Epoch 93/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2232 - accuracy: 0.4766 - val_loss: 0.7872 - val_accuracy: 0.6875\n",
      "Epoch 94/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2224 - accuracy: 0.4782 - val_loss: 0.7838 - val_accuracy: 0.6875\n",
      "Epoch 95/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2224 - accuracy: 0.4738 - val_loss: 0.7884 - val_accuracy: 0.6875\n",
      "Epoch 96/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2225 - accuracy: 0.4782 - val_loss: 0.7917 - val_accuracy: 0.6875\n",
      "Epoch 97/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2219 - accuracy: 0.4760 - val_loss: 0.7872 - val_accuracy: 0.6875\n",
      "Epoch 98/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2209 - accuracy: 0.4766 - val_loss: 0.7874 - val_accuracy: 0.6875\n",
      "Epoch 99/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2205 - accuracy: 0.4782 - val_loss: 0.7836 - val_accuracy: 0.6875\n",
      "Epoch 100/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2201 - accuracy: 0.4771 - val_loss: 0.7841 - val_accuracy: 0.6875\n",
      "Epoch 101/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2194 - accuracy: 0.4760 - val_loss: 0.7842 - val_accuracy: 0.6875\n",
      "Epoch 102/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2191 - accuracy: 0.4771 - val_loss: 0.7832 - val_accuracy: 0.6875\n",
      "Epoch 103/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2194 - accuracy: 0.4749 - val_loss: 0.7848 - val_accuracy: 0.6875\n",
      "Epoch 104/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2187 - accuracy: 0.4754 - val_loss: 0.7859 - val_accuracy: 0.6875\n",
      "Epoch 105/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2192 - accuracy: 0.4782 - val_loss: 0.7836 - val_accuracy: 0.6875\n",
      "Epoch 106/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2189 - accuracy: 0.4799 - val_loss: 0.7835 - val_accuracy: 0.6875\n",
      "Epoch 107/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2181 - accuracy: 0.4754 - val_loss: 0.7817 - val_accuracy: 0.6875\n",
      "Epoch 108/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2187 - accuracy: 0.4760 - val_loss: 0.7832 - val_accuracy: 0.6875\n",
      "Epoch 109/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2177 - accuracy: 0.4766 - val_loss: 0.7802 - val_accuracy: 0.6875\n",
      "Epoch 110/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2180 - accuracy: 0.4766 - val_loss: 0.7842 - val_accuracy: 0.6875\n",
      "Epoch 111/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2179 - accuracy: 0.4760 - val_loss: 0.7807 - val_accuracy: 0.6875\n",
      "Epoch 112/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2171 - accuracy: 0.4782 - val_loss: 0.7812 - val_accuracy: 0.6875\n",
      "Epoch 113/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2171 - accuracy: 0.4782 - val_loss: 0.7794 - val_accuracy: 0.6875\n",
      "Epoch 114/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2166 - accuracy: 0.4782 - val_loss: 0.7808 - val_accuracy: 0.6875\n",
      "Epoch 115/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2163 - accuracy: 0.4782 - val_loss: 0.7795 - val_accuracy: 0.6875\n",
      "Epoch 116/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2168 - accuracy: 0.4782 - val_loss: 0.7805 - val_accuracy: 0.6875\n",
      "Epoch 117/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2165 - accuracy: 0.4782 - val_loss: 0.7783 - val_accuracy: 0.6875\n",
      "Epoch 118/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2165 - accuracy: 0.4782 - val_loss: 0.7816 - val_accuracy: 0.6875\n",
      "Epoch 119/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2161 - accuracy: 0.4766 - val_loss: 0.7766 - val_accuracy: 0.6875\n",
      "Epoch 120/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2156 - accuracy: 0.4805 - val_loss: 0.7789 - val_accuracy: 0.6875\n",
      "Epoch 121/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2153 - accuracy: 0.4766 - val_loss: 0.7769 - val_accuracy: 0.6875\n",
      "Epoch 122/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2160 - accuracy: 0.4760 - val_loss: 0.7784 - val_accuracy: 0.6875\n",
      "Epoch 123/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2154 - accuracy: 0.4788 - val_loss: 0.7772 - val_accuracy: 0.6875\n",
      "Epoch 124/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2156 - accuracy: 0.4810 - val_loss: 0.7747 - val_accuracy: 0.6875\n",
      "Epoch 125/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2152 - accuracy: 0.4777 - val_loss: 0.7792 - val_accuracy: 0.6875\n",
      "Epoch 126/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2151 - accuracy: 0.4782 - val_loss: 0.7763 - val_accuracy: 0.6875\n",
      "Epoch 127/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2150 - accuracy: 0.4777 - val_loss: 0.7779 - val_accuracy: 0.6875\n",
      "Epoch 128/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2145 - accuracy: 0.4799 - val_loss: 0.7771 - val_accuracy: 0.6875\n",
      "Epoch 129/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2149 - accuracy: 0.4788 - val_loss: 0.7803 - val_accuracy: 0.6875\n",
      "Epoch 130/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2165 - accuracy: 0.4782 - val_loss: 0.7737 - val_accuracy: 0.6875\n",
      "Epoch 131/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2147 - accuracy: 0.4799 - val_loss: 0.7782 - val_accuracy: 0.6875\n",
      "Epoch 132/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2154 - accuracy: 0.4782 - val_loss: 0.7750 - val_accuracy: 0.6875\n",
      "Epoch 133/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2140 - accuracy: 0.4788 - val_loss: 0.7743 - val_accuracy: 0.6875\n",
      "Epoch 134/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2144 - accuracy: 0.4794 - val_loss: 0.7757 - val_accuracy: 0.6875\n",
      "Epoch 135/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2138 - accuracy: 0.4799 - val_loss: 0.7745 - val_accuracy: 0.6875\n",
      "Epoch 136/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2132 - accuracy: 0.4788 - val_loss: 0.7764 - val_accuracy: 0.6875\n",
      "Epoch 137/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2140 - accuracy: 0.4771 - val_loss: 0.7740 - val_accuracy: 0.6875\n",
      "Epoch 138/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2134 - accuracy: 0.4777 - val_loss: 0.7758 - val_accuracy: 0.6875\n",
      "Epoch 139/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2143 - accuracy: 0.4777 - val_loss: 0.7781 - val_accuracy: 0.6875\n",
      "Epoch 140/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2134 - accuracy: 0.4777 - val_loss: 0.7763 - val_accuracy: 0.6875\n",
      "Epoch 141/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2128 - accuracy: 0.4766 - val_loss: 0.7717 - val_accuracy: 0.6875\n",
      "Epoch 142/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2131 - accuracy: 0.4754 - val_loss: 0.7755 - val_accuracy: 0.6875\n",
      "Epoch 143/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2123 - accuracy: 0.4771 - val_loss: 0.7725 - val_accuracy: 0.6875\n",
      "Epoch 144/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2125 - accuracy: 0.4771 - val_loss: 0.7737 - val_accuracy: 0.6875\n",
      "Epoch 145/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2122 - accuracy: 0.4782 - val_loss: 0.7743 - val_accuracy: 0.6875\n",
      "Epoch 146/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2129 - accuracy: 0.4782 - val_loss: 0.7733 - val_accuracy: 0.6875\n",
      "Epoch 147/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2123 - accuracy: 0.4760 - val_loss: 0.7732 - val_accuracy: 0.6875\n",
      "Epoch 148/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2131 - accuracy: 0.4749 - val_loss: 0.7737 - val_accuracy: 0.6875\n",
      "Epoch 149/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2126 - accuracy: 0.4788 - val_loss: 0.7684 - val_accuracy: 0.6875\n",
      "Epoch 150/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2127 - accuracy: 0.4777 - val_loss: 0.7762 - val_accuracy: 0.6875\n",
      "Epoch 151/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2127 - accuracy: 0.4799 - val_loss: 0.7705 - val_accuracy: 0.6875\n",
      "Epoch 152/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2129 - accuracy: 0.4782 - val_loss: 0.7722 - val_accuracy: 0.6875\n",
      "Epoch 153/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2116 - accuracy: 0.4788 - val_loss: 0.7699 - val_accuracy: 0.6875\n",
      "Epoch 154/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2118 - accuracy: 0.4777 - val_loss: 0.7712 - val_accuracy: 0.6875\n",
      "Epoch 155/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2131 - accuracy: 0.4794 - val_loss: 0.7704 - val_accuracy: 0.6875\n",
      "Epoch 156/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2121 - accuracy: 0.4760 - val_loss: 0.7701 - val_accuracy: 0.6875\n",
      "Epoch 157/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2118 - accuracy: 0.4799 - val_loss: 0.7707 - val_accuracy: 0.6875\n",
      "Epoch 158/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2117 - accuracy: 0.4771 - val_loss: 0.7697 - val_accuracy: 0.6875\n",
      "Epoch 159/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2113 - accuracy: 0.4777 - val_loss: 0.7720 - val_accuracy: 0.6875\n",
      "Epoch 160/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2120 - accuracy: 0.4771 - val_loss: 0.7715 - val_accuracy: 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2118 - accuracy: 0.4794 - val_loss: 0.7696 - val_accuracy: 0.6875\n",
      "Epoch 162/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2123 - accuracy: 0.4777 - val_loss: 0.7733 - val_accuracy: 0.6875\n",
      "Epoch 163/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2118 - accuracy: 0.4760 - val_loss: 0.7725 - val_accuracy: 0.6875\n",
      "Epoch 164/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2124 - accuracy: 0.4777 - val_loss: 0.7724 - val_accuracy: 0.6875\n",
      "Epoch 165/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2114 - accuracy: 0.4766 - val_loss: 0.7683 - val_accuracy: 0.6875\n",
      "Epoch 166/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2110 - accuracy: 0.4760 - val_loss: 0.7688 - val_accuracy: 0.6875\n",
      "Epoch 167/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2114 - accuracy: 0.4766 - val_loss: 0.7718 - val_accuracy: 0.6875\n",
      "Epoch 168/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2118 - accuracy: 0.4771 - val_loss: 0.7711 - val_accuracy: 0.6875\n",
      "Epoch 169/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2109 - accuracy: 0.4777 - val_loss: 0.7670 - val_accuracy: 0.6875\n",
      "Epoch 170/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2117 - accuracy: 0.4777 - val_loss: 0.7674 - val_accuracy: 0.6875\n",
      "Epoch 171/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2113 - accuracy: 0.4794 - val_loss: 0.7697 - val_accuracy: 0.6875\n",
      "Epoch 172/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2109 - accuracy: 0.4766 - val_loss: 0.7705 - val_accuracy: 0.6875\n",
      "Epoch 173/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2107 - accuracy: 0.4771 - val_loss: 0.7685 - val_accuracy: 0.6875\n",
      "Epoch 174/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2109 - accuracy: 0.4782 - val_loss: 0.7658 - val_accuracy: 0.6875\n",
      "Epoch 175/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2107 - accuracy: 0.4766 - val_loss: 0.7695 - val_accuracy: 0.6875\n",
      "Epoch 176/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2113 - accuracy: 0.4782 - val_loss: 0.7671 - val_accuracy: 0.6875\n",
      "Epoch 177/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2121 - accuracy: 0.4788 - val_loss: 0.7663 - val_accuracy: 0.6875\n",
      "Epoch 178/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2123 - accuracy: 0.4788 - val_loss: 0.7673 - val_accuracy: 0.6875\n",
      "Epoch 179/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2115 - accuracy: 0.4766 - val_loss: 0.7691 - val_accuracy: 0.6875\n",
      "Epoch 180/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2115 - accuracy: 0.4777 - val_loss: 0.7697 - val_accuracy: 0.6875\n",
      "Epoch 181/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2109 - accuracy: 0.4777 - val_loss: 0.7716 - val_accuracy: 0.6875\n",
      "Epoch 182/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2112 - accuracy: 0.4794 - val_loss: 0.7663 - val_accuracy: 0.6875\n",
      "Epoch 183/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2107 - accuracy: 0.4799 - val_loss: 0.7687 - val_accuracy: 0.6875\n",
      "Epoch 184/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2111 - accuracy: 0.4782 - val_loss: 0.7655 - val_accuracy: 0.6875\n",
      "Epoch 185/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2109 - accuracy: 0.4782 - val_loss: 0.7677 - val_accuracy: 0.6875\n",
      "Epoch 186/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2107 - accuracy: 0.4777 - val_loss: 0.7680 - val_accuracy: 0.6875\n",
      "Epoch 187/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2109 - accuracy: 0.4777 - val_loss: 0.7691 - val_accuracy: 0.6875\n",
      "Epoch 188/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2110 - accuracy: 0.4788 - val_loss: 0.7667 - val_accuracy: 0.6875\n",
      "Epoch 189/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2106 - accuracy: 0.4760 - val_loss: 0.7682 - val_accuracy: 0.6875\n",
      "Epoch 190/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2108 - accuracy: 0.4777 - val_loss: 0.7677 - val_accuracy: 0.6875\n",
      "Epoch 191/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2103 - accuracy: 0.4771 - val_loss: 0.7693 - val_accuracy: 0.6875\n",
      "Epoch 192/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2110 - accuracy: 0.4771 - val_loss: 0.7676 - val_accuracy: 0.6875\n",
      "Epoch 193/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2106 - accuracy: 0.4799 - val_loss: 0.7685 - val_accuracy: 0.6875\n",
      "Epoch 194/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2114 - accuracy: 0.4777 - val_loss: 0.7684 - val_accuracy: 0.6875\n",
      "Epoch 195/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2118 - accuracy: 0.4794 - val_loss: 0.7655 - val_accuracy: 0.6875\n",
      "Epoch 196/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2104 - accuracy: 0.4782 - val_loss: 0.7683 - val_accuracy: 0.6875\n",
      "Epoch 197/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2110 - accuracy: 0.4766 - val_loss: 0.7681 - val_accuracy: 0.6875\n",
      "Epoch 198/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2107 - accuracy: 0.4788 - val_loss: 0.7659 - val_accuracy: 0.6875\n",
      "Epoch 199/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2115 - accuracy: 0.4805 - val_loss: 0.7678 - val_accuracy: 0.6875\n",
      "Epoch 200/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2114 - accuracy: 0.4771 - val_loss: 0.7663 - val_accuracy: 0.6875\n",
      "Epoch 201/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2099 - accuracy: 0.4771 - val_loss: 0.7678 - val_accuracy: 0.6875\n",
      "Epoch 202/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2111 - accuracy: 0.4777 - val_loss: 0.7655 - val_accuracy: 0.6875\n",
      "Epoch 203/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2108 - accuracy: 0.4799 - val_loss: 0.7651 - val_accuracy: 0.6875\n",
      "Epoch 204/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2119 - accuracy: 0.4766 - val_loss: 0.7693 - val_accuracy: 0.6875\n",
      "Epoch 205/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2103 - accuracy: 0.4816 - val_loss: 0.7648 - val_accuracy: 0.6875\n",
      "Epoch 206/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2107 - accuracy: 0.4782 - val_loss: 0.7649 - val_accuracy: 0.6875\n",
      "Epoch 207/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2105 - accuracy: 0.4760 - val_loss: 0.7681 - val_accuracy: 0.6875\n",
      "Epoch 208/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2108 - accuracy: 0.4788 - val_loss: 0.7676 - val_accuracy: 0.6875\n",
      "Epoch 209/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2105 - accuracy: 0.4782 - val_loss: 0.7694 - val_accuracy: 0.6875\n",
      "Epoch 210/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2107 - accuracy: 0.4771 - val_loss: 0.7678 - val_accuracy: 0.6875\n",
      "Epoch 211/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2101 - accuracy: 0.4782 - val_loss: 0.7667 - val_accuracy: 0.6875\n",
      "Epoch 212/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2113 - accuracy: 0.4777 - val_loss: 0.7700 - val_accuracy: 0.6875\n",
      "Epoch 213/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2106 - accuracy: 0.4777 - val_loss: 0.7689 - val_accuracy: 0.6875\n",
      "Epoch 214/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2103 - accuracy: 0.4777 - val_loss: 0.7645 - val_accuracy: 0.6875\n",
      "Epoch 215/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2099 - accuracy: 0.4782 - val_loss: 0.7689 - val_accuracy: 0.6875\n",
      "Epoch 216/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2099 - accuracy: 0.4794 - val_loss: 0.7677 - val_accuracy: 0.6875\n",
      "Epoch 217/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2098 - accuracy: 0.4771 - val_loss: 0.7660 - val_accuracy: 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 218/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2102 - accuracy: 0.4760 - val_loss: 0.7680 - val_accuracy: 0.6875\n",
      "Epoch 219/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2100 - accuracy: 0.4794 - val_loss: 0.7663 - val_accuracy: 0.6875\n",
      "Epoch 220/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2098 - accuracy: 0.4749 - val_loss: 0.7685 - val_accuracy: 0.6875\n",
      "Epoch 221/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2102 - accuracy: 0.4782 - val_loss: 0.7688 - val_accuracy: 0.6875\n",
      "Epoch 222/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2102 - accuracy: 0.4760 - val_loss: 0.7672 - val_accuracy: 0.6875\n",
      "Epoch 223/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2105 - accuracy: 0.4771 - val_loss: 0.7650 - val_accuracy: 0.6875\n",
      "Epoch 224/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2106 - accuracy: 0.4805 - val_loss: 0.7666 - val_accuracy: 0.6875\n",
      "Epoch 225/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2096 - accuracy: 0.4794 - val_loss: 0.7664 - val_accuracy: 0.6875\n",
      "Epoch 226/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2101 - accuracy: 0.4771 - val_loss: 0.7655 - val_accuracy: 0.6875\n",
      "Epoch 227/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2101 - accuracy: 0.4805 - val_loss: 0.7639 - val_accuracy: 0.6875\n",
      "Epoch 228/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2104 - accuracy: 0.4794 - val_loss: 0.7668 - val_accuracy: 0.6875\n",
      "Epoch 229/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2099 - accuracy: 0.4799 - val_loss: 0.7690 - val_accuracy: 0.6875\n",
      "Epoch 230/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2097 - accuracy: 0.4771 - val_loss: 0.7665 - val_accuracy: 0.6875\n",
      "Epoch 231/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2096 - accuracy: 0.4766 - val_loss: 0.7648 - val_accuracy: 0.6875\n",
      "Epoch 232/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2106 - accuracy: 0.4782 - val_loss: 0.7655 - val_accuracy: 0.6875\n",
      "Epoch 233/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2103 - accuracy: 0.4788 - val_loss: 0.7647 - val_accuracy: 0.6875\n",
      "Epoch 234/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2093 - accuracy: 0.4788 - val_loss: 0.7630 - val_accuracy: 0.6875\n",
      "Epoch 235/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2097 - accuracy: 0.4788 - val_loss: 0.7661 - val_accuracy: 0.6875\n",
      "Epoch 236/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2095 - accuracy: 0.4766 - val_loss: 0.7666 - val_accuracy: 0.6875\n",
      "Epoch 237/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2104 - accuracy: 0.4799 - val_loss: 0.7666 - val_accuracy: 0.6875\n",
      "Epoch 238/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2099 - accuracy: 0.4794 - val_loss: 0.7681 - val_accuracy: 0.6875\n",
      "Epoch 239/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2100 - accuracy: 0.4777 - val_loss: 0.7631 - val_accuracy: 0.6875\n",
      "Epoch 240/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2104 - accuracy: 0.4805 - val_loss: 0.7649 - val_accuracy: 0.6875\n",
      "Epoch 241/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2107 - accuracy: 0.4788 - val_loss: 0.7703 - val_accuracy: 0.6875\n",
      "Epoch 242/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2101 - accuracy: 0.4799 - val_loss: 0.7676 - val_accuracy: 0.6875\n",
      "Epoch 243/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2098 - accuracy: 0.4777 - val_loss: 0.7683 - val_accuracy: 0.6875\n",
      "Epoch 244/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2099 - accuracy: 0.4771 - val_loss: 0.7670 - val_accuracy: 0.6875\n",
      "Epoch 245/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2102 - accuracy: 0.4799 - val_loss: 0.7646 - val_accuracy: 0.6875\n",
      "Epoch 246/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2107 - accuracy: 0.4766 - val_loss: 0.7692 - val_accuracy: 0.6875\n",
      "Epoch 247/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2090 - accuracy: 0.4777 - val_loss: 0.7648 - val_accuracy: 0.6875\n",
      "Epoch 248/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2105 - accuracy: 0.4805 - val_loss: 0.7697 - val_accuracy: 0.6875\n",
      "Epoch 249/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2106 - accuracy: 0.4827 - val_loss: 0.7656 - val_accuracy: 0.6875\n",
      "Epoch 250/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2096 - accuracy: 0.4760 - val_loss: 0.7687 - val_accuracy: 0.6875\n",
      "Epoch 251/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2092 - accuracy: 0.4782 - val_loss: 0.7676 - val_accuracy: 0.6875\n",
      "Epoch 252/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2109 - accuracy: 0.4782 - val_loss: 0.7659 - val_accuracy: 0.6875\n",
      "Epoch 253/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2090 - accuracy: 0.4788 - val_loss: 0.7662 - val_accuracy: 0.6875\n",
      "Epoch 254/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2108 - accuracy: 0.4833 - val_loss: 0.7664 - val_accuracy: 0.6875\n",
      "Epoch 255/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2106 - accuracy: 0.4777 - val_loss: 0.7663 - val_accuracy: 0.6875\n",
      "Epoch 256/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2092 - accuracy: 0.4788 - val_loss: 0.7661 - val_accuracy: 0.6875\n",
      "Epoch 257/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2093 - accuracy: 0.4805 - val_loss: 0.7648 - val_accuracy: 0.6875\n",
      "Epoch 258/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2098 - accuracy: 0.4777 - val_loss: 0.7622 - val_accuracy: 0.6875\n",
      "Epoch 259/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2095 - accuracy: 0.4794 - val_loss: 0.7656 - val_accuracy: 0.6875\n",
      "Epoch 260/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2099 - accuracy: 0.4782 - val_loss: 0.7682 - val_accuracy: 0.6875\n",
      "Epoch 261/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2102 - accuracy: 0.4794 - val_loss: 0.7644 - val_accuracy: 0.6875\n",
      "Epoch 262/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2094 - accuracy: 0.4805 - val_loss: 0.7668 - val_accuracy: 0.6875\n",
      "Epoch 263/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2098 - accuracy: 0.4799 - val_loss: 0.7652 - val_accuracy: 0.6875\n",
      "Epoch 264/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2091 - accuracy: 0.4777 - val_loss: 0.7640 - val_accuracy: 0.6875\n",
      "Epoch 265/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2089 - accuracy: 0.4799 - val_loss: 0.7648 - val_accuracy: 0.6875\n",
      "Epoch 266/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2092 - accuracy: 0.4771 - val_loss: 0.7649 - val_accuracy: 0.6875\n",
      "Epoch 267/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2096 - accuracy: 0.4788 - val_loss: 0.7676 - val_accuracy: 0.6875\n",
      "Epoch 268/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2091 - accuracy: 0.4838 - val_loss: 0.7654 - val_accuracy: 0.6875\n",
      "Epoch 269/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2094 - accuracy: 0.4816 - val_loss: 0.7654 - val_accuracy: 0.6875\n",
      "Epoch 270/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2096 - accuracy: 0.4788 - val_loss: 0.7638 - val_accuracy: 0.6875\n",
      "Epoch 271/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2108 - accuracy: 0.4794 - val_loss: 0.7632 - val_accuracy: 0.6875\n",
      "Epoch 272/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2092 - accuracy: 0.4777 - val_loss: 0.7656 - val_accuracy: 0.6875\n",
      "Epoch 273/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2098 - accuracy: 0.4849 - val_loss: 0.7643 - val_accuracy: 0.6875\n",
      "Epoch 274/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2090 - accuracy: 0.4782 - val_loss: 0.7647 - val_accuracy: 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 275/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2095 - accuracy: 0.4788 - val_loss: 0.7642 - val_accuracy: 0.6875\n",
      "Epoch 276/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2088 - accuracy: 0.4782 - val_loss: 0.7661 - val_accuracy: 0.6875\n",
      "Epoch 277/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2101 - accuracy: 0.4810 - val_loss: 0.7650 - val_accuracy: 0.6875\n",
      "Epoch 278/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2092 - accuracy: 0.4777 - val_loss: 0.7661 - val_accuracy: 0.6875\n",
      "Epoch 279/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2091 - accuracy: 0.4833 - val_loss: 0.7620 - val_accuracy: 0.6875\n",
      "Epoch 280/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2085 - accuracy: 0.4805 - val_loss: 0.7647 - val_accuracy: 0.6875\n",
      "Epoch 281/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2086 - accuracy: 0.4799 - val_loss: 0.7651 - val_accuracy: 0.6875\n",
      "Epoch 282/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2088 - accuracy: 0.4782 - val_loss: 0.7670 - val_accuracy: 0.6875\n",
      "Epoch 283/300\n",
      "28/28 [==============================] - 1s 29ms/step - loss: 1.2087 - accuracy: 0.4810 - val_loss: 0.7633 - val_accuracy: 0.6875\n",
      "Epoch 284/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2093 - accuracy: 0.4782 - val_loss: 0.7673 - val_accuracy: 0.6875\n",
      "Epoch 285/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2085 - accuracy: 0.4821 - val_loss: 0.7653 - val_accuracy: 0.6875\n",
      "Epoch 286/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2088 - accuracy: 0.4766 - val_loss: 0.7654 - val_accuracy: 0.6875\n",
      "Epoch 287/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2086 - accuracy: 0.4788 - val_loss: 0.7636 - val_accuracy: 0.6875\n",
      "Epoch 288/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2085 - accuracy: 0.4799 - val_loss: 0.7641 - val_accuracy: 0.6875\n",
      "Epoch 289/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2090 - accuracy: 0.4760 - val_loss: 0.7633 - val_accuracy: 0.6875\n",
      "Epoch 290/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2086 - accuracy: 0.4838 - val_loss: 0.7659 - val_accuracy: 0.6875\n",
      "Epoch 291/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2087 - accuracy: 0.4821 - val_loss: 0.7640 - val_accuracy: 0.6875\n",
      "Epoch 292/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2088 - accuracy: 0.4816 - val_loss: 0.7638 - val_accuracy: 0.6875\n",
      "Epoch 293/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2082 - accuracy: 0.4794 - val_loss: 0.7630 - val_accuracy: 0.6875\n",
      "Epoch 294/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2102 - accuracy: 0.4771 - val_loss: 0.7628 - val_accuracy: 0.6875\n",
      "Epoch 295/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2091 - accuracy: 0.4771 - val_loss: 0.7657 - val_accuracy: 0.6875\n",
      "Epoch 296/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2091 - accuracy: 0.4788 - val_loss: 0.7657 - val_accuracy: 0.6875\n",
      "Epoch 297/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.2084 - accuracy: 0.4788 - val_loss: 0.7642 - val_accuracy: 0.6875\n",
      "Epoch 298/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.2092 - accuracy: 0.4788 - val_loss: 0.7637 - val_accuracy: 0.6875\n",
      "Epoch 299/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.2101 - accuracy: 0.4794 - val_loss: 0.7660 - val_accuracy: 0.6875\n",
      "Epoch 300/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.2091 - accuracy: 0.4799 - val_loss: 0.7680 - val_accuracy: 0.6875\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "56/56 [==============================] - 0s 1ms/step\n",
      "Recall: [0.93 0.01 0.57]\n",
      "Precision: [0.4  1.   0.69]\n",
      "F1-score: [0.56 0.02 0.62]\n",
      "Accuracy: 0.48\n",
      "Brier score:0.20314802137288648\n",
      "Brier climat:0.2109052579365079\n",
      "Brier skill score:0.03678066938452862\n",
      "Recall: [0.93 0.01 0.57]\n",
      "Precision: [0.4  1.   0.69]\n",
      "F1-score: [0.56 0.02 0.62]\n",
      "Accuracy: 0.48\n",
      "Brier score:0.22431836477012643\n",
      "Brier climat:0.22462549603174603\n",
      "Brier skill score:0.0013673036545067685\n",
      "Recall: [0.93 0.01 0.57]\n",
      "Precision: [0.4  1.   0.69]\n",
      "F1-score: [0.56 0.02 0.62]\n",
      "Accuracy: 0.48\n",
      "Brier score:0.15849979125548921\n",
      "Brier climat:0.2236607142857143\n",
      "Brier skill score:0.29133825865809215\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "Recall: [0.95 0.   0.85]\n",
      "Precision: [0.57 0.   0.81]\n",
      "F1-score: [0.71 0.   0.83]\n",
      "Accuracy: 0.69\n",
      "Brier score:0.1347886386325643\n",
      "Brier climat:0.20071180555555557\n",
      "Brier skill score:0.32844688303471126\n",
      "Recall: [0.95 0.   0.85]\n",
      "Precision: [0.57 0.   0.81]\n",
      "F1-score: [0.71 0.   0.83]\n",
      "Accuracy: 0.69\n",
      "Brier score:0.1667944875659295\n",
      "Brier climat:0.17503472222222222\n",
      "Brier skill score:0.04707771436247388\n",
      "Recall: [0.95 0.   0.85]\n",
      "Precision: [0.57 0.   0.81]\n",
      "F1-score: [0.71 0.   0.83]\n",
      "Accuracy: 0.69\n",
      "Brier score:0.10748264727854287\n",
      "Brier climat:0.25546875\n",
      "Brier skill score:0.5792728179922482\n",
      "******** 1\n",
      "validation years [1982, 1983]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981}\n",
      "{0: 2.0, 1: 1.0, 2: 1.0}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 60)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 183       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183\n",
      "Trainable params: 183\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - ETA: 0s - loss: 1.8470 - accuracy: 0.3689WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "28/28 [==============================] - 4s 108ms/step - loss: 1.8470 - accuracy: 0.3689 - val_loss: 1.3428 - val_accuracy: 0.4141\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.6553 - accuracy: 0.4369 - val_loss: 1.1249 - val_accuracy: 0.6094\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.5815 - accuracy: 0.4559 - val_loss: 1.0490 - val_accuracy: 0.6797\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.5354 - accuracy: 0.4570 - val_loss: 1.0090 - val_accuracy: 0.7188\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.5058 - accuracy: 0.4509 - val_loss: 0.9929 - val_accuracy: 0.7031\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.4828 - accuracy: 0.4537 - val_loss: 0.9672 - val_accuracy: 0.7031\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4648 - accuracy: 0.4548 - val_loss: 0.9524 - val_accuracy: 0.7031\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.4496 - accuracy: 0.4548 - val_loss: 0.9443 - val_accuracy: 0.7031\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.4363 - accuracy: 0.4593 - val_loss: 0.9283 - val_accuracy: 0.7031\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4250 - accuracy: 0.4626 - val_loss: 0.9188 - val_accuracy: 0.7031\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.4155 - accuracy: 0.4626 - val_loss: 0.9112 - val_accuracy: 0.7031\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4050 - accuracy: 0.4654 - val_loss: 0.8985 - val_accuracy: 0.7031\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3962 - accuracy: 0.4637 - val_loss: 0.8926 - val_accuracy: 0.7031\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3888 - accuracy: 0.4676 - val_loss: 0.8852 - val_accuracy: 0.7031\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3793 - accuracy: 0.4693 - val_loss: 0.8687 - val_accuracy: 0.7031\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3707 - accuracy: 0.4682 - val_loss: 0.8721 - val_accuracy: 0.7031\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3635 - accuracy: 0.4682 - val_loss: 0.8631 - val_accuracy: 0.7031\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3573 - accuracy: 0.4727 - val_loss: 0.8507 - val_accuracy: 0.7031\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3505 - accuracy: 0.4693 - val_loss: 0.8501 - val_accuracy: 0.7031\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3438 - accuracy: 0.4738 - val_loss: 0.8429 - val_accuracy: 0.7031\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3377 - accuracy: 0.4743 - val_loss: 0.8390 - val_accuracy: 0.7031\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3324 - accuracy: 0.4727 - val_loss: 0.8337 - val_accuracy: 0.7031\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.3267 - accuracy: 0.4743 - val_loss: 0.8243 - val_accuracy: 0.7031\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3216 - accuracy: 0.4715 - val_loss: 0.8230 - val_accuracy: 0.7031\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3178 - accuracy: 0.4749 - val_loss: 0.8185 - val_accuracy: 0.7031\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3118 - accuracy: 0.4738 - val_loss: 0.8156 - val_accuracy: 0.7031\n",
      "Epoch 27/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3075 - accuracy: 0.4738 - val_loss: 0.8076 - val_accuracy: 0.7031\n",
      "Epoch 28/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.3032 - accuracy: 0.4738 - val_loss: 0.8048 - val_accuracy: 0.7031\n",
      "Epoch 29/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2986 - accuracy: 0.4710 - val_loss: 0.7978 - val_accuracy: 0.7031\n",
      "Epoch 30/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2951 - accuracy: 0.4721 - val_loss: 0.7997 - val_accuracy: 0.7031\n",
      "Epoch 31/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2919 - accuracy: 0.4738 - val_loss: 0.7891 - val_accuracy: 0.7031\n",
      "Epoch 32/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2878 - accuracy: 0.4721 - val_loss: 0.7911 - val_accuracy: 0.7031\n",
      "Epoch 33/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2851 - accuracy: 0.4743 - val_loss: 0.7893 - val_accuracy: 0.7031\n",
      "Epoch 34/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2825 - accuracy: 0.4743 - val_loss: 0.7814 - val_accuracy: 0.7031\n",
      "Epoch 35/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2789 - accuracy: 0.4732 - val_loss: 0.7821 - val_accuracy: 0.7031\n",
      "Epoch 36/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2768 - accuracy: 0.4715 - val_loss: 0.7824 - val_accuracy: 0.7031\n",
      "Epoch 37/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2750 - accuracy: 0.4738 - val_loss: 0.7760 - val_accuracy: 0.7031\n",
      "Epoch 38/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2721 - accuracy: 0.4754 - val_loss: 0.7759 - val_accuracy: 0.7031\n",
      "Epoch 39/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2699 - accuracy: 0.4754 - val_loss: 0.7726 - val_accuracy: 0.7031\n",
      "Epoch 40/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2680 - accuracy: 0.4743 - val_loss: 0.7706 - val_accuracy: 0.7031\n",
      "Epoch 41/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2665 - accuracy: 0.4754 - val_loss: 0.7717 - val_accuracy: 0.7031\n",
      "Epoch 42/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2649 - accuracy: 0.4738 - val_loss: 0.7669 - val_accuracy: 0.7031\n",
      "Epoch 43/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2632 - accuracy: 0.4777 - val_loss: 0.7620 - val_accuracy: 0.7031\n",
      "Epoch 44/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2613 - accuracy: 0.4754 - val_loss: 0.7643 - val_accuracy: 0.7031\n",
      "Epoch 45/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2594 - accuracy: 0.4754 - val_loss: 0.7642 - val_accuracy: 0.7031\n",
      "Epoch 46/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2575 - accuracy: 0.4749 - val_loss: 0.7617 - val_accuracy: 0.7031\n",
      "Epoch 47/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2565 - accuracy: 0.4754 - val_loss: 0.7593 - val_accuracy: 0.7031\n",
      "Epoch 48/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2541 - accuracy: 0.4766 - val_loss: 0.7584 - val_accuracy: 0.7031\n",
      "Epoch 49/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2524 - accuracy: 0.4771 - val_loss: 0.7576 - val_accuracy: 0.7031\n",
      "Epoch 50/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2509 - accuracy: 0.4760 - val_loss: 0.7538 - val_accuracy: 0.7031\n",
      "Epoch 51/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2496 - accuracy: 0.4754 - val_loss: 0.7536 - val_accuracy: 0.7031\n",
      "Epoch 52/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2487 - accuracy: 0.4788 - val_loss: 0.7517 - val_accuracy: 0.7031\n",
      "Epoch 53/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2472 - accuracy: 0.4777 - val_loss: 0.7537 - val_accuracy: 0.7031\n",
      "Epoch 54/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2455 - accuracy: 0.4794 - val_loss: 0.7512 - val_accuracy: 0.7031\n",
      "Epoch 55/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2453 - accuracy: 0.4771 - val_loss: 0.7508 - val_accuracy: 0.7031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2435 - accuracy: 0.4799 - val_loss: 0.7527 - val_accuracy: 0.7031\n",
      "Epoch 57/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2424 - accuracy: 0.4777 - val_loss: 0.7493 - val_accuracy: 0.7031\n",
      "Epoch 58/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2410 - accuracy: 0.4788 - val_loss: 0.7463 - val_accuracy: 0.7031\n",
      "Epoch 59/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2408 - accuracy: 0.4788 - val_loss: 0.7522 - val_accuracy: 0.7031\n",
      "Epoch 60/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2395 - accuracy: 0.4777 - val_loss: 0.7450 - val_accuracy: 0.7031\n",
      "Epoch 61/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2386 - accuracy: 0.4782 - val_loss: 0.7469 - val_accuracy: 0.7031\n",
      "Epoch 62/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2376 - accuracy: 0.4777 - val_loss: 0.7477 - val_accuracy: 0.7031\n",
      "Epoch 63/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2366 - accuracy: 0.4794 - val_loss: 0.7486 - val_accuracy: 0.7031\n",
      "Epoch 64/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2363 - accuracy: 0.4777 - val_loss: 0.7421 - val_accuracy: 0.7031\n",
      "Epoch 65/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2343 - accuracy: 0.4794 - val_loss: 0.7471 - val_accuracy: 0.7031\n",
      "Epoch 66/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2338 - accuracy: 0.4782 - val_loss: 0.7462 - val_accuracy: 0.7031\n",
      "Epoch 67/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2332 - accuracy: 0.4782 - val_loss: 0.7455 - val_accuracy: 0.7031\n",
      "Epoch 68/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2325 - accuracy: 0.4788 - val_loss: 0.7448 - val_accuracy: 0.7031\n",
      "Epoch 69/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2312 - accuracy: 0.4782 - val_loss: 0.7435 - val_accuracy: 0.7031\n",
      "Epoch 70/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2318 - accuracy: 0.4788 - val_loss: 0.7425 - val_accuracy: 0.7031\n",
      "Epoch 71/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2312 - accuracy: 0.4782 - val_loss: 0.7423 - val_accuracy: 0.7031\n",
      "Epoch 72/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2302 - accuracy: 0.4788 - val_loss: 0.7394 - val_accuracy: 0.7031\n",
      "Epoch 73/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2293 - accuracy: 0.4794 - val_loss: 0.7419 - val_accuracy: 0.7031\n",
      "Epoch 74/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2294 - accuracy: 0.4782 - val_loss: 0.7417 - val_accuracy: 0.7031\n",
      "Epoch 75/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2282 - accuracy: 0.4794 - val_loss: 0.7405 - val_accuracy: 0.7031\n",
      "Epoch 76/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2276 - accuracy: 0.4788 - val_loss: 0.7415 - val_accuracy: 0.7031\n",
      "Epoch 77/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2267 - accuracy: 0.4794 - val_loss: 0.7375 - val_accuracy: 0.7031\n",
      "Epoch 78/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2260 - accuracy: 0.4782 - val_loss: 0.7395 - val_accuracy: 0.7031\n",
      "Epoch 79/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2260 - accuracy: 0.4799 - val_loss: 0.7382 - val_accuracy: 0.7031\n",
      "Epoch 80/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2264 - accuracy: 0.4788 - val_loss: 0.7335 - val_accuracy: 0.7031\n",
      "Epoch 81/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2247 - accuracy: 0.4788 - val_loss: 0.7370 - val_accuracy: 0.7031\n",
      "Epoch 82/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2259 - accuracy: 0.4788 - val_loss: 0.7405 - val_accuracy: 0.7031\n",
      "Epoch 83/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2241 - accuracy: 0.4794 - val_loss: 0.7365 - val_accuracy: 0.7031\n",
      "Epoch 84/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2236 - accuracy: 0.4771 - val_loss: 0.7356 - val_accuracy: 0.7031\n",
      "Epoch 85/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2237 - accuracy: 0.4799 - val_loss: 0.7341 - val_accuracy: 0.7031\n",
      "Epoch 86/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2224 - accuracy: 0.4794 - val_loss: 0.7339 - val_accuracy: 0.7031\n",
      "Epoch 87/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2222 - accuracy: 0.4788 - val_loss: 0.7390 - val_accuracy: 0.7031\n",
      "Epoch 88/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2217 - accuracy: 0.4805 - val_loss: 0.7304 - val_accuracy: 0.7031\n",
      "Epoch 89/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2214 - accuracy: 0.4816 - val_loss: 0.7335 - val_accuracy: 0.7031\n",
      "Epoch 90/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2207 - accuracy: 0.4771 - val_loss: 0.7321 - val_accuracy: 0.7031\n",
      "Epoch 91/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2199 - accuracy: 0.4794 - val_loss: 0.7326 - val_accuracy: 0.7031\n",
      "Epoch 92/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2197 - accuracy: 0.4810 - val_loss: 0.7310 - val_accuracy: 0.7031\n",
      "Epoch 93/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2210 - accuracy: 0.4760 - val_loss: 0.7276 - val_accuracy: 0.7031\n",
      "Epoch 94/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2191 - accuracy: 0.4794 - val_loss: 0.7335 - val_accuracy: 0.7031\n",
      "Epoch 95/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2179 - accuracy: 0.4794 - val_loss: 0.7322 - val_accuracy: 0.7031\n",
      "Epoch 96/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2184 - accuracy: 0.4799 - val_loss: 0.7306 - val_accuracy: 0.7031\n",
      "Epoch 97/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2184 - accuracy: 0.4788 - val_loss: 0.7319 - val_accuracy: 0.7031\n",
      "Epoch 98/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2170 - accuracy: 0.4805 - val_loss: 0.7309 - val_accuracy: 0.7031\n",
      "Epoch 99/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2173 - accuracy: 0.4788 - val_loss: 0.7337 - val_accuracy: 0.7031\n",
      "Epoch 100/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2166 - accuracy: 0.4810 - val_loss: 0.7322 - val_accuracy: 0.7031\n",
      "Epoch 101/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2162 - accuracy: 0.4799 - val_loss: 0.7321 - val_accuracy: 0.7031\n",
      "Epoch 102/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2172 - accuracy: 0.4788 - val_loss: 0.7343 - val_accuracy: 0.7031\n",
      "Epoch 103/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2160 - accuracy: 0.4805 - val_loss: 0.7341 - val_accuracy: 0.7031\n",
      "Epoch 104/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2166 - accuracy: 0.4782 - val_loss: 0.7341 - val_accuracy: 0.7031\n",
      "Epoch 105/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2156 - accuracy: 0.4821 - val_loss: 0.7297 - val_accuracy: 0.7031\n",
      "Epoch 106/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2165 - accuracy: 0.4788 - val_loss: 0.7256 - val_accuracy: 0.7031\n",
      "Epoch 107/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2158 - accuracy: 0.4766 - val_loss: 0.7329 - val_accuracy: 0.7031\n",
      "Epoch 108/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2150 - accuracy: 0.4799 - val_loss: 0.7299 - val_accuracy: 0.7031\n",
      "Epoch 109/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2149 - accuracy: 0.4799 - val_loss: 0.7269 - val_accuracy: 0.7031\n",
      "Epoch 110/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2149 - accuracy: 0.4810 - val_loss: 0.7318 - val_accuracy: 0.7031\n",
      "Epoch 111/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2154 - accuracy: 0.4799 - val_loss: 0.7283 - val_accuracy: 0.7031\n",
      "Epoch 112/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2160 - accuracy: 0.4788 - val_loss: 0.7275 - val_accuracy: 0.7031\n",
      "Epoch 113/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2160 - accuracy: 0.4788 - val_loss: 0.7291 - val_accuracy: 0.7031\n",
      "Epoch 114/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2145 - accuracy: 0.4844 - val_loss: 0.7313 - val_accuracy: 0.7031\n",
      "Epoch 115/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2147 - accuracy: 0.4782 - val_loss: 0.7296 - val_accuracy: 0.7031\n",
      "Epoch 116/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2146 - accuracy: 0.4810 - val_loss: 0.7261 - val_accuracy: 0.7031\n",
      "Epoch 117/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2143 - accuracy: 0.4805 - val_loss: 0.7303 - val_accuracy: 0.7031\n",
      "Epoch 118/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2146 - accuracy: 0.4810 - val_loss: 0.7300 - val_accuracy: 0.7031\n",
      "Epoch 119/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2140 - accuracy: 0.4821 - val_loss: 0.7273 - val_accuracy: 0.7031\n",
      "Epoch 120/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2154 - accuracy: 0.4782 - val_loss: 0.7283 - val_accuracy: 0.7031\n",
      "Epoch 121/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2150 - accuracy: 0.4816 - val_loss: 0.7287 - val_accuracy: 0.7031\n",
      "Epoch 122/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2144 - accuracy: 0.4782 - val_loss: 0.7268 - val_accuracy: 0.7031\n",
      "Epoch 123/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2142 - accuracy: 0.4799 - val_loss: 0.7291 - val_accuracy: 0.7031\n",
      "Epoch 124/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2137 - accuracy: 0.4821 - val_loss: 0.7329 - val_accuracy: 0.7031\n",
      "Epoch 125/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2151 - accuracy: 0.4788 - val_loss: 0.7313 - val_accuracy: 0.7031\n",
      "Epoch 126/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2146 - accuracy: 0.4844 - val_loss: 0.7274 - val_accuracy: 0.7031\n",
      "Epoch 127/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2141 - accuracy: 0.4788 - val_loss: 0.7256 - val_accuracy: 0.7031\n",
      "Epoch 128/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2143 - accuracy: 0.4816 - val_loss: 0.7253 - val_accuracy: 0.7031\n",
      "Epoch 129/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2137 - accuracy: 0.4782 - val_loss: 0.7291 - val_accuracy: 0.7031\n",
      "Epoch 130/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2144 - accuracy: 0.4788 - val_loss: 0.7255 - val_accuracy: 0.7031\n",
      "Epoch 131/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2148 - accuracy: 0.4771 - val_loss: 0.7283 - val_accuracy: 0.7031\n",
      "Epoch 132/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2131 - accuracy: 0.4821 - val_loss: 0.7272 - val_accuracy: 0.7031\n",
      "Epoch 133/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2137 - accuracy: 0.4771 - val_loss: 0.7290 - val_accuracy: 0.7031\n",
      "Epoch 134/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2147 - accuracy: 0.4794 - val_loss: 0.7286 - val_accuracy: 0.7031\n",
      "Epoch 135/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2135 - accuracy: 0.4805 - val_loss: 0.7279 - val_accuracy: 0.7031\n",
      "Epoch 136/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2139 - accuracy: 0.4777 - val_loss: 0.7255 - val_accuracy: 0.7031\n",
      "Epoch 137/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2134 - accuracy: 0.4782 - val_loss: 0.7288 - val_accuracy: 0.7031\n",
      "Epoch 138/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2152 - accuracy: 0.4810 - val_loss: 0.7274 - val_accuracy: 0.7031\n",
      "Epoch 139/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2132 - accuracy: 0.4782 - val_loss: 0.7288 - val_accuracy: 0.7031\n",
      "Epoch 140/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2135 - accuracy: 0.4777 - val_loss: 0.7270 - val_accuracy: 0.7031\n",
      "Epoch 141/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2149 - accuracy: 0.4794 - val_loss: 0.7225 - val_accuracy: 0.7031\n",
      "Epoch 142/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2138 - accuracy: 0.4782 - val_loss: 0.7311 - val_accuracy: 0.7031\n",
      "Epoch 143/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2145 - accuracy: 0.4777 - val_loss: 0.7300 - val_accuracy: 0.7031\n",
      "Epoch 144/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2132 - accuracy: 0.4810 - val_loss: 0.7235 - val_accuracy: 0.7031\n",
      "Epoch 145/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2142 - accuracy: 0.4794 - val_loss: 0.7292 - val_accuracy: 0.7031\n",
      "Epoch 146/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2133 - accuracy: 0.4788 - val_loss: 0.7268 - val_accuracy: 0.7031\n",
      "Epoch 147/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2132 - accuracy: 0.4794 - val_loss: 0.7251 - val_accuracy: 0.7031\n",
      "Epoch 148/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2133 - accuracy: 0.4816 - val_loss: 0.7269 - val_accuracy: 0.7031\n",
      "Epoch 149/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2134 - accuracy: 0.4810 - val_loss: 0.7290 - val_accuracy: 0.7031\n",
      "Epoch 150/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2136 - accuracy: 0.4827 - val_loss: 0.7255 - val_accuracy: 0.7031\n",
      "Epoch 151/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2128 - accuracy: 0.4805 - val_loss: 0.7247 - val_accuracy: 0.7031\n",
      "Epoch 152/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2129 - accuracy: 0.4799 - val_loss: 0.7297 - val_accuracy: 0.7031\n",
      "Epoch 153/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2134 - accuracy: 0.4777 - val_loss: 0.7293 - val_accuracy: 0.7031\n",
      "Epoch 154/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2131 - accuracy: 0.4833 - val_loss: 0.7257 - val_accuracy: 0.7031\n",
      "Epoch 155/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2138 - accuracy: 0.4788 - val_loss: 0.7231 - val_accuracy: 0.7031\n",
      "Epoch 156/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2136 - accuracy: 0.4782 - val_loss: 0.7269 - val_accuracy: 0.7031\n",
      "Epoch 157/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2127 - accuracy: 0.4827 - val_loss: 0.7227 - val_accuracy: 0.7031\n",
      "Epoch 158/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2127 - accuracy: 0.4788 - val_loss: 0.7290 - val_accuracy: 0.7031\n",
      "Epoch 159/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2127 - accuracy: 0.4805 - val_loss: 0.7284 - val_accuracy: 0.7031\n",
      "Epoch 160/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2130 - accuracy: 0.4794 - val_loss: 0.7257 - val_accuracy: 0.7031\n",
      "Epoch 161/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2133 - accuracy: 0.4810 - val_loss: 0.7263 - val_accuracy: 0.7031\n",
      "Epoch 162/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2124 - accuracy: 0.4805 - val_loss: 0.7253 - val_accuracy: 0.7031\n",
      "Epoch 163/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2129 - accuracy: 0.4799 - val_loss: 0.7272 - val_accuracy: 0.7031\n",
      "Epoch 164/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2122 - accuracy: 0.4782 - val_loss: 0.7269 - val_accuracy: 0.7031\n",
      "Epoch 165/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2128 - accuracy: 0.4816 - val_loss: 0.7247 - val_accuracy: 0.7031\n",
      "Epoch 166/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2125 - accuracy: 0.4816 - val_loss: 0.7285 - val_accuracy: 0.7031\n",
      "Epoch 167/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2132 - accuracy: 0.4816 - val_loss: 0.7244 - val_accuracy: 0.7031\n",
      "Epoch 168/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2124 - accuracy: 0.4821 - val_loss: 0.7305 - val_accuracy: 0.7031\n",
      "Epoch 169/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2131 - accuracy: 0.4805 - val_loss: 0.7250 - val_accuracy: 0.7031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2127 - accuracy: 0.4816 - val_loss: 0.7247 - val_accuracy: 0.7031\n",
      "Epoch 171/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2126 - accuracy: 0.4827 - val_loss: 0.7216 - val_accuracy: 0.7031\n",
      "Epoch 172/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2133 - accuracy: 0.4799 - val_loss: 0.7233 - val_accuracy: 0.7031\n",
      "Epoch 173/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2124 - accuracy: 0.4805 - val_loss: 0.7273 - val_accuracy: 0.7031\n",
      "Epoch 174/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2125 - accuracy: 0.4810 - val_loss: 0.7252 - val_accuracy: 0.7031\n",
      "Epoch 175/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2124 - accuracy: 0.4821 - val_loss: 0.7213 - val_accuracy: 0.7031\n",
      "Epoch 176/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2130 - accuracy: 0.4821 - val_loss: 0.7263 - val_accuracy: 0.7031\n",
      "Epoch 177/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2126 - accuracy: 0.4788 - val_loss: 0.7256 - val_accuracy: 0.7031\n",
      "Epoch 178/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2134 - accuracy: 0.4827 - val_loss: 0.7217 - val_accuracy: 0.7031\n",
      "Epoch 179/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2124 - accuracy: 0.4799 - val_loss: 0.7265 - val_accuracy: 0.7031\n",
      "Epoch 180/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2123 - accuracy: 0.4810 - val_loss: 0.7237 - val_accuracy: 0.7031\n",
      "Epoch 181/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2131 - accuracy: 0.4794 - val_loss: 0.7256 - val_accuracy: 0.7031\n",
      "Epoch 182/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2129 - accuracy: 0.4833 - val_loss: 0.7243 - val_accuracy: 0.7031\n",
      "Epoch 183/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2124 - accuracy: 0.4827 - val_loss: 0.7229 - val_accuracy: 0.7031\n",
      "Epoch 184/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2118 - accuracy: 0.4810 - val_loss: 0.7263 - val_accuracy: 0.7031\n",
      "Epoch 185/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2122 - accuracy: 0.4794 - val_loss: 0.7278 - val_accuracy: 0.7031\n",
      "Epoch 186/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2125 - accuracy: 0.4799 - val_loss: 0.7240 - val_accuracy: 0.7031\n",
      "Epoch 187/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2121 - accuracy: 0.4799 - val_loss: 0.7220 - val_accuracy: 0.7031\n",
      "Epoch 188/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2117 - accuracy: 0.4788 - val_loss: 0.7302 - val_accuracy: 0.7031\n",
      "Epoch 189/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2123 - accuracy: 0.4788 - val_loss: 0.7275 - val_accuracy: 0.7031\n",
      "Epoch 190/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2117 - accuracy: 0.4821 - val_loss: 0.7211 - val_accuracy: 0.7031\n",
      "Epoch 191/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2125 - accuracy: 0.4821 - val_loss: 0.7270 - val_accuracy: 0.7031\n",
      "Epoch 192/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2127 - accuracy: 0.4805 - val_loss: 0.7246 - val_accuracy: 0.7031\n",
      "Epoch 193/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2121 - accuracy: 0.4788 - val_loss: 0.7251 - val_accuracy: 0.7031\n",
      "Epoch 194/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2125 - accuracy: 0.4821 - val_loss: 0.7228 - val_accuracy: 0.7031\n",
      "Epoch 195/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2125 - accuracy: 0.4810 - val_loss: 0.7256 - val_accuracy: 0.7031\n",
      "Epoch 196/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2121 - accuracy: 0.4810 - val_loss: 0.7206 - val_accuracy: 0.7031\n",
      "Epoch 197/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2121 - accuracy: 0.4771 - val_loss: 0.7225 - val_accuracy: 0.7031\n",
      "Epoch 198/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2126 - accuracy: 0.4799 - val_loss: 0.7251 - val_accuracy: 0.7031\n",
      "Epoch 199/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2119 - accuracy: 0.4821 - val_loss: 0.7238 - val_accuracy: 0.7031\n",
      "Epoch 200/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2118 - accuracy: 0.4827 - val_loss: 0.7269 - val_accuracy: 0.7031\n",
      "Epoch 201/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2119 - accuracy: 0.4821 - val_loss: 0.7255 - val_accuracy: 0.7031\n",
      "Epoch 202/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2119 - accuracy: 0.4821 - val_loss: 0.7241 - val_accuracy: 0.7031\n",
      "Epoch 203/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2121 - accuracy: 0.4821 - val_loss: 0.7236 - val_accuracy: 0.7031\n",
      "Epoch 204/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2121 - accuracy: 0.4799 - val_loss: 0.7246 - val_accuracy: 0.7031\n",
      "Epoch 205/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2127 - accuracy: 0.4810 - val_loss: 0.7279 - val_accuracy: 0.7031\n",
      "Epoch 206/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2117 - accuracy: 0.4844 - val_loss: 0.7206 - val_accuracy: 0.7031\n",
      "Epoch 207/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2129 - accuracy: 0.4777 - val_loss: 0.7258 - val_accuracy: 0.7031\n",
      "Epoch 208/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2122 - accuracy: 0.4816 - val_loss: 0.7286 - val_accuracy: 0.7031\n",
      "Epoch 209/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2129 - accuracy: 0.4782 - val_loss: 0.7205 - val_accuracy: 0.7031\n",
      "Epoch 210/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2112 - accuracy: 0.4833 - val_loss: 0.7283 - val_accuracy: 0.7031\n",
      "Epoch 211/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2118 - accuracy: 0.4810 - val_loss: 0.7207 - val_accuracy: 0.7031\n",
      "Epoch 212/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2121 - accuracy: 0.4810 - val_loss: 0.7234 - val_accuracy: 0.7031\n",
      "Epoch 213/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2117 - accuracy: 0.4788 - val_loss: 0.7235 - val_accuracy: 0.7031\n",
      "Epoch 214/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2115 - accuracy: 0.4821 - val_loss: 0.7213 - val_accuracy: 0.7031\n",
      "Epoch 215/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2117 - accuracy: 0.4821 - val_loss: 0.7238 - val_accuracy: 0.7031\n",
      "Epoch 216/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2119 - accuracy: 0.4805 - val_loss: 0.7217 - val_accuracy: 0.7031\n",
      "Epoch 217/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2120 - accuracy: 0.4821 - val_loss: 0.7203 - val_accuracy: 0.7031\n",
      "Epoch 218/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2122 - accuracy: 0.4821 - val_loss: 0.7270 - val_accuracy: 0.7031\n",
      "Epoch 219/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2115 - accuracy: 0.4782 - val_loss: 0.7229 - val_accuracy: 0.7031\n",
      "Epoch 220/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2117 - accuracy: 0.4833 - val_loss: 0.7242 - val_accuracy: 0.7031\n",
      "Epoch 221/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2115 - accuracy: 0.4810 - val_loss: 0.7215 - val_accuracy: 0.7031\n",
      "Epoch 222/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2114 - accuracy: 0.4799 - val_loss: 0.7262 - val_accuracy: 0.7031\n",
      "Epoch 223/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2116 - accuracy: 0.4810 - val_loss: 0.7219 - val_accuracy: 0.7031\n",
      "Epoch 224/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2117 - accuracy: 0.4782 - val_loss: 0.7282 - val_accuracy: 0.7031\n",
      "Epoch 225/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2114 - accuracy: 0.4794 - val_loss: 0.7232 - val_accuracy: 0.7031\n",
      "Epoch 226/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2119 - accuracy: 0.4816 - val_loss: 0.7253 - val_accuracy: 0.7031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2116 - accuracy: 0.4777 - val_loss: 0.7236 - val_accuracy: 0.7031\n",
      "Epoch 228/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2112 - accuracy: 0.4816 - val_loss: 0.7276 - val_accuracy: 0.7031\n",
      "Epoch 229/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2117 - accuracy: 0.4799 - val_loss: 0.7271 - val_accuracy: 0.7031\n",
      "Epoch 230/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2118 - accuracy: 0.4799 - val_loss: 0.7213 - val_accuracy: 0.7031\n",
      "Epoch 231/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2117 - accuracy: 0.4833 - val_loss: 0.7239 - val_accuracy: 0.7031\n",
      "Epoch 232/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2115 - accuracy: 0.4821 - val_loss: 0.7239 - val_accuracy: 0.7031\n",
      "Epoch 233/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2113 - accuracy: 0.4805 - val_loss: 0.7231 - val_accuracy: 0.7031\n",
      "Epoch 234/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2113 - accuracy: 0.4794 - val_loss: 0.7275 - val_accuracy: 0.7031\n",
      "Epoch 235/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2122 - accuracy: 0.4794 - val_loss: 0.7224 - val_accuracy: 0.7031\n",
      "Epoch 236/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2110 - accuracy: 0.4794 - val_loss: 0.7257 - val_accuracy: 0.7031\n",
      "Epoch 237/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2111 - accuracy: 0.4827 - val_loss: 0.7235 - val_accuracy: 0.7031\n",
      "Epoch 238/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2114 - accuracy: 0.4810 - val_loss: 0.7244 - val_accuracy: 0.7031\n",
      "Epoch 239/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2109 - accuracy: 0.4788 - val_loss: 0.7222 - val_accuracy: 0.7031\n",
      "Epoch 240/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2113 - accuracy: 0.4844 - val_loss: 0.7240 - val_accuracy: 0.7031\n",
      "Epoch 241/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2108 - accuracy: 0.4794 - val_loss: 0.7226 - val_accuracy: 0.7031\n",
      "Epoch 242/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2108 - accuracy: 0.4827 - val_loss: 0.7236 - val_accuracy: 0.7031\n",
      "Epoch 243/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2113 - accuracy: 0.4805 - val_loss: 0.7263 - val_accuracy: 0.7031\n",
      "Epoch 244/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2121 - accuracy: 0.4794 - val_loss: 0.7237 - val_accuracy: 0.7031\n",
      "Epoch 245/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2110 - accuracy: 0.4838 - val_loss: 0.7228 - val_accuracy: 0.7031\n",
      "Epoch 246/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2112 - accuracy: 0.4805 - val_loss: 0.7183 - val_accuracy: 0.7031\n",
      "Epoch 247/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2117 - accuracy: 0.4833 - val_loss: 0.7213 - val_accuracy: 0.7031\n",
      "Epoch 248/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2112 - accuracy: 0.4799 - val_loss: 0.7231 - val_accuracy: 0.7031\n",
      "Epoch 249/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2108 - accuracy: 0.4816 - val_loss: 0.7251 - val_accuracy: 0.7031\n",
      "Epoch 250/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2112 - accuracy: 0.4816 - val_loss: 0.7233 - val_accuracy: 0.7031\n",
      "Epoch 251/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2106 - accuracy: 0.4816 - val_loss: 0.7216 - val_accuracy: 0.7031\n",
      "Epoch 252/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2107 - accuracy: 0.4782 - val_loss: 0.7259 - val_accuracy: 0.7031\n",
      "Epoch 253/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2111 - accuracy: 0.4810 - val_loss: 0.7249 - val_accuracy: 0.7031\n",
      "Epoch 254/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2107 - accuracy: 0.4805 - val_loss: 0.7194 - val_accuracy: 0.7031\n",
      "Epoch 255/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2126 - accuracy: 0.4821 - val_loss: 0.7261 - val_accuracy: 0.7031\n",
      "Epoch 256/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2106 - accuracy: 0.4794 - val_loss: 0.7234 - val_accuracy: 0.7031\n",
      "Epoch 257/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2110 - accuracy: 0.4777 - val_loss: 0.7198 - val_accuracy: 0.7031\n",
      "Epoch 258/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2107 - accuracy: 0.4816 - val_loss: 0.7253 - val_accuracy: 0.7031\n",
      "Epoch 259/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2113 - accuracy: 0.4799 - val_loss: 0.7246 - val_accuracy: 0.7031\n",
      "Epoch 260/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2116 - accuracy: 0.4805 - val_loss: 0.7204 - val_accuracy: 0.7031\n",
      "Epoch 261/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2116 - accuracy: 0.4805 - val_loss: 0.7229 - val_accuracy: 0.7031\n",
      "Epoch 262/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2105 - accuracy: 0.4838 - val_loss: 0.7203 - val_accuracy: 0.7031\n",
      "Epoch 263/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2115 - accuracy: 0.4794 - val_loss: 0.7281 - val_accuracy: 0.7031\n",
      "Epoch 264/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2115 - accuracy: 0.4821 - val_loss: 0.7217 - val_accuracy: 0.7031\n",
      "Epoch 265/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2116 - accuracy: 0.4855 - val_loss: 0.7276 - val_accuracy: 0.7031\n",
      "Epoch 266/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2111 - accuracy: 0.4794 - val_loss: 0.7239 - val_accuracy: 0.7031\n",
      "Epoch 267/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2111 - accuracy: 0.4821 - val_loss: 0.7243 - val_accuracy: 0.7031\n",
      "Epoch 268/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2109 - accuracy: 0.4838 - val_loss: 0.7242 - val_accuracy: 0.7031\n",
      "Epoch 269/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2114 - accuracy: 0.4794 - val_loss: 0.7273 - val_accuracy: 0.7031\n",
      "Epoch 270/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2114 - accuracy: 0.4810 - val_loss: 0.7251 - val_accuracy: 0.7031\n",
      "Epoch 271/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2105 - accuracy: 0.4805 - val_loss: 0.7236 - val_accuracy: 0.7031\n",
      "Epoch 272/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2105 - accuracy: 0.4827 - val_loss: 0.7231 - val_accuracy: 0.7031\n",
      "Epoch 273/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2105 - accuracy: 0.4827 - val_loss: 0.7196 - val_accuracy: 0.7031\n",
      "Epoch 274/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2118 - accuracy: 0.4760 - val_loss: 0.7237 - val_accuracy: 0.7031\n",
      "Epoch 275/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2107 - accuracy: 0.4833 - val_loss: 0.7236 - val_accuracy: 0.7031\n",
      "Epoch 276/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2105 - accuracy: 0.4805 - val_loss: 0.7242 - val_accuracy: 0.7031\n",
      "Epoch 277/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2106 - accuracy: 0.4805 - val_loss: 0.7231 - val_accuracy: 0.7031\n",
      "Epoch 278/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2109 - accuracy: 0.4805 - val_loss: 0.7229 - val_accuracy: 0.7031\n",
      "Epoch 279/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2105 - accuracy: 0.4810 - val_loss: 0.7254 - val_accuracy: 0.7031\n",
      "Epoch 280/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2111 - accuracy: 0.4799 - val_loss: 0.7230 - val_accuracy: 0.7031\n",
      "Epoch 281/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2108 - accuracy: 0.4816 - val_loss: 0.7253 - val_accuracy: 0.7031\n",
      "Epoch 282/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2114 - accuracy: 0.4799 - val_loss: 0.7253 - val_accuracy: 0.7031\n",
      "Epoch 283/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2103 - accuracy: 0.4788 - val_loss: 0.7199 - val_accuracy: 0.7031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 284/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2109 - accuracy: 0.4810 - val_loss: 0.7195 - val_accuracy: 0.7031\n",
      "Epoch 285/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2107 - accuracy: 0.4810 - val_loss: 0.7274 - val_accuracy: 0.7031\n",
      "Epoch 286/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2105 - accuracy: 0.4821 - val_loss: 0.7260 - val_accuracy: 0.7031\n",
      "Epoch 287/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2112 - accuracy: 0.4799 - val_loss: 0.7211 - val_accuracy: 0.7031\n",
      "Epoch 288/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2109 - accuracy: 0.4821 - val_loss: 0.7210 - val_accuracy: 0.7031\n",
      "Epoch 289/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2107 - accuracy: 0.4777 - val_loss: 0.7236 - val_accuracy: 0.7031\n",
      "Epoch 290/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2108 - accuracy: 0.4788 - val_loss: 0.7248 - val_accuracy: 0.7031\n",
      "Epoch 291/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2105 - accuracy: 0.4821 - val_loss: 0.7213 - val_accuracy: 0.7031\n",
      "Epoch 292/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2108 - accuracy: 0.4805 - val_loss: 0.7230 - val_accuracy: 0.7031\n",
      "Epoch 293/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2110 - accuracy: 0.4838 - val_loss: 0.7208 - val_accuracy: 0.7031\n",
      "Epoch 294/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2114 - accuracy: 0.4810 - val_loss: 0.7193 - val_accuracy: 0.7031\n",
      "Epoch 295/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2112 - accuracy: 0.4799 - val_loss: 0.7192 - val_accuracy: 0.7031\n",
      "Epoch 296/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2108 - accuracy: 0.4816 - val_loss: 0.7230 - val_accuracy: 0.7031\n",
      "Epoch 297/300\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1.2106 - accuracy: 0.4794 - val_loss: 0.7245 - val_accuracy: 0.7031\n",
      "Epoch 298/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.2118 - accuracy: 0.4821 - val_loss: 0.7190 - val_accuracy: 0.7031\n",
      "Epoch 299/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.2117 - accuracy: 0.4805 - val_loss: 0.7193 - val_accuracy: 0.7031\n",
      "Epoch 300/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.2108 - accuracy: 0.4810 - val_loss: 0.7226 - val_accuracy: 0.7031\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "56/56 [==============================] - 0s 1ms/step\n",
      "Recall: [0.93 0.   0.56]\n",
      "Precision: [0.4  0.   0.69]\n",
      "F1-score: [0.56 0.   0.62]\n",
      "Accuracy: 0.48\n",
      "Brier score:0.2041258082669242\n",
      "Brier climat:0.21012400793650793\n",
      "Brier skill score:0.02854599875801045\n",
      "Recall: [0.93 0.   0.56]\n",
      "Precision: [0.4  0.   0.69]\n",
      "F1-score: [0.56 0.   0.62]\n",
      "Accuracy: 0.48\n",
      "Brier score:0.22402450400276694\n",
      "Brier climat:0.22239335317460318\n",
      "Brier skill score:-0.007334530483395829\n",
      "Recall: [0.93 0.   0.56]\n",
      "Precision: [0.4  0.   0.69]\n",
      "F1-score: [0.56 0.   0.62]\n",
      "Accuracy: 0.48\n",
      "Brier score:0.16103531096509446\n",
      "Brier climat:0.22492559523809527\n",
      "Brier skill score:0.2840507511178071\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "Recall: [0.95 0.   1.  ]\n",
      "Precision: [0.6  0.   0.81]\n",
      "F1-score: [0.74 0.   0.89]\n",
      "Accuracy: 0.7\n",
      "Brier score:0.13666054470740518\n",
      "Brier climat:0.21164930555555556\n",
      "Brier skill score:0.3543066708927456\n",
      "Recall: [0.95 0.   1.  ]\n",
      "Precision: [0.6  0.   0.81]\n",
      "F1-score: [0.74 0.   0.89]\n",
      "Accuracy: 0.7\n",
      "Brier score:0.1873222421417646\n",
      "Brier climat:0.20628472222222222\n",
      "Brier skill score:0.0919238219688907\n",
      "Recall: [0.95 0.   1.  ]\n",
      "Precision: [0.6  0.   0.81]\n",
      "F1-score: [0.74 0.   0.89]\n",
      "Accuracy: 0.7\n",
      "Brier score:0.06607053735707442\n",
      "Brier climat:0.2377604166666667\n",
      "Brier skill score:0.7221129644565545\n",
      "******** 2\n",
      "validation years [1983, 1984]\n",
      "train years {1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982}\n",
      "{0: 2.0, 1: 1.0, 2: 1.0}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 60)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 183       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183\n",
      "Trainable params: 183\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 1.7044 - accuracy: 0.4543WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "28/28 [==============================] - 4s 107ms/step - loss: 1.7007 - accuracy: 0.4537 - val_loss: 1.3297 - val_accuracy: 0.4688\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.5797 - accuracy: 0.4771 - val_loss: 1.3245 - val_accuracy: 0.4844\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.5114 - accuracy: 0.4927 - val_loss: 1.3262 - val_accuracy: 0.4766\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.4711 - accuracy: 0.4883 - val_loss: 1.3250 - val_accuracy: 0.4219\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4464 - accuracy: 0.4939 - val_loss: 1.3294 - val_accuracy: 0.4141\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.4274 - accuracy: 0.5022 - val_loss: 1.3226 - val_accuracy: 0.3984\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.4124 - accuracy: 0.5061 - val_loss: 1.3163 - val_accuracy: 0.3672\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.3997 - accuracy: 0.5039 - val_loss: 1.3171 - val_accuracy: 0.3672\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3874 - accuracy: 0.5056 - val_loss: 1.3130 - val_accuracy: 0.3672\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3766 - accuracy: 0.5039 - val_loss: 1.3134 - val_accuracy: 0.3672\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3658 - accuracy: 0.5000 - val_loss: 1.3118 - val_accuracy: 0.3672\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3557 - accuracy: 0.5045 - val_loss: 1.3063 - val_accuracy: 0.3672\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3471 - accuracy: 0.5039 - val_loss: 1.3000 - val_accuracy: 0.3672\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.3395 - accuracy: 0.4983 - val_loss: 1.2928 - val_accuracy: 0.3672\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3310 - accuracy: 0.4983 - val_loss: 1.2911 - val_accuracy: 0.3672\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3234 - accuracy: 0.4978 - val_loss: 1.2870 - val_accuracy: 0.3672\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3173 - accuracy: 0.5000 - val_loss: 1.2878 - val_accuracy: 0.3672\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3103 - accuracy: 0.4972 - val_loss: 1.2731 - val_accuracy: 0.3672\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3041 - accuracy: 0.4989 - val_loss: 1.2749 - val_accuracy: 0.3672\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2985 - accuracy: 0.4983 - val_loss: 1.2711 - val_accuracy: 0.3672\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2934 - accuracy: 0.4967 - val_loss: 1.2638 - val_accuracy: 0.3672\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2876 - accuracy: 0.4983 - val_loss: 1.2584 - val_accuracy: 0.3672\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2820 - accuracy: 0.4955 - val_loss: 1.2609 - val_accuracy: 0.3672\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2784 - accuracy: 0.4967 - val_loss: 1.2497 - val_accuracy: 0.3672\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2725 - accuracy: 0.4944 - val_loss: 1.2460 - val_accuracy: 0.3672\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2683 - accuracy: 0.4983 - val_loss: 1.2478 - val_accuracy: 0.3672\n",
      "Epoch 27/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2645 - accuracy: 0.4989 - val_loss: 1.2409 - val_accuracy: 0.3672\n",
      "Epoch 28/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2618 - accuracy: 0.4978 - val_loss: 1.2381 - val_accuracy: 0.3672\n",
      "Epoch 29/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2581 - accuracy: 0.4972 - val_loss: 1.2288 - val_accuracy: 0.3672\n",
      "Epoch 30/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2550 - accuracy: 0.4961 - val_loss: 1.2331 - val_accuracy: 0.3672\n",
      "Epoch 31/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2518 - accuracy: 0.4972 - val_loss: 1.2251 - val_accuracy: 0.3672\n",
      "Epoch 32/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2491 - accuracy: 0.5011 - val_loss: 1.2166 - val_accuracy: 0.3672\n",
      "Epoch 33/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2465 - accuracy: 0.4983 - val_loss: 1.2165 - val_accuracy: 0.3672\n",
      "Epoch 34/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2436 - accuracy: 0.4972 - val_loss: 1.2141 - val_accuracy: 0.3672\n",
      "Epoch 35/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2411 - accuracy: 0.5000 - val_loss: 1.2146 - val_accuracy: 0.3672\n",
      "Epoch 36/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2392 - accuracy: 0.5006 - val_loss: 1.2143 - val_accuracy: 0.3672\n",
      "Epoch 37/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2370 - accuracy: 0.5011 - val_loss: 1.2185 - val_accuracy: 0.3672\n",
      "Epoch 38/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2351 - accuracy: 0.4972 - val_loss: 1.2094 - val_accuracy: 0.3672\n",
      "Epoch 39/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2338 - accuracy: 0.4994 - val_loss: 1.2178 - val_accuracy: 0.3672\n",
      "Epoch 40/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2311 - accuracy: 0.4989 - val_loss: 1.2101 - val_accuracy: 0.3672\n",
      "Epoch 41/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2307 - accuracy: 0.4983 - val_loss: 1.2065 - val_accuracy: 0.3672\n",
      "Epoch 42/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2288 - accuracy: 0.4961 - val_loss: 1.2007 - val_accuracy: 0.3672\n",
      "Epoch 43/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2264 - accuracy: 0.5011 - val_loss: 1.2048 - val_accuracy: 0.3672\n",
      "Epoch 44/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2242 - accuracy: 0.5000 - val_loss: 1.2036 - val_accuracy: 0.3672\n",
      "Epoch 45/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2231 - accuracy: 0.5006 - val_loss: 1.2026 - val_accuracy: 0.3672\n",
      "Epoch 46/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2219 - accuracy: 0.4989 - val_loss: 1.2052 - val_accuracy: 0.3672\n",
      "Epoch 47/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2201 - accuracy: 0.4989 - val_loss: 1.2050 - val_accuracy: 0.3672\n",
      "Epoch 48/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2191 - accuracy: 0.5000 - val_loss: 1.1981 - val_accuracy: 0.3672\n",
      "Epoch 49/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2177 - accuracy: 0.4994 - val_loss: 1.1953 - val_accuracy: 0.3672\n",
      "Epoch 50/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2164 - accuracy: 0.5022 - val_loss: 1.1980 - val_accuracy: 0.3672\n",
      "Epoch 51/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2153 - accuracy: 0.5017 - val_loss: 1.1962 - val_accuracy: 0.3672\n",
      "Epoch 52/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2136 - accuracy: 0.5006 - val_loss: 1.1994 - val_accuracy: 0.3672\n",
      "Epoch 53/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2121 - accuracy: 0.5011 - val_loss: 1.1965 - val_accuracy: 0.3672\n",
      "Epoch 54/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2112 - accuracy: 0.4967 - val_loss: 1.1938 - val_accuracy: 0.3672\n",
      "Epoch 55/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2097 - accuracy: 0.5006 - val_loss: 1.1899 - val_accuracy: 0.3672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2093 - accuracy: 0.4989 - val_loss: 1.1919 - val_accuracy: 0.3672\n",
      "Epoch 57/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2078 - accuracy: 0.5011 - val_loss: 1.1879 - val_accuracy: 0.3672\n",
      "Epoch 58/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2065 - accuracy: 0.5017 - val_loss: 1.1897 - val_accuracy: 0.3672\n",
      "Epoch 59/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2065 - accuracy: 0.5022 - val_loss: 1.1919 - val_accuracy: 0.3672\n",
      "Epoch 60/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2055 - accuracy: 0.5000 - val_loss: 1.1888 - val_accuracy: 0.3672\n",
      "Epoch 61/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2037 - accuracy: 0.4989 - val_loss: 1.1910 - val_accuracy: 0.3672\n",
      "Epoch 62/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2028 - accuracy: 0.4994 - val_loss: 1.1873 - val_accuracy: 0.3672\n",
      "Epoch 63/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2033 - accuracy: 0.5017 - val_loss: 1.1832 - val_accuracy: 0.3672\n",
      "Epoch 64/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2018 - accuracy: 0.4994 - val_loss: 1.1885 - val_accuracy: 0.3672\n",
      "Epoch 65/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2013 - accuracy: 0.4989 - val_loss: 1.1782 - val_accuracy: 0.3672\n",
      "Epoch 66/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2001 - accuracy: 0.5022 - val_loss: 1.1882 - val_accuracy: 0.3672\n",
      "Epoch 67/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1994 - accuracy: 0.4983 - val_loss: 1.1836 - val_accuracy: 0.3672\n",
      "Epoch 68/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1986 - accuracy: 0.5033 - val_loss: 1.1784 - val_accuracy: 0.3672\n",
      "Epoch 69/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1982 - accuracy: 0.5017 - val_loss: 1.1751 - val_accuracy: 0.3672\n",
      "Epoch 70/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1990 - accuracy: 0.5000 - val_loss: 1.1815 - val_accuracy: 0.3672\n",
      "Epoch 71/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1979 - accuracy: 0.5033 - val_loss: 1.1810 - val_accuracy: 0.3672\n",
      "Epoch 72/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1963 - accuracy: 0.5011 - val_loss: 1.1811 - val_accuracy: 0.3672\n",
      "Epoch 73/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1966 - accuracy: 0.5033 - val_loss: 1.1798 - val_accuracy: 0.3672\n",
      "Epoch 74/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1964 - accuracy: 0.5022 - val_loss: 1.1782 - val_accuracy: 0.3672\n",
      "Epoch 75/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.1962 - accuracy: 0.5033 - val_loss: 1.1770 - val_accuracy: 0.3672\n",
      "Epoch 76/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1950 - accuracy: 0.5017 - val_loss: 1.1797 - val_accuracy: 0.3672\n",
      "Epoch 77/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1948 - accuracy: 0.5017 - val_loss: 1.1716 - val_accuracy: 0.3672\n",
      "Epoch 78/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1951 - accuracy: 0.4994 - val_loss: 1.1695 - val_accuracy: 0.3672\n",
      "Epoch 79/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1941 - accuracy: 0.5050 - val_loss: 1.1736 - val_accuracy: 0.3672\n",
      "Epoch 80/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1936 - accuracy: 0.5033 - val_loss: 1.1699 - val_accuracy: 0.3672\n",
      "Epoch 81/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1939 - accuracy: 0.5022 - val_loss: 1.1737 - val_accuracy: 0.3672\n",
      "Epoch 82/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1931 - accuracy: 0.5033 - val_loss: 1.1715 - val_accuracy: 0.3672\n",
      "Epoch 83/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.1936 - accuracy: 0.5022 - val_loss: 1.1731 - val_accuracy: 0.3672\n",
      "Epoch 84/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1924 - accuracy: 0.5006 - val_loss: 1.1612 - val_accuracy: 0.3672\n",
      "Epoch 85/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1922 - accuracy: 0.5039 - val_loss: 1.1753 - val_accuracy: 0.3672\n",
      "Epoch 86/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1923 - accuracy: 0.5006 - val_loss: 1.1701 - val_accuracy: 0.3672\n",
      "Epoch 87/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1939 - accuracy: 0.5006 - val_loss: 1.1721 - val_accuracy: 0.3672\n",
      "Epoch 88/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1921 - accuracy: 0.5011 - val_loss: 1.1633 - val_accuracy: 0.3672\n",
      "Epoch 89/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1910 - accuracy: 0.5061 - val_loss: 1.1684 - val_accuracy: 0.3672\n",
      "Epoch 90/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1911 - accuracy: 0.5022 - val_loss: 1.1661 - val_accuracy: 0.3672\n",
      "Epoch 91/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1905 - accuracy: 0.5033 - val_loss: 1.1713 - val_accuracy: 0.3672\n",
      "Epoch 92/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1910 - accuracy: 0.5022 - val_loss: 1.1663 - val_accuracy: 0.3672\n",
      "Epoch 93/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1907 - accuracy: 0.5033 - val_loss: 1.1674 - val_accuracy: 0.3672\n",
      "Epoch 94/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1901 - accuracy: 0.5039 - val_loss: 1.1636 - val_accuracy: 0.3672\n",
      "Epoch 95/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1902 - accuracy: 0.5045 - val_loss: 1.1646 - val_accuracy: 0.3672\n",
      "Epoch 96/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1900 - accuracy: 0.5028 - val_loss: 1.1725 - val_accuracy: 0.3672\n",
      "Epoch 97/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1903 - accuracy: 0.5022 - val_loss: 1.1547 - val_accuracy: 0.3672\n",
      "Epoch 98/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1899 - accuracy: 0.5045 - val_loss: 1.1639 - val_accuracy: 0.3672\n",
      "Epoch 99/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1901 - accuracy: 0.5033 - val_loss: 1.1684 - val_accuracy: 0.3672\n",
      "Epoch 100/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1892 - accuracy: 0.5028 - val_loss: 1.1581 - val_accuracy: 0.3672\n",
      "Epoch 101/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1900 - accuracy: 0.5039 - val_loss: 1.1624 - val_accuracy: 0.3672\n",
      "Epoch 102/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1892 - accuracy: 0.5028 - val_loss: 1.1645 - val_accuracy: 0.3672\n",
      "Epoch 103/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1885 - accuracy: 0.5039 - val_loss: 1.1675 - val_accuracy: 0.3672\n",
      "Epoch 104/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1882 - accuracy: 0.5045 - val_loss: 1.1590 - val_accuracy: 0.3672\n",
      "Epoch 105/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1883 - accuracy: 0.5033 - val_loss: 1.1619 - val_accuracy: 0.3672\n",
      "Epoch 106/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1882 - accuracy: 0.5039 - val_loss: 1.1571 - val_accuracy: 0.3672\n",
      "Epoch 107/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1886 - accuracy: 0.5045 - val_loss: 1.1670 - val_accuracy: 0.3672\n",
      "Epoch 108/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1885 - accuracy: 0.5022 - val_loss: 1.1551 - val_accuracy: 0.3672\n",
      "Epoch 109/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1894 - accuracy: 0.5045 - val_loss: 1.1649 - val_accuracy: 0.3672\n",
      "Epoch 110/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1883 - accuracy: 0.5039 - val_loss: 1.1582 - val_accuracy: 0.3672\n",
      "Epoch 111/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1882 - accuracy: 0.5039 - val_loss: 1.1580 - val_accuracy: 0.3672\n",
      "Epoch 112/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1881 - accuracy: 0.5050 - val_loss: 1.1543 - val_accuracy: 0.3672\n",
      "Epoch 113/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1875 - accuracy: 0.5011 - val_loss: 1.1597 - val_accuracy: 0.3672\n",
      "Epoch 114/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1872 - accuracy: 0.5045 - val_loss: 1.1599 - val_accuracy: 0.3672\n",
      "Epoch 115/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1882 - accuracy: 0.5033 - val_loss: 1.1566 - val_accuracy: 0.3672\n",
      "Epoch 116/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1873 - accuracy: 0.5039 - val_loss: 1.1576 - val_accuracy: 0.3672\n",
      "Epoch 117/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1871 - accuracy: 0.5050 - val_loss: 1.1609 - val_accuracy: 0.3672\n",
      "Epoch 118/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1872 - accuracy: 0.5033 - val_loss: 1.1626 - val_accuracy: 0.3672\n",
      "Epoch 119/300\n",
      "28/28 [==============================] - 1s 49ms/step - loss: 1.1876 - accuracy: 0.5050 - val_loss: 1.1597 - val_accuracy: 0.3672\n",
      "Epoch 120/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1880 - accuracy: 0.5033 - val_loss: 1.1600 - val_accuracy: 0.3672\n",
      "Epoch 121/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1864 - accuracy: 0.5061 - val_loss: 1.1598 - val_accuracy: 0.3672\n",
      "Epoch 122/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1868 - accuracy: 0.5017 - val_loss: 1.1596 - val_accuracy: 0.3672\n",
      "Epoch 123/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1868 - accuracy: 0.5028 - val_loss: 1.1599 - val_accuracy: 0.3672\n",
      "Epoch 124/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1868 - accuracy: 0.5045 - val_loss: 1.1628 - val_accuracy: 0.3672\n",
      "Epoch 125/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1868 - accuracy: 0.5039 - val_loss: 1.1475 - val_accuracy: 0.3672\n",
      "Epoch 126/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1872 - accuracy: 0.5033 - val_loss: 1.1575 - val_accuracy: 0.3672\n",
      "Epoch 127/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1861 - accuracy: 0.5033 - val_loss: 1.1555 - val_accuracy: 0.3672\n",
      "Epoch 128/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1865 - accuracy: 0.5000 - val_loss: 1.1604 - val_accuracy: 0.3672\n",
      "Epoch 129/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1863 - accuracy: 0.5056 - val_loss: 1.1566 - val_accuracy: 0.3672\n",
      "Epoch 130/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1865 - accuracy: 0.5056 - val_loss: 1.1646 - val_accuracy: 0.3672\n",
      "Epoch 131/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1874 - accuracy: 0.5033 - val_loss: 1.1633 - val_accuracy: 0.3672\n",
      "Epoch 132/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1861 - accuracy: 0.5011 - val_loss: 1.1541 - val_accuracy: 0.3672\n",
      "Epoch 133/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1860 - accuracy: 0.5045 - val_loss: 1.1568 - val_accuracy: 0.3672\n",
      "Epoch 134/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1858 - accuracy: 0.5022 - val_loss: 1.1578 - val_accuracy: 0.3672\n",
      "Epoch 135/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1860 - accuracy: 0.5039 - val_loss: 1.1571 - val_accuracy: 0.3672\n",
      "Epoch 136/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1864 - accuracy: 0.5033 - val_loss: 1.1586 - val_accuracy: 0.3672\n",
      "Epoch 137/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1865 - accuracy: 0.5028 - val_loss: 1.1547 - val_accuracy: 0.3672\n",
      "Epoch 138/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1852 - accuracy: 0.5033 - val_loss: 1.1581 - val_accuracy: 0.3672\n",
      "Epoch 139/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1858 - accuracy: 0.5028 - val_loss: 1.1514 - val_accuracy: 0.3672\n",
      "Epoch 140/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1856 - accuracy: 0.5039 - val_loss: 1.1588 - val_accuracy: 0.3672\n",
      "Epoch 141/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1861 - accuracy: 0.5050 - val_loss: 1.1635 - val_accuracy: 0.3672\n",
      "Epoch 142/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1857 - accuracy: 0.5022 - val_loss: 1.1558 - val_accuracy: 0.3672\n",
      "Epoch 143/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1852 - accuracy: 0.5045 - val_loss: 1.1552 - val_accuracy: 0.3672\n",
      "Epoch 144/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1852 - accuracy: 0.5061 - val_loss: 1.1605 - val_accuracy: 0.3672\n",
      "Epoch 145/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1851 - accuracy: 0.5022 - val_loss: 1.1535 - val_accuracy: 0.3672\n",
      "Epoch 146/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1850 - accuracy: 0.5045 - val_loss: 1.1574 - val_accuracy: 0.3672\n",
      "Epoch 147/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1851 - accuracy: 0.5000 - val_loss: 1.1551 - val_accuracy: 0.3672\n",
      "Epoch 148/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1845 - accuracy: 0.5050 - val_loss: 1.1583 - val_accuracy: 0.3672\n",
      "Epoch 149/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1852 - accuracy: 0.5039 - val_loss: 1.1517 - val_accuracy: 0.3672\n",
      "Epoch 150/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1859 - accuracy: 0.5039 - val_loss: 1.1668 - val_accuracy: 0.3672\n",
      "Epoch 151/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1854 - accuracy: 0.5022 - val_loss: 1.1533 - val_accuracy: 0.3672\n",
      "Epoch 152/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1852 - accuracy: 0.5033 - val_loss: 1.1641 - val_accuracy: 0.3672\n",
      "Epoch 153/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1849 - accuracy: 0.5067 - val_loss: 1.1547 - val_accuracy: 0.3672\n",
      "Epoch 154/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1850 - accuracy: 0.5017 - val_loss: 1.1570 - val_accuracy: 0.3672\n",
      "Epoch 155/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1845 - accuracy: 0.5039 - val_loss: 1.1579 - val_accuracy: 0.3672\n",
      "Epoch 156/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1854 - accuracy: 0.5050 - val_loss: 1.1584 - val_accuracy: 0.3672\n",
      "Epoch 157/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1846 - accuracy: 0.5017 - val_loss: 1.1532 - val_accuracy: 0.3672\n",
      "Epoch 158/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1846 - accuracy: 0.5045 - val_loss: 1.1544 - val_accuracy: 0.3672\n",
      "Epoch 159/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1850 - accuracy: 0.5045 - val_loss: 1.1596 - val_accuracy: 0.3672\n",
      "Epoch 160/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1841 - accuracy: 0.5050 - val_loss: 1.1578 - val_accuracy: 0.3672\n",
      "Epoch 161/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1844 - accuracy: 0.5039 - val_loss: 1.1523 - val_accuracy: 0.3672\n",
      "Epoch 162/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1836 - accuracy: 0.5033 - val_loss: 1.1537 - val_accuracy: 0.3672\n",
      "Epoch 163/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1834 - accuracy: 0.5045 - val_loss: 1.1551 - val_accuracy: 0.3672\n",
      "Epoch 164/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1838 - accuracy: 0.5017 - val_loss: 1.1582 - val_accuracy: 0.3672\n",
      "Epoch 165/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1842 - accuracy: 0.5011 - val_loss: 1.1544 - val_accuracy: 0.3672\n",
      "Epoch 166/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1846 - accuracy: 0.5056 - val_loss: 1.1611 - val_accuracy: 0.3672\n",
      "Epoch 167/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1846 - accuracy: 0.5028 - val_loss: 1.1633 - val_accuracy: 0.3672\n",
      "Epoch 168/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1844 - accuracy: 0.5022 - val_loss: 1.1512 - val_accuracy: 0.3672\n",
      "Epoch 169/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1835 - accuracy: 0.5061 - val_loss: 1.1568 - val_accuracy: 0.3672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1834 - accuracy: 0.5033 - val_loss: 1.1553 - val_accuracy: 0.3672\n",
      "Epoch 171/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1836 - accuracy: 0.5039 - val_loss: 1.1530 - val_accuracy: 0.3672\n",
      "Epoch 172/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1852 - accuracy: 0.5028 - val_loss: 1.1522 - val_accuracy: 0.3672\n",
      "Epoch 173/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1837 - accuracy: 0.5050 - val_loss: 1.1539 - val_accuracy: 0.3672\n",
      "Epoch 174/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1841 - accuracy: 0.5045 - val_loss: 1.1583 - val_accuracy: 0.3672\n",
      "Epoch 175/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1840 - accuracy: 0.5033 - val_loss: 1.1542 - val_accuracy: 0.3672\n",
      "Epoch 176/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1834 - accuracy: 0.5006 - val_loss: 1.1559 - val_accuracy: 0.3672\n",
      "Epoch 177/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1842 - accuracy: 0.5045 - val_loss: 1.1595 - val_accuracy: 0.3672\n",
      "Epoch 178/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1848 - accuracy: 0.5000 - val_loss: 1.1620 - val_accuracy: 0.3672\n",
      "Epoch 179/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1841 - accuracy: 0.5050 - val_loss: 1.1542 - val_accuracy: 0.3672\n",
      "Epoch 180/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1834 - accuracy: 0.5017 - val_loss: 1.1554 - val_accuracy: 0.3672\n",
      "Epoch 181/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1834 - accuracy: 0.5050 - val_loss: 1.1539 - val_accuracy: 0.3672\n",
      "Epoch 182/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1836 - accuracy: 0.5028 - val_loss: 1.1591 - val_accuracy: 0.3672\n",
      "Epoch 183/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1842 - accuracy: 0.5011 - val_loss: 1.1563 - val_accuracy: 0.3672\n",
      "Epoch 184/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1829 - accuracy: 0.5028 - val_loss: 1.1573 - val_accuracy: 0.3672\n",
      "Epoch 185/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1836 - accuracy: 0.5028 - val_loss: 1.1461 - val_accuracy: 0.3672\n",
      "Epoch 186/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1833 - accuracy: 0.5022 - val_loss: 1.1518 - val_accuracy: 0.3672\n",
      "Epoch 187/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1837 - accuracy: 0.5067 - val_loss: 1.1527 - val_accuracy: 0.3672\n",
      "Epoch 188/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1836 - accuracy: 0.5033 - val_loss: 1.1517 - val_accuracy: 0.3672\n",
      "Epoch 189/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1831 - accuracy: 0.5045 - val_loss: 1.1541 - val_accuracy: 0.3672\n",
      "Epoch 190/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1834 - accuracy: 0.5033 - val_loss: 1.1497 - val_accuracy: 0.3672\n",
      "Epoch 191/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1831 - accuracy: 0.5039 - val_loss: 1.1586 - val_accuracy: 0.3672\n",
      "Epoch 192/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1827 - accuracy: 0.5050 - val_loss: 1.1518 - val_accuracy: 0.3672\n",
      "Epoch 193/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1825 - accuracy: 0.5028 - val_loss: 1.1563 - val_accuracy: 0.3672\n",
      "Epoch 194/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1832 - accuracy: 0.5039 - val_loss: 1.1516 - val_accuracy: 0.3672\n",
      "Epoch 195/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1826 - accuracy: 0.5045 - val_loss: 1.1511 - val_accuracy: 0.3672\n",
      "Epoch 196/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1832 - accuracy: 0.5011 - val_loss: 1.1570 - val_accuracy: 0.3672\n",
      "Epoch 197/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1835 - accuracy: 0.5061 - val_loss: 1.1522 - val_accuracy: 0.3672\n",
      "Epoch 198/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1830 - accuracy: 0.5045 - val_loss: 1.1584 - val_accuracy: 0.3672\n",
      "Epoch 199/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1825 - accuracy: 0.5056 - val_loss: 1.1508 - val_accuracy: 0.3672\n",
      "Epoch 200/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1827 - accuracy: 0.5039 - val_loss: 1.1489 - val_accuracy: 0.3672\n",
      "Epoch 201/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1826 - accuracy: 0.5061 - val_loss: 1.1518 - val_accuracy: 0.3672\n",
      "Epoch 202/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1827 - accuracy: 0.5033 - val_loss: 1.1586 - val_accuracy: 0.3672\n",
      "Epoch 203/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1839 - accuracy: 0.5022 - val_loss: 1.1512 - val_accuracy: 0.3672\n",
      "Epoch 204/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1827 - accuracy: 0.5045 - val_loss: 1.1516 - val_accuracy: 0.3672\n",
      "Epoch 205/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1825 - accuracy: 0.5006 - val_loss: 1.1513 - val_accuracy: 0.3672\n",
      "Epoch 206/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1822 - accuracy: 0.5050 - val_loss: 1.1482 - val_accuracy: 0.3672\n",
      "Epoch 207/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1825 - accuracy: 0.5022 - val_loss: 1.1526 - val_accuracy: 0.3672\n",
      "Epoch 208/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1826 - accuracy: 0.5050 - val_loss: 1.1561 - val_accuracy: 0.3672\n",
      "Epoch 209/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1826 - accuracy: 0.5033 - val_loss: 1.1585 - val_accuracy: 0.3672\n",
      "Epoch 210/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1836 - accuracy: 0.5039 - val_loss: 1.1516 - val_accuracy: 0.3672\n",
      "Epoch 211/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1821 - accuracy: 0.5045 - val_loss: 1.1528 - val_accuracy: 0.3672\n",
      "Epoch 212/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1825 - accuracy: 0.5050 - val_loss: 1.1563 - val_accuracy: 0.3672\n",
      "Epoch 213/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1822 - accuracy: 0.5017 - val_loss: 1.1532 - val_accuracy: 0.3672\n",
      "Epoch 214/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1819 - accuracy: 0.5056 - val_loss: 1.1510 - val_accuracy: 0.3672\n",
      "Epoch 215/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1823 - accuracy: 0.5050 - val_loss: 1.1482 - val_accuracy: 0.3672\n",
      "Epoch 216/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1820 - accuracy: 0.5050 - val_loss: 1.1540 - val_accuracy: 0.3672\n",
      "Epoch 217/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1824 - accuracy: 0.5011 - val_loss: 1.1494 - val_accuracy: 0.3672\n",
      "Epoch 218/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1820 - accuracy: 0.5050 - val_loss: 1.1542 - val_accuracy: 0.3672\n",
      "Epoch 219/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1823 - accuracy: 0.5050 - val_loss: 1.1567 - val_accuracy: 0.3672\n",
      "Epoch 220/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1828 - accuracy: 0.5006 - val_loss: 1.1537 - val_accuracy: 0.3672\n",
      "Epoch 221/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1833 - accuracy: 0.5056 - val_loss: 1.1518 - val_accuracy: 0.3672\n",
      "Epoch 222/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1823 - accuracy: 0.5028 - val_loss: 1.1578 - val_accuracy: 0.3672\n",
      "Epoch 223/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1824 - accuracy: 0.5061 - val_loss: 1.1568 - val_accuracy: 0.3672\n",
      "Epoch 224/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1816 - accuracy: 0.5039 - val_loss: 1.1523 - val_accuracy: 0.3672\n",
      "Epoch 225/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1817 - accuracy: 0.5061 - val_loss: 1.1492 - val_accuracy: 0.3672\n",
      "Epoch 226/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1824 - accuracy: 0.5045 - val_loss: 1.1489 - val_accuracy: 0.3672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1821 - accuracy: 0.5050 - val_loss: 1.1472 - val_accuracy: 0.3672\n",
      "Epoch 228/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1818 - accuracy: 0.5056 - val_loss: 1.1582 - val_accuracy: 0.3672\n",
      "Epoch 229/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1819 - accuracy: 0.5067 - val_loss: 1.1504 - val_accuracy: 0.3672\n",
      "Epoch 230/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1813 - accuracy: 0.5056 - val_loss: 1.1530 - val_accuracy: 0.3672\n",
      "Epoch 231/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1819 - accuracy: 0.5056 - val_loss: 1.1509 - val_accuracy: 0.3672\n",
      "Epoch 232/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1832 - accuracy: 0.5050 - val_loss: 1.1541 - val_accuracy: 0.3672\n",
      "Epoch 233/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1826 - accuracy: 0.5061 - val_loss: 1.1587 - val_accuracy: 0.3672\n",
      "Epoch 234/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1818 - accuracy: 0.5033 - val_loss: 1.1457 - val_accuracy: 0.3672\n",
      "Epoch 235/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1814 - accuracy: 0.5056 - val_loss: 1.1507 - val_accuracy: 0.3672\n",
      "Epoch 236/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1812 - accuracy: 0.5050 - val_loss: 1.1490 - val_accuracy: 0.3672\n",
      "Epoch 237/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1821 - accuracy: 0.5056 - val_loss: 1.1471 - val_accuracy: 0.3672\n",
      "Epoch 238/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1822 - accuracy: 0.5056 - val_loss: 1.1553 - val_accuracy: 0.3672\n",
      "Epoch 239/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1831 - accuracy: 0.5061 - val_loss: 1.1433 - val_accuracy: 0.3672\n",
      "Epoch 240/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1811 - accuracy: 0.5045 - val_loss: 1.1540 - val_accuracy: 0.3672\n",
      "Epoch 241/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1811 - accuracy: 0.5045 - val_loss: 1.1526 - val_accuracy: 0.3672\n",
      "Epoch 242/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1820 - accuracy: 0.5045 - val_loss: 1.1512 - val_accuracy: 0.3672\n",
      "Epoch 243/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1818 - accuracy: 0.5056 - val_loss: 1.1455 - val_accuracy: 0.3672\n",
      "Epoch 244/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1817 - accuracy: 0.5039 - val_loss: 1.1561 - val_accuracy: 0.3672\n",
      "Epoch 245/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1824 - accuracy: 0.5028 - val_loss: 1.1622 - val_accuracy: 0.3672\n",
      "Epoch 246/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1822 - accuracy: 0.5067 - val_loss: 1.1566 - val_accuracy: 0.3672\n",
      "Epoch 247/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1816 - accuracy: 0.5050 - val_loss: 1.1527 - val_accuracy: 0.3672\n",
      "Epoch 248/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1817 - accuracy: 0.5039 - val_loss: 1.1474 - val_accuracy: 0.3672\n",
      "Epoch 249/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1813 - accuracy: 0.5061 - val_loss: 1.1461 - val_accuracy: 0.3672\n",
      "Epoch 250/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1823 - accuracy: 0.5039 - val_loss: 1.1568 - val_accuracy: 0.3672\n",
      "Epoch 251/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1814 - accuracy: 0.5050 - val_loss: 1.1484 - val_accuracy: 0.3672\n",
      "Epoch 252/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1824 - accuracy: 0.5050 - val_loss: 1.1521 - val_accuracy: 0.3672\n",
      "Epoch 253/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1817 - accuracy: 0.5050 - val_loss: 1.1553 - val_accuracy: 0.3672\n",
      "Epoch 254/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1814 - accuracy: 0.5056 - val_loss: 1.1490 - val_accuracy: 0.3672\n",
      "Epoch 255/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1817 - accuracy: 0.5073 - val_loss: 1.1533 - val_accuracy: 0.3672\n",
      "Epoch 256/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1818 - accuracy: 0.5045 - val_loss: 1.1553 - val_accuracy: 0.3672\n",
      "Epoch 257/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1820 - accuracy: 0.5067 - val_loss: 1.1584 - val_accuracy: 0.3672\n",
      "Epoch 258/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1826 - accuracy: 0.5039 - val_loss: 1.1494 - val_accuracy: 0.3672\n",
      "Epoch 259/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1819 - accuracy: 0.5028 - val_loss: 1.1506 - val_accuracy: 0.3672\n",
      "Epoch 260/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1820 - accuracy: 0.5045 - val_loss: 1.1503 - val_accuracy: 0.3672\n",
      "Epoch 261/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1819 - accuracy: 0.5033 - val_loss: 1.1499 - val_accuracy: 0.3672\n",
      "Epoch 262/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1820 - accuracy: 0.5056 - val_loss: 1.1479 - val_accuracy: 0.3672\n",
      "Epoch 263/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1826 - accuracy: 0.5050 - val_loss: 1.1550 - val_accuracy: 0.3672\n",
      "Epoch 264/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1823 - accuracy: 0.5033 - val_loss: 1.1592 - val_accuracy: 0.3672\n",
      "Epoch 265/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1811 - accuracy: 0.5067 - val_loss: 1.1500 - val_accuracy: 0.3672\n",
      "Epoch 266/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1816 - accuracy: 0.5039 - val_loss: 1.1562 - val_accuracy: 0.3672\n",
      "Epoch 267/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1813 - accuracy: 0.5033 - val_loss: 1.1495 - val_accuracy: 0.3672\n",
      "Epoch 268/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1810 - accuracy: 0.5045 - val_loss: 1.1476 - val_accuracy: 0.3672\n",
      "Epoch 269/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1820 - accuracy: 0.5028 - val_loss: 1.1541 - val_accuracy: 0.3672\n",
      "Epoch 270/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1813 - accuracy: 0.5045 - val_loss: 1.1508 - val_accuracy: 0.3672\n",
      "Epoch 271/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1813 - accuracy: 0.5045 - val_loss: 1.1481 - val_accuracy: 0.3672\n",
      "Epoch 272/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1814 - accuracy: 0.5050 - val_loss: 1.1535 - val_accuracy: 0.3672\n",
      "Epoch 273/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1814 - accuracy: 0.5045 - val_loss: 1.1466 - val_accuracy: 0.3672\n",
      "Epoch 274/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1812 - accuracy: 0.5017 - val_loss: 1.1528 - val_accuracy: 0.3672\n",
      "Epoch 275/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1817 - accuracy: 0.5045 - val_loss: 1.1578 - val_accuracy: 0.3672\n",
      "Epoch 276/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1815 - accuracy: 0.5050 - val_loss: 1.1605 - val_accuracy: 0.3672\n",
      "Epoch 277/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1815 - accuracy: 0.5039 - val_loss: 1.1570 - val_accuracy: 0.3672\n",
      "Epoch 278/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1817 - accuracy: 0.5028 - val_loss: 1.1493 - val_accuracy: 0.3672\n",
      "Epoch 279/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1813 - accuracy: 0.5067 - val_loss: 1.1576 - val_accuracy: 0.3672\n",
      "Epoch 280/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1814 - accuracy: 0.5022 - val_loss: 1.1592 - val_accuracy: 0.3672\n",
      "Epoch 281/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1820 - accuracy: 0.5050 - val_loss: 1.1502 - val_accuracy: 0.3672\n",
      "Epoch 282/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1818 - accuracy: 0.5039 - val_loss: 1.1564 - val_accuracy: 0.3672\n",
      "Epoch 283/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1819 - accuracy: 0.5039 - val_loss: 1.1558 - val_accuracy: 0.3672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 284/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1813 - accuracy: 0.5056 - val_loss: 1.1517 - val_accuracy: 0.3672\n",
      "Epoch 285/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1813 - accuracy: 0.5045 - val_loss: 1.1504 - val_accuracy: 0.3672\n",
      "Epoch 286/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1808 - accuracy: 0.5033 - val_loss: 1.1514 - val_accuracy: 0.3672\n",
      "Epoch 287/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1810 - accuracy: 0.5050 - val_loss: 1.1533 - val_accuracy: 0.3672\n",
      "Epoch 288/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1812 - accuracy: 0.5028 - val_loss: 1.1521 - val_accuracy: 0.3672\n",
      "Epoch 289/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1811 - accuracy: 0.5022 - val_loss: 1.1530 - val_accuracy: 0.3672\n",
      "Epoch 290/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1816 - accuracy: 0.5050 - val_loss: 1.1552 - val_accuracy: 0.3672\n",
      "Epoch 291/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1821 - accuracy: 0.5039 - val_loss: 1.1478 - val_accuracy: 0.3672\n",
      "Epoch 292/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1818 - accuracy: 0.5045 - val_loss: 1.1452 - val_accuracy: 0.3672\n",
      "Epoch 293/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1818 - accuracy: 0.5039 - val_loss: 1.1545 - val_accuracy: 0.3672\n",
      "Epoch 294/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1812 - accuracy: 0.5056 - val_loss: 1.1491 - val_accuracy: 0.3672\n",
      "Epoch 295/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1813 - accuracy: 0.5028 - val_loss: 1.1470 - val_accuracy: 0.3672\n",
      "Epoch 296/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1810 - accuracy: 0.5045 - val_loss: 1.1511 - val_accuracy: 0.3672\n",
      "Epoch 297/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1.1814 - accuracy: 0.5011 - val_loss: 1.1512 - val_accuracy: 0.3672\n",
      "Epoch 298/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1814 - accuracy: 0.5056 - val_loss: 1.1571 - val_accuracy: 0.3672\n",
      "Epoch 299/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1813 - accuracy: 0.5050 - val_loss: 1.1585 - val_accuracy: 0.3672\n",
      "Epoch 300/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1817 - accuracy: 0.5022 - val_loss: 1.1499 - val_accuracy: 0.3672\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "56/56 [==============================] - 0s 1ms/step\n",
      "Recall: [0.93 0.   0.6 ]\n",
      "Precision: [0.41 0.   0.7 ]\n",
      "F1-score: [0.57 0.   0.65]\n",
      "Accuracy: 0.5\n",
      "Brier score:0.196196340491392\n",
      "Brier climat:0.20930555555555558\n",
      "Brier skill score:0.06263194987523402\n",
      "Recall: [0.93 0.   0.6 ]\n",
      "Precision: [0.41 0.   0.7 ]\n",
      "F1-score: [0.57 0.   0.65]\n",
      "Accuracy: 0.5\n",
      "Brier score:0.2174331673325844\n",
      "Brier climat:0.21774305555555554\n",
      "Brier skill score:0.0014231830364486164\n",
      "Recall: [0.93 0.   0.6 ]\n",
      "Precision: [0.41 0.   0.7 ]\n",
      "F1-score: [0.57 0.   0.65]\n",
      "Accuracy: 0.5\n",
      "Brier score:0.1563441871327884\n",
      "Brier climat:0.22883184523809527\n",
      "Brier skill score:0.31677259793052326\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "4/4 [==============================] - 0s 993us/step\n",
      "Recall: [1. 0. 0.]\n",
      "Precision: [0.37 0.   0.  ]\n",
      "F1-score: [0.54 0.   0.  ]\n",
      "Accuracy: 0.37\n",
      "Brier score:0.2863971331695284\n",
      "Brier climat:0.22310763888888888\n",
      "Brier skill score:-0.283672466778059\n",
      "Recall: [1. 0. 0.]\n",
      "Precision: [0.37 0.   0.  ]\n",
      "F1-score: [0.54 0.   0.  ]\n",
      "Accuracy: 0.37\n",
      "Brier score:0.2863286299794648\n",
      "Brier climat:0.2713888888888889\n",
      "Brier skill score:-0.05504919951491627\n",
      "Recall: [1. 0. 0.]\n",
      "Precision: [0.37 0.   0.  ]\n",
      "F1-score: [0.54 0.   0.  ]\n",
      "Accuracy: 0.37\n",
      "Brier score:0.12700237971085895\n",
      "Brier climat:0.1830729166666667\n",
      "Brier skill score:0.30627434126643205\n",
      "******** 3\n",
      "validation years [1984, 1985]\n",
      "train years {1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "{0: 2.0, 1: 1.0, 2: 1.0}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 60)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 183       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183\n",
      "Trainable params: 183\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 1.7122 - accuracy: 0.4132WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "28/28 [==============================] - 4s 109ms/step - loss: 1.7020 - accuracy: 0.4191 - val_loss: 1.5557 - val_accuracy: 0.2344\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.5417 - accuracy: 0.4911 - val_loss: 1.4611 - val_accuracy: 0.2969\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.4939 - accuracy: 0.4961 - val_loss: 1.4336 - val_accuracy: 0.2578\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.4634 - accuracy: 0.4978 - val_loss: 1.4363 - val_accuracy: 0.2031\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4423 - accuracy: 0.5006 - val_loss: 1.4312 - val_accuracy: 0.2031\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4244 - accuracy: 0.4983 - val_loss: 1.4483 - val_accuracy: 0.2031\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4114 - accuracy: 0.5017 - val_loss: 1.4300 - val_accuracy: 0.2031\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3971 - accuracy: 0.5095 - val_loss: 1.4411 - val_accuracy: 0.2031\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3856 - accuracy: 0.5100 - val_loss: 1.4455 - val_accuracy: 0.2031\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3753 - accuracy: 0.5095 - val_loss: 1.4508 - val_accuracy: 0.2031\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3657 - accuracy: 0.5128 - val_loss: 1.4380 - val_accuracy: 0.2031\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3571 - accuracy: 0.5134 - val_loss: 1.4382 - val_accuracy: 0.2031\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3471 - accuracy: 0.5106 - val_loss: 1.4293 - val_accuracy: 0.2031\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3395 - accuracy: 0.5106 - val_loss: 1.4309 - val_accuracy: 0.2031\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3308 - accuracy: 0.5117 - val_loss: 1.4330 - val_accuracy: 0.2031\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3236 - accuracy: 0.5112 - val_loss: 1.4353 - val_accuracy: 0.2031\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3165 - accuracy: 0.5123 - val_loss: 1.4379 - val_accuracy: 0.2031\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3102 - accuracy: 0.5112 - val_loss: 1.4361 - val_accuracy: 0.2031\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3033 - accuracy: 0.5128 - val_loss: 1.4419 - val_accuracy: 0.2031\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2975 - accuracy: 0.5112 - val_loss: 1.4399 - val_accuracy: 0.2031\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2919 - accuracy: 0.5128 - val_loss: 1.4375 - val_accuracy: 0.2031\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2856 - accuracy: 0.5106 - val_loss: 1.4364 - val_accuracy: 0.2031\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2812 - accuracy: 0.5134 - val_loss: 1.4464 - val_accuracy: 0.2031\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2771 - accuracy: 0.5128 - val_loss: 1.4336 - val_accuracy: 0.2031\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2729 - accuracy: 0.5089 - val_loss: 1.4387 - val_accuracy: 0.2031\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2689 - accuracy: 0.5117 - val_loss: 1.4265 - val_accuracy: 0.2031\n",
      "Epoch 27/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2655 - accuracy: 0.5145 - val_loss: 1.4300 - val_accuracy: 0.2031\n",
      "Epoch 28/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2610 - accuracy: 0.5123 - val_loss: 1.4308 - val_accuracy: 0.2031\n",
      "Epoch 29/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2582 - accuracy: 0.5117 - val_loss: 1.4274 - val_accuracy: 0.2031\n",
      "Epoch 30/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2551 - accuracy: 0.5151 - val_loss: 1.4246 - val_accuracy: 0.2031\n",
      "Epoch 31/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2523 - accuracy: 0.5095 - val_loss: 1.4181 - val_accuracy: 0.2031\n",
      "Epoch 32/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2493 - accuracy: 0.5123 - val_loss: 1.4236 - val_accuracy: 0.2031\n",
      "Epoch 33/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2475 - accuracy: 0.5140 - val_loss: 1.4162 - val_accuracy: 0.2031\n",
      "Epoch 34/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2442 - accuracy: 0.5112 - val_loss: 1.4275 - val_accuracy: 0.2031\n",
      "Epoch 35/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2419 - accuracy: 0.5089 - val_loss: 1.4107 - val_accuracy: 0.2031\n",
      "Epoch 36/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2398 - accuracy: 0.5140 - val_loss: 1.4034 - val_accuracy: 0.2031\n",
      "Epoch 37/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2371 - accuracy: 0.5106 - val_loss: 1.4051 - val_accuracy: 0.2031\n",
      "Epoch 38/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2348 - accuracy: 0.5123 - val_loss: 1.4095 - val_accuracy: 0.2031\n",
      "Epoch 39/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2321 - accuracy: 0.5089 - val_loss: 1.4044 - val_accuracy: 0.2031\n",
      "Epoch 40/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2310 - accuracy: 0.5151 - val_loss: 1.4086 - val_accuracy: 0.2031\n",
      "Epoch 41/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2286 - accuracy: 0.5106 - val_loss: 1.4081 - val_accuracy: 0.2031\n",
      "Epoch 42/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2264 - accuracy: 0.5123 - val_loss: 1.3953 - val_accuracy: 0.2031\n",
      "Epoch 43/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2249 - accuracy: 0.5117 - val_loss: 1.3981 - val_accuracy: 0.2031\n",
      "Epoch 44/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2221 - accuracy: 0.5167 - val_loss: 1.3909 - val_accuracy: 0.2031\n",
      "Epoch 45/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2200 - accuracy: 0.5123 - val_loss: 1.3946 - val_accuracy: 0.2031\n",
      "Epoch 46/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2183 - accuracy: 0.5123 - val_loss: 1.3894 - val_accuracy: 0.2031\n",
      "Epoch 47/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2172 - accuracy: 0.5162 - val_loss: 1.3915 - val_accuracy: 0.2031\n",
      "Epoch 48/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2151 - accuracy: 0.5112 - val_loss: 1.3897 - val_accuracy: 0.2031\n",
      "Epoch 49/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2137 - accuracy: 0.5145 - val_loss: 1.3908 - val_accuracy: 0.2031\n",
      "Epoch 50/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2121 - accuracy: 0.5151 - val_loss: 1.3994 - val_accuracy: 0.2031\n",
      "Epoch 51/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2105 - accuracy: 0.5128 - val_loss: 1.3838 - val_accuracy: 0.2031\n",
      "Epoch 52/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2094 - accuracy: 0.5123 - val_loss: 1.3771 - val_accuracy: 0.2031\n",
      "Epoch 53/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2084 - accuracy: 0.5145 - val_loss: 1.3844 - val_accuracy: 0.2031\n",
      "Epoch 54/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2066 - accuracy: 0.5140 - val_loss: 1.3854 - val_accuracy: 0.2031\n",
      "Epoch 55/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2061 - accuracy: 0.5151 - val_loss: 1.3955 - val_accuracy: 0.2031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2049 - accuracy: 0.5134 - val_loss: 1.3868 - val_accuracy: 0.2031\n",
      "Epoch 57/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2027 - accuracy: 0.5145 - val_loss: 1.3802 - val_accuracy: 0.2031\n",
      "Epoch 58/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2024 - accuracy: 0.5140 - val_loss: 1.3826 - val_accuracy: 0.2031\n",
      "Epoch 59/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2018 - accuracy: 0.5162 - val_loss: 1.3867 - val_accuracy: 0.2031\n",
      "Epoch 60/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2015 - accuracy: 0.5140 - val_loss: 1.3768 - val_accuracy: 0.2031\n",
      "Epoch 61/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2001 - accuracy: 0.5156 - val_loss: 1.3875 - val_accuracy: 0.2031\n",
      "Epoch 62/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1999 - accuracy: 0.5156 - val_loss: 1.3873 - val_accuracy: 0.2031\n",
      "Epoch 63/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1996 - accuracy: 0.5151 - val_loss: 1.3825 - val_accuracy: 0.2031\n",
      "Epoch 64/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1986 - accuracy: 0.5151 - val_loss: 1.3717 - val_accuracy: 0.2031\n",
      "Epoch 65/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1977 - accuracy: 0.5151 - val_loss: 1.3849 - val_accuracy: 0.2031\n",
      "Epoch 66/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1980 - accuracy: 0.5179 - val_loss: 1.3833 - val_accuracy: 0.2031\n",
      "Epoch 67/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1966 - accuracy: 0.5117 - val_loss: 1.3782 - val_accuracy: 0.2031\n",
      "Epoch 68/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1971 - accuracy: 0.5151 - val_loss: 1.3697 - val_accuracy: 0.2031\n",
      "Epoch 69/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1967 - accuracy: 0.5179 - val_loss: 1.3714 - val_accuracy: 0.2031\n",
      "Epoch 70/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1951 - accuracy: 0.5151 - val_loss: 1.3820 - val_accuracy: 0.2031\n",
      "Epoch 71/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1949 - accuracy: 0.5156 - val_loss: 1.3805 - val_accuracy: 0.2031\n",
      "Epoch 72/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1951 - accuracy: 0.5151 - val_loss: 1.3673 - val_accuracy: 0.2031\n",
      "Epoch 73/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1949 - accuracy: 0.5190 - val_loss: 1.3805 - val_accuracy: 0.2031\n",
      "Epoch 74/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1944 - accuracy: 0.5145 - val_loss: 1.3673 - val_accuracy: 0.2031\n",
      "Epoch 75/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1937 - accuracy: 0.5151 - val_loss: 1.3791 - val_accuracy: 0.2031\n",
      "Epoch 76/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1935 - accuracy: 0.5156 - val_loss: 1.3723 - val_accuracy: 0.2031\n",
      "Epoch 77/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1931 - accuracy: 0.5162 - val_loss: 1.3852 - val_accuracy: 0.2031\n",
      "Epoch 78/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1928 - accuracy: 0.5145 - val_loss: 1.3737 - val_accuracy: 0.2031\n",
      "Epoch 79/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1924 - accuracy: 0.5179 - val_loss: 1.3794 - val_accuracy: 0.2031\n",
      "Epoch 80/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1916 - accuracy: 0.5145 - val_loss: 1.3677 - val_accuracy: 0.2031\n",
      "Epoch 81/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1923 - accuracy: 0.5156 - val_loss: 1.3876 - val_accuracy: 0.2031\n",
      "Epoch 82/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1918 - accuracy: 0.5162 - val_loss: 1.3697 - val_accuracy: 0.2031\n",
      "Epoch 83/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1915 - accuracy: 0.5145 - val_loss: 1.3644 - val_accuracy: 0.2031\n",
      "Epoch 84/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1906 - accuracy: 0.5151 - val_loss: 1.3656 - val_accuracy: 0.2031\n",
      "Epoch 85/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1904 - accuracy: 0.5140 - val_loss: 1.3687 - val_accuracy: 0.2031\n",
      "Epoch 86/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1900 - accuracy: 0.5151 - val_loss: 1.3612 - val_accuracy: 0.2031\n",
      "Epoch 87/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1898 - accuracy: 0.5173 - val_loss: 1.3764 - val_accuracy: 0.2031\n",
      "Epoch 88/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1901 - accuracy: 0.5151 - val_loss: 1.3639 - val_accuracy: 0.2031\n",
      "Epoch 89/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1898 - accuracy: 0.5140 - val_loss: 1.3653 - val_accuracy: 0.2031\n",
      "Epoch 90/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1895 - accuracy: 0.5151 - val_loss: 1.3719 - val_accuracy: 0.2031\n",
      "Epoch 91/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1894 - accuracy: 0.5128 - val_loss: 1.3631 - val_accuracy: 0.2031\n",
      "Epoch 92/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1878 - accuracy: 0.5167 - val_loss: 1.3702 - val_accuracy: 0.2031\n",
      "Epoch 93/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1885 - accuracy: 0.5156 - val_loss: 1.3656 - val_accuracy: 0.2031\n",
      "Epoch 94/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1888 - accuracy: 0.5123 - val_loss: 1.3657 - val_accuracy: 0.2031\n",
      "Epoch 95/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1888 - accuracy: 0.5140 - val_loss: 1.3661 - val_accuracy: 0.2031\n",
      "Epoch 96/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1876 - accuracy: 0.5140 - val_loss: 1.3562 - val_accuracy: 0.2031\n",
      "Epoch 97/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1873 - accuracy: 0.5151 - val_loss: 1.3727 - val_accuracy: 0.2031\n",
      "Epoch 98/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1876 - accuracy: 0.5134 - val_loss: 1.3599 - val_accuracy: 0.2031\n",
      "Epoch 99/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1873 - accuracy: 0.5140 - val_loss: 1.3552 - val_accuracy: 0.2031\n",
      "Epoch 100/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1877 - accuracy: 0.5140 - val_loss: 1.3600 - val_accuracy: 0.2031\n",
      "Epoch 101/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1879 - accuracy: 0.5140 - val_loss: 1.3612 - val_accuracy: 0.2031\n",
      "Epoch 102/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1868 - accuracy: 0.5167 - val_loss: 1.3521 - val_accuracy: 0.2031\n",
      "Epoch 103/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1866 - accuracy: 0.5140 - val_loss: 1.3630 - val_accuracy: 0.2031\n",
      "Epoch 104/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1867 - accuracy: 0.5179 - val_loss: 1.3628 - val_accuracy: 0.2031\n",
      "Epoch 105/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1864 - accuracy: 0.5145 - val_loss: 1.3622 - val_accuracy: 0.2031\n",
      "Epoch 106/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1861 - accuracy: 0.5151 - val_loss: 1.3591 - val_accuracy: 0.2031\n",
      "Epoch 107/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1853 - accuracy: 0.5167 - val_loss: 1.3601 - val_accuracy: 0.2031\n",
      "Epoch 108/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1852 - accuracy: 0.5128 - val_loss: 1.3636 - val_accuracy: 0.2031\n",
      "Epoch 109/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1852 - accuracy: 0.5145 - val_loss: 1.3663 - val_accuracy: 0.2031\n",
      "Epoch 110/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1864 - accuracy: 0.5156 - val_loss: 1.3759 - val_accuracy: 0.2031\n",
      "Epoch 111/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1862 - accuracy: 0.5134 - val_loss: 1.3559 - val_accuracy: 0.2031\n",
      "Epoch 112/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1851 - accuracy: 0.5134 - val_loss: 1.3551 - val_accuracy: 0.2031\n",
      "Epoch 113/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1847 - accuracy: 0.5128 - val_loss: 1.3630 - val_accuracy: 0.2031\n",
      "Epoch 114/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1842 - accuracy: 0.5128 - val_loss: 1.3566 - val_accuracy: 0.2031\n",
      "Epoch 115/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1845 - accuracy: 0.5156 - val_loss: 1.3529 - val_accuracy: 0.2031\n",
      "Epoch 116/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1844 - accuracy: 0.5167 - val_loss: 1.3642 - val_accuracy: 0.2031\n",
      "Epoch 117/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1842 - accuracy: 0.5128 - val_loss: 1.3713 - val_accuracy: 0.2031\n",
      "Epoch 118/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1841 - accuracy: 0.5151 - val_loss: 1.3510 - val_accuracy: 0.2031\n",
      "Epoch 119/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1841 - accuracy: 0.5145 - val_loss: 1.3532 - val_accuracy: 0.2031\n",
      "Epoch 120/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1839 - accuracy: 0.5145 - val_loss: 1.3551 - val_accuracy: 0.2031\n",
      "Epoch 121/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1834 - accuracy: 0.5134 - val_loss: 1.3561 - val_accuracy: 0.2031\n",
      "Epoch 122/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1835 - accuracy: 0.5151 - val_loss: 1.3599 - val_accuracy: 0.2031\n",
      "Epoch 123/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1836 - accuracy: 0.5140 - val_loss: 1.3569 - val_accuracy: 0.2031\n",
      "Epoch 124/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1833 - accuracy: 0.5162 - val_loss: 1.3547 - val_accuracy: 0.2031\n",
      "Epoch 125/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1838 - accuracy: 0.5123 - val_loss: 1.3668 - val_accuracy: 0.2031\n",
      "Epoch 126/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1827 - accuracy: 0.5145 - val_loss: 1.3618 - val_accuracy: 0.2031\n",
      "Epoch 127/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1835 - accuracy: 0.5145 - val_loss: 1.3634 - val_accuracy: 0.2031\n",
      "Epoch 128/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1828 - accuracy: 0.5151 - val_loss: 1.3575 - val_accuracy: 0.2031\n",
      "Epoch 129/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1834 - accuracy: 0.5134 - val_loss: 1.3529 - val_accuracy: 0.2031\n",
      "Epoch 130/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1831 - accuracy: 0.5151 - val_loss: 1.3520 - val_accuracy: 0.2031\n",
      "Epoch 131/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1833 - accuracy: 0.5128 - val_loss: 1.3615 - val_accuracy: 0.2031\n",
      "Epoch 132/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1826 - accuracy: 0.5140 - val_loss: 1.3456 - val_accuracy: 0.2031\n",
      "Epoch 133/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1833 - accuracy: 0.5151 - val_loss: 1.3678 - val_accuracy: 0.2031\n",
      "Epoch 134/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1841 - accuracy: 0.5140 - val_loss: 1.3470 - val_accuracy: 0.2031\n",
      "Epoch 135/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1824 - accuracy: 0.5151 - val_loss: 1.3599 - val_accuracy: 0.2031\n",
      "Epoch 136/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1828 - accuracy: 0.5145 - val_loss: 1.3433 - val_accuracy: 0.2031\n",
      "Epoch 137/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1824 - accuracy: 0.5140 - val_loss: 1.3543 - val_accuracy: 0.2031\n",
      "Epoch 138/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1821 - accuracy: 0.5162 - val_loss: 1.3565 - val_accuracy: 0.2031\n",
      "Epoch 139/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1826 - accuracy: 0.5140 - val_loss: 1.3533 - val_accuracy: 0.2031\n",
      "Epoch 140/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1824 - accuracy: 0.5134 - val_loss: 1.3559 - val_accuracy: 0.2031\n",
      "Epoch 141/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1817 - accuracy: 0.5128 - val_loss: 1.3514 - val_accuracy: 0.2031\n",
      "Epoch 142/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1818 - accuracy: 0.5140 - val_loss: 1.3384 - val_accuracy: 0.2031\n",
      "Epoch 143/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1815 - accuracy: 0.5151 - val_loss: 1.3559 - val_accuracy: 0.2031\n",
      "Epoch 144/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1821 - accuracy: 0.5117 - val_loss: 1.3405 - val_accuracy: 0.2031\n",
      "Epoch 145/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1818 - accuracy: 0.5162 - val_loss: 1.3552 - val_accuracy: 0.2031\n",
      "Epoch 146/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1820 - accuracy: 0.5140 - val_loss: 1.3527 - val_accuracy: 0.2031\n",
      "Epoch 147/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1818 - accuracy: 0.5134 - val_loss: 1.3550 - val_accuracy: 0.2031\n",
      "Epoch 148/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1812 - accuracy: 0.5140 - val_loss: 1.3515 - val_accuracy: 0.2031\n",
      "Epoch 149/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1816 - accuracy: 0.5123 - val_loss: 1.3379 - val_accuracy: 0.2031\n",
      "Epoch 150/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1812 - accuracy: 0.5128 - val_loss: 1.3537 - val_accuracy: 0.2031\n",
      "Epoch 151/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1806 - accuracy: 0.5140 - val_loss: 1.3546 - val_accuracy: 0.2031\n",
      "Epoch 152/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1816 - accuracy: 0.5162 - val_loss: 1.3465 - val_accuracy: 0.2031\n",
      "Epoch 153/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1813 - accuracy: 0.5134 - val_loss: 1.3483 - val_accuracy: 0.2031\n",
      "Epoch 154/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1816 - accuracy: 0.5140 - val_loss: 1.3511 - val_accuracy: 0.2031\n",
      "Epoch 155/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1815 - accuracy: 0.5134 - val_loss: 1.3492 - val_accuracy: 0.2031\n",
      "Epoch 156/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1816 - accuracy: 0.5128 - val_loss: 1.3489 - val_accuracy: 0.2031\n",
      "Epoch 157/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1807 - accuracy: 0.5140 - val_loss: 1.3518 - val_accuracy: 0.2031\n",
      "Epoch 158/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1806 - accuracy: 0.5162 - val_loss: 1.3611 - val_accuracy: 0.2031\n",
      "Epoch 159/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1809 - accuracy: 0.5128 - val_loss: 1.3572 - val_accuracy: 0.2031\n",
      "Epoch 160/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1806 - accuracy: 0.5128 - val_loss: 1.3491 - val_accuracy: 0.2031\n",
      "Epoch 161/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1819 - accuracy: 0.5140 - val_loss: 1.3693 - val_accuracy: 0.2031\n",
      "Epoch 162/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1820 - accuracy: 0.5123 - val_loss: 1.3563 - val_accuracy: 0.2031\n",
      "Epoch 163/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1812 - accuracy: 0.5134 - val_loss: 1.3415 - val_accuracy: 0.2031\n",
      "Epoch 164/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1809 - accuracy: 0.5140 - val_loss: 1.3680 - val_accuracy: 0.2031\n",
      "Epoch 165/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1804 - accuracy: 0.5128 - val_loss: 1.3425 - val_accuracy: 0.2031\n",
      "Epoch 166/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1802 - accuracy: 0.5134 - val_loss: 1.3586 - val_accuracy: 0.2031\n",
      "Epoch 167/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1802 - accuracy: 0.5123 - val_loss: 1.3523 - val_accuracy: 0.2031\n",
      "Epoch 168/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1800 - accuracy: 0.5134 - val_loss: 1.3515 - val_accuracy: 0.2031\n",
      "Epoch 169/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1801 - accuracy: 0.5151 - val_loss: 1.3542 - val_accuracy: 0.2031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1799 - accuracy: 0.5134 - val_loss: 1.3510 - val_accuracy: 0.2031\n",
      "Epoch 171/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1801 - accuracy: 0.5128 - val_loss: 1.3504 - val_accuracy: 0.2031\n",
      "Epoch 172/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1809 - accuracy: 0.5145 - val_loss: 1.3563 - val_accuracy: 0.2031\n",
      "Epoch 173/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1808 - accuracy: 0.5140 - val_loss: 1.3603 - val_accuracy: 0.2031\n",
      "Epoch 174/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1815 - accuracy: 0.5128 - val_loss: 1.3446 - val_accuracy: 0.2031\n",
      "Epoch 175/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1813 - accuracy: 0.5134 - val_loss: 1.3471 - val_accuracy: 0.2031\n",
      "Epoch 176/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1795 - accuracy: 0.5145 - val_loss: 1.3537 - val_accuracy: 0.2031\n",
      "Epoch 177/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1803 - accuracy: 0.5128 - val_loss: 1.3489 - val_accuracy: 0.2031\n",
      "Epoch 178/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1801 - accuracy: 0.5145 - val_loss: 1.3568 - val_accuracy: 0.2031\n",
      "Epoch 179/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1800 - accuracy: 0.5123 - val_loss: 1.3394 - val_accuracy: 0.2031\n",
      "Epoch 180/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1800 - accuracy: 0.5145 - val_loss: 1.3490 - val_accuracy: 0.2031\n",
      "Epoch 181/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1807 - accuracy: 0.5151 - val_loss: 1.3600 - val_accuracy: 0.2031\n",
      "Epoch 182/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1798 - accuracy: 0.5145 - val_loss: 1.3494 - val_accuracy: 0.2031\n",
      "Epoch 183/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1792 - accuracy: 0.5156 - val_loss: 1.3657 - val_accuracy: 0.2031\n",
      "Epoch 184/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1799 - accuracy: 0.5140 - val_loss: 1.3480 - val_accuracy: 0.2031\n",
      "Epoch 185/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1792 - accuracy: 0.5134 - val_loss: 1.3478 - val_accuracy: 0.2031\n",
      "Epoch 186/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1795 - accuracy: 0.5123 - val_loss: 1.3486 - val_accuracy: 0.2031\n",
      "Epoch 187/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1791 - accuracy: 0.5151 - val_loss: 1.3407 - val_accuracy: 0.2031\n",
      "Epoch 188/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1795 - accuracy: 0.5134 - val_loss: 1.3484 - val_accuracy: 0.2031\n",
      "Epoch 189/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1794 - accuracy: 0.5123 - val_loss: 1.3466 - val_accuracy: 0.2031\n",
      "Epoch 190/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1798 - accuracy: 0.5128 - val_loss: 1.3368 - val_accuracy: 0.2031\n",
      "Epoch 191/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1796 - accuracy: 0.5140 - val_loss: 1.3438 - val_accuracy: 0.2031\n",
      "Epoch 192/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1789 - accuracy: 0.5134 - val_loss: 1.3505 - val_accuracy: 0.2031\n",
      "Epoch 193/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1785 - accuracy: 0.5128 - val_loss: 1.3507 - val_accuracy: 0.2031\n",
      "Epoch 194/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1793 - accuracy: 0.5128 - val_loss: 1.3705 - val_accuracy: 0.2031\n",
      "Epoch 195/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1792 - accuracy: 0.5162 - val_loss: 1.3527 - val_accuracy: 0.2031\n",
      "Epoch 196/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1793 - accuracy: 0.5128 - val_loss: 1.3372 - val_accuracy: 0.2031\n",
      "Epoch 197/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1803 - accuracy: 0.5173 - val_loss: 1.3628 - val_accuracy: 0.2031\n",
      "Epoch 198/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1803 - accuracy: 0.5145 - val_loss: 1.3506 - val_accuracy: 0.2031\n",
      "Epoch 199/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1785 - accuracy: 0.5140 - val_loss: 1.3477 - val_accuracy: 0.2031\n",
      "Epoch 200/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1787 - accuracy: 0.5140 - val_loss: 1.3490 - val_accuracy: 0.2031\n",
      "Epoch 201/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1787 - accuracy: 0.5167 - val_loss: 1.3574 - val_accuracy: 0.2031\n",
      "Epoch 202/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1796 - accuracy: 0.5128 - val_loss: 1.3314 - val_accuracy: 0.2031\n",
      "Epoch 203/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1788 - accuracy: 0.5162 - val_loss: 1.3432 - val_accuracy: 0.2031\n",
      "Epoch 204/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1794 - accuracy: 0.5145 - val_loss: 1.3433 - val_accuracy: 0.2031\n",
      "Epoch 205/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1789 - accuracy: 0.5145 - val_loss: 1.3514 - val_accuracy: 0.2031\n",
      "Epoch 206/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1789 - accuracy: 0.5134 - val_loss: 1.3526 - val_accuracy: 0.2031\n",
      "Epoch 207/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1786 - accuracy: 0.5117 - val_loss: 1.3412 - val_accuracy: 0.2031\n",
      "Epoch 208/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1786 - accuracy: 0.5134 - val_loss: 1.3476 - val_accuracy: 0.2031\n",
      "Epoch 209/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1788 - accuracy: 0.5140 - val_loss: 1.3434 - val_accuracy: 0.2031\n",
      "Epoch 210/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1785 - accuracy: 0.5151 - val_loss: 1.3513 - val_accuracy: 0.2031\n",
      "Epoch 211/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1785 - accuracy: 0.5134 - val_loss: 1.3518 - val_accuracy: 0.2031\n",
      "Epoch 212/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1787 - accuracy: 0.5134 - val_loss: 1.3485 - val_accuracy: 0.2031\n",
      "Epoch 213/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1794 - accuracy: 0.5128 - val_loss: 1.3595 - val_accuracy: 0.2031\n",
      "Epoch 214/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1792 - accuracy: 0.5112 - val_loss: 1.3520 - val_accuracy: 0.2031\n",
      "Epoch 215/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1791 - accuracy: 0.5156 - val_loss: 1.3626 - val_accuracy: 0.2031\n",
      "Epoch 216/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1791 - accuracy: 0.5128 - val_loss: 1.3486 - val_accuracy: 0.2031\n",
      "Epoch 217/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1789 - accuracy: 0.5140 - val_loss: 1.3578 - val_accuracy: 0.2031\n",
      "Epoch 218/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1783 - accuracy: 0.5140 - val_loss: 1.3510 - val_accuracy: 0.2031\n",
      "Epoch 219/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1780 - accuracy: 0.5140 - val_loss: 1.3414 - val_accuracy: 0.2031\n",
      "Epoch 220/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1782 - accuracy: 0.5145 - val_loss: 1.3604 - val_accuracy: 0.2031\n",
      "Epoch 221/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1782 - accuracy: 0.5156 - val_loss: 1.3523 - val_accuracy: 0.2031\n",
      "Epoch 222/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1777 - accuracy: 0.5134 - val_loss: 1.3519 - val_accuracy: 0.2031\n",
      "Epoch 223/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1780 - accuracy: 0.5117 - val_loss: 1.3501 - val_accuracy: 0.2031\n",
      "Epoch 224/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1780 - accuracy: 0.5151 - val_loss: 1.3594 - val_accuracy: 0.2031\n",
      "Epoch 225/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1781 - accuracy: 0.5123 - val_loss: 1.3494 - val_accuracy: 0.2031\n",
      "Epoch 226/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1781 - accuracy: 0.5134 - val_loss: 1.3400 - val_accuracy: 0.2031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1791 - accuracy: 0.5151 - val_loss: 1.3639 - val_accuracy: 0.2031\n",
      "Epoch 228/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1782 - accuracy: 0.5128 - val_loss: 1.3456 - val_accuracy: 0.2031\n",
      "Epoch 229/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1783 - accuracy: 0.5156 - val_loss: 1.3568 - val_accuracy: 0.2031\n",
      "Epoch 230/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1787 - accuracy: 0.5145 - val_loss: 1.3446 - val_accuracy: 0.2031\n",
      "Epoch 231/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1776 - accuracy: 0.5140 - val_loss: 1.3518 - val_accuracy: 0.2031\n",
      "Epoch 232/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1779 - accuracy: 0.5145 - val_loss: 1.3493 - val_accuracy: 0.2031\n",
      "Epoch 233/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1776 - accuracy: 0.5128 - val_loss: 1.3528 - val_accuracy: 0.2031\n",
      "Epoch 234/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1782 - accuracy: 0.5123 - val_loss: 1.3445 - val_accuracy: 0.2031\n",
      "Epoch 235/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1777 - accuracy: 0.5134 - val_loss: 1.3532 - val_accuracy: 0.2031\n",
      "Epoch 236/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1777 - accuracy: 0.5134 - val_loss: 1.3569 - val_accuracy: 0.2031\n",
      "Epoch 237/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1781 - accuracy: 0.5134 - val_loss: 1.3435 - val_accuracy: 0.2031\n",
      "Epoch 238/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1785 - accuracy: 0.5123 - val_loss: 1.3482 - val_accuracy: 0.2031\n",
      "Epoch 239/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1778 - accuracy: 0.5140 - val_loss: 1.3547 - val_accuracy: 0.2031\n",
      "Epoch 240/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1780 - accuracy: 0.5145 - val_loss: 1.3643 - val_accuracy: 0.2031\n",
      "Epoch 241/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1782 - accuracy: 0.5134 - val_loss: 1.3600 - val_accuracy: 0.2031\n",
      "Epoch 242/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1780 - accuracy: 0.5128 - val_loss: 1.3530 - val_accuracy: 0.2031\n",
      "Epoch 243/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1775 - accuracy: 0.5134 - val_loss: 1.3521 - val_accuracy: 0.2031\n",
      "Epoch 244/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1775 - accuracy: 0.5140 - val_loss: 1.3538 - val_accuracy: 0.2031\n",
      "Epoch 245/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1784 - accuracy: 0.5151 - val_loss: 1.3411 - val_accuracy: 0.2031\n",
      "Epoch 246/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1772 - accuracy: 0.5151 - val_loss: 1.3638 - val_accuracy: 0.2031\n",
      "Epoch 247/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1776 - accuracy: 0.5140 - val_loss: 1.3585 - val_accuracy: 0.2031\n",
      "Epoch 248/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1775 - accuracy: 0.5134 - val_loss: 1.3523 - val_accuracy: 0.2031\n",
      "Epoch 249/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1779 - accuracy: 0.5123 - val_loss: 1.3620 - val_accuracy: 0.2031\n",
      "Epoch 250/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1783 - accuracy: 0.5128 - val_loss: 1.3525 - val_accuracy: 0.2031\n",
      "Epoch 251/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1782 - accuracy: 0.5145 - val_loss: 1.3451 - val_accuracy: 0.2031\n",
      "Epoch 252/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1771 - accuracy: 0.5134 - val_loss: 1.3608 - val_accuracy: 0.2031\n",
      "Epoch 253/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1783 - accuracy: 0.5117 - val_loss: 1.3463 - val_accuracy: 0.2031\n",
      "Epoch 254/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1781 - accuracy: 0.5134 - val_loss: 1.3470 - val_accuracy: 0.2031\n",
      "Epoch 255/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1774 - accuracy: 0.5134 - val_loss: 1.3481 - val_accuracy: 0.2031\n",
      "Epoch 256/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1774 - accuracy: 0.5123 - val_loss: 1.3526 - val_accuracy: 0.2031\n",
      "Epoch 257/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1782 - accuracy: 0.5128 - val_loss: 1.3611 - val_accuracy: 0.2031\n",
      "Epoch 258/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1781 - accuracy: 0.5151 - val_loss: 1.3641 - val_accuracy: 0.2031\n",
      "Epoch 259/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1780 - accuracy: 0.5134 - val_loss: 1.3592 - val_accuracy: 0.2031\n",
      "Epoch 260/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1770 - accuracy: 0.5128 - val_loss: 1.3526 - val_accuracy: 0.2031\n",
      "Epoch 261/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1774 - accuracy: 0.5151 - val_loss: 1.3628 - val_accuracy: 0.2031\n",
      "Epoch 262/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1772 - accuracy: 0.5128 - val_loss: 1.3538 - val_accuracy: 0.2031\n",
      "Epoch 263/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1775 - accuracy: 0.5128 - val_loss: 1.3546 - val_accuracy: 0.2031\n",
      "Epoch 264/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1787 - accuracy: 0.5145 - val_loss: 1.3592 - val_accuracy: 0.2031\n",
      "Epoch 265/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1776 - accuracy: 0.5134 - val_loss: 1.3641 - val_accuracy: 0.2031\n",
      "Epoch 266/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1776 - accuracy: 0.5128 - val_loss: 1.3439 - val_accuracy: 0.2031\n",
      "Epoch 267/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1770 - accuracy: 0.5134 - val_loss: 1.3601 - val_accuracy: 0.2031\n",
      "Epoch 268/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1771 - accuracy: 0.5145 - val_loss: 1.3441 - val_accuracy: 0.2031\n",
      "Epoch 269/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1778 - accuracy: 0.5156 - val_loss: 1.3491 - val_accuracy: 0.2031\n",
      "Epoch 270/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1772 - accuracy: 0.5140 - val_loss: 1.3473 - val_accuracy: 0.2031\n",
      "Epoch 271/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1767 - accuracy: 0.5134 - val_loss: 1.3547 - val_accuracy: 0.2031\n",
      "Epoch 272/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1776 - accuracy: 0.5140 - val_loss: 1.3565 - val_accuracy: 0.2031\n",
      "Epoch 273/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1771 - accuracy: 0.5140 - val_loss: 1.3439 - val_accuracy: 0.2031\n",
      "Epoch 274/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1772 - accuracy: 0.5128 - val_loss: 1.3504 - val_accuracy: 0.2031\n",
      "Epoch 275/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1784 - accuracy: 0.5145 - val_loss: 1.3606 - val_accuracy: 0.2031\n",
      "Epoch 276/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1772 - accuracy: 0.5151 - val_loss: 1.3504 - val_accuracy: 0.2031\n",
      "Epoch 277/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1766 - accuracy: 0.5134 - val_loss: 1.3509 - val_accuracy: 0.2031\n",
      "Epoch 278/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1769 - accuracy: 0.5140 - val_loss: 1.3399 - val_accuracy: 0.2031\n",
      "Epoch 279/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1767 - accuracy: 0.5128 - val_loss: 1.3554 - val_accuracy: 0.2031\n",
      "Epoch 280/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1777 - accuracy: 0.5151 - val_loss: 1.3659 - val_accuracy: 0.2031\n",
      "Epoch 281/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1777 - accuracy: 0.5140 - val_loss: 1.3546 - val_accuracy: 0.2031\n",
      "Epoch 282/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1775 - accuracy: 0.5134 - val_loss: 1.3499 - val_accuracy: 0.2031\n",
      "Epoch 283/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1781 - accuracy: 0.5128 - val_loss: 1.3640 - val_accuracy: 0.2031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 284/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1773 - accuracy: 0.5128 - val_loss: 1.3630 - val_accuracy: 0.2031\n",
      "Epoch 285/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1774 - accuracy: 0.5134 - val_loss: 1.3476 - val_accuracy: 0.2031\n",
      "Epoch 286/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1773 - accuracy: 0.5134 - val_loss: 1.3627 - val_accuracy: 0.2031\n",
      "Epoch 287/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1778 - accuracy: 0.5134 - val_loss: 1.3537 - val_accuracy: 0.2031\n",
      "Epoch 288/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1771 - accuracy: 0.5151 - val_loss: 1.3553 - val_accuracy: 0.2031\n",
      "Epoch 289/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1773 - accuracy: 0.5140 - val_loss: 1.3448 - val_accuracy: 0.2031\n",
      "Epoch 290/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1775 - accuracy: 0.5134 - val_loss: 1.3552 - val_accuracy: 0.2031\n",
      "Epoch 291/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1769 - accuracy: 0.5145 - val_loss: 1.3526 - val_accuracy: 0.2031\n",
      "Epoch 292/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1772 - accuracy: 0.5145 - val_loss: 1.3494 - val_accuracy: 0.2031\n",
      "Epoch 293/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1775 - accuracy: 0.5140 - val_loss: 1.3430 - val_accuracy: 0.2031\n",
      "Epoch 294/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1779 - accuracy: 0.5134 - val_loss: 1.3573 - val_accuracy: 0.2031\n",
      "Epoch 295/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1772 - accuracy: 0.5123 - val_loss: 1.3471 - val_accuracy: 0.2031\n",
      "Epoch 296/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1769 - accuracy: 0.5134 - val_loss: 1.3533 - val_accuracy: 0.2031\n",
      "Epoch 297/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1770 - accuracy: 0.5134 - val_loss: 1.3579 - val_accuracy: 0.2031\n",
      "Epoch 298/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1768 - accuracy: 0.5145 - val_loss: 1.3598 - val_accuracy: 0.2031\n",
      "Epoch 299/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1770 - accuracy: 0.5123 - val_loss: 1.3403 - val_accuracy: 0.2031\n",
      "Epoch 300/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1773 - accuracy: 0.5140 - val_loss: 1.3518 - val_accuracy: 0.2031\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "56/56 [==============================] - 0s 1ms/step\n",
      "Recall: [0.94 0.   0.6 ]\n",
      "Precision: [0.43 0.   0.71]\n",
      "F1-score: [0.59 0.   0.65]\n",
      "Accuracy: 0.51\n",
      "Brier score:0.1966391840243837\n",
      "Brier climat:0.21310019841269842\n",
      "Brier skill score:0.07724542028081849\n",
      "Recall: [0.94 0.   0.6 ]\n",
      "Precision: [0.43 0.   0.71]\n",
      "F1-score: [0.59 0.   0.65]\n",
      "Accuracy: 0.51\n",
      "Brier score:0.21447879689441549\n",
      "Brier climat:0.21402281746031745\n",
      "Brier skill score:-0.002130517855567282\n",
      "Recall: [0.94 0.   0.6 ]\n",
      "Precision: [0.43 0.   0.71]\n",
      "F1-score: [0.59 0.   0.65]\n",
      "Accuracy: 0.51\n",
      "Brier score:0.1553924396604602\n",
      "Brier climat:0.22883184523809527\n",
      "Brier skill score:0.320931754499566\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "Recall: [1. 0. 0.]\n",
      "Precision: [0.2 0.  0. ]\n",
      "F1-score: [0.34 0.   0.  ]\n",
      "Accuracy: 0.2\n",
      "Brier score:0.32425089049282524\n",
      "Brier climat:0.16998263888888887\n",
      "Brier skill score:-0.9075529866598648\n",
      "Recall: [1. 0. 0.]\n",
      "Precision: [0.2 0.  0. ]\n",
      "F1-score: [0.34 0.   0.  ]\n",
      "Accuracy: 0.2\n",
      "Brier score:0.3845914937026856\n",
      "Brier climat:0.32347222222222227\n",
      "Brier skill score:-0.18894751166137214\n",
      "Recall: [1. 0. 0.]\n",
      "Precision: [0.2 0.  0. ]\n",
      "F1-score: [0.34 0.   0.  ]\n",
      "Accuracy: 0.2\n",
      "Brier score:0.13490711260046007\n",
      "Brier climat:0.1830729166666667\n",
      "Brier skill score:0.26309628394627804\n",
      "******** 4\n",
      "validation years [1985, 1986]\n",
      "train years {1984, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "{0: 2.0, 1: 1.0, 2: 1.0}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 60)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 183       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183\n",
      "Trainable params: 183\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.6581 - accuracy: 0.4263WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "28/28 [==============================] - 4s 107ms/step - loss: 1.6581 - accuracy: 0.4263 - val_loss: 1.4665 - val_accuracy: 0.3203\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.5445 - accuracy: 0.4727 - val_loss: 1.3910 - val_accuracy: 0.3359\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4898 - accuracy: 0.4922 - val_loss: 1.3575 - val_accuracy: 0.3359\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4595 - accuracy: 0.5123 - val_loss: 1.3314 - val_accuracy: 0.2891\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4398 - accuracy: 0.5100 - val_loss: 1.3151 - val_accuracy: 0.3125\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4235 - accuracy: 0.5050 - val_loss: 1.3037 - val_accuracy: 0.3281\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4106 - accuracy: 0.5073 - val_loss: 1.2856 - val_accuracy: 0.3594\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3992 - accuracy: 0.5006 - val_loss: 1.2830 - val_accuracy: 0.3438\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3892 - accuracy: 0.4994 - val_loss: 1.2598 - val_accuracy: 0.3984\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3787 - accuracy: 0.4955 - val_loss: 1.2668 - val_accuracy: 0.3828\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3693 - accuracy: 0.4967 - val_loss: 1.2525 - val_accuracy: 0.4297\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3598 - accuracy: 0.4961 - val_loss: 1.2447 - val_accuracy: 0.4766\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3526 - accuracy: 0.4933 - val_loss: 1.2408 - val_accuracy: 0.5000\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3452 - accuracy: 0.4933 - val_loss: 1.2267 - val_accuracy: 0.5000\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3382 - accuracy: 0.4933 - val_loss: 1.2280 - val_accuracy: 0.5000\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3316 - accuracy: 0.4911 - val_loss: 1.2118 - val_accuracy: 0.5000\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3237 - accuracy: 0.4916 - val_loss: 1.2165 - val_accuracy: 0.5000\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3166 - accuracy: 0.4883 - val_loss: 1.2016 - val_accuracy: 0.5078\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3105 - accuracy: 0.4888 - val_loss: 1.1963 - val_accuracy: 0.5078\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3058 - accuracy: 0.4855 - val_loss: 1.1940 - val_accuracy: 0.5078\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2997 - accuracy: 0.4916 - val_loss: 1.1817 - val_accuracy: 0.5000\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2942 - accuracy: 0.4860 - val_loss: 1.1786 - val_accuracy: 0.5078\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2888 - accuracy: 0.4911 - val_loss: 1.1699 - val_accuracy: 0.5000\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2838 - accuracy: 0.4883 - val_loss: 1.1718 - val_accuracy: 0.5078\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2787 - accuracy: 0.4900 - val_loss: 1.1662 - val_accuracy: 0.5078\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2748 - accuracy: 0.4905 - val_loss: 1.1568 - val_accuracy: 0.5078\n",
      "Epoch 27/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2710 - accuracy: 0.4944 - val_loss: 1.1588 - val_accuracy: 0.5078\n",
      "Epoch 28/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2672 - accuracy: 0.4905 - val_loss: 1.1462 - val_accuracy: 0.5000\n",
      "Epoch 29/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2625 - accuracy: 0.4911 - val_loss: 1.1408 - val_accuracy: 0.5078\n",
      "Epoch 30/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2588 - accuracy: 0.4900 - val_loss: 1.1379 - val_accuracy: 0.5078\n",
      "Epoch 31/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2552 - accuracy: 0.4900 - val_loss: 1.1332 - val_accuracy: 0.5078\n",
      "Epoch 32/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2527 - accuracy: 0.4905 - val_loss: 1.1391 - val_accuracy: 0.5078\n",
      "Epoch 33/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2494 - accuracy: 0.4916 - val_loss: 1.1278 - val_accuracy: 0.5078\n",
      "Epoch 34/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2464 - accuracy: 0.4900 - val_loss: 1.1313 - val_accuracy: 0.5156\n",
      "Epoch 35/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2436 - accuracy: 0.4905 - val_loss: 1.1233 - val_accuracy: 0.5078\n",
      "Epoch 36/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2406 - accuracy: 0.4939 - val_loss: 1.1222 - val_accuracy: 0.5078\n",
      "Epoch 37/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2378 - accuracy: 0.4911 - val_loss: 1.1223 - val_accuracy: 0.5078\n",
      "Epoch 38/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2362 - accuracy: 0.4911 - val_loss: 1.1144 - val_accuracy: 0.5078\n",
      "Epoch 39/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2336 - accuracy: 0.4916 - val_loss: 1.1217 - val_accuracy: 0.5156\n",
      "Epoch 40/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2312 - accuracy: 0.4900 - val_loss: 1.1182 - val_accuracy: 0.5078\n",
      "Epoch 41/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2284 - accuracy: 0.4900 - val_loss: 1.1128 - val_accuracy: 0.5078\n",
      "Epoch 42/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2273 - accuracy: 0.4911 - val_loss: 1.1106 - val_accuracy: 0.5078\n",
      "Epoch 43/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2254 - accuracy: 0.4972 - val_loss: 1.1033 - val_accuracy: 0.5078\n",
      "Epoch 44/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2238 - accuracy: 0.4905 - val_loss: 1.1095 - val_accuracy: 0.5156\n",
      "Epoch 45/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2214 - accuracy: 0.4972 - val_loss: 1.1066 - val_accuracy: 0.5078\n",
      "Epoch 46/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2205 - accuracy: 0.4944 - val_loss: 1.1007 - val_accuracy: 0.5156\n",
      "Epoch 47/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2189 - accuracy: 0.4944 - val_loss: 1.1050 - val_accuracy: 0.5078\n",
      "Epoch 48/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2176 - accuracy: 0.4989 - val_loss: 1.1034 - val_accuracy: 0.5078\n",
      "Epoch 49/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2161 - accuracy: 0.4922 - val_loss: 1.0985 - val_accuracy: 0.5078\n",
      "Epoch 50/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2147 - accuracy: 0.4939 - val_loss: 1.0986 - val_accuracy: 0.5078\n",
      "Epoch 51/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2135 - accuracy: 0.4989 - val_loss: 1.1003 - val_accuracy: 0.5078\n",
      "Epoch 52/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2119 - accuracy: 0.4967 - val_loss: 1.0994 - val_accuracy: 0.5078\n",
      "Epoch 53/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2120 - accuracy: 0.4950 - val_loss: 1.0992 - val_accuracy: 0.5078\n",
      "Epoch 54/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2109 - accuracy: 0.4916 - val_loss: 1.1031 - val_accuracy: 0.5078\n",
      "Epoch 55/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2089 - accuracy: 0.4944 - val_loss: 1.0954 - val_accuracy: 0.5078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2083 - accuracy: 0.4911 - val_loss: 1.0973 - val_accuracy: 0.5078\n",
      "Epoch 57/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2078 - accuracy: 0.4939 - val_loss: 1.0849 - val_accuracy: 0.5078\n",
      "Epoch 58/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2070 - accuracy: 0.4916 - val_loss: 1.0932 - val_accuracy: 0.5078\n",
      "Epoch 59/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2063 - accuracy: 0.4961 - val_loss: 1.0945 - val_accuracy: 0.5078\n",
      "Epoch 60/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2058 - accuracy: 0.4922 - val_loss: 1.0944 - val_accuracy: 0.5078\n",
      "Epoch 61/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2050 - accuracy: 0.4927 - val_loss: 1.0898 - val_accuracy: 0.5156\n",
      "Epoch 62/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2043 - accuracy: 0.4939 - val_loss: 1.0925 - val_accuracy: 0.5078\n",
      "Epoch 63/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2039 - accuracy: 0.4916 - val_loss: 1.0930 - val_accuracy: 0.5078\n",
      "Epoch 64/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2035 - accuracy: 0.4922 - val_loss: 1.0893 - val_accuracy: 0.5078\n",
      "Epoch 65/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2028 - accuracy: 0.4983 - val_loss: 1.0933 - val_accuracy: 0.5078\n",
      "Epoch 66/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2015 - accuracy: 0.4927 - val_loss: 1.0864 - val_accuracy: 0.5156\n",
      "Epoch 67/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2011 - accuracy: 0.4905 - val_loss: 1.0862 - val_accuracy: 0.5156\n",
      "Epoch 68/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2004 - accuracy: 0.4916 - val_loss: 1.0914 - val_accuracy: 0.5156\n",
      "Epoch 69/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2002 - accuracy: 0.4911 - val_loss: 1.0940 - val_accuracy: 0.5078\n",
      "Epoch 70/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1998 - accuracy: 0.4950 - val_loss: 1.0889 - val_accuracy: 0.5156\n",
      "Epoch 71/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2001 - accuracy: 0.4905 - val_loss: 1.0850 - val_accuracy: 0.5156\n",
      "Epoch 72/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1988 - accuracy: 0.4922 - val_loss: 1.0863 - val_accuracy: 0.5156\n",
      "Epoch 73/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1981 - accuracy: 0.4911 - val_loss: 1.0845 - val_accuracy: 0.5156\n",
      "Epoch 74/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1975 - accuracy: 0.4916 - val_loss: 1.0912 - val_accuracy: 0.5156\n",
      "Epoch 75/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1982 - accuracy: 0.4911 - val_loss: 1.0867 - val_accuracy: 0.5156\n",
      "Epoch 76/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1974 - accuracy: 0.4922 - val_loss: 1.0829 - val_accuracy: 0.5156\n",
      "Epoch 77/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1966 - accuracy: 0.4922 - val_loss: 1.0852 - val_accuracy: 0.5156\n",
      "Epoch 78/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1970 - accuracy: 0.4944 - val_loss: 1.0769 - val_accuracy: 0.5156\n",
      "Epoch 79/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1975 - accuracy: 0.4905 - val_loss: 1.0911 - val_accuracy: 0.5156\n",
      "Epoch 80/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1959 - accuracy: 0.4927 - val_loss: 1.0833 - val_accuracy: 0.5156\n",
      "Epoch 81/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1949 - accuracy: 0.4905 - val_loss: 1.0884 - val_accuracy: 0.5156\n",
      "Epoch 82/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1942 - accuracy: 0.4933 - val_loss: 1.0836 - val_accuracy: 0.5156\n",
      "Epoch 83/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1942 - accuracy: 0.4933 - val_loss: 1.0838 - val_accuracy: 0.5156\n",
      "Epoch 84/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1942 - accuracy: 0.4927 - val_loss: 1.0839 - val_accuracy: 0.5156\n",
      "Epoch 85/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1942 - accuracy: 0.4916 - val_loss: 1.0853 - val_accuracy: 0.5156\n",
      "Epoch 86/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1933 - accuracy: 0.4922 - val_loss: 1.0808 - val_accuracy: 0.5156\n",
      "Epoch 87/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1929 - accuracy: 0.4905 - val_loss: 1.0837 - val_accuracy: 0.5156\n",
      "Epoch 88/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1928 - accuracy: 0.4950 - val_loss: 1.0875 - val_accuracy: 0.5156\n",
      "Epoch 89/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1927 - accuracy: 0.4922 - val_loss: 1.0762 - val_accuracy: 0.5156\n",
      "Epoch 90/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1926 - accuracy: 0.4927 - val_loss: 1.0879 - val_accuracy: 0.5156\n",
      "Epoch 91/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1925 - accuracy: 0.4944 - val_loss: 1.0868 - val_accuracy: 0.5156\n",
      "Epoch 92/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1931 - accuracy: 0.4927 - val_loss: 1.0783 - val_accuracy: 0.5156\n",
      "Epoch 93/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1919 - accuracy: 0.4922 - val_loss: 1.0800 - val_accuracy: 0.5156\n",
      "Epoch 94/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1909 - accuracy: 0.4927 - val_loss: 1.0771 - val_accuracy: 0.5156\n",
      "Epoch 95/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1911 - accuracy: 0.4905 - val_loss: 1.0853 - val_accuracy: 0.5156\n",
      "Epoch 96/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1910 - accuracy: 0.4911 - val_loss: 1.0778 - val_accuracy: 0.5156\n",
      "Epoch 97/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1906 - accuracy: 0.4933 - val_loss: 1.0776 - val_accuracy: 0.5156\n",
      "Epoch 98/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1909 - accuracy: 0.4911 - val_loss: 1.0752 - val_accuracy: 0.5156\n",
      "Epoch 99/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1910 - accuracy: 0.4933 - val_loss: 1.0709 - val_accuracy: 0.5156\n",
      "Epoch 100/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1903 - accuracy: 0.4911 - val_loss: 1.0803 - val_accuracy: 0.5156\n",
      "Epoch 101/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1909 - accuracy: 0.4916 - val_loss: 1.0753 - val_accuracy: 0.5156\n",
      "Epoch 102/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1906 - accuracy: 0.4911 - val_loss: 1.0781 - val_accuracy: 0.5156\n",
      "Epoch 103/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1899 - accuracy: 0.4905 - val_loss: 1.0760 - val_accuracy: 0.5156\n",
      "Epoch 104/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1896 - accuracy: 0.4961 - val_loss: 1.0701 - val_accuracy: 0.5156\n",
      "Epoch 105/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1897 - accuracy: 0.4944 - val_loss: 1.0756 - val_accuracy: 0.5156\n",
      "Epoch 106/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1892 - accuracy: 0.4927 - val_loss: 1.0742 - val_accuracy: 0.5156\n",
      "Epoch 107/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1892 - accuracy: 0.4944 - val_loss: 1.0717 - val_accuracy: 0.5156\n",
      "Epoch 108/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1889 - accuracy: 0.4922 - val_loss: 1.0786 - val_accuracy: 0.5156\n",
      "Epoch 109/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1892 - accuracy: 0.4933 - val_loss: 1.0786 - val_accuracy: 0.5156\n",
      "Epoch 110/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1896 - accuracy: 0.4911 - val_loss: 1.0737 - val_accuracy: 0.5156\n",
      "Epoch 111/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1899 - accuracy: 0.4916 - val_loss: 1.0717 - val_accuracy: 0.5156\n",
      "Epoch 112/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1889 - accuracy: 0.4939 - val_loss: 1.0778 - val_accuracy: 0.5156\n",
      "Epoch 113/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1882 - accuracy: 0.4944 - val_loss: 1.0734 - val_accuracy: 0.5156\n",
      "Epoch 114/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1886 - accuracy: 0.4911 - val_loss: 1.0754 - val_accuracy: 0.5234\n",
      "Epoch 115/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1885 - accuracy: 0.4939 - val_loss: 1.0701 - val_accuracy: 0.5156\n",
      "Epoch 116/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1884 - accuracy: 0.4916 - val_loss: 1.0737 - val_accuracy: 0.5234\n",
      "Epoch 117/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1876 - accuracy: 0.4939 - val_loss: 1.0749 - val_accuracy: 0.5156\n",
      "Epoch 118/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1883 - accuracy: 0.4955 - val_loss: 1.0719 - val_accuracy: 0.5156\n",
      "Epoch 119/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1879 - accuracy: 0.4927 - val_loss: 1.0740 - val_accuracy: 0.5234\n",
      "Epoch 120/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1878 - accuracy: 0.4939 - val_loss: 1.0706 - val_accuracy: 0.5156\n",
      "Epoch 121/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1881 - accuracy: 0.4905 - val_loss: 1.0709 - val_accuracy: 0.5234\n",
      "Epoch 122/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1883 - accuracy: 0.4911 - val_loss: 1.0816 - val_accuracy: 0.5234\n",
      "Epoch 123/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1882 - accuracy: 0.4933 - val_loss: 1.0729 - val_accuracy: 0.5156\n",
      "Epoch 124/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1872 - accuracy: 0.4916 - val_loss: 1.0765 - val_accuracy: 0.5234\n",
      "Epoch 125/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1877 - accuracy: 0.4922 - val_loss: 1.0670 - val_accuracy: 0.5156\n",
      "Epoch 126/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1874 - accuracy: 0.4927 - val_loss: 1.0762 - val_accuracy: 0.5234\n",
      "Epoch 127/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1885 - accuracy: 0.4978 - val_loss: 1.0769 - val_accuracy: 0.5234\n",
      "Epoch 128/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1887 - accuracy: 0.4922 - val_loss: 1.0724 - val_accuracy: 0.5156\n",
      "Epoch 129/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1872 - accuracy: 0.4927 - val_loss: 1.0741 - val_accuracy: 0.5234\n",
      "Epoch 130/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1876 - accuracy: 0.4961 - val_loss: 1.0746 - val_accuracy: 0.5234\n",
      "Epoch 131/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1873 - accuracy: 0.4905 - val_loss: 1.0703 - val_accuracy: 0.5234\n",
      "Epoch 132/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1870 - accuracy: 0.4967 - val_loss: 1.0693 - val_accuracy: 0.5234\n",
      "Epoch 133/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1864 - accuracy: 0.4933 - val_loss: 1.0738 - val_accuracy: 0.5234\n",
      "Epoch 134/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1869 - accuracy: 0.4911 - val_loss: 1.0716 - val_accuracy: 0.5234\n",
      "Epoch 135/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1868 - accuracy: 0.4933 - val_loss: 1.0696 - val_accuracy: 0.5234\n",
      "Epoch 136/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1871 - accuracy: 0.4922 - val_loss: 1.0716 - val_accuracy: 0.5234\n",
      "Epoch 137/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1867 - accuracy: 0.4922 - val_loss: 1.0708 - val_accuracy: 0.5234\n",
      "Epoch 138/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1867 - accuracy: 0.4916 - val_loss: 1.0704 - val_accuracy: 0.5234\n",
      "Epoch 139/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1866 - accuracy: 0.4927 - val_loss: 1.0760 - val_accuracy: 0.5234\n",
      "Epoch 140/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1865 - accuracy: 0.4927 - val_loss: 1.0692 - val_accuracy: 0.5234\n",
      "Epoch 141/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1863 - accuracy: 0.4916 - val_loss: 1.0692 - val_accuracy: 0.5234\n",
      "Epoch 142/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1859 - accuracy: 0.4927 - val_loss: 1.0726 - val_accuracy: 0.5234\n",
      "Epoch 143/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1862 - accuracy: 0.4933 - val_loss: 1.0676 - val_accuracy: 0.5234\n",
      "Epoch 144/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1859 - accuracy: 0.4916 - val_loss: 1.0714 - val_accuracy: 0.5234\n",
      "Epoch 145/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1865 - accuracy: 0.4955 - val_loss: 1.0723 - val_accuracy: 0.5234\n",
      "Epoch 146/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1860 - accuracy: 0.4955 - val_loss: 1.0761 - val_accuracy: 0.5234\n",
      "Epoch 147/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1863 - accuracy: 0.4916 - val_loss: 1.0719 - val_accuracy: 0.5234\n",
      "Epoch 148/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1867 - accuracy: 0.4916 - val_loss: 1.0664 - val_accuracy: 0.5234\n",
      "Epoch 149/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1861 - accuracy: 0.4922 - val_loss: 1.0665 - val_accuracy: 0.5234\n",
      "Epoch 150/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1856 - accuracy: 0.4939 - val_loss: 1.0699 - val_accuracy: 0.5234\n",
      "Epoch 151/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1858 - accuracy: 0.4911 - val_loss: 1.0734 - val_accuracy: 0.5234\n",
      "Epoch 152/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1859 - accuracy: 0.4944 - val_loss: 1.0771 - val_accuracy: 0.5234\n",
      "Epoch 153/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1854 - accuracy: 0.4939 - val_loss: 1.0656 - val_accuracy: 0.5234\n",
      "Epoch 154/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1859 - accuracy: 0.4911 - val_loss: 1.0764 - val_accuracy: 0.5234\n",
      "Epoch 155/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1856 - accuracy: 0.4911 - val_loss: 1.0656 - val_accuracy: 0.5234\n",
      "Epoch 156/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1857 - accuracy: 0.4905 - val_loss: 1.0694 - val_accuracy: 0.5234\n",
      "Epoch 157/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1862 - accuracy: 0.4911 - val_loss: 1.0670 - val_accuracy: 0.5234\n",
      "Epoch 158/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1867 - accuracy: 0.4939 - val_loss: 1.0671 - val_accuracy: 0.5234\n",
      "Epoch 159/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1865 - accuracy: 0.4939 - val_loss: 1.0673 - val_accuracy: 0.5234\n",
      "Epoch 160/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1847 - accuracy: 0.4911 - val_loss: 1.0679 - val_accuracy: 0.5234\n",
      "Epoch 161/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1851 - accuracy: 0.4922 - val_loss: 1.0753 - val_accuracy: 0.5234\n",
      "Epoch 162/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1861 - accuracy: 0.4916 - val_loss: 1.0663 - val_accuracy: 0.5234\n",
      "Epoch 163/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1853 - accuracy: 0.4955 - val_loss: 1.0719 - val_accuracy: 0.5234\n",
      "Epoch 164/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1852 - accuracy: 0.4911 - val_loss: 1.0708 - val_accuracy: 0.5234\n",
      "Epoch 165/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1857 - accuracy: 0.4955 - val_loss: 1.0693 - val_accuracy: 0.5234\n",
      "Epoch 166/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1853 - accuracy: 0.4922 - val_loss: 1.0728 - val_accuracy: 0.5234\n",
      "Epoch 167/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1850 - accuracy: 0.4972 - val_loss: 1.0745 - val_accuracy: 0.5234\n",
      "Epoch 168/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1853 - accuracy: 0.4933 - val_loss: 1.0637 - val_accuracy: 0.5234\n",
      "Epoch 169/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1846 - accuracy: 0.4916 - val_loss: 1.0719 - val_accuracy: 0.5234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1857 - accuracy: 0.4955 - val_loss: 1.0662 - val_accuracy: 0.5234\n",
      "Epoch 171/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1852 - accuracy: 0.4911 - val_loss: 1.0782 - val_accuracy: 0.5234\n",
      "Epoch 172/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1854 - accuracy: 0.4955 - val_loss: 1.0655 - val_accuracy: 0.5234\n",
      "Epoch 173/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1852 - accuracy: 0.4927 - val_loss: 1.0658 - val_accuracy: 0.5234\n",
      "Epoch 174/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1844 - accuracy: 0.4933 - val_loss: 1.0683 - val_accuracy: 0.5234\n",
      "Epoch 175/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1846 - accuracy: 0.4927 - val_loss: 1.0632 - val_accuracy: 0.5234\n",
      "Epoch 176/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1855 - accuracy: 0.4950 - val_loss: 1.0774 - val_accuracy: 0.5234\n",
      "Epoch 177/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1844 - accuracy: 0.4927 - val_loss: 1.0694 - val_accuracy: 0.5234\n",
      "Epoch 178/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1855 - accuracy: 0.4922 - val_loss: 1.0696 - val_accuracy: 0.5234\n",
      "Epoch 179/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1843 - accuracy: 0.4900 - val_loss: 1.0597 - val_accuracy: 0.5234\n",
      "Epoch 180/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1847 - accuracy: 0.4916 - val_loss: 1.0692 - val_accuracy: 0.5234\n",
      "Epoch 181/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1842 - accuracy: 0.4933 - val_loss: 1.0689 - val_accuracy: 0.5234\n",
      "Epoch 182/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1842 - accuracy: 0.4955 - val_loss: 1.0718 - val_accuracy: 0.5234\n",
      "Epoch 183/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1847 - accuracy: 0.4967 - val_loss: 1.0662 - val_accuracy: 0.5234\n",
      "Epoch 184/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1852 - accuracy: 0.4911 - val_loss: 1.0680 - val_accuracy: 0.5234\n",
      "Epoch 185/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1836 - accuracy: 0.4944 - val_loss: 1.0685 - val_accuracy: 0.5234\n",
      "Epoch 186/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1833 - accuracy: 0.4944 - val_loss: 1.0673 - val_accuracy: 0.5234\n",
      "Epoch 187/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1841 - accuracy: 0.4944 - val_loss: 1.0710 - val_accuracy: 0.5234\n",
      "Epoch 188/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1839 - accuracy: 0.4905 - val_loss: 1.0682 - val_accuracy: 0.5234\n",
      "Epoch 189/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1846 - accuracy: 0.4944 - val_loss: 1.0699 - val_accuracy: 0.5234\n",
      "Epoch 190/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1844 - accuracy: 0.4939 - val_loss: 1.0665 - val_accuracy: 0.5234\n",
      "Epoch 191/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1842 - accuracy: 0.4950 - val_loss: 1.0646 - val_accuracy: 0.5234\n",
      "Epoch 192/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1838 - accuracy: 0.4911 - val_loss: 1.0693 - val_accuracy: 0.5234\n",
      "Epoch 193/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1837 - accuracy: 0.4950 - val_loss: 1.0643 - val_accuracy: 0.5234\n",
      "Epoch 194/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1842 - accuracy: 0.4927 - val_loss: 1.0757 - val_accuracy: 0.5234\n",
      "Epoch 195/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1838 - accuracy: 0.4916 - val_loss: 1.0676 - val_accuracy: 0.5234\n",
      "Epoch 196/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1842 - accuracy: 0.4933 - val_loss: 1.0734 - val_accuracy: 0.5234\n",
      "Epoch 197/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1847 - accuracy: 0.4939 - val_loss: 1.0735 - val_accuracy: 0.5234\n",
      "Epoch 198/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1843 - accuracy: 0.4905 - val_loss: 1.0667 - val_accuracy: 0.5234\n",
      "Epoch 199/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1835 - accuracy: 0.4955 - val_loss: 1.0656 - val_accuracy: 0.5234\n",
      "Epoch 200/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1836 - accuracy: 0.4939 - val_loss: 1.0652 - val_accuracy: 0.5234\n",
      "Epoch 201/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1833 - accuracy: 0.4900 - val_loss: 1.0686 - val_accuracy: 0.5234\n",
      "Epoch 202/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1844 - accuracy: 0.4972 - val_loss: 1.0714 - val_accuracy: 0.5234\n",
      "Epoch 203/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1854 - accuracy: 0.4894 - val_loss: 1.0588 - val_accuracy: 0.5234\n",
      "Epoch 204/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1839 - accuracy: 0.4939 - val_loss: 1.0682 - val_accuracy: 0.5234\n",
      "Epoch 205/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1835 - accuracy: 0.4905 - val_loss: 1.0628 - val_accuracy: 0.5234\n",
      "Epoch 206/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1838 - accuracy: 0.4927 - val_loss: 1.0644 - val_accuracy: 0.5234\n",
      "Epoch 207/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1843 - accuracy: 0.4933 - val_loss: 1.0673 - val_accuracy: 0.5234\n",
      "Epoch 208/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1832 - accuracy: 0.4978 - val_loss: 1.0655 - val_accuracy: 0.5234\n",
      "Epoch 209/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1832 - accuracy: 0.4916 - val_loss: 1.0655 - val_accuracy: 0.5234\n",
      "Epoch 210/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1836 - accuracy: 0.4927 - val_loss: 1.0725 - val_accuracy: 0.5234\n",
      "Epoch 211/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1847 - accuracy: 0.4933 - val_loss: 1.0761 - val_accuracy: 0.5234\n",
      "Epoch 212/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1847 - accuracy: 0.4894 - val_loss: 1.0667 - val_accuracy: 0.5234\n",
      "Epoch 213/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1848 - accuracy: 0.4972 - val_loss: 1.0646 - val_accuracy: 0.5234\n",
      "Epoch 214/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1836 - accuracy: 0.4911 - val_loss: 1.0692 - val_accuracy: 0.5234\n",
      "Epoch 215/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1836 - accuracy: 0.4905 - val_loss: 1.0697 - val_accuracy: 0.5234\n",
      "Epoch 216/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1843 - accuracy: 0.4933 - val_loss: 1.0657 - val_accuracy: 0.5234\n",
      "Epoch 217/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1832 - accuracy: 0.4916 - val_loss: 1.0643 - val_accuracy: 0.5234\n",
      "Epoch 218/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1831 - accuracy: 0.4911 - val_loss: 1.0663 - val_accuracy: 0.5234\n",
      "Epoch 219/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1830 - accuracy: 0.4911 - val_loss: 1.0721 - val_accuracy: 0.5234\n",
      "Epoch 220/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1841 - accuracy: 0.4961 - val_loss: 1.0754 - val_accuracy: 0.5234\n",
      "Epoch 221/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1845 - accuracy: 0.4922 - val_loss: 1.0608 - val_accuracy: 0.5234\n",
      "Epoch 222/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1837 - accuracy: 0.4916 - val_loss: 1.0650 - val_accuracy: 0.5234\n",
      "Epoch 223/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1835 - accuracy: 0.4950 - val_loss: 1.0616 - val_accuracy: 0.5234\n",
      "Epoch 224/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1836 - accuracy: 0.4950 - val_loss: 1.0724 - val_accuracy: 0.5234\n",
      "Epoch 225/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1837 - accuracy: 0.4900 - val_loss: 1.0663 - val_accuracy: 0.5234\n",
      "Epoch 226/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1837 - accuracy: 0.4927 - val_loss: 1.0651 - val_accuracy: 0.5234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1834 - accuracy: 0.4916 - val_loss: 1.0624 - val_accuracy: 0.5234\n",
      "Epoch 228/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1839 - accuracy: 0.4922 - val_loss: 1.0714 - val_accuracy: 0.5234\n",
      "Epoch 229/300\n",
      "28/28 [==============================] - 2s 58ms/step - loss: 1.1835 - accuracy: 0.4905 - val_loss: 1.0642 - val_accuracy: 0.5234\n",
      "Epoch 230/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1834 - accuracy: 0.4916 - val_loss: 1.0651 - val_accuracy: 0.5234\n",
      "Epoch 231/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1838 - accuracy: 0.4933 - val_loss: 1.0720 - val_accuracy: 0.5234\n",
      "Epoch 232/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1834 - accuracy: 0.4933 - val_loss: 1.0613 - val_accuracy: 0.5234\n",
      "Epoch 233/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1839 - accuracy: 0.4933 - val_loss: 1.0736 - val_accuracy: 0.5234\n",
      "Epoch 234/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1838 - accuracy: 0.4933 - val_loss: 1.0587 - val_accuracy: 0.5234\n",
      "Epoch 235/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1833 - accuracy: 0.4911 - val_loss: 1.0650 - val_accuracy: 0.5234\n",
      "Epoch 236/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1828 - accuracy: 0.4955 - val_loss: 1.0672 - val_accuracy: 0.5234\n",
      "Epoch 237/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1836 - accuracy: 0.4916 - val_loss: 1.0706 - val_accuracy: 0.5234\n",
      "Epoch 238/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1835 - accuracy: 0.4905 - val_loss: 1.0639 - val_accuracy: 0.5234\n",
      "Epoch 239/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1834 - accuracy: 0.4955 - val_loss: 1.0654 - val_accuracy: 0.5234\n",
      "Epoch 240/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1833 - accuracy: 0.4933 - val_loss: 1.0618 - val_accuracy: 0.5234\n",
      "Epoch 241/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1830 - accuracy: 0.4911 - val_loss: 1.0679 - val_accuracy: 0.5234\n",
      "Epoch 242/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1836 - accuracy: 0.4922 - val_loss: 1.0576 - val_accuracy: 0.5234\n",
      "Epoch 243/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1839 - accuracy: 0.4944 - val_loss: 1.0646 - val_accuracy: 0.5234\n",
      "Epoch 244/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1830 - accuracy: 0.4939 - val_loss: 1.0640 - val_accuracy: 0.5234\n",
      "Epoch 245/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1827 - accuracy: 0.4911 - val_loss: 1.0616 - val_accuracy: 0.5234\n",
      "Epoch 246/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1830 - accuracy: 0.4905 - val_loss: 1.0618 - val_accuracy: 0.5234\n",
      "Epoch 247/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1828 - accuracy: 0.4911 - val_loss: 1.0651 - val_accuracy: 0.5234\n",
      "Epoch 248/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1826 - accuracy: 0.4905 - val_loss: 1.0619 - val_accuracy: 0.5234\n",
      "Epoch 249/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1826 - accuracy: 0.4916 - val_loss: 1.0647 - val_accuracy: 0.5234\n",
      "Epoch 250/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1831 - accuracy: 0.4911 - val_loss: 1.0670 - val_accuracy: 0.5234\n",
      "Epoch 251/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1831 - accuracy: 0.4916 - val_loss: 1.0659 - val_accuracy: 0.5234\n",
      "Epoch 252/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1828 - accuracy: 0.4933 - val_loss: 1.0660 - val_accuracy: 0.5234\n",
      "Epoch 253/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1835 - accuracy: 0.4927 - val_loss: 1.0584 - val_accuracy: 0.5234\n",
      "Epoch 254/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1827 - accuracy: 0.4927 - val_loss: 1.0666 - val_accuracy: 0.5234\n",
      "Epoch 255/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1822 - accuracy: 0.4961 - val_loss: 1.0644 - val_accuracy: 0.5234\n",
      "Epoch 256/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1833 - accuracy: 0.4911 - val_loss: 1.0606 - val_accuracy: 0.5234\n",
      "Epoch 257/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1826 - accuracy: 0.4955 - val_loss: 1.0678 - val_accuracy: 0.5234\n",
      "Epoch 258/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1827 - accuracy: 0.4933 - val_loss: 1.0636 - val_accuracy: 0.5234\n",
      "Epoch 259/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1836 - accuracy: 0.4911 - val_loss: 1.0645 - val_accuracy: 0.5234\n",
      "Epoch 260/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1832 - accuracy: 0.4972 - val_loss: 1.0646 - val_accuracy: 0.5234\n",
      "Epoch 261/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1825 - accuracy: 0.4927 - val_loss: 1.0624 - val_accuracy: 0.5234\n",
      "Epoch 262/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1825 - accuracy: 0.4905 - val_loss: 1.0625 - val_accuracy: 0.5234\n",
      "Epoch 263/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1831 - accuracy: 0.4944 - val_loss: 1.0687 - val_accuracy: 0.5234\n",
      "Epoch 264/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1833 - accuracy: 0.4939 - val_loss: 1.0633 - val_accuracy: 0.5234\n",
      "Epoch 265/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1832 - accuracy: 0.4905 - val_loss: 1.0635 - val_accuracy: 0.5234\n",
      "Epoch 266/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1825 - accuracy: 0.4939 - val_loss: 1.0580 - val_accuracy: 0.5234\n",
      "Epoch 267/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1826 - accuracy: 0.4927 - val_loss: 1.0625 - val_accuracy: 0.5234\n",
      "Epoch 268/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1818 - accuracy: 0.4933 - val_loss: 1.0655 - val_accuracy: 0.5234\n",
      "Epoch 269/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1828 - accuracy: 0.4933 - val_loss: 1.0686 - val_accuracy: 0.5234\n",
      "Epoch 270/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1826 - accuracy: 0.4972 - val_loss: 1.0676 - val_accuracy: 0.5234\n",
      "Epoch 271/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1822 - accuracy: 0.4927 - val_loss: 1.0684 - val_accuracy: 0.5234\n",
      "Epoch 272/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1830 - accuracy: 0.4916 - val_loss: 1.0683 - val_accuracy: 0.5234\n",
      "Epoch 273/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1822 - accuracy: 0.4950 - val_loss: 1.0635 - val_accuracy: 0.5234\n",
      "Epoch 274/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1824 - accuracy: 0.4955 - val_loss: 1.0699 - val_accuracy: 0.5234\n",
      "Epoch 275/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1820 - accuracy: 0.4944 - val_loss: 1.0650 - val_accuracy: 0.5234\n",
      "Epoch 276/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1831 - accuracy: 0.4955 - val_loss: 1.0750 - val_accuracy: 0.5234\n",
      "Epoch 277/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1835 - accuracy: 0.4916 - val_loss: 1.0747 - val_accuracy: 0.5234\n",
      "Epoch 278/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1826 - accuracy: 0.4955 - val_loss: 1.0662 - val_accuracy: 0.5234\n",
      "Epoch 279/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1822 - accuracy: 0.4927 - val_loss: 1.0696 - val_accuracy: 0.5234\n",
      "Epoch 280/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1829 - accuracy: 0.4916 - val_loss: 1.0657 - val_accuracy: 0.5234\n",
      "Epoch 281/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1827 - accuracy: 0.4922 - val_loss: 1.0685 - val_accuracy: 0.5234\n",
      "Epoch 282/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1824 - accuracy: 0.4933 - val_loss: 1.0635 - val_accuracy: 0.5234\n",
      "Epoch 283/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1834 - accuracy: 0.4967 - val_loss: 1.0760 - val_accuracy: 0.5234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 284/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1846 - accuracy: 0.4989 - val_loss: 1.0738 - val_accuracy: 0.5234\n",
      "Epoch 285/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1829 - accuracy: 0.4911 - val_loss: 1.0656 - val_accuracy: 0.5234\n",
      "Epoch 286/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1819 - accuracy: 0.4955 - val_loss: 1.0678 - val_accuracy: 0.5234\n",
      "Epoch 287/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1830 - accuracy: 0.4933 - val_loss: 1.0667 - val_accuracy: 0.5234\n",
      "Epoch 288/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1834 - accuracy: 0.4978 - val_loss: 1.0624 - val_accuracy: 0.5234\n",
      "Epoch 289/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1823 - accuracy: 0.4916 - val_loss: 1.0636 - val_accuracy: 0.5234\n",
      "Epoch 290/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1818 - accuracy: 0.4922 - val_loss: 1.0643 - val_accuracy: 0.5234\n",
      "Epoch 291/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1823 - accuracy: 0.4950 - val_loss: 1.0682 - val_accuracy: 0.5234\n",
      "Epoch 292/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1828 - accuracy: 0.4922 - val_loss: 1.0622 - val_accuracy: 0.5234\n",
      "Epoch 293/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1822 - accuracy: 0.4950 - val_loss: 1.0671 - val_accuracy: 0.5234\n",
      "Epoch 294/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1823 - accuracy: 0.4944 - val_loss: 1.0692 - val_accuracy: 0.5234\n",
      "Epoch 295/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1830 - accuracy: 0.4927 - val_loss: 1.0642 - val_accuracy: 0.5234\n",
      "Epoch 296/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1833 - accuracy: 0.4950 - val_loss: 1.0608 - val_accuracy: 0.5234\n",
      "Epoch 297/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1.1824 - accuracy: 0.4916 - val_loss: 1.0677 - val_accuracy: 0.5234\n",
      "Epoch 298/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1827 - accuracy: 0.4972 - val_loss: 1.0654 - val_accuracy: 0.5234\n",
      "Epoch 299/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1833 - accuracy: 0.4950 - val_loss: 1.0735 - val_accuracy: 0.5234\n",
      "Epoch 300/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1824 - accuracy: 0.4944 - val_loss: 1.0658 - val_accuracy: 0.5234\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "56/56 [==============================] - 0s 1ms/step\n",
      "Recall: [0.92 0.   0.6 ]\n",
      "Precision: [0.4 0.  0.7]\n",
      "F1-score: [0.56 0.   0.64]\n",
      "Accuracy: 0.49\n",
      "Brier score:0.19773045717116822\n",
      "Brier climat:0.20584573412698415\n",
      "Brier skill score:0.03942407157590011\n",
      "Recall: [0.92 0.   0.6 ]\n",
      "Precision: [0.4 0.  0.7]\n",
      "F1-score: [0.56 0.   0.64]\n",
      "Accuracy: 0.49\n",
      "Brier score:0.21939660481740672\n",
      "Brier climat:0.21949156746031745\n",
      "Brier skill score:0.00043264825163680243\n",
      "Recall: [0.92 0.   0.6 ]\n",
      "Precision: [0.4 0.  0.7]\n",
      "F1-score: [0.56 0.   0.64]\n",
      "Accuracy: 0.49\n",
      "Brier score:0.15787867611678513\n",
      "Brier climat:0.22935267857142858\n",
      "Brier skill score:0.3116336068095402\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "Recall: [1.   0.   0.57]\n",
      "Precision: [0.51 0.   0.59]\n",
      "F1-score: [0.68 0.   0.58]\n",
      "Accuracy: 0.52\n",
      "Brier score:0.22989862394333618\n",
      "Brier climat:0.27154513888888887\n",
      "Brier skill score:0.15336866318418485\n",
      "Recall: [1.   0.   0.57]\n",
      "Precision: [0.51 0.   0.59]\n",
      "F1-score: [0.68 0.   0.58]\n",
      "Accuracy: 0.52\n",
      "Brier score:0.25255377691895503\n",
      "Brier climat:0.24690972222222224\n",
      "Brier skill score:-0.022858778690184822\n",
      "Recall: [1.   0.   0.57]\n",
      "Precision: [0.51 0.   0.59]\n",
      "F1-score: [0.68 0.   0.58]\n",
      "Accuracy: 0.52\n",
      "Brier score:0.12395118808890751\n",
      "Brier climat:0.17578125\n",
      "Brier skill score:0.2948554633164373\n",
      "******** 5\n",
      "validation years [1986, 1987]\n",
      "train years {1984, 1985, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "{0: 2.0, 1: 1.0, 2: 1.0}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 60)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 183       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183\n",
      "Trainable params: 183\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "Epoch 1/300\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.7353 - accuracy: 0.4336WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "28/28 [==============================] - 4s 107ms/step - loss: 1.7353 - accuracy: 0.4336 - val_loss: 1.4275 - val_accuracy: 0.3438\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.5810 - accuracy: 0.4704 - val_loss: 1.3851 - val_accuracy: 0.3359\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.5057 - accuracy: 0.4905 - val_loss: 1.3653 - val_accuracy: 0.4141\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4685 - accuracy: 0.5017 - val_loss: 1.3545 - val_accuracy: 0.4219\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.4468 - accuracy: 0.5078 - val_loss: 1.3501 - val_accuracy: 0.4375\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4320 - accuracy: 0.5095 - val_loss: 1.3471 - val_accuracy: 0.4453\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4196 - accuracy: 0.4994 - val_loss: 1.3333 - val_accuracy: 0.4531\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.4086 - accuracy: 0.4983 - val_loss: 1.3324 - val_accuracy: 0.4531\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3975 - accuracy: 0.5028 - val_loss: 1.3160 - val_accuracy: 0.4609\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3884 - accuracy: 0.4983 - val_loss: 1.3125 - val_accuracy: 0.4688\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3793 - accuracy: 0.4967 - val_loss: 1.3015 - val_accuracy: 0.4609\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3716 - accuracy: 0.4989 - val_loss: 1.2910 - val_accuracy: 0.4531\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3628 - accuracy: 0.4983 - val_loss: 1.2817 - val_accuracy: 0.4531\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3557 - accuracy: 0.4994 - val_loss: 1.2748 - val_accuracy: 0.4688\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3483 - accuracy: 0.4944 - val_loss: 1.2596 - val_accuracy: 0.4844\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3417 - accuracy: 0.4939 - val_loss: 1.2512 - val_accuracy: 0.5078\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3358 - accuracy: 0.4922 - val_loss: 1.2453 - val_accuracy: 0.4922\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3297 - accuracy: 0.4933 - val_loss: 1.2325 - val_accuracy: 0.5391\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3236 - accuracy: 0.4911 - val_loss: 1.2229 - val_accuracy: 0.5469\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3184 - accuracy: 0.4872 - val_loss: 1.2157 - val_accuracy: 0.5469\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3130 - accuracy: 0.4877 - val_loss: 1.2081 - val_accuracy: 0.5547\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3070 - accuracy: 0.4855 - val_loss: 1.2008 - val_accuracy: 0.5547\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.3018 - accuracy: 0.4855 - val_loss: 1.1943 - val_accuracy: 0.5547\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2967 - accuracy: 0.4816 - val_loss: 1.1870 - val_accuracy: 0.5859\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2922 - accuracy: 0.4833 - val_loss: 1.1795 - val_accuracy: 0.5859\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2880 - accuracy: 0.4810 - val_loss: 1.1746 - val_accuracy: 0.5703\n",
      "Epoch 27/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2827 - accuracy: 0.4810 - val_loss: 1.1689 - val_accuracy: 0.5703\n",
      "Epoch 28/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2790 - accuracy: 0.4827 - val_loss: 1.1656 - val_accuracy: 0.5859\n",
      "Epoch 29/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2752 - accuracy: 0.4827 - val_loss: 1.1630 - val_accuracy: 0.5859\n",
      "Epoch 30/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2716 - accuracy: 0.4844 - val_loss: 1.1592 - val_accuracy: 0.5703\n",
      "Epoch 31/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2680 - accuracy: 0.4833 - val_loss: 1.1532 - val_accuracy: 0.5547\n",
      "Epoch 32/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2643 - accuracy: 0.4849 - val_loss: 1.1504 - val_accuracy: 0.5859\n",
      "Epoch 33/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2628 - accuracy: 0.4855 - val_loss: 1.1543 - val_accuracy: 0.5703\n",
      "Epoch 34/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2590 - accuracy: 0.4810 - val_loss: 1.1460 - val_accuracy: 0.5625\n",
      "Epoch 35/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2546 - accuracy: 0.4821 - val_loss: 1.1391 - val_accuracy: 0.5859\n",
      "Epoch 36/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2517 - accuracy: 0.4833 - val_loss: 1.1385 - val_accuracy: 0.5859\n",
      "Epoch 37/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2486 - accuracy: 0.4816 - val_loss: 1.1358 - val_accuracy: 0.5859\n",
      "Epoch 38/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2468 - accuracy: 0.4821 - val_loss: 1.1317 - val_accuracy: 0.5859\n",
      "Epoch 39/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2436 - accuracy: 0.4827 - val_loss: 1.1298 - val_accuracy: 0.5938\n",
      "Epoch 40/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2419 - accuracy: 0.4816 - val_loss: 1.1288 - val_accuracy: 0.6016\n",
      "Epoch 41/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2388 - accuracy: 0.4860 - val_loss: 1.1274 - val_accuracy: 0.6016\n",
      "Epoch 42/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2363 - accuracy: 0.4799 - val_loss: 1.1228 - val_accuracy: 0.6016\n",
      "Epoch 43/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2344 - accuracy: 0.4810 - val_loss: 1.1211 - val_accuracy: 0.5938\n",
      "Epoch 44/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2318 - accuracy: 0.4799 - val_loss: 1.1162 - val_accuracy: 0.6016\n",
      "Epoch 45/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2292 - accuracy: 0.4805 - val_loss: 1.1137 - val_accuracy: 0.6094\n",
      "Epoch 46/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2280 - accuracy: 0.4827 - val_loss: 1.1140 - val_accuracy: 0.6016\n",
      "Epoch 47/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2263 - accuracy: 0.4816 - val_loss: 1.1126 - val_accuracy: 0.6094\n",
      "Epoch 48/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2234 - accuracy: 0.4788 - val_loss: 1.1096 - val_accuracy: 0.6016\n",
      "Epoch 49/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2222 - accuracy: 0.4816 - val_loss: 1.1111 - val_accuracy: 0.6094\n",
      "Epoch 50/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2199 - accuracy: 0.4799 - val_loss: 1.1084 - val_accuracy: 0.6094\n",
      "Epoch 51/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2194 - accuracy: 0.4816 - val_loss: 1.1058 - val_accuracy: 0.6094\n",
      "Epoch 52/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2168 - accuracy: 0.4810 - val_loss: 1.1072 - val_accuracy: 0.6016\n",
      "Epoch 53/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2154 - accuracy: 0.4771 - val_loss: 1.1050 - val_accuracy: 0.6094\n",
      "Epoch 54/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2140 - accuracy: 0.4844 - val_loss: 1.1012 - val_accuracy: 0.6016\n",
      "Epoch 55/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2123 - accuracy: 0.4788 - val_loss: 1.1027 - val_accuracy: 0.6016\n",
      "Epoch 56/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2114 - accuracy: 0.4860 - val_loss: 1.1023 - val_accuracy: 0.5938\n",
      "Epoch 57/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2102 - accuracy: 0.4827 - val_loss: 1.1015 - val_accuracy: 0.6016\n",
      "Epoch 58/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2082 - accuracy: 0.4833 - val_loss: 1.0983 - val_accuracy: 0.6016\n",
      "Epoch 59/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2086 - accuracy: 0.4838 - val_loss: 1.0999 - val_accuracy: 0.5938\n",
      "Epoch 60/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2062 - accuracy: 0.4794 - val_loss: 1.0976 - val_accuracy: 0.5938\n",
      "Epoch 61/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2062 - accuracy: 0.4833 - val_loss: 1.0964 - val_accuracy: 0.6016\n",
      "Epoch 62/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2048 - accuracy: 0.4821 - val_loss: 1.0911 - val_accuracy: 0.6016\n",
      "Epoch 63/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2028 - accuracy: 0.4788 - val_loss: 1.0945 - val_accuracy: 0.6016\n",
      "Epoch 64/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2025 - accuracy: 0.4805 - val_loss: 1.0920 - val_accuracy: 0.5938\n",
      "Epoch 65/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2016 - accuracy: 0.4855 - val_loss: 1.0923 - val_accuracy: 0.5938\n",
      "Epoch 66/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2004 - accuracy: 0.4827 - val_loss: 1.0903 - val_accuracy: 0.5938\n",
      "Epoch 67/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2004 - accuracy: 0.4771 - val_loss: 1.0906 - val_accuracy: 0.5938\n",
      "Epoch 68/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1993 - accuracy: 0.4827 - val_loss: 1.0916 - val_accuracy: 0.5938\n",
      "Epoch 69/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1981 - accuracy: 0.4833 - val_loss: 1.0924 - val_accuracy: 0.5938\n",
      "Epoch 70/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1968 - accuracy: 0.4810 - val_loss: 1.0887 - val_accuracy: 0.5938\n",
      "Epoch 71/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1965 - accuracy: 0.4766 - val_loss: 1.0890 - val_accuracy: 0.5938\n",
      "Epoch 72/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1952 - accuracy: 0.4816 - val_loss: 1.0879 - val_accuracy: 0.5938\n",
      "Epoch 73/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1943 - accuracy: 0.4827 - val_loss: 1.0868 - val_accuracy: 0.5938\n",
      "Epoch 74/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1940 - accuracy: 0.4799 - val_loss: 1.0885 - val_accuracy: 0.5938\n",
      "Epoch 75/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1935 - accuracy: 0.4844 - val_loss: 1.0836 - val_accuracy: 0.5938\n",
      "Epoch 76/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1929 - accuracy: 0.4788 - val_loss: 1.0847 - val_accuracy: 0.5938\n",
      "Epoch 77/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1919 - accuracy: 0.4810 - val_loss: 1.0834 - val_accuracy: 0.5938\n",
      "Epoch 78/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1916 - accuracy: 0.4799 - val_loss: 1.0870 - val_accuracy: 0.5938\n",
      "Epoch 79/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1915 - accuracy: 0.4805 - val_loss: 1.0841 - val_accuracy: 0.5938\n",
      "Epoch 80/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1916 - accuracy: 0.4827 - val_loss: 1.0836 - val_accuracy: 0.5938\n",
      "Epoch 81/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1911 - accuracy: 0.4821 - val_loss: 1.0801 - val_accuracy: 0.6016\n",
      "Epoch 82/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1897 - accuracy: 0.4827 - val_loss: 1.0801 - val_accuracy: 0.6016\n",
      "Epoch 83/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1899 - accuracy: 0.4805 - val_loss: 1.0815 - val_accuracy: 0.6016\n",
      "Epoch 84/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1891 - accuracy: 0.4799 - val_loss: 1.0834 - val_accuracy: 0.6016\n",
      "Epoch 85/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1889 - accuracy: 0.4810 - val_loss: 1.0814 - val_accuracy: 0.6016\n",
      "Epoch 86/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1883 - accuracy: 0.4833 - val_loss: 1.0763 - val_accuracy: 0.6016\n",
      "Epoch 87/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1878 - accuracy: 0.4810 - val_loss: 1.0765 - val_accuracy: 0.5938\n",
      "Epoch 88/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1873 - accuracy: 0.4799 - val_loss: 1.0745 - val_accuracy: 0.6016\n",
      "Epoch 89/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1865 - accuracy: 0.4782 - val_loss: 1.0757 - val_accuracy: 0.6016\n",
      "Epoch 90/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1863 - accuracy: 0.4816 - val_loss: 1.0755 - val_accuracy: 0.6016\n",
      "Epoch 91/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1858 - accuracy: 0.4827 - val_loss: 1.0752 - val_accuracy: 0.6016\n",
      "Epoch 92/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1866 - accuracy: 0.4838 - val_loss: 1.0735 - val_accuracy: 0.6016\n",
      "Epoch 93/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1851 - accuracy: 0.4838 - val_loss: 1.0764 - val_accuracy: 0.6016\n",
      "Epoch 94/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1851 - accuracy: 0.4810 - val_loss: 1.0736 - val_accuracy: 0.6016\n",
      "Epoch 95/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1843 - accuracy: 0.4805 - val_loss: 1.0734 - val_accuracy: 0.6016\n",
      "Epoch 96/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1842 - accuracy: 0.4833 - val_loss: 1.0745 - val_accuracy: 0.6016\n",
      "Epoch 97/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1838 - accuracy: 0.4833 - val_loss: 1.0755 - val_accuracy: 0.6016\n",
      "Epoch 98/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1835 - accuracy: 0.4821 - val_loss: 1.0723 - val_accuracy: 0.6016\n",
      "Epoch 99/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1832 - accuracy: 0.4838 - val_loss: 1.0708 - val_accuracy: 0.6094\n",
      "Epoch 100/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1832 - accuracy: 0.4827 - val_loss: 1.0705 - val_accuracy: 0.6094\n",
      "Epoch 101/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1830 - accuracy: 0.4805 - val_loss: 1.0742 - val_accuracy: 0.6016\n",
      "Epoch 102/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1840 - accuracy: 0.4866 - val_loss: 1.0692 - val_accuracy: 0.6016\n",
      "Epoch 103/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1825 - accuracy: 0.4821 - val_loss: 1.0718 - val_accuracy: 0.6094\n",
      "Epoch 104/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1832 - accuracy: 0.4849 - val_loss: 1.0721 - val_accuracy: 0.6016\n",
      "Epoch 105/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1831 - accuracy: 0.4844 - val_loss: 1.0693 - val_accuracy: 0.6016\n",
      "Epoch 106/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1818 - accuracy: 0.4833 - val_loss: 1.0746 - val_accuracy: 0.6016\n",
      "Epoch 107/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1820 - accuracy: 0.4827 - val_loss: 1.0704 - val_accuracy: 0.6016\n",
      "Epoch 108/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1824 - accuracy: 0.4872 - val_loss: 1.0711 - val_accuracy: 0.6016\n",
      "Epoch 109/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1818 - accuracy: 0.4844 - val_loss: 1.0730 - val_accuracy: 0.6094\n",
      "Epoch 110/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1816 - accuracy: 0.4827 - val_loss: 1.0699 - val_accuracy: 0.6094\n",
      "Epoch 111/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1812 - accuracy: 0.4866 - val_loss: 1.0741 - val_accuracy: 0.6016\n",
      "Epoch 112/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1815 - accuracy: 0.4849 - val_loss: 1.0693 - val_accuracy: 0.6094\n",
      "Epoch 113/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1806 - accuracy: 0.4838 - val_loss: 1.0701 - val_accuracy: 0.6016\n",
      "Epoch 114/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1804 - accuracy: 0.4844 - val_loss: 1.0696 - val_accuracy: 0.6094\n",
      "Epoch 115/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1800 - accuracy: 0.4833 - val_loss: 1.0697 - val_accuracy: 0.6094\n",
      "Epoch 116/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1799 - accuracy: 0.4849 - val_loss: 1.0683 - val_accuracy: 0.6094\n",
      "Epoch 117/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1806 - accuracy: 0.4860 - val_loss: 1.0693 - val_accuracy: 0.6094\n",
      "Epoch 118/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1806 - accuracy: 0.4844 - val_loss: 1.0735 - val_accuracy: 0.6094\n",
      "Epoch 119/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1800 - accuracy: 0.4833 - val_loss: 1.0677 - val_accuracy: 0.6016\n",
      "Epoch 120/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1806 - accuracy: 0.4866 - val_loss: 1.0688 - val_accuracy: 0.6250\n",
      "Epoch 121/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1808 - accuracy: 0.4833 - val_loss: 1.0666 - val_accuracy: 0.6016\n",
      "Epoch 122/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1803 - accuracy: 0.4827 - val_loss: 1.0703 - val_accuracy: 0.6016\n",
      "Epoch 123/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1796 - accuracy: 0.4838 - val_loss: 1.0668 - val_accuracy: 0.6016\n",
      "Epoch 124/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1795 - accuracy: 0.4883 - val_loss: 1.0673 - val_accuracy: 0.6094\n",
      "Epoch 125/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1797 - accuracy: 0.4838 - val_loss: 1.0678 - val_accuracy: 0.6016\n",
      "Epoch 126/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1793 - accuracy: 0.4860 - val_loss: 1.0694 - val_accuracy: 0.6172\n",
      "Epoch 127/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1795 - accuracy: 0.4855 - val_loss: 1.0657 - val_accuracy: 0.6094\n",
      "Epoch 128/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1789 - accuracy: 0.4855 - val_loss: 1.0670 - val_accuracy: 0.6172\n",
      "Epoch 129/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1793 - accuracy: 0.4855 - val_loss: 1.0660 - val_accuracy: 0.6406\n",
      "Epoch 130/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1805 - accuracy: 0.4855 - val_loss: 1.0685 - val_accuracy: 0.6094\n",
      "Epoch 131/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1794 - accuracy: 0.4833 - val_loss: 1.0686 - val_accuracy: 0.6172\n",
      "Epoch 132/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1803 - accuracy: 0.4872 - val_loss: 1.0638 - val_accuracy: 0.6172\n",
      "Epoch 133/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1790 - accuracy: 0.4866 - val_loss: 1.0657 - val_accuracy: 0.6406\n",
      "Epoch 134/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1788 - accuracy: 0.4838 - val_loss: 1.0655 - val_accuracy: 0.6250\n",
      "Epoch 135/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1786 - accuracy: 0.4844 - val_loss: 1.0678 - val_accuracy: 0.6406\n",
      "Epoch 136/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1797 - accuracy: 0.4838 - val_loss: 1.0686 - val_accuracy: 0.6406\n",
      "Epoch 137/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1790 - accuracy: 0.4866 - val_loss: 1.0646 - val_accuracy: 0.6328\n",
      "Epoch 138/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1791 - accuracy: 0.4872 - val_loss: 1.0700 - val_accuracy: 0.6094\n",
      "Epoch 139/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1786 - accuracy: 0.4860 - val_loss: 1.0658 - val_accuracy: 0.6094\n",
      "Epoch 140/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1791 - accuracy: 0.4860 - val_loss: 1.0667 - val_accuracy: 0.6094\n",
      "Epoch 141/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1788 - accuracy: 0.4860 - val_loss: 1.0654 - val_accuracy: 0.6250\n",
      "Epoch 142/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1789 - accuracy: 0.4838 - val_loss: 1.0671 - val_accuracy: 0.6172\n",
      "Epoch 143/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1792 - accuracy: 0.4894 - val_loss: 1.0645 - val_accuracy: 0.6484\n",
      "Epoch 144/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1788 - accuracy: 0.4833 - val_loss: 1.0678 - val_accuracy: 0.6406\n",
      "Epoch 145/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1789 - accuracy: 0.4849 - val_loss: 1.0672 - val_accuracy: 0.6094\n",
      "Epoch 146/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1786 - accuracy: 0.4872 - val_loss: 1.0643 - val_accuracy: 0.6406\n",
      "Epoch 147/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1783 - accuracy: 0.4844 - val_loss: 1.0677 - val_accuracy: 0.6406\n",
      "Epoch 148/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1790 - accuracy: 0.4888 - val_loss: 1.0673 - val_accuracy: 0.6406\n",
      "Epoch 149/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1786 - accuracy: 0.4855 - val_loss: 1.0631 - val_accuracy: 0.6406\n",
      "Epoch 150/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1782 - accuracy: 0.4844 - val_loss: 1.0667 - val_accuracy: 0.6484\n",
      "Epoch 151/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1786 - accuracy: 0.4866 - val_loss: 1.0662 - val_accuracy: 0.6250\n",
      "Epoch 152/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1785 - accuracy: 0.4866 - val_loss: 1.0657 - val_accuracy: 0.6094\n",
      "Epoch 153/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1785 - accuracy: 0.4877 - val_loss: 1.0663 - val_accuracy: 0.6484\n",
      "Epoch 154/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1778 - accuracy: 0.4866 - val_loss: 1.0646 - val_accuracy: 0.6484\n",
      "Epoch 155/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1781 - accuracy: 0.4849 - val_loss: 1.0657 - val_accuracy: 0.6094\n",
      "Epoch 156/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.1786 - accuracy: 0.4888 - val_loss: 1.0637 - val_accuracy: 0.6484\n",
      "Epoch 157/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1780 - accuracy: 0.4838 - val_loss: 1.0653 - val_accuracy: 0.6094\n",
      "Epoch 158/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1779 - accuracy: 0.4872 - val_loss: 1.0661 - val_accuracy: 0.6484\n",
      "Epoch 159/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1796 - accuracy: 0.4872 - val_loss: 1.0653 - val_accuracy: 0.6484\n",
      "Epoch 160/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1784 - accuracy: 0.4838 - val_loss: 1.0660 - val_accuracy: 0.6094\n",
      "Epoch 161/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1776 - accuracy: 0.4883 - val_loss: 1.0635 - val_accuracy: 0.6484\n",
      "Epoch 162/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1792 - accuracy: 0.4866 - val_loss: 1.0668 - val_accuracy: 0.6484\n",
      "Epoch 163/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1792 - accuracy: 0.4849 - val_loss: 1.0631 - val_accuracy: 0.6172\n",
      "Epoch 164/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1775 - accuracy: 0.4872 - val_loss: 1.0655 - val_accuracy: 0.6172\n",
      "Epoch 165/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1778 - accuracy: 0.4849 - val_loss: 1.0623 - val_accuracy: 0.6406\n",
      "Epoch 166/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1788 - accuracy: 0.4872 - val_loss: 1.0628 - val_accuracy: 0.6484\n",
      "Epoch 167/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1771 - accuracy: 0.4872 - val_loss: 1.0662 - val_accuracy: 0.6406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1778 - accuracy: 0.4827 - val_loss: 1.0622 - val_accuracy: 0.6406\n",
      "Epoch 169/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1776 - accuracy: 0.4883 - val_loss: 1.0604 - val_accuracy: 0.6484\n",
      "Epoch 170/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1776 - accuracy: 0.4855 - val_loss: 1.0646 - val_accuracy: 0.6484\n",
      "Epoch 171/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1771 - accuracy: 0.4855 - val_loss: 1.0648 - val_accuracy: 0.6406\n",
      "Epoch 172/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1782 - accuracy: 0.4872 - val_loss: 1.0638 - val_accuracy: 0.6484\n",
      "Epoch 173/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1775 - accuracy: 0.4849 - val_loss: 1.0633 - val_accuracy: 0.6406\n",
      "Epoch 174/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1782 - accuracy: 0.4877 - val_loss: 1.0624 - val_accuracy: 0.6484\n",
      "Epoch 175/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1781 - accuracy: 0.4860 - val_loss: 1.0619 - val_accuracy: 0.6328\n",
      "Epoch 176/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1773 - accuracy: 0.4866 - val_loss: 1.0616 - val_accuracy: 0.6406\n",
      "Epoch 177/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1774 - accuracy: 0.4849 - val_loss: 1.0593 - val_accuracy: 0.6406\n",
      "Epoch 178/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1779 - accuracy: 0.4883 - val_loss: 1.0626 - val_accuracy: 0.6641\n",
      "Epoch 179/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1768 - accuracy: 0.4838 - val_loss: 1.0641 - val_accuracy: 0.6484\n",
      "Epoch 180/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1777 - accuracy: 0.4877 - val_loss: 1.0610 - val_accuracy: 0.6328\n",
      "Epoch 181/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1778 - accuracy: 0.4860 - val_loss: 1.0638 - val_accuracy: 0.6484\n",
      "Epoch 182/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1777 - accuracy: 0.4833 - val_loss: 1.0613 - val_accuracy: 0.6406\n",
      "Epoch 183/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1771 - accuracy: 0.4860 - val_loss: 1.0630 - val_accuracy: 0.6484\n",
      "Epoch 184/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1783 - accuracy: 0.4833 - val_loss: 1.0644 - val_accuracy: 0.6406\n",
      "Epoch 185/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1776 - accuracy: 0.4911 - val_loss: 1.0636 - val_accuracy: 0.6562\n",
      "Epoch 186/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1770 - accuracy: 0.4833 - val_loss: 1.0631 - val_accuracy: 0.6484\n",
      "Epoch 187/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1775 - accuracy: 0.4872 - val_loss: 1.0635 - val_accuracy: 0.6484\n",
      "Epoch 188/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1771 - accuracy: 0.4833 - val_loss: 1.0610 - val_accuracy: 0.6484\n",
      "Epoch 189/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1774 - accuracy: 0.4894 - val_loss: 1.0648 - val_accuracy: 0.6406\n",
      "Epoch 190/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1771 - accuracy: 0.4866 - val_loss: 1.0620 - val_accuracy: 0.6406\n",
      "Epoch 191/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1769 - accuracy: 0.4833 - val_loss: 1.0627 - val_accuracy: 0.6484\n",
      "Epoch 192/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1776 - accuracy: 0.4883 - val_loss: 1.0617 - val_accuracy: 0.6406\n",
      "Epoch 193/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1772 - accuracy: 0.4833 - val_loss: 1.0624 - val_accuracy: 0.6641\n",
      "Epoch 194/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1771 - accuracy: 0.4849 - val_loss: 1.0626 - val_accuracy: 0.6562\n",
      "Epoch 195/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1776 - accuracy: 0.4855 - val_loss: 1.0622 - val_accuracy: 0.6484\n",
      "Epoch 196/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1768 - accuracy: 0.4827 - val_loss: 1.0629 - val_accuracy: 0.6484\n",
      "Epoch 197/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1766 - accuracy: 0.4838 - val_loss: 1.0646 - val_accuracy: 0.6562\n",
      "Epoch 198/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1770 - accuracy: 0.4872 - val_loss: 1.0611 - val_accuracy: 0.6484\n",
      "Epoch 199/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1763 - accuracy: 0.4849 - val_loss: 1.0612 - val_accuracy: 0.6484\n",
      "Epoch 200/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1768 - accuracy: 0.4844 - val_loss: 1.0615 - val_accuracy: 0.6484\n",
      "Epoch 201/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1775 - accuracy: 0.4877 - val_loss: 1.0591 - val_accuracy: 0.6562\n",
      "Epoch 202/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1777 - accuracy: 0.4849 - val_loss: 1.0585 - val_accuracy: 0.6641\n",
      "Epoch 203/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1765 - accuracy: 0.4838 - val_loss: 1.0614 - val_accuracy: 0.6484\n",
      "Epoch 204/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1775 - accuracy: 0.4844 - val_loss: 1.0628 - val_accuracy: 0.6328\n",
      "Epoch 205/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1780 - accuracy: 0.4911 - val_loss: 1.0624 - val_accuracy: 0.6641\n",
      "Epoch 206/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1774 - accuracy: 0.4833 - val_loss: 1.0620 - val_accuracy: 0.6484\n",
      "Epoch 207/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1764 - accuracy: 0.4849 - val_loss: 1.0630 - val_accuracy: 0.6484\n",
      "Epoch 208/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1760 - accuracy: 0.4833 - val_loss: 1.0615 - val_accuracy: 0.6484\n",
      "Epoch 209/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1769 - accuracy: 0.4872 - val_loss: 1.0652 - val_accuracy: 0.6484\n",
      "Epoch 210/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1771 - accuracy: 0.4844 - val_loss: 1.0640 - val_accuracy: 0.6484\n",
      "Epoch 211/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1767 - accuracy: 0.4872 - val_loss: 1.0608 - val_accuracy: 0.6562\n",
      "Epoch 212/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1765 - accuracy: 0.4827 - val_loss: 1.0626 - val_accuracy: 0.6328\n",
      "Epoch 213/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1761 - accuracy: 0.4877 - val_loss: 1.0631 - val_accuracy: 0.6562\n",
      "Epoch 214/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1763 - accuracy: 0.4844 - val_loss: 1.0607 - val_accuracy: 0.6406\n",
      "Epoch 215/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1763 - accuracy: 0.4866 - val_loss: 1.0609 - val_accuracy: 0.6641\n",
      "Epoch 216/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1764 - accuracy: 0.4844 - val_loss: 1.0593 - val_accuracy: 0.6562\n",
      "Epoch 217/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1767 - accuracy: 0.4844 - val_loss: 1.0653 - val_accuracy: 0.6406\n",
      "Epoch 218/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1768 - accuracy: 0.4855 - val_loss: 1.0613 - val_accuracy: 0.6562\n",
      "Epoch 219/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1768 - accuracy: 0.4833 - val_loss: 1.0610 - val_accuracy: 0.6484\n",
      "Epoch 220/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1768 - accuracy: 0.4849 - val_loss: 1.0623 - val_accuracy: 0.6562\n",
      "Epoch 221/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1761 - accuracy: 0.4872 - val_loss: 1.0610 - val_accuracy: 0.6562\n",
      "Epoch 222/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1769 - accuracy: 0.4872 - val_loss: 1.0617 - val_accuracy: 0.6484\n",
      "Epoch 223/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1773 - accuracy: 0.4872 - val_loss: 1.0602 - val_accuracy: 0.6641\n",
      "Epoch 224/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1769 - accuracy: 0.4827 - val_loss: 1.0625 - val_accuracy: 0.6562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1768 - accuracy: 0.4849 - val_loss: 1.0621 - val_accuracy: 0.6484\n",
      "Epoch 226/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1764 - accuracy: 0.4883 - val_loss: 1.0591 - val_accuracy: 0.6562\n",
      "Epoch 227/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1764 - accuracy: 0.4866 - val_loss: 1.0623 - val_accuracy: 0.6484\n",
      "Epoch 228/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1766 - accuracy: 0.4860 - val_loss: 1.0601 - val_accuracy: 0.6562\n",
      "Epoch 229/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1769 - accuracy: 0.4860 - val_loss: 1.0600 - val_accuracy: 0.6484\n",
      "Epoch 230/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1769 - accuracy: 0.4849 - val_loss: 1.0648 - val_accuracy: 0.6484\n",
      "Epoch 231/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1768 - accuracy: 0.4883 - val_loss: 1.0584 - val_accuracy: 0.6562\n",
      "Epoch 232/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1778 - accuracy: 0.4844 - val_loss: 1.0621 - val_accuracy: 0.6562\n",
      "Epoch 233/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1768 - accuracy: 0.4888 - val_loss: 1.0630 - val_accuracy: 0.6484\n",
      "Epoch 234/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1766 - accuracy: 0.4833 - val_loss: 1.0610 - val_accuracy: 0.6719\n",
      "Epoch 235/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1762 - accuracy: 0.4844 - val_loss: 1.0636 - val_accuracy: 0.6562\n",
      "Epoch 236/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1766 - accuracy: 0.4855 - val_loss: 1.0596 - val_accuracy: 0.6562\n",
      "Epoch 237/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1767 - accuracy: 0.4844 - val_loss: 1.0636 - val_accuracy: 0.6562\n",
      "Epoch 238/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1766 - accuracy: 0.4866 - val_loss: 1.0612 - val_accuracy: 0.6562\n",
      "Epoch 239/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1762 - accuracy: 0.4844 - val_loss: 1.0627 - val_accuracy: 0.6484\n",
      "Epoch 240/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1767 - accuracy: 0.4894 - val_loss: 1.0586 - val_accuracy: 0.6562\n",
      "Epoch 241/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1782 - accuracy: 0.4844 - val_loss: 1.0622 - val_accuracy: 0.6484\n",
      "Epoch 242/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1760 - accuracy: 0.4860 - val_loss: 1.0600 - val_accuracy: 0.6562\n",
      "Epoch 243/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1768 - accuracy: 0.4866 - val_loss: 1.0619 - val_accuracy: 0.6562\n",
      "Epoch 244/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1763 - accuracy: 0.4877 - val_loss: 1.0591 - val_accuracy: 0.6484\n",
      "Epoch 245/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1764 - accuracy: 0.4844 - val_loss: 1.0624 - val_accuracy: 0.6562\n",
      "Epoch 246/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1766 - accuracy: 0.4860 - val_loss: 1.0608 - val_accuracy: 0.6406\n",
      "Epoch 247/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1764 - accuracy: 0.4849 - val_loss: 1.0625 - val_accuracy: 0.6562\n",
      "Epoch 248/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1769 - accuracy: 0.4894 - val_loss: 1.0634 - val_accuracy: 0.6719\n",
      "Epoch 249/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1773 - accuracy: 0.4833 - val_loss: 1.0610 - val_accuracy: 0.6406\n",
      "Epoch 250/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1764 - accuracy: 0.4849 - val_loss: 1.0633 - val_accuracy: 0.6562\n",
      "Epoch 251/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1764 - accuracy: 0.4883 - val_loss: 1.0605 - val_accuracy: 0.6719\n",
      "Epoch 252/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1770 - accuracy: 0.4872 - val_loss: 1.0596 - val_accuracy: 0.6484\n",
      "Epoch 253/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1768 - accuracy: 0.4849 - val_loss: 1.0615 - val_accuracy: 0.6719\n",
      "Epoch 254/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1771 - accuracy: 0.4833 - val_loss: 1.0603 - val_accuracy: 0.6562\n",
      "Epoch 255/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1764 - accuracy: 0.4872 - val_loss: 1.0606 - val_accuracy: 0.6562\n",
      "Epoch 256/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1767 - accuracy: 0.4844 - val_loss: 1.0618 - val_accuracy: 0.6562\n",
      "Epoch 257/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1760 - accuracy: 0.4872 - val_loss: 1.0613 - val_accuracy: 0.6484\n",
      "Epoch 258/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1764 - accuracy: 0.4860 - val_loss: 1.0604 - val_accuracy: 0.6562\n",
      "Epoch 259/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1759 - accuracy: 0.4872 - val_loss: 1.0622 - val_accuracy: 0.6719\n",
      "Epoch 260/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1766 - accuracy: 0.4872 - val_loss: 1.0595 - val_accuracy: 0.6562\n",
      "Epoch 261/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1762 - accuracy: 0.4883 - val_loss: 1.0622 - val_accuracy: 0.6641\n",
      "Epoch 262/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1768 - accuracy: 0.4838 - val_loss: 1.0617 - val_accuracy: 0.6328\n",
      "Epoch 263/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1763 - accuracy: 0.4866 - val_loss: 1.0610 - val_accuracy: 0.6484\n",
      "Epoch 264/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1764 - accuracy: 0.4894 - val_loss: 1.0600 - val_accuracy: 0.6562\n",
      "Epoch 265/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1769 - accuracy: 0.4844 - val_loss: 1.0604 - val_accuracy: 0.6328\n",
      "Epoch 266/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1757 - accuracy: 0.4888 - val_loss: 1.0626 - val_accuracy: 0.6562\n",
      "Epoch 267/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1763 - accuracy: 0.4855 - val_loss: 1.0612 - val_accuracy: 0.6484\n",
      "Epoch 268/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1762 - accuracy: 0.4866 - val_loss: 1.0597 - val_accuracy: 0.6719\n",
      "Epoch 269/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1769 - accuracy: 0.4888 - val_loss: 1.0621 - val_accuracy: 0.6562\n",
      "Epoch 270/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1767 - accuracy: 0.4844 - val_loss: 1.0606 - val_accuracy: 0.6562\n",
      "Epoch 271/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1766 - accuracy: 0.4872 - val_loss: 1.0655 - val_accuracy: 0.6562\n",
      "Epoch 272/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1770 - accuracy: 0.4855 - val_loss: 1.0615 - val_accuracy: 0.6562\n",
      "Epoch 273/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1778 - accuracy: 0.4894 - val_loss: 1.0656 - val_accuracy: 0.6484\n",
      "Epoch 274/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1769 - accuracy: 0.4872 - val_loss: 1.0629 - val_accuracy: 0.6484\n",
      "Epoch 275/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1761 - accuracy: 0.4849 - val_loss: 1.0601 - val_accuracy: 0.6562\n",
      "Epoch 276/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1765 - accuracy: 0.4860 - val_loss: 1.0628 - val_accuracy: 0.6484\n",
      "Epoch 277/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1766 - accuracy: 0.4866 - val_loss: 1.0620 - val_accuracy: 0.6484\n",
      "Epoch 278/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1769 - accuracy: 0.4900 - val_loss: 1.0607 - val_accuracy: 0.6562\n",
      "Epoch 279/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1767 - accuracy: 0.4827 - val_loss: 1.0618 - val_accuracy: 0.6562\n",
      "Epoch 280/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1764 - accuracy: 0.4866 - val_loss: 1.0610 - val_accuracy: 0.6484\n",
      "Epoch 281/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1763 - accuracy: 0.4900 - val_loss: 1.0598 - val_accuracy: 0.6641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 282/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1780 - accuracy: 0.4849 - val_loss: 1.0628 - val_accuracy: 0.6484\n",
      "Epoch 283/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1770 - accuracy: 0.4827 - val_loss: 1.0581 - val_accuracy: 0.6484\n",
      "Epoch 284/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1772 - accuracy: 0.4900 - val_loss: 1.0613 - val_accuracy: 0.6641\n",
      "Epoch 285/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1766 - accuracy: 0.4844 - val_loss: 1.0587 - val_accuracy: 0.6484\n",
      "Epoch 286/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1761 - accuracy: 0.4900 - val_loss: 1.0608 - val_accuracy: 0.6562\n",
      "Epoch 287/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1765 - accuracy: 0.4860 - val_loss: 1.0607 - val_accuracy: 0.6484\n",
      "Epoch 288/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1766 - accuracy: 0.4855 - val_loss: 1.0589 - val_accuracy: 0.6562\n",
      "Epoch 289/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1777 - accuracy: 0.4883 - val_loss: 1.0594 - val_accuracy: 0.6719\n",
      "Epoch 290/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1773 - accuracy: 0.4844 - val_loss: 1.0624 - val_accuracy: 0.6484\n",
      "Epoch 291/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1764 - accuracy: 0.4860 - val_loss: 1.0647 - val_accuracy: 0.6562\n",
      "Epoch 292/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1773 - accuracy: 0.4872 - val_loss: 1.0641 - val_accuracy: 0.6641\n",
      "Epoch 293/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1759 - accuracy: 0.4877 - val_loss: 1.0609 - val_accuracy: 0.6484\n",
      "Epoch 294/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1773 - accuracy: 0.4860 - val_loss: 1.0665 - val_accuracy: 0.6719\n",
      "Epoch 295/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1758 - accuracy: 0.4866 - val_loss: 1.0594 - val_accuracy: 0.6641\n",
      "Epoch 296/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1763 - accuracy: 0.4877 - val_loss: 1.0617 - val_accuracy: 0.6641\n",
      "Epoch 297/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1.1762 - accuracy: 0.4894 - val_loss: 1.0622 - val_accuracy: 0.6719\n",
      "Epoch 298/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1764 - accuracy: 0.4838 - val_loss: 1.0638 - val_accuracy: 0.6562\n",
      "Epoch 299/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1775 - accuracy: 0.4849 - val_loss: 1.0652 - val_accuracy: 0.6562\n",
      "Epoch 300/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1764 - accuracy: 0.4939 - val_loss: 1.0601 - val_accuracy: 0.6562\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "56/56 [==============================] - 0s 1ms/step\n",
      "Recall: [0.92 0.   0.6 ]\n",
      "Precision: [0.39 0.   0.7 ]\n",
      "F1-score: [0.55 0.   0.65]\n",
      "Accuracy: 0.49\n",
      "Brier score:0.19690738513663827\n",
      "Brier climat:0.20476686507936506\n",
      "Brier skill score:0.03838257688655122\n",
      "Recall: [0.92 0.   0.6 ]\n",
      "Precision: [0.39 0.   0.7 ]\n",
      "F1-score: [0.55 0.   0.65]\n",
      "Accuracy: 0.49\n",
      "Brier score:0.22133488300845064\n",
      "Brier climat:0.22384424603174605\n",
      "Brier skill score:0.011210308362983423\n",
      "Recall: [0.92 0.   0.6 ]\n",
      "Precision: [0.39 0.   0.7 ]\n",
      "F1-score: [0.55 0.   0.65]\n",
      "Accuracy: 0.49\n",
      "Brier score:0.1531356022360883\n",
      "Brier climat:0.22823660714285715\n",
      "Brier skill score:0.32904890169420487\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "Recall: [0.97 0.   0.7 ]\n",
      "Precision: [0.73 0.   0.52]\n",
      "F1-score: [0.83 0.   0.6 ]\n",
      "Accuracy: 0.66\n",
      "Brier score:0.21255897148908603\n",
      "Brier climat:0.28664930555555557\n",
      "Brier skill score:0.2584703072029947\n",
      "Recall: [0.97 0.   0.7 ]\n",
      "Precision: [0.73 0.   0.52]\n",
      "F1-score: [0.83 0.   0.6 ]\n",
      "Accuracy: 0.66\n",
      "Brier score:0.2046255683066157\n",
      "Brier climat:0.18597222222222223\n",
      "Brier skill score:-0.10030178626410224\n",
      "Recall: [0.97 0.   0.7 ]\n",
      "Precision: [0.73 0.   0.52]\n",
      "F1-score: [0.83 0.   0.6 ]\n",
      "Accuracy: 0.66\n",
      "Brier score:0.16912961376968083\n",
      "Brier climat:0.19140625\n",
      "Brier skill score:0.11638405867268786\n",
      "******** 6\n",
      "validation years [1987, 1988]\n",
      "train years {1984, 1985, 1986, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "{0: 2.0, 1: 1.0, 2: 1.0}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 60)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 183       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183\n",
      "Trainable params: 183\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.9729 - accuracy: 0.4035WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "28/28 [==============================] - 4s 108ms/step - loss: 1.9729 - accuracy: 0.4035 - val_loss: 2.3005 - val_accuracy: 0.2812\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.8003 - accuracy: 0.4358 - val_loss: 2.0930 - val_accuracy: 0.2656\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.6684 - accuracy: 0.4408 - val_loss: 1.8851 - val_accuracy: 0.2422\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.5736 - accuracy: 0.4498 - val_loss: 1.7139 - val_accuracy: 0.2422\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.5059 - accuracy: 0.4626 - val_loss: 1.5681 - val_accuracy: 0.2422\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4572 - accuracy: 0.4621 - val_loss: 1.5035 - val_accuracy: 0.2422\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4259 - accuracy: 0.4771 - val_loss: 1.4274 - val_accuracy: 0.2422\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4000 - accuracy: 0.4833 - val_loss: 1.4019 - val_accuracy: 0.2422\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3812 - accuracy: 0.4805 - val_loss: 1.3717 - val_accuracy: 0.2422\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3664 - accuracy: 0.4888 - val_loss: 1.3343 - val_accuracy: 0.2500\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3536 - accuracy: 0.4955 - val_loss: 1.3242 - val_accuracy: 0.3125\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3438 - accuracy: 0.4972 - val_loss: 1.3067 - val_accuracy: 0.3203\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3344 - accuracy: 0.4978 - val_loss: 1.3155 - val_accuracy: 0.3203\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3269 - accuracy: 0.5011 - val_loss: 1.3065 - val_accuracy: 0.3281\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3188 - accuracy: 0.5006 - val_loss: 1.2913 - val_accuracy: 0.3359\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3117 - accuracy: 0.5006 - val_loss: 1.2849 - val_accuracy: 0.3438\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.3051 - accuracy: 0.5045 - val_loss: 1.2747 - val_accuracy: 0.3438\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2994 - accuracy: 0.4989 - val_loss: 1.2820 - val_accuracy: 0.3438\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2944 - accuracy: 0.5045 - val_loss: 1.2709 - val_accuracy: 0.3438\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2894 - accuracy: 0.5006 - val_loss: 1.2620 - val_accuracy: 0.3516\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2843 - accuracy: 0.5011 - val_loss: 1.2664 - val_accuracy: 0.3516\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2807 - accuracy: 0.5000 - val_loss: 1.2619 - val_accuracy: 0.3516\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2765 - accuracy: 0.4989 - val_loss: 1.2587 - val_accuracy: 0.3516\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2726 - accuracy: 0.4989 - val_loss: 1.2601 - val_accuracy: 0.3516\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2688 - accuracy: 0.4967 - val_loss: 1.2552 - val_accuracy: 0.3516\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2655 - accuracy: 0.4972 - val_loss: 1.2454 - val_accuracy: 0.3594\n",
      "Epoch 27/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2626 - accuracy: 0.5006 - val_loss: 1.2488 - val_accuracy: 0.3516\n",
      "Epoch 28/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2589 - accuracy: 0.4955 - val_loss: 1.2516 - val_accuracy: 0.3516\n",
      "Epoch 29/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2559 - accuracy: 0.4967 - val_loss: 1.2440 - val_accuracy: 0.3594\n",
      "Epoch 30/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2530 - accuracy: 0.4950 - val_loss: 1.2447 - val_accuracy: 0.3594\n",
      "Epoch 31/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2502 - accuracy: 0.4972 - val_loss: 1.2446 - val_accuracy: 0.3594\n",
      "Epoch 32/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2482 - accuracy: 0.4944 - val_loss: 1.2387 - val_accuracy: 0.3672\n",
      "Epoch 33/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2461 - accuracy: 0.4944 - val_loss: 1.2417 - val_accuracy: 0.3750\n",
      "Epoch 34/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2436 - accuracy: 0.4961 - val_loss: 1.2346 - val_accuracy: 0.3672\n",
      "Epoch 35/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2411 - accuracy: 0.4967 - val_loss: 1.2296 - val_accuracy: 0.3750\n",
      "Epoch 36/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2382 - accuracy: 0.4950 - val_loss: 1.2322 - val_accuracy: 0.3750\n",
      "Epoch 37/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2363 - accuracy: 0.4939 - val_loss: 1.2256 - val_accuracy: 0.3828\n",
      "Epoch 38/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2346 - accuracy: 0.4950 - val_loss: 1.2319 - val_accuracy: 0.3750\n",
      "Epoch 39/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2326 - accuracy: 0.4961 - val_loss: 1.2330 - val_accuracy: 0.3828\n",
      "Epoch 40/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2305 - accuracy: 0.4944 - val_loss: 1.2210 - val_accuracy: 0.3750\n",
      "Epoch 41/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2296 - accuracy: 0.4950 - val_loss: 1.2290 - val_accuracy: 0.3828\n",
      "Epoch 42/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2284 - accuracy: 0.4950 - val_loss: 1.2250 - val_accuracy: 0.3828\n",
      "Epoch 43/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2262 - accuracy: 0.4950 - val_loss: 1.2196 - val_accuracy: 0.3828\n",
      "Epoch 44/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2249 - accuracy: 0.4944 - val_loss: 1.2154 - val_accuracy: 0.3828\n",
      "Epoch 45/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2230 - accuracy: 0.4950 - val_loss: 1.2136 - val_accuracy: 0.3828\n",
      "Epoch 46/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2213 - accuracy: 0.4944 - val_loss: 1.2180 - val_accuracy: 0.3906\n",
      "Epoch 47/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2199 - accuracy: 0.4955 - val_loss: 1.2102 - val_accuracy: 0.3828\n",
      "Epoch 48/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2184 - accuracy: 0.4939 - val_loss: 1.2109 - val_accuracy: 0.3828\n",
      "Epoch 49/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2176 - accuracy: 0.4944 - val_loss: 1.2025 - val_accuracy: 0.3906\n",
      "Epoch 50/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2156 - accuracy: 0.4939 - val_loss: 1.2090 - val_accuracy: 0.3828\n",
      "Epoch 51/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2151 - accuracy: 0.4922 - val_loss: 1.2130 - val_accuracy: 0.3906\n",
      "Epoch 52/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2130 - accuracy: 0.4927 - val_loss: 1.2112 - val_accuracy: 0.3906\n",
      "Epoch 53/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2117 - accuracy: 0.4933 - val_loss: 1.2074 - val_accuracy: 0.3828\n",
      "Epoch 54/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2110 - accuracy: 0.4950 - val_loss: 1.2147 - val_accuracy: 0.3828\n",
      "Epoch 55/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2098 - accuracy: 0.4961 - val_loss: 1.2009 - val_accuracy: 0.3906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2089 - accuracy: 0.4944 - val_loss: 1.2055 - val_accuracy: 0.3828\n",
      "Epoch 57/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2077 - accuracy: 0.4961 - val_loss: 1.2082 - val_accuracy: 0.3828\n",
      "Epoch 58/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2072 - accuracy: 0.4950 - val_loss: 1.2068 - val_accuracy: 0.3828\n",
      "Epoch 59/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2058 - accuracy: 0.4967 - val_loss: 1.2048 - val_accuracy: 0.3906\n",
      "Epoch 60/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2063 - accuracy: 0.4933 - val_loss: 1.1964 - val_accuracy: 0.3828\n",
      "Epoch 61/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2047 - accuracy: 0.4989 - val_loss: 1.1984 - val_accuracy: 0.3906\n",
      "Epoch 62/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2029 - accuracy: 0.4967 - val_loss: 1.1997 - val_accuracy: 0.3828\n",
      "Epoch 63/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2028 - accuracy: 0.4972 - val_loss: 1.2028 - val_accuracy: 0.3750\n",
      "Epoch 64/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.2009 - accuracy: 0.4989 - val_loss: 1.1865 - val_accuracy: 0.3828\n",
      "Epoch 65/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2005 - accuracy: 0.4967 - val_loss: 1.1909 - val_accuracy: 0.3906\n",
      "Epoch 66/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2000 - accuracy: 0.5017 - val_loss: 1.1962 - val_accuracy: 0.3906\n",
      "Epoch 67/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1992 - accuracy: 0.4972 - val_loss: 1.1964 - val_accuracy: 0.4219\n",
      "Epoch 68/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1986 - accuracy: 0.4983 - val_loss: 1.1937 - val_accuracy: 0.4062\n",
      "Epoch 69/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1992 - accuracy: 0.4978 - val_loss: 1.1816 - val_accuracy: 0.4141\n",
      "Epoch 70/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1972 - accuracy: 0.4978 - val_loss: 1.1827 - val_accuracy: 0.4062\n",
      "Epoch 71/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1964 - accuracy: 0.4989 - val_loss: 1.1926 - val_accuracy: 0.4141\n",
      "Epoch 72/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1960 - accuracy: 0.4994 - val_loss: 1.1871 - val_accuracy: 0.4219\n",
      "Epoch 73/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1958 - accuracy: 0.4989 - val_loss: 1.1814 - val_accuracy: 0.4141\n",
      "Epoch 74/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1957 - accuracy: 0.4972 - val_loss: 1.1979 - val_accuracy: 0.4219\n",
      "Epoch 75/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1968 - accuracy: 0.4989 - val_loss: 1.1786 - val_accuracy: 0.4141\n",
      "Epoch 76/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1950 - accuracy: 0.4978 - val_loss: 1.1864 - val_accuracy: 0.4219\n",
      "Epoch 77/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1940 - accuracy: 0.4983 - val_loss: 1.1868 - val_accuracy: 0.4141\n",
      "Epoch 78/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1940 - accuracy: 0.4983 - val_loss: 1.1857 - val_accuracy: 0.4141\n",
      "Epoch 79/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1934 - accuracy: 0.4989 - val_loss: 1.1824 - val_accuracy: 0.4141\n",
      "Epoch 80/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1935 - accuracy: 0.4978 - val_loss: 1.1860 - val_accuracy: 0.4141\n",
      "Epoch 81/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1931 - accuracy: 0.5011 - val_loss: 1.1706 - val_accuracy: 0.4141\n",
      "Epoch 82/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1930 - accuracy: 0.4961 - val_loss: 1.1845 - val_accuracy: 0.4141\n",
      "Epoch 83/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1925 - accuracy: 0.5006 - val_loss: 1.1788 - val_accuracy: 0.4141\n",
      "Epoch 84/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1921 - accuracy: 0.4983 - val_loss: 1.1868 - val_accuracy: 0.4141\n",
      "Epoch 85/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1915 - accuracy: 0.5017 - val_loss: 1.1769 - val_accuracy: 0.4141\n",
      "Epoch 86/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1914 - accuracy: 0.4972 - val_loss: 1.1765 - val_accuracy: 0.4141\n",
      "Epoch 87/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1913 - accuracy: 0.5017 - val_loss: 1.1801 - val_accuracy: 0.4141\n",
      "Epoch 88/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1911 - accuracy: 0.4989 - val_loss: 1.1847 - val_accuracy: 0.4141\n",
      "Epoch 89/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1904 - accuracy: 0.5000 - val_loss: 1.1714 - val_accuracy: 0.4141\n",
      "Epoch 90/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1901 - accuracy: 0.5006 - val_loss: 1.1760 - val_accuracy: 0.4141\n",
      "Epoch 91/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1910 - accuracy: 0.4983 - val_loss: 1.1686 - val_accuracy: 0.4219\n",
      "Epoch 92/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1895 - accuracy: 0.5006 - val_loss: 1.1764 - val_accuracy: 0.4219\n",
      "Epoch 93/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1894 - accuracy: 0.4994 - val_loss: 1.1690 - val_accuracy: 0.4219\n",
      "Epoch 94/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1893 - accuracy: 0.4994 - val_loss: 1.1704 - val_accuracy: 0.4141\n",
      "Epoch 95/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1895 - accuracy: 0.4983 - val_loss: 1.1752 - val_accuracy: 0.4141\n",
      "Epoch 96/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1893 - accuracy: 0.4983 - val_loss: 1.1713 - val_accuracy: 0.4219\n",
      "Epoch 97/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1892 - accuracy: 0.4978 - val_loss: 1.1836 - val_accuracy: 0.4141\n",
      "Epoch 98/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1887 - accuracy: 0.5006 - val_loss: 1.1669 - val_accuracy: 0.4219\n",
      "Epoch 99/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1884 - accuracy: 0.5011 - val_loss: 1.1749 - val_accuracy: 0.4219\n",
      "Epoch 100/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1880 - accuracy: 0.4978 - val_loss: 1.1666 - val_accuracy: 0.4219\n",
      "Epoch 101/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1881 - accuracy: 0.4978 - val_loss: 1.1783 - val_accuracy: 0.4219\n",
      "Epoch 102/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1882 - accuracy: 0.4961 - val_loss: 1.1742 - val_accuracy: 0.4141\n",
      "Epoch 103/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1891 - accuracy: 0.5000 - val_loss: 1.1751 - val_accuracy: 0.4297\n",
      "Epoch 104/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1887 - accuracy: 0.4994 - val_loss: 1.1666 - val_accuracy: 0.4219\n",
      "Epoch 105/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1878 - accuracy: 0.4978 - val_loss: 1.1676 - val_accuracy: 0.4141\n",
      "Epoch 106/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1877 - accuracy: 0.5000 - val_loss: 1.1727 - val_accuracy: 0.4141\n",
      "Epoch 107/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1868 - accuracy: 0.4994 - val_loss: 1.1555 - val_accuracy: 0.4141\n",
      "Epoch 108/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1877 - accuracy: 0.4967 - val_loss: 1.1642 - val_accuracy: 0.4141\n",
      "Epoch 109/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1869 - accuracy: 0.5006 - val_loss: 1.1679 - val_accuracy: 0.4219\n",
      "Epoch 110/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1868 - accuracy: 0.4994 - val_loss: 1.1622 - val_accuracy: 0.4141\n",
      "Epoch 111/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1866 - accuracy: 0.5000 - val_loss: 1.1644 - val_accuracy: 0.4219\n",
      "Epoch 112/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1878 - accuracy: 0.5000 - val_loss: 1.1691 - val_accuracy: 0.4141\n",
      "Epoch 113/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1863 - accuracy: 0.4994 - val_loss: 1.1678 - val_accuracy: 0.4219\n",
      "Epoch 114/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1859 - accuracy: 0.5006 - val_loss: 1.1635 - val_accuracy: 0.4141\n",
      "Epoch 115/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1863 - accuracy: 0.4983 - val_loss: 1.1583 - val_accuracy: 0.4141\n",
      "Epoch 116/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1855 - accuracy: 0.4989 - val_loss: 1.1564 - val_accuracy: 0.4219\n",
      "Epoch 117/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1864 - accuracy: 0.4978 - val_loss: 1.1579 - val_accuracy: 0.4219\n",
      "Epoch 118/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1866 - accuracy: 0.5006 - val_loss: 1.1487 - val_accuracy: 0.4219\n",
      "Epoch 119/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1856 - accuracy: 0.4989 - val_loss: 1.1549 - val_accuracy: 0.4219\n",
      "Epoch 120/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1858 - accuracy: 0.5000 - val_loss: 1.1548 - val_accuracy: 0.4141\n",
      "Epoch 121/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1847 - accuracy: 0.5000 - val_loss: 1.1649 - val_accuracy: 0.4219\n",
      "Epoch 122/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1854 - accuracy: 0.5011 - val_loss: 1.1688 - val_accuracy: 0.4219\n",
      "Epoch 123/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1851 - accuracy: 0.5011 - val_loss: 1.1627 - val_accuracy: 0.4219\n",
      "Epoch 124/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1855 - accuracy: 0.4994 - val_loss: 1.1542 - val_accuracy: 0.4141\n",
      "Epoch 125/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1847 - accuracy: 0.5022 - val_loss: 1.1546 - val_accuracy: 0.4219\n",
      "Epoch 126/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1850 - accuracy: 0.4983 - val_loss: 1.1690 - val_accuracy: 0.4219\n",
      "Epoch 127/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1840 - accuracy: 0.5006 - val_loss: 1.1527 - val_accuracy: 0.4219\n",
      "Epoch 128/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1849 - accuracy: 0.4994 - val_loss: 1.1563 - val_accuracy: 0.4219\n",
      "Epoch 129/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1841 - accuracy: 0.4994 - val_loss: 1.1488 - val_accuracy: 0.4219\n",
      "Epoch 130/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1847 - accuracy: 0.5000 - val_loss: 1.1519 - val_accuracy: 0.4219\n",
      "Epoch 131/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1844 - accuracy: 0.5011 - val_loss: 1.1531 - val_accuracy: 0.4219\n",
      "Epoch 132/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1845 - accuracy: 0.5017 - val_loss: 1.1555 - val_accuracy: 0.4219\n",
      "Epoch 133/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1846 - accuracy: 0.4994 - val_loss: 1.1617 - val_accuracy: 0.4219\n",
      "Epoch 134/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1847 - accuracy: 0.5006 - val_loss: 1.1610 - val_accuracy: 0.4141\n",
      "Epoch 135/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1843 - accuracy: 0.5022 - val_loss: 1.1542 - val_accuracy: 0.4219\n",
      "Epoch 136/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1836 - accuracy: 0.5006 - val_loss: 1.1505 - val_accuracy: 0.4219\n",
      "Epoch 137/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1834 - accuracy: 0.4994 - val_loss: 1.1551 - val_accuracy: 0.4219\n",
      "Epoch 138/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1838 - accuracy: 0.5006 - val_loss: 1.1438 - val_accuracy: 0.4219\n",
      "Epoch 139/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1840 - accuracy: 0.5006 - val_loss: 1.1555 - val_accuracy: 0.4219\n",
      "Epoch 140/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1840 - accuracy: 0.4989 - val_loss: 1.1521 - val_accuracy: 0.4453\n",
      "Epoch 141/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1839 - accuracy: 0.5011 - val_loss: 1.1507 - val_accuracy: 0.4219\n",
      "Epoch 142/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1840 - accuracy: 0.5006 - val_loss: 1.1661 - val_accuracy: 0.4219\n",
      "Epoch 143/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1837 - accuracy: 0.4994 - val_loss: 1.1489 - val_accuracy: 0.4219\n",
      "Epoch 144/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1832 - accuracy: 0.5045 - val_loss: 1.1556 - val_accuracy: 0.4219\n",
      "Epoch 145/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1833 - accuracy: 0.5000 - val_loss: 1.1477 - val_accuracy: 0.4141\n",
      "Epoch 146/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1837 - accuracy: 0.5011 - val_loss: 1.1532 - val_accuracy: 0.4219\n",
      "Epoch 147/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1836 - accuracy: 0.5017 - val_loss: 1.1469 - val_accuracy: 0.4219\n",
      "Epoch 148/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1835 - accuracy: 0.5006 - val_loss: 1.1495 - val_accuracy: 0.4219\n",
      "Epoch 149/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1834 - accuracy: 0.5033 - val_loss: 1.1559 - val_accuracy: 0.4219\n",
      "Epoch 150/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1840 - accuracy: 0.5000 - val_loss: 1.1513 - val_accuracy: 0.4297\n",
      "Epoch 151/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1835 - accuracy: 0.5006 - val_loss: 1.1491 - val_accuracy: 0.4297\n",
      "Epoch 152/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1831 - accuracy: 0.5022 - val_loss: 1.1468 - val_accuracy: 0.4219\n",
      "Epoch 153/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1828 - accuracy: 0.5011 - val_loss: 1.1528 - val_accuracy: 0.4219\n",
      "Epoch 154/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1833 - accuracy: 0.5006 - val_loss: 1.1621 - val_accuracy: 0.4453\n",
      "Epoch 155/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1831 - accuracy: 0.5006 - val_loss: 1.1624 - val_accuracy: 0.4219\n",
      "Epoch 156/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1834 - accuracy: 0.5006 - val_loss: 1.1568 - val_accuracy: 0.4219\n",
      "Epoch 157/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1831 - accuracy: 0.5011 - val_loss: 1.1510 - val_accuracy: 0.4297\n",
      "Epoch 158/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1830 - accuracy: 0.5033 - val_loss: 1.1506 - val_accuracy: 0.4297\n",
      "Epoch 159/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1826 - accuracy: 0.5028 - val_loss: 1.1602 - val_accuracy: 0.4219\n",
      "Epoch 160/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1832 - accuracy: 0.5006 - val_loss: 1.1482 - val_accuracy: 0.4219\n",
      "Epoch 161/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1831 - accuracy: 0.5011 - val_loss: 1.1549 - val_accuracy: 0.4141\n",
      "Epoch 162/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1830 - accuracy: 0.5022 - val_loss: 1.1537 - val_accuracy: 0.4297\n",
      "Epoch 163/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1828 - accuracy: 0.5011 - val_loss: 1.1557 - val_accuracy: 0.4297\n",
      "Epoch 164/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1832 - accuracy: 0.5011 - val_loss: 1.1416 - val_accuracy: 0.4219\n",
      "Epoch 165/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1840 - accuracy: 0.5028 - val_loss: 1.1379 - val_accuracy: 0.4219\n",
      "Epoch 166/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1824 - accuracy: 0.5039 - val_loss: 1.1617 - val_accuracy: 0.4453\n",
      "Epoch 167/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1827 - accuracy: 0.5033 - val_loss: 1.1461 - val_accuracy: 0.4219\n",
      "Epoch 168/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1831 - accuracy: 0.5033 - val_loss: 1.1569 - val_accuracy: 0.4297\n",
      "Epoch 169/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1828 - accuracy: 0.5006 - val_loss: 1.1483 - val_accuracy: 0.4219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1828 - accuracy: 0.5011 - val_loss: 1.1511 - val_accuracy: 0.4219\n",
      "Epoch 171/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1825 - accuracy: 0.5033 - val_loss: 1.1479 - val_accuracy: 0.4297\n",
      "Epoch 172/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1824 - accuracy: 0.5006 - val_loss: 1.1475 - val_accuracy: 0.4297\n",
      "Epoch 173/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1827 - accuracy: 0.5022 - val_loss: 1.1529 - val_accuracy: 0.4453\n",
      "Epoch 174/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1825 - accuracy: 0.5022 - val_loss: 1.1525 - val_accuracy: 0.4219\n",
      "Epoch 175/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1822 - accuracy: 0.5011 - val_loss: 1.1466 - val_accuracy: 0.4297\n",
      "Epoch 176/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1827 - accuracy: 0.5011 - val_loss: 1.1486 - val_accuracy: 0.4375\n",
      "Epoch 177/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1827 - accuracy: 0.5011 - val_loss: 1.1490 - val_accuracy: 0.4297\n",
      "Epoch 178/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1824 - accuracy: 0.5022 - val_loss: 1.1499 - val_accuracy: 0.4297\n",
      "Epoch 179/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1825 - accuracy: 0.5028 - val_loss: 1.1506 - val_accuracy: 0.4297\n",
      "Epoch 180/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1828 - accuracy: 0.5033 - val_loss: 1.1466 - val_accuracy: 0.4453\n",
      "Epoch 181/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1823 - accuracy: 0.5006 - val_loss: 1.1457 - val_accuracy: 0.4219\n",
      "Epoch 182/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1822 - accuracy: 0.5045 - val_loss: 1.1523 - val_accuracy: 0.4219\n",
      "Epoch 183/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1819 - accuracy: 0.5039 - val_loss: 1.1518 - val_accuracy: 0.4297\n",
      "Epoch 184/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1821 - accuracy: 0.5017 - val_loss: 1.1481 - val_accuracy: 0.4297\n",
      "Epoch 185/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1825 - accuracy: 0.5033 - val_loss: 1.1589 - val_accuracy: 0.4297\n",
      "Epoch 186/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1819 - accuracy: 0.5011 - val_loss: 1.1471 - val_accuracy: 0.4297\n",
      "Epoch 187/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1824 - accuracy: 0.5033 - val_loss: 1.1561 - val_accuracy: 0.4219\n",
      "Epoch 188/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1818 - accuracy: 0.5022 - val_loss: 1.1480 - val_accuracy: 0.4297\n",
      "Epoch 189/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1822 - accuracy: 0.5039 - val_loss: 1.1579 - val_accuracy: 0.4297\n",
      "Epoch 190/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1835 - accuracy: 0.5028 - val_loss: 1.1531 - val_accuracy: 0.4219\n",
      "Epoch 191/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1819 - accuracy: 0.5022 - val_loss: 1.1575 - val_accuracy: 0.4219\n",
      "Epoch 192/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1822 - accuracy: 0.5039 - val_loss: 1.1510 - val_accuracy: 0.4141\n",
      "Epoch 193/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1820 - accuracy: 0.5017 - val_loss: 1.1557 - val_accuracy: 0.4297\n",
      "Epoch 194/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1826 - accuracy: 0.5039 - val_loss: 1.1511 - val_accuracy: 0.4219\n",
      "Epoch 195/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1817 - accuracy: 0.5022 - val_loss: 1.1518 - val_accuracy: 0.4297\n",
      "Epoch 196/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1821 - accuracy: 0.5045 - val_loss: 1.1452 - val_accuracy: 0.4219\n",
      "Epoch 197/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1820 - accuracy: 0.5011 - val_loss: 1.1490 - val_accuracy: 0.4219\n",
      "Epoch 198/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1820 - accuracy: 0.5033 - val_loss: 1.1397 - val_accuracy: 0.4219\n",
      "Epoch 199/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1816 - accuracy: 0.5039 - val_loss: 1.1587 - val_accuracy: 0.4375\n",
      "Epoch 200/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1827 - accuracy: 0.5039 - val_loss: 1.1567 - val_accuracy: 0.4297\n",
      "Epoch 201/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1840 - accuracy: 0.5000 - val_loss: 1.1672 - val_accuracy: 0.4141\n",
      "Epoch 202/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1822 - accuracy: 0.5033 - val_loss: 1.1482 - val_accuracy: 0.4219\n",
      "Epoch 203/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1817 - accuracy: 0.5028 - val_loss: 1.1561 - val_accuracy: 0.4219\n",
      "Epoch 204/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1823 - accuracy: 0.5045 - val_loss: 1.1430 - val_accuracy: 0.4219\n",
      "Epoch 205/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1817 - accuracy: 0.5045 - val_loss: 1.1477 - val_accuracy: 0.4453\n",
      "Epoch 206/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1811 - accuracy: 0.5022 - val_loss: 1.1537 - val_accuracy: 0.4375\n",
      "Epoch 207/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1814 - accuracy: 0.5050 - val_loss: 1.1534 - val_accuracy: 0.4219\n",
      "Epoch 208/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1821 - accuracy: 0.5033 - val_loss: 1.1387 - val_accuracy: 0.4219\n",
      "Epoch 209/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1818 - accuracy: 0.5028 - val_loss: 1.1513 - val_accuracy: 0.4219\n",
      "Epoch 210/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1815 - accuracy: 0.5039 - val_loss: 1.1476 - val_accuracy: 0.4453\n",
      "Epoch 211/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1819 - accuracy: 0.5050 - val_loss: 1.1463 - val_accuracy: 0.4219\n",
      "Epoch 212/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1820 - accuracy: 0.5017 - val_loss: 1.1469 - val_accuracy: 0.4219\n",
      "Epoch 213/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1821 - accuracy: 0.5033 - val_loss: 1.1485 - val_accuracy: 0.4297\n",
      "Epoch 214/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1816 - accuracy: 0.5050 - val_loss: 1.1534 - val_accuracy: 0.4219\n",
      "Epoch 215/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1816 - accuracy: 0.5039 - val_loss: 1.1557 - val_accuracy: 0.4219\n",
      "Epoch 216/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1814 - accuracy: 0.5050 - val_loss: 1.1542 - val_accuracy: 0.4297\n",
      "Epoch 217/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1818 - accuracy: 0.5033 - val_loss: 1.1445 - val_accuracy: 0.4297\n",
      "Epoch 218/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1816 - accuracy: 0.5028 - val_loss: 1.1529 - val_accuracy: 0.4219\n",
      "Epoch 219/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1819 - accuracy: 0.5056 - val_loss: 1.1541 - val_accuracy: 0.4219\n",
      "Epoch 220/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1813 - accuracy: 0.5061 - val_loss: 1.1546 - val_accuracy: 0.4219\n",
      "Epoch 221/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1813 - accuracy: 0.5022 - val_loss: 1.1516 - val_accuracy: 0.4219\n",
      "Epoch 222/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1810 - accuracy: 0.5039 - val_loss: 1.1458 - val_accuracy: 0.4219\n",
      "Epoch 223/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1812 - accuracy: 0.5028 - val_loss: 1.1454 - val_accuracy: 0.4219\n",
      "Epoch 224/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1814 - accuracy: 0.5033 - val_loss: 1.1454 - val_accuracy: 0.4453\n",
      "Epoch 225/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1816 - accuracy: 0.5033 - val_loss: 1.1504 - val_accuracy: 0.4219\n",
      "Epoch 226/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1822 - accuracy: 0.5050 - val_loss: 1.1481 - val_accuracy: 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1821 - accuracy: 0.5022 - val_loss: 1.1647 - val_accuracy: 0.4219\n",
      "Epoch 228/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1819 - accuracy: 0.5033 - val_loss: 1.1553 - val_accuracy: 0.4219\n",
      "Epoch 229/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1822 - accuracy: 0.5045 - val_loss: 1.1457 - val_accuracy: 0.4219\n",
      "Epoch 230/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1813 - accuracy: 0.5045 - val_loss: 1.1464 - val_accuracy: 0.4219\n",
      "Epoch 231/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1810 - accuracy: 0.5045 - val_loss: 1.1564 - val_accuracy: 0.4219\n",
      "Epoch 232/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1811 - accuracy: 0.5017 - val_loss: 1.1552 - val_accuracy: 0.4219\n",
      "Epoch 233/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1811 - accuracy: 0.5033 - val_loss: 1.1526 - val_accuracy: 0.4219\n",
      "Epoch 234/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1812 - accuracy: 0.5050 - val_loss: 1.1472 - val_accuracy: 0.4219\n",
      "Epoch 235/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1816 - accuracy: 0.5045 - val_loss: 1.1537 - val_accuracy: 0.4297\n",
      "Epoch 236/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1810 - accuracy: 0.5056 - val_loss: 1.1489 - val_accuracy: 0.4375\n",
      "Epoch 237/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1816 - accuracy: 0.5067 - val_loss: 1.1564 - val_accuracy: 0.4219\n",
      "Epoch 238/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1818 - accuracy: 0.5022 - val_loss: 1.1446 - val_accuracy: 0.4219\n",
      "Epoch 239/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1818 - accuracy: 0.5056 - val_loss: 1.1479 - val_accuracy: 0.4297\n",
      "Epoch 240/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1810 - accuracy: 0.5039 - val_loss: 1.1534 - val_accuracy: 0.4297\n",
      "Epoch 241/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1810 - accuracy: 0.5045 - val_loss: 1.1512 - val_accuracy: 0.4297\n",
      "Epoch 242/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1808 - accuracy: 0.5022 - val_loss: 1.1538 - val_accuracy: 0.4219\n",
      "Epoch 243/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1815 - accuracy: 0.5056 - val_loss: 1.1529 - val_accuracy: 0.4219\n",
      "Epoch 244/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1811 - accuracy: 0.5022 - val_loss: 1.1568 - val_accuracy: 0.4219\n",
      "Epoch 245/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1815 - accuracy: 0.5050 - val_loss: 1.1476 - val_accuracy: 0.4297\n",
      "Epoch 246/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1816 - accuracy: 0.5045 - val_loss: 1.1602 - val_accuracy: 0.4219\n",
      "Epoch 247/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1815 - accuracy: 0.5039 - val_loss: 1.1475 - val_accuracy: 0.4297\n",
      "Epoch 248/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1827 - accuracy: 0.5039 - val_loss: 1.1695 - val_accuracy: 0.4219\n",
      "Epoch 249/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1819 - accuracy: 0.5028 - val_loss: 1.1489 - val_accuracy: 0.4219\n",
      "Epoch 250/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1810 - accuracy: 0.5033 - val_loss: 1.1510 - val_accuracy: 0.4219\n",
      "Epoch 251/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1811 - accuracy: 0.5050 - val_loss: 1.1510 - val_accuracy: 0.4219\n",
      "Epoch 252/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1816 - accuracy: 0.5067 - val_loss: 1.1448 - val_accuracy: 0.4141\n",
      "Epoch 253/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1813 - accuracy: 0.5028 - val_loss: 1.1464 - val_accuracy: 0.4219\n",
      "Epoch 254/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1812 - accuracy: 0.5033 - val_loss: 1.1450 - val_accuracy: 0.4219\n",
      "Epoch 255/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1809 - accuracy: 0.5011 - val_loss: 1.1605 - val_accuracy: 0.4219\n",
      "Epoch 256/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1810 - accuracy: 0.5061 - val_loss: 1.1442 - val_accuracy: 0.4297\n",
      "Epoch 257/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1806 - accuracy: 0.5050 - val_loss: 1.1471 - val_accuracy: 0.4219\n",
      "Epoch 258/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1813 - accuracy: 0.5056 - val_loss: 1.1612 - val_accuracy: 0.4219\n",
      "Epoch 259/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1811 - accuracy: 0.5056 - val_loss: 1.1526 - val_accuracy: 0.4297\n",
      "Epoch 260/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1812 - accuracy: 0.5056 - val_loss: 1.1507 - val_accuracy: 0.4219\n",
      "Epoch 261/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1807 - accuracy: 0.5033 - val_loss: 1.1494 - val_accuracy: 0.4375\n",
      "Epoch 262/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1808 - accuracy: 0.5056 - val_loss: 1.1509 - val_accuracy: 0.4297\n",
      "Epoch 263/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1810 - accuracy: 0.5045 - val_loss: 1.1483 - val_accuracy: 0.4219\n",
      "Epoch 264/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1807 - accuracy: 0.5050 - val_loss: 1.1422 - val_accuracy: 0.4375\n",
      "Epoch 265/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1813 - accuracy: 0.5017 - val_loss: 1.1521 - val_accuracy: 0.4219\n",
      "Epoch 266/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1811 - accuracy: 0.5056 - val_loss: 1.1482 - val_accuracy: 0.4375\n",
      "Epoch 267/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1813 - accuracy: 0.5061 - val_loss: 1.1632 - val_accuracy: 0.4219\n",
      "Epoch 268/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1828 - accuracy: 0.5033 - val_loss: 1.1613 - val_accuracy: 0.4297\n",
      "Epoch 269/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1833 - accuracy: 0.4989 - val_loss: 1.1470 - val_accuracy: 0.4219\n",
      "Epoch 270/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1809 - accuracy: 0.5050 - val_loss: 1.1469 - val_accuracy: 0.4297\n",
      "Epoch 271/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1806 - accuracy: 0.5017 - val_loss: 1.1563 - val_accuracy: 0.4219\n",
      "Epoch 272/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1812 - accuracy: 0.5039 - val_loss: 1.1436 - val_accuracy: 0.4297\n",
      "Epoch 273/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1807 - accuracy: 0.5050 - val_loss: 1.1569 - val_accuracy: 0.4219\n",
      "Epoch 274/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1806 - accuracy: 0.5045 - val_loss: 1.1457 - val_accuracy: 0.4219\n",
      "Epoch 275/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1812 - accuracy: 0.5011 - val_loss: 1.1571 - val_accuracy: 0.4219\n",
      "Epoch 276/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1811 - accuracy: 0.5067 - val_loss: 1.1555 - val_accuracy: 0.4453\n",
      "Epoch 277/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1802 - accuracy: 0.5045 - val_loss: 1.1529 - val_accuracy: 0.4297\n",
      "Epoch 278/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1807 - accuracy: 0.5039 - val_loss: 1.1565 - val_accuracy: 0.4219\n",
      "Epoch 279/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1811 - accuracy: 0.5045 - val_loss: 1.1652 - val_accuracy: 0.4219\n",
      "Epoch 280/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1808 - accuracy: 0.5067 - val_loss: 1.1474 - val_accuracy: 0.4297\n",
      "Epoch 281/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1811 - accuracy: 0.5056 - val_loss: 1.1601 - val_accuracy: 0.4219\n",
      "Epoch 282/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1813 - accuracy: 0.5067 - val_loss: 1.1515 - val_accuracy: 0.4375\n",
      "Epoch 283/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1809 - accuracy: 0.5022 - val_loss: 1.1479 - val_accuracy: 0.4219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 284/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1808 - accuracy: 0.5045 - val_loss: 1.1518 - val_accuracy: 0.4297\n",
      "Epoch 285/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1807 - accuracy: 0.5028 - val_loss: 1.1570 - val_accuracy: 0.4219\n",
      "Epoch 286/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1805 - accuracy: 0.5056 - val_loss: 1.1548 - val_accuracy: 0.4375\n",
      "Epoch 287/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1803 - accuracy: 0.5033 - val_loss: 1.1485 - val_accuracy: 0.4219\n",
      "Epoch 288/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1808 - accuracy: 0.5061 - val_loss: 1.1468 - val_accuracy: 0.4219\n",
      "Epoch 289/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1804 - accuracy: 0.5017 - val_loss: 1.1536 - val_accuracy: 0.4219\n",
      "Epoch 290/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1805 - accuracy: 0.5050 - val_loss: 1.1523 - val_accuracy: 0.4297\n",
      "Epoch 291/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1818 - accuracy: 0.5033 - val_loss: 1.1602 - val_accuracy: 0.4453\n",
      "Epoch 292/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1814 - accuracy: 0.5006 - val_loss: 1.1508 - val_accuracy: 0.4141\n",
      "Epoch 293/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1812 - accuracy: 0.5050 - val_loss: 1.1459 - val_accuracy: 0.4219\n",
      "Epoch 294/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1806 - accuracy: 0.5073 - val_loss: 1.1468 - val_accuracy: 0.4297\n",
      "Epoch 295/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1803 - accuracy: 0.5045 - val_loss: 1.1479 - val_accuracy: 0.4219\n",
      "Epoch 296/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1806 - accuracy: 0.5039 - val_loss: 1.1582 - val_accuracy: 0.4219\n",
      "Epoch 297/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1.1802 - accuracy: 0.5061 - val_loss: 1.1542 - val_accuracy: 0.4219\n",
      "Epoch 298/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1802 - accuracy: 0.5061 - val_loss: 1.1418 - val_accuracy: 0.4219\n",
      "Epoch 299/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1815 - accuracy: 0.5067 - val_loss: 1.1628 - val_accuracy: 0.4297\n",
      "Epoch 300/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1809 - accuracy: 0.5028 - val_loss: 1.1502 - val_accuracy: 0.4141\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "56/56 [==============================] - 0s 1ms/step\n",
      "Recall: [0.92 0.   0.61]\n",
      "Precision: [0.41 1.   0.72]\n",
      "F1-score: [0.57 0.   0.66]\n",
      "Accuracy: 0.51\n",
      "Brier score:0.19645319038021208\n",
      "Brier climat:0.20926835317460318\n",
      "Brier skill score:0.06123793970748537\n",
      "Recall: [0.92 0.   0.61]\n",
      "Precision: [0.41 1.   0.72]\n",
      "F1-score: [0.57 0.   0.66]\n",
      "Accuracy: 0.51\n",
      "Brier score:0.2138621966126618\n",
      "Brier climat:0.2178174603174603\n",
      "Brier skill score:0.018158616389309934\n",
      "Recall: [0.92 0.   0.61]\n",
      "Precision: [0.41 1.   0.72]\n",
      "F1-score: [0.57 0.   0.66]\n",
      "Accuracy: 0.51\n",
      "Brier score:0.1577942109551895\n",
      "Brier climat:0.2318824404761905\n",
      "Brier skill score:0.31950771851829085\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "4/4 [==============================] - 0s 970us/step\n",
      "Recall: [0.92 0.   0.9 ]\n",
      "Precision: [0.46 0.   0.28]\n",
      "F1-score: [0.61 0.   0.43]\n",
      "Accuracy: 0.41\n",
      "Brier score:0.24468351203326777\n",
      "Brier climat:0.22362847222222224\n",
      "Brier skill score:-0.09415187432002359\n",
      "Recall: [0.92 0.   0.9 ]\n",
      "Precision: [0.46 0.   0.28]\n",
      "F1-score: [0.61 0.   0.43]\n",
      "Accuracy: 0.41\n",
      "Brier score:0.3466527073962667\n",
      "Brier climat:0.2703472222222222\n",
      "Brier skill score:-0.2822499323160137\n",
      "Recall: [0.92 0.   0.9 ]\n",
      "Precision: [0.46 0.   0.28]\n",
      "F1-score: [0.61 0.   0.43]\n",
      "Accuracy: 0.41\n",
      "Brier score:0.09374748129589598\n",
      "Brier climat:0.14036458333333332\n",
      "Brier skill score:0.33211441896801375\n",
      "******** 7\n",
      "validation years [1988, 1989]\n",
      "train years {1984, 1985, 1986, 1987, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "{0: 2.0, 1: 1.0, 2: 1.0}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 60)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 183       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183\n",
      "Trainable params: 183\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - ETA: 0s - loss: 1.7754 - accuracy: 0.4604WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "28/28 [==============================] - 4s 109ms/step - loss: 1.7754 - accuracy: 0.4604 - val_loss: 1.8039 - val_accuracy: 0.2812\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.6285 - accuracy: 0.4794 - val_loss: 1.5775 - val_accuracy: 0.3594\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 1s 47ms/step - loss: 1.5347 - accuracy: 0.4927 - val_loss: 1.4711 - val_accuracy: 0.4062\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4807 - accuracy: 0.5100 - val_loss: 1.3686 - val_accuracy: 0.4219\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 0s 17ms/step - loss: 1.4502 - accuracy: 0.5017 - val_loss: 1.3085 - val_accuracy: 0.4453\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 1s 32ms/step - loss: 1.4306 - accuracy: 0.5123 - val_loss: 1.2698 - val_accuracy: 0.4375\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4157 - accuracy: 0.5022 - val_loss: 1.2778 - val_accuracy: 0.4375\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4038 - accuracy: 0.4994 - val_loss: 1.2547 - val_accuracy: 0.4375\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3935 - accuracy: 0.4994 - val_loss: 1.2498 - val_accuracy: 0.4375\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3829 - accuracy: 0.4994 - val_loss: 1.2355 - val_accuracy: 0.4375\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3738 - accuracy: 0.5028 - val_loss: 1.2390 - val_accuracy: 0.4375\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3647 - accuracy: 0.4972 - val_loss: 1.2315 - val_accuracy: 0.4375\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3554 - accuracy: 0.4978 - val_loss: 1.2285 - val_accuracy: 0.4219\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3474 - accuracy: 0.4950 - val_loss: 1.2357 - val_accuracy: 0.4141\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3393 - accuracy: 0.4972 - val_loss: 1.2132 - val_accuracy: 0.4141\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3312 - accuracy: 0.4989 - val_loss: 1.2284 - val_accuracy: 0.4141\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3248 - accuracy: 0.4983 - val_loss: 1.2295 - val_accuracy: 0.4062\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3182 - accuracy: 0.4967 - val_loss: 1.2198 - val_accuracy: 0.4062\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3123 - accuracy: 0.4967 - val_loss: 1.2097 - val_accuracy: 0.3984\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3067 - accuracy: 0.4978 - val_loss: 1.2160 - val_accuracy: 0.3984\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3020 - accuracy: 0.4972 - val_loss: 1.2277 - val_accuracy: 0.3906\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2980 - accuracy: 0.4967 - val_loss: 1.1942 - val_accuracy: 0.3984\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2922 - accuracy: 0.4961 - val_loss: 1.2125 - val_accuracy: 0.3906\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2884 - accuracy: 0.4955 - val_loss: 1.2178 - val_accuracy: 0.3906\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2842 - accuracy: 0.4950 - val_loss: 1.2084 - val_accuracy: 0.3906\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2811 - accuracy: 0.4955 - val_loss: 1.1964 - val_accuracy: 0.3906\n",
      "Epoch 27/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2775 - accuracy: 0.4967 - val_loss: 1.2157 - val_accuracy: 0.3828\n",
      "Epoch 28/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2745 - accuracy: 0.4983 - val_loss: 1.2013 - val_accuracy: 0.3906\n",
      "Epoch 29/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2713 - accuracy: 0.4972 - val_loss: 1.2018 - val_accuracy: 0.3828\n",
      "Epoch 30/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2686 - accuracy: 0.4967 - val_loss: 1.2103 - val_accuracy: 0.3750\n",
      "Epoch 31/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2661 - accuracy: 0.4933 - val_loss: 1.1956 - val_accuracy: 0.3672\n",
      "Epoch 32/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2633 - accuracy: 0.4950 - val_loss: 1.2027 - val_accuracy: 0.3672\n",
      "Epoch 33/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2611 - accuracy: 0.4950 - val_loss: 1.1954 - val_accuracy: 0.3672\n",
      "Epoch 34/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2595 - accuracy: 0.4950 - val_loss: 1.2050 - val_accuracy: 0.3594\n",
      "Epoch 35/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2575 - accuracy: 0.4955 - val_loss: 1.2023 - val_accuracy: 0.3672\n",
      "Epoch 36/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2539 - accuracy: 0.4944 - val_loss: 1.1918 - val_accuracy: 0.3594\n",
      "Epoch 37/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2520 - accuracy: 0.4927 - val_loss: 1.2084 - val_accuracy: 0.3594\n",
      "Epoch 38/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2500 - accuracy: 0.4939 - val_loss: 1.2019 - val_accuracy: 0.3594\n",
      "Epoch 39/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2483 - accuracy: 0.4967 - val_loss: 1.1992 - val_accuracy: 0.3594\n",
      "Epoch 40/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2458 - accuracy: 0.4955 - val_loss: 1.1873 - val_accuracy: 0.3594\n",
      "Epoch 41/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2436 - accuracy: 0.4950 - val_loss: 1.1898 - val_accuracy: 0.3672\n",
      "Epoch 42/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2425 - accuracy: 0.4950 - val_loss: 1.1834 - val_accuracy: 0.3594\n",
      "Epoch 43/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2407 - accuracy: 0.4967 - val_loss: 1.1991 - val_accuracy: 0.3594\n",
      "Epoch 44/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2390 - accuracy: 0.4950 - val_loss: 1.1860 - val_accuracy: 0.3594\n",
      "Epoch 45/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2373 - accuracy: 0.4972 - val_loss: 1.1884 - val_accuracy: 0.3594\n",
      "Epoch 46/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2360 - accuracy: 0.4972 - val_loss: 1.1752 - val_accuracy: 0.3672\n",
      "Epoch 47/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2344 - accuracy: 0.4955 - val_loss: 1.1973 - val_accuracy: 0.3594\n",
      "Epoch 48/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2326 - accuracy: 0.4983 - val_loss: 1.1693 - val_accuracy: 0.3672\n",
      "Epoch 49/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2326 - accuracy: 0.4994 - val_loss: 1.1835 - val_accuracy: 0.3594\n",
      "Epoch 50/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2318 - accuracy: 0.4983 - val_loss: 1.1873 - val_accuracy: 0.3594\n",
      "Epoch 51/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2285 - accuracy: 0.4989 - val_loss: 1.1751 - val_accuracy: 0.3672\n",
      "Epoch 52/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2268 - accuracy: 0.5006 - val_loss: 1.1708 - val_accuracy: 0.3594\n",
      "Epoch 53/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2259 - accuracy: 0.5000 - val_loss: 1.1777 - val_accuracy: 0.3672\n",
      "Epoch 54/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.2257 - accuracy: 0.4994 - val_loss: 1.1698 - val_accuracy: 0.3672\n",
      "Epoch 55/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2236 - accuracy: 0.5011 - val_loss: 1.1731 - val_accuracy: 0.3594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2226 - accuracy: 0.4967 - val_loss: 1.1626 - val_accuracy: 0.3672\n",
      "Epoch 57/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2214 - accuracy: 0.4972 - val_loss: 1.1582 - val_accuracy: 0.3594\n",
      "Epoch 58/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2208 - accuracy: 0.4950 - val_loss: 1.1562 - val_accuracy: 0.3672\n",
      "Epoch 59/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2197 - accuracy: 0.4978 - val_loss: 1.1680 - val_accuracy: 0.3594\n",
      "Epoch 60/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2188 - accuracy: 0.4978 - val_loss: 1.1735 - val_accuracy: 0.3594\n",
      "Epoch 61/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2171 - accuracy: 0.4978 - val_loss: 1.1689 - val_accuracy: 0.3750\n",
      "Epoch 62/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2165 - accuracy: 0.4994 - val_loss: 1.1633 - val_accuracy: 0.3594\n",
      "Epoch 63/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2150 - accuracy: 0.4983 - val_loss: 1.1674 - val_accuracy: 0.3672\n",
      "Epoch 64/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2146 - accuracy: 0.4989 - val_loss: 1.1537 - val_accuracy: 0.3672\n",
      "Epoch 65/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2136 - accuracy: 0.4967 - val_loss: 1.1746 - val_accuracy: 0.3672\n",
      "Epoch 66/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2136 - accuracy: 0.4994 - val_loss: 1.1648 - val_accuracy: 0.3672\n",
      "Epoch 67/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2126 - accuracy: 0.4955 - val_loss: 1.1581 - val_accuracy: 0.3672\n",
      "Epoch 68/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2118 - accuracy: 0.4972 - val_loss: 1.1818 - val_accuracy: 0.3672\n",
      "Epoch 69/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2106 - accuracy: 0.4978 - val_loss: 1.1708 - val_accuracy: 0.3672\n",
      "Epoch 70/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2103 - accuracy: 0.4983 - val_loss: 1.1592 - val_accuracy: 0.3672\n",
      "Epoch 71/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2097 - accuracy: 0.4978 - val_loss: 1.1575 - val_accuracy: 0.3594\n",
      "Epoch 72/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2098 - accuracy: 0.4961 - val_loss: 1.1658 - val_accuracy: 0.3594\n",
      "Epoch 73/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2078 - accuracy: 0.4972 - val_loss: 1.1569 - val_accuracy: 0.3594\n",
      "Epoch 74/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2067 - accuracy: 0.4950 - val_loss: 1.1685 - val_accuracy: 0.3672\n",
      "Epoch 75/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2067 - accuracy: 0.4961 - val_loss: 1.1540 - val_accuracy: 0.3672\n",
      "Epoch 76/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2063 - accuracy: 0.4950 - val_loss: 1.1722 - val_accuracy: 0.3594\n",
      "Epoch 77/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2053 - accuracy: 0.4983 - val_loss: 1.1517 - val_accuracy: 0.3672\n",
      "Epoch 78/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2052 - accuracy: 0.4944 - val_loss: 1.1418 - val_accuracy: 0.3672\n",
      "Epoch 79/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2045 - accuracy: 0.4983 - val_loss: 1.1615 - val_accuracy: 0.3672\n",
      "Epoch 80/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2030 - accuracy: 0.4939 - val_loss: 1.1494 - val_accuracy: 0.3672\n",
      "Epoch 81/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2037 - accuracy: 0.4933 - val_loss: 1.1555 - val_accuracy: 0.3672\n",
      "Epoch 82/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2035 - accuracy: 0.4961 - val_loss: 1.1612 - val_accuracy: 0.3672\n",
      "Epoch 83/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2022 - accuracy: 0.4955 - val_loss: 1.1606 - val_accuracy: 0.3672\n",
      "Epoch 84/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2013 - accuracy: 0.4933 - val_loss: 1.1482 - val_accuracy: 0.3672\n",
      "Epoch 85/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2007 - accuracy: 0.4950 - val_loss: 1.1535 - val_accuracy: 0.3672\n",
      "Epoch 86/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2009 - accuracy: 0.4961 - val_loss: 1.1407 - val_accuracy: 0.3672\n",
      "Epoch 87/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2003 - accuracy: 0.4933 - val_loss: 1.1386 - val_accuracy: 0.3672\n",
      "Epoch 88/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2004 - accuracy: 0.4933 - val_loss: 1.1462 - val_accuracy: 0.3672\n",
      "Epoch 89/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1998 - accuracy: 0.4944 - val_loss: 1.1523 - val_accuracy: 0.3672\n",
      "Epoch 90/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1995 - accuracy: 0.4933 - val_loss: 1.1467 - val_accuracy: 0.3672\n",
      "Epoch 91/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1991 - accuracy: 0.4939 - val_loss: 1.1408 - val_accuracy: 0.3594\n",
      "Epoch 92/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1989 - accuracy: 0.4944 - val_loss: 1.1367 - val_accuracy: 0.3672\n",
      "Epoch 93/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1983 - accuracy: 0.4939 - val_loss: 1.1451 - val_accuracy: 0.3594\n",
      "Epoch 94/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1988 - accuracy: 0.4927 - val_loss: 1.1333 - val_accuracy: 0.3594\n",
      "Epoch 95/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1984 - accuracy: 0.4927 - val_loss: 1.1283 - val_accuracy: 0.3672\n",
      "Epoch 96/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1976 - accuracy: 0.4944 - val_loss: 1.1424 - val_accuracy: 0.3672\n",
      "Epoch 97/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1975 - accuracy: 0.4933 - val_loss: 1.1394 - val_accuracy: 0.3672\n",
      "Epoch 98/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1971 - accuracy: 0.4950 - val_loss: 1.1268 - val_accuracy: 0.3594\n",
      "Epoch 99/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1968 - accuracy: 0.4939 - val_loss: 1.1248 - val_accuracy: 0.3594\n",
      "Epoch 100/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1971 - accuracy: 0.4939 - val_loss: 1.1353 - val_accuracy: 0.3672\n",
      "Epoch 101/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1965 - accuracy: 0.4950 - val_loss: 1.1274 - val_accuracy: 0.3672\n",
      "Epoch 102/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1972 - accuracy: 0.4950 - val_loss: 1.1304 - val_accuracy: 0.3672\n",
      "Epoch 103/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1971 - accuracy: 0.4933 - val_loss: 1.1266 - val_accuracy: 0.3594\n",
      "Epoch 104/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1971 - accuracy: 0.4933 - val_loss: 1.1327 - val_accuracy: 0.3672\n",
      "Epoch 105/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1954 - accuracy: 0.4933 - val_loss: 1.1376 - val_accuracy: 0.3594\n",
      "Epoch 106/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1951 - accuracy: 0.4933 - val_loss: 1.1323 - val_accuracy: 0.3594\n",
      "Epoch 107/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1955 - accuracy: 0.4961 - val_loss: 1.1212 - val_accuracy: 0.3672\n",
      "Epoch 108/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1956 - accuracy: 0.4950 - val_loss: 1.1257 - val_accuracy: 0.3594\n",
      "Epoch 109/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1956 - accuracy: 0.4939 - val_loss: 1.1227 - val_accuracy: 0.3594\n",
      "Epoch 110/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1945 - accuracy: 0.4944 - val_loss: 1.1476 - val_accuracy: 0.3594\n",
      "Epoch 111/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1942 - accuracy: 0.4950 - val_loss: 1.1277 - val_accuracy: 0.3594\n",
      "Epoch 112/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1947 - accuracy: 0.4916 - val_loss: 1.1435 - val_accuracy: 0.3594\n",
      "Epoch 113/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1949 - accuracy: 0.4911 - val_loss: 1.1203 - val_accuracy: 0.3594\n",
      "Epoch 114/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1945 - accuracy: 0.4955 - val_loss: 1.1331 - val_accuracy: 0.3516\n",
      "Epoch 115/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1948 - accuracy: 0.4950 - val_loss: 1.1174 - val_accuracy: 0.3594\n",
      "Epoch 116/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1947 - accuracy: 0.4944 - val_loss: 1.1371 - val_accuracy: 0.3594\n",
      "Epoch 117/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1937 - accuracy: 0.4950 - val_loss: 1.1402 - val_accuracy: 0.3594\n",
      "Epoch 118/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1939 - accuracy: 0.4955 - val_loss: 1.1220 - val_accuracy: 0.3594\n",
      "Epoch 119/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1938 - accuracy: 0.4944 - val_loss: 1.1129 - val_accuracy: 0.3594\n",
      "Epoch 120/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1934 - accuracy: 0.4944 - val_loss: 1.1247 - val_accuracy: 0.3516\n",
      "Epoch 121/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1933 - accuracy: 0.4939 - val_loss: 1.1356 - val_accuracy: 0.3594\n",
      "Epoch 122/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1931 - accuracy: 0.4950 - val_loss: 1.1290 - val_accuracy: 0.3516\n",
      "Epoch 123/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1927 - accuracy: 0.4944 - val_loss: 1.1361 - val_accuracy: 0.3594\n",
      "Epoch 124/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1948 - accuracy: 0.4927 - val_loss: 1.1367 - val_accuracy: 0.3594\n",
      "Epoch 125/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1933 - accuracy: 0.4950 - val_loss: 1.1298 - val_accuracy: 0.3438\n",
      "Epoch 126/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1927 - accuracy: 0.4961 - val_loss: 1.1287 - val_accuracy: 0.3516\n",
      "Epoch 127/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1946 - accuracy: 0.4955 - val_loss: 1.1234 - val_accuracy: 0.3516\n",
      "Epoch 128/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1936 - accuracy: 0.4955 - val_loss: 1.1225 - val_accuracy: 0.3516\n",
      "Epoch 129/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1931 - accuracy: 0.4944 - val_loss: 1.1146 - val_accuracy: 0.3594\n",
      "Epoch 130/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1924 - accuracy: 0.4922 - val_loss: 1.1322 - val_accuracy: 0.3516\n",
      "Epoch 131/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1923 - accuracy: 0.4950 - val_loss: 1.1334 - val_accuracy: 0.3438\n",
      "Epoch 132/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1927 - accuracy: 0.4950 - val_loss: 1.1450 - val_accuracy: 0.3594\n",
      "Epoch 133/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1924 - accuracy: 0.4950 - val_loss: 1.1271 - val_accuracy: 0.3594\n",
      "Epoch 134/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1926 - accuracy: 0.4944 - val_loss: 1.1370 - val_accuracy: 0.3594\n",
      "Epoch 135/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1928 - accuracy: 0.4961 - val_loss: 1.1350 - val_accuracy: 0.3594\n",
      "Epoch 136/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1915 - accuracy: 0.4955 - val_loss: 1.1204 - val_accuracy: 0.3594\n",
      "Epoch 137/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1922 - accuracy: 0.4950 - val_loss: 1.1464 - val_accuracy: 0.3672\n",
      "Epoch 138/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1922 - accuracy: 0.4939 - val_loss: 1.1278 - val_accuracy: 0.3672\n",
      "Epoch 139/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1914 - accuracy: 0.4961 - val_loss: 1.1291 - val_accuracy: 0.3594\n",
      "Epoch 140/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1915 - accuracy: 0.4927 - val_loss: 1.1296 - val_accuracy: 0.3672\n",
      "Epoch 141/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1914 - accuracy: 0.4944 - val_loss: 1.1132 - val_accuracy: 0.3594\n",
      "Epoch 142/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1924 - accuracy: 0.4939 - val_loss: 1.1322 - val_accuracy: 0.3594\n",
      "Epoch 143/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1915 - accuracy: 0.4944 - val_loss: 1.1212 - val_accuracy: 0.3594\n",
      "Epoch 144/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1916 - accuracy: 0.4950 - val_loss: 1.1261 - val_accuracy: 0.3594\n",
      "Epoch 145/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1915 - accuracy: 0.4944 - val_loss: 1.1181 - val_accuracy: 0.3594\n",
      "Epoch 146/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1911 - accuracy: 0.4950 - val_loss: 1.1267 - val_accuracy: 0.3594\n",
      "Epoch 147/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1924 - accuracy: 0.4967 - val_loss: 1.1297 - val_accuracy: 0.3594\n",
      "Epoch 148/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1924 - accuracy: 0.4944 - val_loss: 1.1339 - val_accuracy: 0.3672\n",
      "Epoch 149/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1917 - accuracy: 0.4972 - val_loss: 1.1367 - val_accuracy: 0.3594\n",
      "Epoch 150/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1927 - accuracy: 0.4944 - val_loss: 1.1265 - val_accuracy: 0.3594\n",
      "Epoch 151/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1915 - accuracy: 0.4944 - val_loss: 1.1146 - val_accuracy: 0.3594\n",
      "Epoch 152/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1917 - accuracy: 0.4967 - val_loss: 1.1352 - val_accuracy: 0.3594\n",
      "Epoch 153/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1910 - accuracy: 0.4955 - val_loss: 1.1292 - val_accuracy: 0.3594\n",
      "Epoch 154/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1913 - accuracy: 0.4944 - val_loss: 1.1353 - val_accuracy: 0.3594\n",
      "Epoch 155/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1911 - accuracy: 0.4922 - val_loss: 1.1272 - val_accuracy: 0.3672\n",
      "Epoch 156/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1915 - accuracy: 0.4967 - val_loss: 1.1175 - val_accuracy: 0.3750\n",
      "Epoch 157/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1908 - accuracy: 0.4950 - val_loss: 1.1250 - val_accuracy: 0.3672\n",
      "Epoch 158/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1905 - accuracy: 0.4944 - val_loss: 1.1321 - val_accuracy: 0.3594\n",
      "Epoch 159/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1906 - accuracy: 0.4939 - val_loss: 1.1192 - val_accuracy: 0.3672\n",
      "Epoch 160/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1905 - accuracy: 0.4955 - val_loss: 1.1106 - val_accuracy: 0.3594\n",
      "Epoch 161/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1907 - accuracy: 0.4939 - val_loss: 1.1121 - val_accuracy: 0.3594\n",
      "Epoch 162/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1906 - accuracy: 0.4955 - val_loss: 1.1171 - val_accuracy: 0.3594\n",
      "Epoch 163/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1906 - accuracy: 0.4944 - val_loss: 1.1122 - val_accuracy: 0.3672\n",
      "Epoch 164/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1913 - accuracy: 0.4933 - val_loss: 1.1099 - val_accuracy: 0.3672\n",
      "Epoch 165/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1901 - accuracy: 0.4950 - val_loss: 1.1337 - val_accuracy: 0.3594\n",
      "Epoch 166/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1901 - accuracy: 0.4967 - val_loss: 1.1278 - val_accuracy: 0.3672\n",
      "Epoch 167/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1901 - accuracy: 0.4933 - val_loss: 1.1309 - val_accuracy: 0.3672\n",
      "Epoch 168/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1900 - accuracy: 0.4955 - val_loss: 1.1161 - val_accuracy: 0.3594\n",
      "Epoch 169/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1905 - accuracy: 0.4961 - val_loss: 1.1298 - val_accuracy: 0.3594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1902 - accuracy: 0.4939 - val_loss: 1.1111 - val_accuracy: 0.3672\n",
      "Epoch 171/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1905 - accuracy: 0.4950 - val_loss: 1.1263 - val_accuracy: 0.3672\n",
      "Epoch 172/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1911 - accuracy: 0.4961 - val_loss: 1.1277 - val_accuracy: 0.3594\n",
      "Epoch 173/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1899 - accuracy: 0.4955 - val_loss: 1.1201 - val_accuracy: 0.3672\n",
      "Epoch 174/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1902 - accuracy: 0.4944 - val_loss: 1.1146 - val_accuracy: 0.3594\n",
      "Epoch 175/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1901 - accuracy: 0.4955 - val_loss: 1.1186 - val_accuracy: 0.3594\n",
      "Epoch 176/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1896 - accuracy: 0.4950 - val_loss: 1.1225 - val_accuracy: 0.3672\n",
      "Epoch 177/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1917 - accuracy: 0.4972 - val_loss: 1.1170 - val_accuracy: 0.3672\n",
      "Epoch 178/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1904 - accuracy: 0.4933 - val_loss: 1.1268 - val_accuracy: 0.3672\n",
      "Epoch 179/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1897 - accuracy: 0.4955 - val_loss: 1.1219 - val_accuracy: 0.3594\n",
      "Epoch 180/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1902 - accuracy: 0.4933 - val_loss: 1.1220 - val_accuracy: 0.3672\n",
      "Epoch 181/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1897 - accuracy: 0.4967 - val_loss: 1.1107 - val_accuracy: 0.3672\n",
      "Epoch 182/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1902 - accuracy: 0.4939 - val_loss: 1.1210 - val_accuracy: 0.3672\n",
      "Epoch 183/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1898 - accuracy: 0.4961 - val_loss: 1.1267 - val_accuracy: 0.3672\n",
      "Epoch 184/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1904 - accuracy: 0.4944 - val_loss: 1.1268 - val_accuracy: 0.3672\n",
      "Epoch 185/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1905 - accuracy: 0.4955 - val_loss: 1.1200 - val_accuracy: 0.3672\n",
      "Epoch 186/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1892 - accuracy: 0.4955 - val_loss: 1.1173 - val_accuracy: 0.3672\n",
      "Epoch 187/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1894 - accuracy: 0.4950 - val_loss: 1.1119 - val_accuracy: 0.3672\n",
      "Epoch 188/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1894 - accuracy: 0.4950 - val_loss: 1.1189 - val_accuracy: 0.3672\n",
      "Epoch 189/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1897 - accuracy: 0.4933 - val_loss: 1.1292 - val_accuracy: 0.3672\n",
      "Epoch 190/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1897 - accuracy: 0.4950 - val_loss: 1.1116 - val_accuracy: 0.3672\n",
      "Epoch 191/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1893 - accuracy: 0.4955 - val_loss: 1.1255 - val_accuracy: 0.3672\n",
      "Epoch 192/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1898 - accuracy: 0.4950 - val_loss: 1.1140 - val_accuracy: 0.3672\n",
      "Epoch 193/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1906 - accuracy: 0.4967 - val_loss: 1.1326 - val_accuracy: 0.3594\n",
      "Epoch 194/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1896 - accuracy: 0.4955 - val_loss: 1.1190 - val_accuracy: 0.3672\n",
      "Epoch 195/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1895 - accuracy: 0.4939 - val_loss: 1.1244 - val_accuracy: 0.3672\n",
      "Epoch 196/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1900 - accuracy: 0.4955 - val_loss: 1.1175 - val_accuracy: 0.3594\n",
      "Epoch 197/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1912 - accuracy: 0.4978 - val_loss: 1.1153 - val_accuracy: 0.3672\n",
      "Epoch 198/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1902 - accuracy: 0.4967 - val_loss: 1.1215 - val_accuracy: 0.3594\n",
      "Epoch 199/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1893 - accuracy: 0.4939 - val_loss: 1.1286 - val_accuracy: 0.3594\n",
      "Epoch 200/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1899 - accuracy: 0.4927 - val_loss: 1.1258 - val_accuracy: 0.3672\n",
      "Epoch 201/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1896 - accuracy: 0.4972 - val_loss: 1.1318 - val_accuracy: 0.3594\n",
      "Epoch 202/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1894 - accuracy: 0.4944 - val_loss: 1.1102 - val_accuracy: 0.3594\n",
      "Epoch 203/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1903 - accuracy: 0.4955 - val_loss: 1.1171 - val_accuracy: 0.3594\n",
      "Epoch 204/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1897 - accuracy: 0.4933 - val_loss: 1.1257 - val_accuracy: 0.3672\n",
      "Epoch 205/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1893 - accuracy: 0.4950 - val_loss: 1.1194 - val_accuracy: 0.3594\n",
      "Epoch 206/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1893 - accuracy: 0.4933 - val_loss: 1.1225 - val_accuracy: 0.3594\n",
      "Epoch 207/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1893 - accuracy: 0.4944 - val_loss: 1.1283 - val_accuracy: 0.3672\n",
      "Epoch 208/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1899 - accuracy: 0.4972 - val_loss: 1.1197 - val_accuracy: 0.3594\n",
      "Epoch 209/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1893 - accuracy: 0.4967 - val_loss: 1.1137 - val_accuracy: 0.3672\n",
      "Epoch 210/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1894 - accuracy: 0.4961 - val_loss: 1.1301 - val_accuracy: 0.3672\n",
      "Epoch 211/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1887 - accuracy: 0.4939 - val_loss: 1.1188 - val_accuracy: 0.3672\n",
      "Epoch 212/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1894 - accuracy: 0.4927 - val_loss: 1.1065 - val_accuracy: 0.3672\n",
      "Epoch 213/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1892 - accuracy: 0.4961 - val_loss: 1.1323 - val_accuracy: 0.3672\n",
      "Epoch 214/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1891 - accuracy: 0.4950 - val_loss: 1.1231 - val_accuracy: 0.3594\n",
      "Epoch 215/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1887 - accuracy: 0.4955 - val_loss: 1.1144 - val_accuracy: 0.3594\n",
      "Epoch 216/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1889 - accuracy: 0.4955 - val_loss: 1.1098 - val_accuracy: 0.3594\n",
      "Epoch 217/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1887 - accuracy: 0.4955 - val_loss: 1.1095 - val_accuracy: 0.3672\n",
      "Epoch 218/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1891 - accuracy: 0.4978 - val_loss: 1.1174 - val_accuracy: 0.3672\n",
      "Epoch 219/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1887 - accuracy: 0.4944 - val_loss: 1.1321 - val_accuracy: 0.3594\n",
      "Epoch 220/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1892 - accuracy: 0.4939 - val_loss: 1.1141 - val_accuracy: 0.3672\n",
      "Epoch 221/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1896 - accuracy: 0.4978 - val_loss: 1.1259 - val_accuracy: 0.3672\n",
      "Epoch 222/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1889 - accuracy: 0.4961 - val_loss: 1.1271 - val_accuracy: 0.3672\n",
      "Epoch 223/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1885 - accuracy: 0.4944 - val_loss: 1.1154 - val_accuracy: 0.3672\n",
      "Epoch 224/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1885 - accuracy: 0.4944 - val_loss: 1.1260 - val_accuracy: 0.3672\n",
      "Epoch 225/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1887 - accuracy: 0.4972 - val_loss: 1.1104 - val_accuracy: 0.3672\n",
      "Epoch 226/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1887 - accuracy: 0.4944 - val_loss: 1.1086 - val_accuracy: 0.3594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1889 - accuracy: 0.4944 - val_loss: 1.1151 - val_accuracy: 0.3672\n",
      "Epoch 228/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1883 - accuracy: 0.4944 - val_loss: 1.1150 - val_accuracy: 0.3594\n",
      "Epoch 229/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1891 - accuracy: 0.4927 - val_loss: 1.1293 - val_accuracy: 0.3594\n",
      "Epoch 230/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1894 - accuracy: 0.4967 - val_loss: 1.1291 - val_accuracy: 0.3594\n",
      "Epoch 231/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1887 - accuracy: 0.4961 - val_loss: 1.1191 - val_accuracy: 0.3672\n",
      "Epoch 232/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1897 - accuracy: 0.4967 - val_loss: 1.1155 - val_accuracy: 0.3672\n",
      "Epoch 233/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1896 - accuracy: 0.4955 - val_loss: 1.1371 - val_accuracy: 0.3594\n",
      "Epoch 234/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1891 - accuracy: 0.4978 - val_loss: 1.1176 - val_accuracy: 0.3672\n",
      "Epoch 235/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1886 - accuracy: 0.4961 - val_loss: 1.1182 - val_accuracy: 0.3672\n",
      "Epoch 236/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1881 - accuracy: 0.4950 - val_loss: 1.1220 - val_accuracy: 0.3672\n",
      "Epoch 237/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1888 - accuracy: 0.4950 - val_loss: 1.1222 - val_accuracy: 0.3594\n",
      "Epoch 238/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1896 - accuracy: 0.4972 - val_loss: 1.1042 - val_accuracy: 0.3672\n",
      "Epoch 239/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1884 - accuracy: 0.4978 - val_loss: 1.1250 - val_accuracy: 0.3672\n",
      "Epoch 240/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1883 - accuracy: 0.4955 - val_loss: 1.1279 - val_accuracy: 0.3594\n",
      "Epoch 241/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1883 - accuracy: 0.4950 - val_loss: 1.1232 - val_accuracy: 0.3594\n",
      "Epoch 242/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1886 - accuracy: 0.4972 - val_loss: 1.1148 - val_accuracy: 0.3672\n",
      "Epoch 243/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1885 - accuracy: 0.4961 - val_loss: 1.1358 - val_accuracy: 0.3594\n",
      "Epoch 244/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1881 - accuracy: 0.4955 - val_loss: 1.1062 - val_accuracy: 0.3672\n",
      "Epoch 245/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1883 - accuracy: 0.4955 - val_loss: 1.1298 - val_accuracy: 0.3594\n",
      "Epoch 246/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1890 - accuracy: 0.4972 - val_loss: 1.1247 - val_accuracy: 0.3672\n",
      "Epoch 247/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1888 - accuracy: 0.4961 - val_loss: 1.1129 - val_accuracy: 0.3672\n",
      "Epoch 248/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1891 - accuracy: 0.4972 - val_loss: 1.1344 - val_accuracy: 0.3594\n",
      "Epoch 249/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1885 - accuracy: 0.4961 - val_loss: 1.1173 - val_accuracy: 0.3672\n",
      "Epoch 250/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1887 - accuracy: 0.4978 - val_loss: 1.1296 - val_accuracy: 0.3594\n",
      "Epoch 251/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1886 - accuracy: 0.4944 - val_loss: 1.1245 - val_accuracy: 0.3594\n",
      "Epoch 252/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1881 - accuracy: 0.4950 - val_loss: 1.1174 - val_accuracy: 0.3672\n",
      "Epoch 253/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1888 - accuracy: 0.4961 - val_loss: 1.1100 - val_accuracy: 0.3594\n",
      "Epoch 254/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1891 - accuracy: 0.4950 - val_loss: 1.1373 - val_accuracy: 0.3594\n",
      "Epoch 255/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1885 - accuracy: 0.4961 - val_loss: 1.1264 - val_accuracy: 0.3594\n",
      "Epoch 256/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1879 - accuracy: 0.4978 - val_loss: 1.1173 - val_accuracy: 0.3672\n",
      "Epoch 257/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1875 - accuracy: 0.4961 - val_loss: 1.1184 - val_accuracy: 0.3594\n",
      "Epoch 258/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1879 - accuracy: 0.4961 - val_loss: 1.1269 - val_accuracy: 0.3672\n",
      "Epoch 259/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1886 - accuracy: 0.4961 - val_loss: 1.1104 - val_accuracy: 0.3672\n",
      "Epoch 260/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1895 - accuracy: 0.4961 - val_loss: 1.1090 - val_accuracy: 0.3672\n",
      "Epoch 261/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1893 - accuracy: 0.4967 - val_loss: 1.1087 - val_accuracy: 0.3672\n",
      "Epoch 262/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1885 - accuracy: 0.4967 - val_loss: 1.1139 - val_accuracy: 0.3672\n",
      "Epoch 263/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1885 - accuracy: 0.4978 - val_loss: 1.1199 - val_accuracy: 0.3672\n",
      "Epoch 264/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1887 - accuracy: 0.4972 - val_loss: 1.1265 - val_accuracy: 0.3672\n",
      "Epoch 265/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1883 - accuracy: 0.4972 - val_loss: 1.1296 - val_accuracy: 0.3594\n",
      "Epoch 266/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1884 - accuracy: 0.4967 - val_loss: 1.1288 - val_accuracy: 0.3594\n",
      "Epoch 267/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1884 - accuracy: 0.4961 - val_loss: 1.1139 - val_accuracy: 0.3672\n",
      "Epoch 268/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1881 - accuracy: 0.4967 - val_loss: 1.1242 - val_accuracy: 0.3672\n",
      "Epoch 269/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1884 - accuracy: 0.4978 - val_loss: 1.1205 - val_accuracy: 0.3750\n",
      "Epoch 270/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1884 - accuracy: 0.4961 - val_loss: 1.1191 - val_accuracy: 0.3672\n",
      "Epoch 271/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1881 - accuracy: 0.4955 - val_loss: 1.1253 - val_accuracy: 0.3672\n",
      "Epoch 272/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1888 - accuracy: 0.4961 - val_loss: 1.1183 - val_accuracy: 0.3594\n",
      "Epoch 273/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1887 - accuracy: 0.4950 - val_loss: 1.1319 - val_accuracy: 0.3672\n",
      "Epoch 274/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1884 - accuracy: 0.4978 - val_loss: 1.1183 - val_accuracy: 0.3672\n",
      "Epoch 275/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1883 - accuracy: 0.4950 - val_loss: 1.1260 - val_accuracy: 0.3672\n",
      "Epoch 276/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1885 - accuracy: 0.4983 - val_loss: 1.1158 - val_accuracy: 0.3672\n",
      "Epoch 277/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1884 - accuracy: 0.4950 - val_loss: 1.1204 - val_accuracy: 0.3672\n",
      "Epoch 278/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1884 - accuracy: 0.4994 - val_loss: 1.1189 - val_accuracy: 0.3672\n",
      "Epoch 279/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1885 - accuracy: 0.4989 - val_loss: 1.1133 - val_accuracy: 0.3750\n",
      "Epoch 280/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1881 - accuracy: 0.4961 - val_loss: 1.1321 - val_accuracy: 0.3672\n",
      "Epoch 281/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1885 - accuracy: 0.4978 - val_loss: 1.1099 - val_accuracy: 0.3672\n",
      "Epoch 282/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1878 - accuracy: 0.4967 - val_loss: 1.1177 - val_accuracy: 0.3750\n",
      "Epoch 283/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1886 - accuracy: 0.4972 - val_loss: 1.1257 - val_accuracy: 0.3594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 284/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1884 - accuracy: 0.4939 - val_loss: 1.1241 - val_accuracy: 0.3672\n",
      "Epoch 285/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1880 - accuracy: 0.4955 - val_loss: 1.1221 - val_accuracy: 0.3828\n",
      "Epoch 286/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1890 - accuracy: 0.4955 - val_loss: 1.1188 - val_accuracy: 0.3672\n",
      "Epoch 287/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1898 - accuracy: 0.4978 - val_loss: 1.1410 - val_accuracy: 0.3750\n",
      "Epoch 288/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1888 - accuracy: 0.4967 - val_loss: 1.1319 - val_accuracy: 0.3672\n",
      "Epoch 289/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1879 - accuracy: 0.4955 - val_loss: 1.1268 - val_accuracy: 0.3672\n",
      "Epoch 290/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1894 - accuracy: 0.4978 - val_loss: 1.1420 - val_accuracy: 0.3672\n",
      "Epoch 291/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1888 - accuracy: 0.4989 - val_loss: 1.1182 - val_accuracy: 0.3750\n",
      "Epoch 292/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1879 - accuracy: 0.4944 - val_loss: 1.1244 - val_accuracy: 0.3828\n",
      "Epoch 293/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1884 - accuracy: 0.4950 - val_loss: 1.1211 - val_accuracy: 0.3672\n",
      "Epoch 294/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1898 - accuracy: 0.4994 - val_loss: 1.1094 - val_accuracy: 0.3672\n",
      "Epoch 295/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1882 - accuracy: 0.4972 - val_loss: 1.1393 - val_accuracy: 0.3594\n",
      "Epoch 296/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1881 - accuracy: 0.4967 - val_loss: 1.1216 - val_accuracy: 0.3672\n",
      "Epoch 297/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1.1882 - accuracy: 0.4972 - val_loss: 1.1054 - val_accuracy: 0.3672\n",
      "Epoch 298/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1884 - accuracy: 0.4961 - val_loss: 1.1322 - val_accuracy: 0.3750\n",
      "Epoch 299/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1883 - accuracy: 0.4978 - val_loss: 1.1212 - val_accuracy: 0.3672\n",
      "Epoch 300/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1884 - accuracy: 0.4978 - val_loss: 1.1190 - val_accuracy: 0.3750\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "56/56 [==============================] - 0s 1ms/step\n",
      "Recall: [0.93 0.   0.58]\n",
      "Precision: [0.41 1.   0.7 ]\n",
      "F1-score: [0.57 0.   0.63]\n",
      "Accuracy: 0.49\n",
      "Brier score:0.1995516660779359\n",
      "Brier climat:0.2121701388888889\n",
      "Brier skill score:0.059473368293175066\n",
      "Recall: [0.93 0.   0.58]\n",
      "Precision: [0.41 1.   0.7 ]\n",
      "F1-score: [0.57 0.   0.63]\n",
      "Accuracy: 0.49\n",
      "Brier score:0.2168896059597786\n",
      "Brier climat:0.22004960317460315\n",
      "Brier skill score:0.014360385882255744\n",
      "Recall: [0.93 0.   0.58]\n",
      "Precision: [0.41 1.   0.7 ]\n",
      "F1-score: [0.57 0.   0.63]\n",
      "Accuracy: 0.49\n",
      "Brier score:0.15587778316760598\n",
      "Brier climat:0.22600446428571427\n",
      "Brier skill score:0.3102889199102471\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "Recall: [1.   0.   0.41]\n",
      "Precision: [0.29 0.   0.85]\n",
      "F1-score: [0.45 0.   0.56]\n",
      "Accuracy: 0.38\n",
      "Brier score:0.24083779308556522\n",
      "Brier climat:0.18300347222222224\n",
      "Brier skill score:-0.3160285439454089\n",
      "Recall: [1.   0.   0.41]\n",
      "Precision: [0.29 0.   0.85]\n",
      "F1-score: [0.45 0.   0.56]\n",
      "Accuracy: 0.38\n",
      "Brier score:0.2851041938343044\n",
      "Brier climat:0.23909722222222224\n",
      "Brier skill score:-0.19241951531048018\n",
      "Recall: [1.   0.   0.41]\n",
      "Precision: [0.29 0.   0.85]\n",
      "F1-score: [0.45 0.   0.56]\n",
      "Accuracy: 0.38\n",
      "Brier score:0.15572387217364844\n",
      "Brier climat:0.22265625\n",
      "Brier skill score:0.30060857409729824\n",
      "******** 8\n",
      "validation years [1989, 1990]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "{0: 2.0, 1: 1.0, 2: 1.0}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 60)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 183       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183\n",
      "Trainable params: 183\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "Epoch 1/300\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 1.7099 - accuracy: 0.3625WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "28/28 [==============================] - 4s 109ms/step - loss: 1.6920 - accuracy: 0.3694 - val_loss: 1.3162 - val_accuracy: 0.4141\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.5285 - accuracy: 0.4738 - val_loss: 1.3123 - val_accuracy: 0.3281\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.4818 - accuracy: 0.4866 - val_loss: 1.2889 - val_accuracy: 0.3906\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4543 - accuracy: 0.5078 - val_loss: 1.2906 - val_accuracy: 0.3672\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4337 - accuracy: 0.5045 - val_loss: 1.2874 - val_accuracy: 0.3672\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.4180 - accuracy: 0.5017 - val_loss: 1.2900 - val_accuracy: 0.3672\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.4049 - accuracy: 0.5000 - val_loss: 1.2958 - val_accuracy: 0.3516\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3923 - accuracy: 0.5006 - val_loss: 1.2892 - val_accuracy: 0.3516\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3815 - accuracy: 0.4961 - val_loss: 1.2919 - val_accuracy: 0.3438\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3707 - accuracy: 0.4955 - val_loss: 1.2891 - val_accuracy: 0.3359\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3619 - accuracy: 0.4972 - val_loss: 1.2905 - val_accuracy: 0.3359\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3529 - accuracy: 0.4961 - val_loss: 1.2922 - val_accuracy: 0.3438\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3440 - accuracy: 0.4983 - val_loss: 1.2948 - val_accuracy: 0.3516\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3353 - accuracy: 0.4955 - val_loss: 1.2872 - val_accuracy: 0.3359\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3291 - accuracy: 0.4967 - val_loss: 1.2884 - val_accuracy: 0.3359\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3205 - accuracy: 0.4983 - val_loss: 1.2836 - val_accuracy: 0.3359\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3143 - accuracy: 0.4983 - val_loss: 1.2874 - val_accuracy: 0.3438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3070 - accuracy: 0.4972 - val_loss: 1.2819 - val_accuracy: 0.3281\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3011 - accuracy: 0.4961 - val_loss: 1.2832 - val_accuracy: 0.3047\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2946 - accuracy: 0.4967 - val_loss: 1.2814 - val_accuracy: 0.3047\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2896 - accuracy: 0.4967 - val_loss: 1.2814 - val_accuracy: 0.2969\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2846 - accuracy: 0.4955 - val_loss: 1.2825 - val_accuracy: 0.2969\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2799 - accuracy: 0.4983 - val_loss: 1.2827 - val_accuracy: 0.2734\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2756 - accuracy: 0.4989 - val_loss: 1.2829 - val_accuracy: 0.2734\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2721 - accuracy: 0.4972 - val_loss: 1.2835 - val_accuracy: 0.2812\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2675 - accuracy: 0.4967 - val_loss: 1.2821 - val_accuracy: 0.2734\n",
      "Epoch 27/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2644 - accuracy: 0.5017 - val_loss: 1.2809 - val_accuracy: 0.2734\n",
      "Epoch 28/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2619 - accuracy: 0.5011 - val_loss: 1.2813 - val_accuracy: 0.2656\n",
      "Epoch 29/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2588 - accuracy: 0.5028 - val_loss: 1.2761 - val_accuracy: 0.2656\n",
      "Epoch 30/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2566 - accuracy: 0.5028 - val_loss: 1.2793 - val_accuracy: 0.2734\n",
      "Epoch 31/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2531 - accuracy: 0.4983 - val_loss: 1.2764 - val_accuracy: 0.2656\n",
      "Epoch 32/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2506 - accuracy: 0.5000 - val_loss: 1.2697 - val_accuracy: 0.2656\n",
      "Epoch 33/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2474 - accuracy: 0.5000 - val_loss: 1.2772 - val_accuracy: 0.2656\n",
      "Epoch 34/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2458 - accuracy: 0.4983 - val_loss: 1.2799 - val_accuracy: 0.2656\n",
      "Epoch 35/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2438 - accuracy: 0.5000 - val_loss: 1.2783 - val_accuracy: 0.2656\n",
      "Epoch 36/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2415 - accuracy: 0.5000 - val_loss: 1.2754 - val_accuracy: 0.2656\n",
      "Epoch 37/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2411 - accuracy: 0.5006 - val_loss: 1.2699 - val_accuracy: 0.2656\n",
      "Epoch 38/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2376 - accuracy: 0.4994 - val_loss: 1.2746 - val_accuracy: 0.2578\n",
      "Epoch 39/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2363 - accuracy: 0.5011 - val_loss: 1.2697 - val_accuracy: 0.2656\n",
      "Epoch 40/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2343 - accuracy: 0.5028 - val_loss: 1.2701 - val_accuracy: 0.2578\n",
      "Epoch 41/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2329 - accuracy: 0.5000 - val_loss: 1.2690 - val_accuracy: 0.2578\n",
      "Epoch 42/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2313 - accuracy: 0.5022 - val_loss: 1.2732 - val_accuracy: 0.2578\n",
      "Epoch 43/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2295 - accuracy: 0.5028 - val_loss: 1.2730 - val_accuracy: 0.2578\n",
      "Epoch 44/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2282 - accuracy: 0.5039 - val_loss: 1.2707 - val_accuracy: 0.2578\n",
      "Epoch 45/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2276 - accuracy: 0.5017 - val_loss: 1.2755 - val_accuracy: 0.2578\n",
      "Epoch 46/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2253 - accuracy: 0.5056 - val_loss: 1.2692 - val_accuracy: 0.2578\n",
      "Epoch 47/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2237 - accuracy: 0.5033 - val_loss: 1.2725 - val_accuracy: 0.2500\n",
      "Epoch 48/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2217 - accuracy: 0.5039 - val_loss: 1.2672 - val_accuracy: 0.2500\n",
      "Epoch 49/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2211 - accuracy: 0.5033 - val_loss: 1.2663 - val_accuracy: 0.2578\n",
      "Epoch 50/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2199 - accuracy: 0.5061 - val_loss: 1.2667 - val_accuracy: 0.2500\n",
      "Epoch 51/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2186 - accuracy: 0.5028 - val_loss: 1.2655 - val_accuracy: 0.2578\n",
      "Epoch 52/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2182 - accuracy: 0.5045 - val_loss: 1.2630 - val_accuracy: 0.2578\n",
      "Epoch 53/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2161 - accuracy: 0.5067 - val_loss: 1.2638 - val_accuracy: 0.2422\n",
      "Epoch 54/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2149 - accuracy: 0.5061 - val_loss: 1.2664 - val_accuracy: 0.2500\n",
      "Epoch 55/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2149 - accuracy: 0.5039 - val_loss: 1.2698 - val_accuracy: 0.2500\n",
      "Epoch 56/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2137 - accuracy: 0.5061 - val_loss: 1.2640 - val_accuracy: 0.2500\n",
      "Epoch 57/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2127 - accuracy: 0.5050 - val_loss: 1.2624 - val_accuracy: 0.2422\n",
      "Epoch 58/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2118 - accuracy: 0.5067 - val_loss: 1.2582 - val_accuracy: 0.2578\n",
      "Epoch 59/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2104 - accuracy: 0.5078 - val_loss: 1.2607 - val_accuracy: 0.2500\n",
      "Epoch 60/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2103 - accuracy: 0.5067 - val_loss: 1.2592 - val_accuracy: 0.2500\n",
      "Epoch 61/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2096 - accuracy: 0.5073 - val_loss: 1.2529 - val_accuracy: 0.2500\n",
      "Epoch 62/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2088 - accuracy: 0.5089 - val_loss: 1.2533 - val_accuracy: 0.2656\n",
      "Epoch 63/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2075 - accuracy: 0.5084 - val_loss: 1.2529 - val_accuracy: 0.2500\n",
      "Epoch 64/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2073 - accuracy: 0.5067 - val_loss: 1.2571 - val_accuracy: 0.2500\n",
      "Epoch 65/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2065 - accuracy: 0.5078 - val_loss: 1.2503 - val_accuracy: 0.2656\n",
      "Epoch 66/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2053 - accuracy: 0.5073 - val_loss: 1.2451 - val_accuracy: 0.2578\n",
      "Epoch 67/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2052 - accuracy: 0.5078 - val_loss: 1.2522 - val_accuracy: 0.2500\n",
      "Epoch 68/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2050 - accuracy: 0.5061 - val_loss: 1.2486 - val_accuracy: 0.2500\n",
      "Epoch 69/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2041 - accuracy: 0.5089 - val_loss: 1.2503 - val_accuracy: 0.2500\n",
      "Epoch 70/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2036 - accuracy: 0.5078 - val_loss: 1.2413 - val_accuracy: 0.2656\n",
      "Epoch 71/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2027 - accuracy: 0.5061 - val_loss: 1.2449 - val_accuracy: 0.2500\n",
      "Epoch 72/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2030 - accuracy: 0.5061 - val_loss: 1.2477 - val_accuracy: 0.2500\n",
      "Epoch 73/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2018 - accuracy: 0.5073 - val_loss: 1.2445 - val_accuracy: 0.2656\n",
      "Epoch 74/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2024 - accuracy: 0.5095 - val_loss: 1.2427 - val_accuracy: 0.2500\n",
      "Epoch 75/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2007 - accuracy: 0.5095 - val_loss: 1.2443 - val_accuracy: 0.2656\n",
      "Epoch 76/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2006 - accuracy: 0.5073 - val_loss: 1.2373 - val_accuracy: 0.2500\n",
      "Epoch 77/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1997 - accuracy: 0.5084 - val_loss: 1.2397 - val_accuracy: 0.2656\n",
      "Epoch 78/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1993 - accuracy: 0.5073 - val_loss: 1.2406 - val_accuracy: 0.2500\n",
      "Epoch 79/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1985 - accuracy: 0.5078 - val_loss: 1.2426 - val_accuracy: 0.2656\n",
      "Epoch 80/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1989 - accuracy: 0.5067 - val_loss: 1.2380 - val_accuracy: 0.2578\n",
      "Epoch 81/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1983 - accuracy: 0.5073 - val_loss: 1.2415 - val_accuracy: 0.2500\n",
      "Epoch 82/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1977 - accuracy: 0.5084 - val_loss: 1.2439 - val_accuracy: 0.2422\n",
      "Epoch 83/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1968 - accuracy: 0.5078 - val_loss: 1.2423 - val_accuracy: 0.2500\n",
      "Epoch 84/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1964 - accuracy: 0.5078 - val_loss: 1.2400 - val_accuracy: 0.2500\n",
      "Epoch 85/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1970 - accuracy: 0.5061 - val_loss: 1.2373 - val_accuracy: 0.2422\n",
      "Epoch 86/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1974 - accuracy: 0.5067 - val_loss: 1.2394 - val_accuracy: 0.2500\n",
      "Epoch 87/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1956 - accuracy: 0.5078 - val_loss: 1.2349 - val_accuracy: 0.2500\n",
      "Epoch 88/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1961 - accuracy: 0.5073 - val_loss: 1.2410 - val_accuracy: 0.2422\n",
      "Epoch 89/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1966 - accuracy: 0.5028 - val_loss: 1.2302 - val_accuracy: 0.2656\n",
      "Epoch 90/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1960 - accuracy: 0.5061 - val_loss: 1.2380 - val_accuracy: 0.2500\n",
      "Epoch 91/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1952 - accuracy: 0.5045 - val_loss: 1.2335 - val_accuracy: 0.2578\n",
      "Epoch 92/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1951 - accuracy: 0.5073 - val_loss: 1.2371 - val_accuracy: 0.2500\n",
      "Epoch 93/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1951 - accuracy: 0.5033 - val_loss: 1.2374 - val_accuracy: 0.2500\n",
      "Epoch 94/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1949 - accuracy: 0.5067 - val_loss: 1.2399 - val_accuracy: 0.2500\n",
      "Epoch 95/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1949 - accuracy: 0.5067 - val_loss: 1.2352 - val_accuracy: 0.2500\n",
      "Epoch 96/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1945 - accuracy: 0.5061 - val_loss: 1.2401 - val_accuracy: 0.2422\n",
      "Epoch 97/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1944 - accuracy: 0.5050 - val_loss: 1.2358 - val_accuracy: 0.2500\n",
      "Epoch 98/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1938 - accuracy: 0.5050 - val_loss: 1.2332 - val_accuracy: 0.2578\n",
      "Epoch 99/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1954 - accuracy: 0.4994 - val_loss: 1.2397 - val_accuracy: 0.2422\n",
      "Epoch 100/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1941 - accuracy: 0.5061 - val_loss: 1.2367 - val_accuracy: 0.2500\n",
      "Epoch 101/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1938 - accuracy: 0.5067 - val_loss: 1.2318 - val_accuracy: 0.2500\n",
      "Epoch 102/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1938 - accuracy: 0.5067 - val_loss: 1.2396 - val_accuracy: 0.2500\n",
      "Epoch 103/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1930 - accuracy: 0.5056 - val_loss: 1.2322 - val_accuracy: 0.2578\n",
      "Epoch 104/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1929 - accuracy: 0.5033 - val_loss: 1.2381 - val_accuracy: 0.2422\n",
      "Epoch 105/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1932 - accuracy: 0.5061 - val_loss: 1.2370 - val_accuracy: 0.2500\n",
      "Epoch 106/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1932 - accuracy: 0.5039 - val_loss: 1.2382 - val_accuracy: 0.2422\n",
      "Epoch 107/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1930 - accuracy: 0.5061 - val_loss: 1.2374 - val_accuracy: 0.2422\n",
      "Epoch 108/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1924 - accuracy: 0.5061 - val_loss: 1.2363 - val_accuracy: 0.2422\n",
      "Epoch 109/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1923 - accuracy: 0.5050 - val_loss: 1.2357 - val_accuracy: 0.2500\n",
      "Epoch 110/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1920 - accuracy: 0.5050 - val_loss: 1.2355 - val_accuracy: 0.2500\n",
      "Epoch 111/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1918 - accuracy: 0.5061 - val_loss: 1.2363 - val_accuracy: 0.2500\n",
      "Epoch 112/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1921 - accuracy: 0.5067 - val_loss: 1.2365 - val_accuracy: 0.2500\n",
      "Epoch 113/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1916 - accuracy: 0.5061 - val_loss: 1.2359 - val_accuracy: 0.2500\n",
      "Epoch 114/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1929 - accuracy: 0.5089 - val_loss: 1.2325 - val_accuracy: 0.2500\n",
      "Epoch 115/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1919 - accuracy: 0.5050 - val_loss: 1.2332 - val_accuracy: 0.2500\n",
      "Epoch 116/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1915 - accuracy: 0.5033 - val_loss: 1.2324 - val_accuracy: 0.2422\n",
      "Epoch 117/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1910 - accuracy: 0.5050 - val_loss: 1.2378 - val_accuracy: 0.2500\n",
      "Epoch 118/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1909 - accuracy: 0.5056 - val_loss: 1.2370 - val_accuracy: 0.2422\n",
      "Epoch 119/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1910 - accuracy: 0.5045 - val_loss: 1.2385 - val_accuracy: 0.2500\n",
      "Epoch 120/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1923 - accuracy: 0.5045 - val_loss: 1.2376 - val_accuracy: 0.2500\n",
      "Epoch 121/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1910 - accuracy: 0.5050 - val_loss: 1.2335 - val_accuracy: 0.2500\n",
      "Epoch 122/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1913 - accuracy: 0.5045 - val_loss: 1.2376 - val_accuracy: 0.2500\n",
      "Epoch 123/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1927 - accuracy: 0.5056 - val_loss: 1.2431 - val_accuracy: 0.2422\n",
      "Epoch 124/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1919 - accuracy: 0.5056 - val_loss: 1.2359 - val_accuracy: 0.2500\n",
      "Epoch 125/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1917 - accuracy: 0.5028 - val_loss: 1.2355 - val_accuracy: 0.2422\n",
      "Epoch 126/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1902 - accuracy: 0.5056 - val_loss: 1.2322 - val_accuracy: 0.2422\n",
      "Epoch 127/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1905 - accuracy: 0.5028 - val_loss: 1.2366 - val_accuracy: 0.2500\n",
      "Epoch 128/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1910 - accuracy: 0.5061 - val_loss: 1.2395 - val_accuracy: 0.2422\n",
      "Epoch 129/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1910 - accuracy: 0.5056 - val_loss: 1.2324 - val_accuracy: 0.2500\n",
      "Epoch 130/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1905 - accuracy: 0.5056 - val_loss: 1.2327 - val_accuracy: 0.2422\n",
      "Epoch 131/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1902 - accuracy: 0.5045 - val_loss: 1.2376 - val_accuracy: 0.2422\n",
      "Epoch 132/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1911 - accuracy: 0.5022 - val_loss: 1.2348 - val_accuracy: 0.2500\n",
      "Epoch 133/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1908 - accuracy: 0.5050 - val_loss: 1.2391 - val_accuracy: 0.2500\n",
      "Epoch 134/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1908 - accuracy: 0.5056 - val_loss: 1.2407 - val_accuracy: 0.2422\n",
      "Epoch 135/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1903 - accuracy: 0.5028 - val_loss: 1.2334 - val_accuracy: 0.2500\n",
      "Epoch 136/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1901 - accuracy: 0.5039 - val_loss: 1.2351 - val_accuracy: 0.2500\n",
      "Epoch 137/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1900 - accuracy: 0.5022 - val_loss: 1.2389 - val_accuracy: 0.2422\n",
      "Epoch 138/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1897 - accuracy: 0.5056 - val_loss: 1.2332 - val_accuracy: 0.2500\n",
      "Epoch 139/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1895 - accuracy: 0.5067 - val_loss: 1.2380 - val_accuracy: 0.2500\n",
      "Epoch 140/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1897 - accuracy: 0.5061 - val_loss: 1.2399 - val_accuracy: 0.2422\n",
      "Epoch 141/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1910 - accuracy: 0.5073 - val_loss: 1.2351 - val_accuracy: 0.2422\n",
      "Epoch 142/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1899 - accuracy: 0.5050 - val_loss: 1.2381 - val_accuracy: 0.2500\n",
      "Epoch 143/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1900 - accuracy: 0.5056 - val_loss: 1.2396 - val_accuracy: 0.2422\n",
      "Epoch 144/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1890 - accuracy: 0.5050 - val_loss: 1.2341 - val_accuracy: 0.2578\n",
      "Epoch 145/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1896 - accuracy: 0.5045 - val_loss: 1.2365 - val_accuracy: 0.2422\n",
      "Epoch 146/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1893 - accuracy: 0.5028 - val_loss: 1.2350 - val_accuracy: 0.2422\n",
      "Epoch 147/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1893 - accuracy: 0.5056 - val_loss: 1.2378 - val_accuracy: 0.2422\n",
      "Epoch 148/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1888 - accuracy: 0.5061 - val_loss: 1.2384 - val_accuracy: 0.2422\n",
      "Epoch 149/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1890 - accuracy: 0.5039 - val_loss: 1.2390 - val_accuracy: 0.2422\n",
      "Epoch 150/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1898 - accuracy: 0.5050 - val_loss: 1.2334 - val_accuracy: 0.2500\n",
      "Epoch 151/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1890 - accuracy: 0.5056 - val_loss: 1.2379 - val_accuracy: 0.2422\n",
      "Epoch 152/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1885 - accuracy: 0.5045 - val_loss: 1.2378 - val_accuracy: 0.2422\n",
      "Epoch 153/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1882 - accuracy: 0.5061 - val_loss: 1.2342 - val_accuracy: 0.2500\n",
      "Epoch 154/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1900 - accuracy: 0.5061 - val_loss: 1.2333 - val_accuracy: 0.2500\n",
      "Epoch 155/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1892 - accuracy: 0.5033 - val_loss: 1.2429 - val_accuracy: 0.2422\n",
      "Epoch 156/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1879 - accuracy: 0.5050 - val_loss: 1.2343 - val_accuracy: 0.2422\n",
      "Epoch 157/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1884 - accuracy: 0.5039 - val_loss: 1.2360 - val_accuracy: 0.2500\n",
      "Epoch 158/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1890 - accuracy: 0.5028 - val_loss: 1.2402 - val_accuracy: 0.2422\n",
      "Epoch 159/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1884 - accuracy: 0.5039 - val_loss: 1.2376 - val_accuracy: 0.2422\n",
      "Epoch 160/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1885 - accuracy: 0.5050 - val_loss: 1.2349 - val_accuracy: 0.2422\n",
      "Epoch 161/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1894 - accuracy: 0.5073 - val_loss: 1.2392 - val_accuracy: 0.2500\n",
      "Epoch 162/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1882 - accuracy: 0.5045 - val_loss: 1.2385 - val_accuracy: 0.2422\n",
      "Epoch 163/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1885 - accuracy: 0.5045 - val_loss: 1.2348 - val_accuracy: 0.2422\n",
      "Epoch 164/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1877 - accuracy: 0.5050 - val_loss: 1.2347 - val_accuracy: 0.2500\n",
      "Epoch 165/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1887 - accuracy: 0.5056 - val_loss: 1.2380 - val_accuracy: 0.2500\n",
      "Epoch 166/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1877 - accuracy: 0.5017 - val_loss: 1.2392 - val_accuracy: 0.2422\n",
      "Epoch 167/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1876 - accuracy: 0.5067 - val_loss: 1.2369 - val_accuracy: 0.2500\n",
      "Epoch 168/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1878 - accuracy: 0.5045 - val_loss: 1.2349 - val_accuracy: 0.2422\n",
      "Epoch 169/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1873 - accuracy: 0.5039 - val_loss: 1.2320 - val_accuracy: 0.2500\n",
      "Epoch 170/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1887 - accuracy: 0.5045 - val_loss: 1.2385 - val_accuracy: 0.2422\n",
      "Epoch 171/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1875 - accuracy: 0.5033 - val_loss: 1.2378 - val_accuracy: 0.2344\n",
      "Epoch 172/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1877 - accuracy: 0.5033 - val_loss: 1.2370 - val_accuracy: 0.2500\n",
      "Epoch 173/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1877 - accuracy: 0.5033 - val_loss: 1.2428 - val_accuracy: 0.2344\n",
      "Epoch 174/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1878 - accuracy: 0.5039 - val_loss: 1.2349 - val_accuracy: 0.2500\n",
      "Epoch 175/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1873 - accuracy: 0.5056 - val_loss: 1.2377 - val_accuracy: 0.2422\n",
      "Epoch 176/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1876 - accuracy: 0.5033 - val_loss: 1.2360 - val_accuracy: 0.2422\n",
      "Epoch 177/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1876 - accuracy: 0.5067 - val_loss: 1.2411 - val_accuracy: 0.2422\n",
      "Epoch 178/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1866 - accuracy: 0.5045 - val_loss: 1.2356 - val_accuracy: 0.2422\n",
      "Epoch 179/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1872 - accuracy: 0.5056 - val_loss: 1.2345 - val_accuracy: 0.2422\n",
      "Epoch 180/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1873 - accuracy: 0.5039 - val_loss: 1.2391 - val_accuracy: 0.2266\n",
      "Epoch 181/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1871 - accuracy: 0.5039 - val_loss: 1.2357 - val_accuracy: 0.2500\n",
      "Epoch 182/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1873 - accuracy: 0.5056 - val_loss: 1.2370 - val_accuracy: 0.2422\n",
      "Epoch 183/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1863 - accuracy: 0.5067 - val_loss: 1.2394 - val_accuracy: 0.2422\n",
      "Epoch 184/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1869 - accuracy: 0.5045 - val_loss: 1.2357 - val_accuracy: 0.2422\n",
      "Epoch 185/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1867 - accuracy: 0.5056 - val_loss: 1.2381 - val_accuracy: 0.2422\n",
      "Epoch 186/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1868 - accuracy: 0.5039 - val_loss: 1.2385 - val_accuracy: 0.2500\n",
      "Epoch 187/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1872 - accuracy: 0.5067 - val_loss: 1.2359 - val_accuracy: 0.2344\n",
      "Epoch 188/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1870 - accuracy: 0.5033 - val_loss: 1.2410 - val_accuracy: 0.2422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 189/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1870 - accuracy: 0.5045 - val_loss: 1.2375 - val_accuracy: 0.2422\n",
      "Epoch 190/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1867 - accuracy: 0.5039 - val_loss: 1.2386 - val_accuracy: 0.2500\n",
      "Epoch 191/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1869 - accuracy: 0.5045 - val_loss: 1.2345 - val_accuracy: 0.2500\n",
      "Epoch 192/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1866 - accuracy: 0.5039 - val_loss: 1.2379 - val_accuracy: 0.2500\n",
      "Epoch 193/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1871 - accuracy: 0.5056 - val_loss: 1.2397 - val_accuracy: 0.2500\n",
      "Epoch 194/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1869 - accuracy: 0.5045 - val_loss: 1.2359 - val_accuracy: 0.2500\n",
      "Epoch 195/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1873 - accuracy: 0.5028 - val_loss: 1.2405 - val_accuracy: 0.2500\n",
      "Epoch 196/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1873 - accuracy: 0.5045 - val_loss: 1.2381 - val_accuracy: 0.2344\n",
      "Epoch 197/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1868 - accuracy: 0.5033 - val_loss: 1.2318 - val_accuracy: 0.2422\n",
      "Epoch 198/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1864 - accuracy: 0.5028 - val_loss: 1.2421 - val_accuracy: 0.2422\n",
      "Epoch 199/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1863 - accuracy: 0.5056 - val_loss: 1.2375 - val_accuracy: 0.2500\n",
      "Epoch 200/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1871 - accuracy: 0.5017 - val_loss: 1.2393 - val_accuracy: 0.2500\n",
      "Epoch 201/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1873 - accuracy: 0.5061 - val_loss: 1.2390 - val_accuracy: 0.2500\n",
      "Epoch 202/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1877 - accuracy: 0.5033 - val_loss: 1.2369 - val_accuracy: 0.2422\n",
      "Epoch 203/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1869 - accuracy: 0.5028 - val_loss: 1.2391 - val_accuracy: 0.2500\n",
      "Epoch 204/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1857 - accuracy: 0.5045 - val_loss: 1.2397 - val_accuracy: 0.2422\n",
      "Epoch 205/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1856 - accuracy: 0.5045 - val_loss: 1.2398 - val_accuracy: 0.2344\n",
      "Epoch 206/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1878 - accuracy: 0.5022 - val_loss: 1.2373 - val_accuracy: 0.2500\n",
      "Epoch 207/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1861 - accuracy: 0.5045 - val_loss: 1.2371 - val_accuracy: 0.2422\n",
      "Epoch 208/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1870 - accuracy: 0.5011 - val_loss: 1.2403 - val_accuracy: 0.2500\n",
      "Epoch 209/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1866 - accuracy: 0.5061 - val_loss: 1.2393 - val_accuracy: 0.2500\n",
      "Epoch 210/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1874 - accuracy: 0.5056 - val_loss: 1.2426 - val_accuracy: 0.2266\n",
      "Epoch 211/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1866 - accuracy: 0.5006 - val_loss: 1.2370 - val_accuracy: 0.2344\n",
      "Epoch 212/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1872 - accuracy: 0.5061 - val_loss: 1.2386 - val_accuracy: 0.2344\n",
      "Epoch 213/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1862 - accuracy: 0.5033 - val_loss: 1.2349 - val_accuracy: 0.2500\n",
      "Epoch 214/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1859 - accuracy: 0.5039 - val_loss: 1.2394 - val_accuracy: 0.2500\n",
      "Epoch 215/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1856 - accuracy: 0.5039 - val_loss: 1.2421 - val_accuracy: 0.2500\n",
      "Epoch 216/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1860 - accuracy: 0.5039 - val_loss: 1.2398 - val_accuracy: 0.2422\n",
      "Epoch 217/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1857 - accuracy: 0.5033 - val_loss: 1.2352 - val_accuracy: 0.2500\n",
      "Epoch 218/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1864 - accuracy: 0.5039 - val_loss: 1.2326 - val_accuracy: 0.2500\n",
      "Epoch 219/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1864 - accuracy: 0.5033 - val_loss: 1.2393 - val_accuracy: 0.2344\n",
      "Epoch 220/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1858 - accuracy: 0.5022 - val_loss: 1.2361 - val_accuracy: 0.2500\n",
      "Epoch 221/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1858 - accuracy: 0.5050 - val_loss: 1.2404 - val_accuracy: 0.2266\n",
      "Epoch 222/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1859 - accuracy: 0.5045 - val_loss: 1.2379 - val_accuracy: 0.2344\n",
      "Epoch 223/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1861 - accuracy: 0.5067 - val_loss: 1.2395 - val_accuracy: 0.2344\n",
      "Epoch 224/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1861 - accuracy: 0.5050 - val_loss: 1.2377 - val_accuracy: 0.2344\n",
      "Epoch 225/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1854 - accuracy: 0.5022 - val_loss: 1.2402 - val_accuracy: 0.2500\n",
      "Epoch 226/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1857 - accuracy: 0.5067 - val_loss: 1.2370 - val_accuracy: 0.2422\n",
      "Epoch 227/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1857 - accuracy: 0.5028 - val_loss: 1.2374 - val_accuracy: 0.2500\n",
      "Epoch 228/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1867 - accuracy: 0.5033 - val_loss: 1.2336 - val_accuracy: 0.2500\n",
      "Epoch 229/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1857 - accuracy: 0.5056 - val_loss: 1.2429 - val_accuracy: 0.2344\n",
      "Epoch 230/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1854 - accuracy: 0.5017 - val_loss: 1.2356 - val_accuracy: 0.2500\n",
      "Epoch 231/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1857 - accuracy: 0.5033 - val_loss: 1.2385 - val_accuracy: 0.2500\n",
      "Epoch 232/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1855 - accuracy: 0.5050 - val_loss: 1.2361 - val_accuracy: 0.2422\n",
      "Epoch 233/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1870 - accuracy: 0.5022 - val_loss: 1.2369 - val_accuracy: 0.2500\n",
      "Epoch 234/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1865 - accuracy: 0.5050 - val_loss: 1.2365 - val_accuracy: 0.2344\n",
      "Epoch 235/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1854 - accuracy: 0.5039 - val_loss: 1.2414 - val_accuracy: 0.2500\n",
      "Epoch 236/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1855 - accuracy: 0.5028 - val_loss: 1.2343 - val_accuracy: 0.2500\n",
      "Epoch 237/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1861 - accuracy: 0.5039 - val_loss: 1.2369 - val_accuracy: 0.2422\n",
      "Epoch 238/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1854 - accuracy: 0.5033 - val_loss: 1.2395 - val_accuracy: 0.2344\n",
      "Epoch 239/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1856 - accuracy: 0.5045 - val_loss: 1.2365 - val_accuracy: 0.2422\n",
      "Epoch 240/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1858 - accuracy: 0.5028 - val_loss: 1.2422 - val_accuracy: 0.2266\n",
      "Epoch 241/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1855 - accuracy: 0.5022 - val_loss: 1.2365 - val_accuracy: 0.2500\n",
      "Epoch 242/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1864 - accuracy: 0.5061 - val_loss: 1.2356 - val_accuracy: 0.2266\n",
      "Epoch 243/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1858 - accuracy: 0.5033 - val_loss: 1.2383 - val_accuracy: 0.2422\n",
      "Epoch 244/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1853 - accuracy: 0.5045 - val_loss: 1.2407 - val_accuracy: 0.2500\n",
      "Epoch 245/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1851 - accuracy: 0.5050 - val_loss: 1.2409 - val_accuracy: 0.2422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 246/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1855 - accuracy: 0.5056 - val_loss: 1.2392 - val_accuracy: 0.2422\n",
      "Epoch 247/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1860 - accuracy: 0.5033 - val_loss: 1.2377 - val_accuracy: 0.2422\n",
      "Epoch 248/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1853 - accuracy: 0.4989 - val_loss: 1.2375 - val_accuracy: 0.2422\n",
      "Epoch 249/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1850 - accuracy: 0.5045 - val_loss: 1.2381 - val_accuracy: 0.2500\n",
      "Epoch 250/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1847 - accuracy: 0.5033 - val_loss: 1.2418 - val_accuracy: 0.2344\n",
      "Epoch 251/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1851 - accuracy: 0.5022 - val_loss: 1.2396 - val_accuracy: 0.2422\n",
      "Epoch 252/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1846 - accuracy: 0.5045 - val_loss: 1.2385 - val_accuracy: 0.2422\n",
      "Epoch 253/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1853 - accuracy: 0.5033 - val_loss: 1.2369 - val_accuracy: 0.2422\n",
      "Epoch 254/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1854 - accuracy: 0.5039 - val_loss: 1.2440 - val_accuracy: 0.2422\n",
      "Epoch 255/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1862 - accuracy: 0.5061 - val_loss: 1.2415 - val_accuracy: 0.2422\n",
      "Epoch 256/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1864 - accuracy: 0.5017 - val_loss: 1.2340 - val_accuracy: 0.2344\n",
      "Epoch 257/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1847 - accuracy: 0.5028 - val_loss: 1.2378 - val_accuracy: 0.2266\n",
      "Epoch 258/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1850 - accuracy: 0.5028 - val_loss: 1.2395 - val_accuracy: 0.2422\n",
      "Epoch 259/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1848 - accuracy: 0.5061 - val_loss: 1.2421 - val_accuracy: 0.2422\n",
      "Epoch 260/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1851 - accuracy: 0.5050 - val_loss: 1.2411 - val_accuracy: 0.2422\n",
      "Epoch 261/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1849 - accuracy: 0.5022 - val_loss: 1.2353 - val_accuracy: 0.2422\n",
      "Epoch 262/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1850 - accuracy: 0.5028 - val_loss: 1.2381 - val_accuracy: 0.2422\n",
      "Epoch 263/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1850 - accuracy: 0.5022 - val_loss: 1.2395 - val_accuracy: 0.2422\n",
      "Epoch 264/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1852 - accuracy: 0.5056 - val_loss: 1.2406 - val_accuracy: 0.2266\n",
      "Epoch 265/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1847 - accuracy: 0.5033 - val_loss: 1.2403 - val_accuracy: 0.2500\n",
      "Epoch 266/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1853 - accuracy: 0.5045 - val_loss: 1.2452 - val_accuracy: 0.2344\n",
      "Epoch 267/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1863 - accuracy: 0.5045 - val_loss: 1.2342 - val_accuracy: 0.2500\n",
      "Epoch 268/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1854 - accuracy: 0.5028 - val_loss: 1.2378 - val_accuracy: 0.2422\n",
      "Epoch 269/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1845 - accuracy: 0.5022 - val_loss: 1.2386 - val_accuracy: 0.2422\n",
      "Epoch 270/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1848 - accuracy: 0.5022 - val_loss: 1.2372 - val_accuracy: 0.2422\n",
      "Epoch 271/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1855 - accuracy: 0.5039 - val_loss: 1.2423 - val_accuracy: 0.2422\n",
      "Epoch 272/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1848 - accuracy: 0.5033 - val_loss: 1.2407 - val_accuracy: 0.2422\n",
      "Epoch 273/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1851 - accuracy: 0.5022 - val_loss: 1.2397 - val_accuracy: 0.2344\n",
      "Epoch 274/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1847 - accuracy: 0.5045 - val_loss: 1.2406 - val_accuracy: 0.2422\n",
      "Epoch 275/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1848 - accuracy: 0.5011 - val_loss: 1.2390 - val_accuracy: 0.2422\n",
      "Epoch 276/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1855 - accuracy: 0.5056 - val_loss: 1.2387 - val_accuracy: 0.2344\n",
      "Epoch 277/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1846 - accuracy: 0.5045 - val_loss: 1.2349 - val_accuracy: 0.2500\n",
      "Epoch 278/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1859 - accuracy: 0.5033 - val_loss: 1.2436 - val_accuracy: 0.2422\n",
      "Epoch 279/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1852 - accuracy: 0.5028 - val_loss: 1.2341 - val_accuracy: 0.2500\n",
      "Epoch 280/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1859 - accuracy: 0.5039 - val_loss: 1.2391 - val_accuracy: 0.2422\n",
      "Epoch 281/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1848 - accuracy: 0.5022 - val_loss: 1.2403 - val_accuracy: 0.2422\n",
      "Epoch 282/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1855 - accuracy: 0.5028 - val_loss: 1.2408 - val_accuracy: 0.2422\n",
      "Epoch 283/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1843 - accuracy: 0.5033 - val_loss: 1.2406 - val_accuracy: 0.2344\n",
      "Epoch 284/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1850 - accuracy: 0.5039 - val_loss: 1.2419 - val_accuracy: 0.2266\n",
      "Epoch 285/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1842 - accuracy: 0.5028 - val_loss: 1.2362 - val_accuracy: 0.2422\n",
      "Epoch 286/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1846 - accuracy: 0.5039 - val_loss: 1.2375 - val_accuracy: 0.2422\n",
      "Epoch 287/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1844 - accuracy: 0.5056 - val_loss: 1.2423 - val_accuracy: 0.2422\n",
      "Epoch 288/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1845 - accuracy: 0.5022 - val_loss: 1.2389 - val_accuracy: 0.2422\n",
      "Epoch 289/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1846 - accuracy: 0.5039 - val_loss: 1.2398 - val_accuracy: 0.2422\n",
      "Epoch 290/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1855 - accuracy: 0.5067 - val_loss: 1.2402 - val_accuracy: 0.2500\n",
      "Epoch 291/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1860 - accuracy: 0.5011 - val_loss: 1.2393 - val_accuracy: 0.2422\n",
      "Epoch 292/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1847 - accuracy: 0.5033 - val_loss: 1.2379 - val_accuracy: 0.2422\n",
      "Epoch 293/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1852 - accuracy: 0.5011 - val_loss: 1.2396 - val_accuracy: 0.2422\n",
      "Epoch 294/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1849 - accuracy: 0.5033 - val_loss: 1.2356 - val_accuracy: 0.2500\n",
      "Epoch 295/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1851 - accuracy: 0.5045 - val_loss: 1.2417 - val_accuracy: 0.2422\n",
      "Epoch 296/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1850 - accuracy: 0.5056 - val_loss: 1.2380 - val_accuracy: 0.2500\n",
      "Epoch 297/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1.1847 - accuracy: 0.5050 - val_loss: 1.2386 - val_accuracy: 0.2422\n",
      "Epoch 298/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1851 - accuracy: 0.5000 - val_loss: 1.2396 - val_accuracy: 0.2422\n",
      "Epoch 299/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1845 - accuracy: 0.5056 - val_loss: 1.2378 - val_accuracy: 0.2422\n",
      "Epoch 300/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1848 - accuracy: 0.5050 - val_loss: 1.2392 - val_accuracy: 0.2266\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "56/56 [==============================] - 0s 1ms/step\n",
      "Recall: [0.94 0.   0.58]\n",
      "Precision: [0.42 0.   0.73]\n",
      "F1-score: [0.58 0.   0.65]\n",
      "Accuracy: 0.5\n",
      "Brier score:0.20531166643297058\n",
      "Brier climat:0.21555555555555556\n",
      "Brier skill score:0.04752319696044571\n",
      "Recall: [0.94 0.   0.58]\n",
      "Precision: [0.42 0.   0.73]\n",
      "F1-score: [0.58 0.   0.65]\n",
      "Accuracy: 0.5\n",
      "Brier score:0.21751451232659796\n",
      "Brier climat:0.21893353174603172\n",
      "Brier skill score:0.006481507917571294\n",
      "Recall: [0.94 0.   0.58]\n",
      "Precision: [0.42 0.   0.73]\n",
      "F1-score: [0.58 0.   0.65]\n",
      "Accuracy: 0.5\n",
      "Brier score:0.15051079667578343\n",
      "Brier climat:0.22418154761904763\n",
      "Brier skill score:0.3286209401518323\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "Recall: [0.86 0.   0.31]\n",
      "Precision: [0.12 0.   0.59]\n",
      "F1-score: [0.21 0.   0.4 ]\n",
      "Accuracy: 0.23\n",
      "Brier score:0.21830190183910114\n",
      "Brier climat:0.1356076388888889\n",
      "Brier skill score:-0.6098053445054696\n",
      "Recall: [0.86 0.   0.31]\n",
      "Precision: [0.12 0.   0.59]\n",
      "F1-score: [0.21 0.   0.4 ]\n",
      "Accuracy: 0.23\n",
      "Brier score:0.2983348210952194\n",
      "Brier climat:0.2547222222222222\n",
      "Brier skill score:-0.17121630964317336\n",
      "Recall: [0.86 0.   0.31]\n",
      "Precision: [0.12 0.   0.59]\n",
      "F1-score: [0.21 0.   0.4 ]\n",
      "Accuracy: 0.23\n",
      "Brier score:0.2349070295102828\n",
      "Brier climat:0.24817708333333333\n",
      "Brier skill score:0.053470101448598095\n",
      "******** 9\n",
      "validation years [1990, 1991]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "{0: 2.0, 1: 1.0, 2: 1.0}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 60)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 183       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183\n",
      "Trainable params: 183\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.7039 - accuracy: 0.4487WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "28/28 [==============================] - 4s 110ms/step - loss: 1.7039 - accuracy: 0.4487 - val_loss: 1.6554 - val_accuracy: 0.3281\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.5710 - accuracy: 0.4676 - val_loss: 1.6524 - val_accuracy: 0.3516\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4976 - accuracy: 0.4821 - val_loss: 1.6597 - val_accuracy: 0.2734\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4587 - accuracy: 0.5033 - val_loss: 1.6867 - val_accuracy: 0.2188\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4336 - accuracy: 0.5128 - val_loss: 1.6910 - val_accuracy: 0.1875\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4160 - accuracy: 0.5128 - val_loss: 1.6895 - val_accuracy: 0.1562\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4025 - accuracy: 0.5112 - val_loss: 1.6850 - val_accuracy: 0.1484\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3898 - accuracy: 0.5112 - val_loss: 1.6794 - val_accuracy: 0.1250\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3788 - accuracy: 0.5095 - val_loss: 1.6670 - val_accuracy: 0.1250\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.3677 - accuracy: 0.5089 - val_loss: 1.6509 - val_accuracy: 0.1250\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3574 - accuracy: 0.5089 - val_loss: 1.6448 - val_accuracy: 0.1328\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3483 - accuracy: 0.5084 - val_loss: 1.6274 - val_accuracy: 0.1250\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3396 - accuracy: 0.5112 - val_loss: 1.6191 - val_accuracy: 0.1172\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3314 - accuracy: 0.5089 - val_loss: 1.5967 - val_accuracy: 0.1250\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3234 - accuracy: 0.5128 - val_loss: 1.5977 - val_accuracy: 0.1172\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3161 - accuracy: 0.5128 - val_loss: 1.5726 - val_accuracy: 0.1250\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3088 - accuracy: 0.5151 - val_loss: 1.5687 - val_accuracy: 0.1250\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3027 - accuracy: 0.5162 - val_loss: 1.5606 - val_accuracy: 0.1250\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2956 - accuracy: 0.5134 - val_loss: 1.5480 - val_accuracy: 0.1172\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2898 - accuracy: 0.5145 - val_loss: 1.5502 - val_accuracy: 0.1484\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2845 - accuracy: 0.5123 - val_loss: 1.5424 - val_accuracy: 0.1484\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2804 - accuracy: 0.5112 - val_loss: 1.5354 - val_accuracy: 0.1484\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2746 - accuracy: 0.5140 - val_loss: 1.5338 - val_accuracy: 0.1562\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2703 - accuracy: 0.5151 - val_loss: 1.5237 - val_accuracy: 0.1562\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2662 - accuracy: 0.5112 - val_loss: 1.5150 - val_accuracy: 0.1641\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2624 - accuracy: 0.5167 - val_loss: 1.5141 - val_accuracy: 0.1562\n",
      "Epoch 27/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2582 - accuracy: 0.5128 - val_loss: 1.5007 - val_accuracy: 0.1484\n",
      "Epoch 28/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2547 - accuracy: 0.5134 - val_loss: 1.4984 - val_accuracy: 0.1562\n",
      "Epoch 29/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2513 - accuracy: 0.5128 - val_loss: 1.4979 - val_accuracy: 0.1562\n",
      "Epoch 30/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2479 - accuracy: 0.5128 - val_loss: 1.4876 - val_accuracy: 0.1562\n",
      "Epoch 31/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2446 - accuracy: 0.5128 - val_loss: 1.4841 - val_accuracy: 0.1562\n",
      "Epoch 32/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2411 - accuracy: 0.5134 - val_loss: 1.4817 - val_accuracy: 0.1562\n",
      "Epoch 33/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2384 - accuracy: 0.5134 - val_loss: 1.4754 - val_accuracy: 0.1641\n",
      "Epoch 34/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2356 - accuracy: 0.5128 - val_loss: 1.4732 - val_accuracy: 0.1641\n",
      "Epoch 35/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2331 - accuracy: 0.5123 - val_loss: 1.4712 - val_accuracy: 0.1641\n",
      "Epoch 36/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2309 - accuracy: 0.5134 - val_loss: 1.4689 - val_accuracy: 0.1719\n",
      "Epoch 37/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2287 - accuracy: 0.5167 - val_loss: 1.4624 - val_accuracy: 0.1719\n",
      "Epoch 38/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2268 - accuracy: 0.5117 - val_loss: 1.4597 - val_accuracy: 0.1719\n",
      "Epoch 39/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2242 - accuracy: 0.5140 - val_loss: 1.4599 - val_accuracy: 0.1719\n",
      "Epoch 40/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2233 - accuracy: 0.5084 - val_loss: 1.4579 - val_accuracy: 0.1719\n",
      "Epoch 41/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2207 - accuracy: 0.5134 - val_loss: 1.4483 - val_accuracy: 0.1719\n",
      "Epoch 42/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2190 - accuracy: 0.5140 - val_loss: 1.4488 - val_accuracy: 0.1719\n",
      "Epoch 43/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2171 - accuracy: 0.5117 - val_loss: 1.4416 - val_accuracy: 0.1719\n",
      "Epoch 44/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2156 - accuracy: 0.5140 - val_loss: 1.4408 - val_accuracy: 0.1719\n",
      "Epoch 45/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2143 - accuracy: 0.5117 - val_loss: 1.4398 - val_accuracy: 0.1719\n",
      "Epoch 46/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2127 - accuracy: 0.5151 - val_loss: 1.4328 - val_accuracy: 0.1719\n",
      "Epoch 47/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2116 - accuracy: 0.5123 - val_loss: 1.4386 - val_accuracy: 0.1719\n",
      "Epoch 48/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2094 - accuracy: 0.5134 - val_loss: 1.4247 - val_accuracy: 0.1797\n",
      "Epoch 49/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2079 - accuracy: 0.5162 - val_loss: 1.4222 - val_accuracy: 0.1719\n",
      "Epoch 50/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2077 - accuracy: 0.5123 - val_loss: 1.4317 - val_accuracy: 0.1719\n",
      "Epoch 51/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2054 - accuracy: 0.5151 - val_loss: 1.4234 - val_accuracy: 0.1719\n",
      "Epoch 52/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2043 - accuracy: 0.5145 - val_loss: 1.4254 - val_accuracy: 0.1719\n",
      "Epoch 53/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2026 - accuracy: 0.5162 - val_loss: 1.4201 - val_accuracy: 0.1719\n",
      "Epoch 54/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2012 - accuracy: 0.5173 - val_loss: 1.4191 - val_accuracy: 0.1719\n",
      "Epoch 55/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2003 - accuracy: 0.5140 - val_loss: 1.4190 - val_accuracy: 0.1719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1991 - accuracy: 0.5156 - val_loss: 1.4187 - val_accuracy: 0.1719\n",
      "Epoch 57/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1982 - accuracy: 0.5162 - val_loss: 1.4226 - val_accuracy: 0.1719\n",
      "Epoch 58/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1967 - accuracy: 0.5145 - val_loss: 1.4142 - val_accuracy: 0.1719\n",
      "Epoch 59/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1954 - accuracy: 0.5112 - val_loss: 1.4178 - val_accuracy: 0.1719\n",
      "Epoch 60/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1937 - accuracy: 0.5134 - val_loss: 1.4147 - val_accuracy: 0.1797\n",
      "Epoch 61/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1928 - accuracy: 0.5179 - val_loss: 1.4135 - val_accuracy: 0.1797\n",
      "Epoch 62/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1924 - accuracy: 0.5167 - val_loss: 1.4015 - val_accuracy: 0.1875\n",
      "Epoch 63/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1911 - accuracy: 0.5140 - val_loss: 1.4017 - val_accuracy: 0.1875\n",
      "Epoch 64/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1910 - accuracy: 0.5128 - val_loss: 1.4031 - val_accuracy: 0.1719\n",
      "Epoch 65/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1894 - accuracy: 0.5123 - val_loss: 1.4048 - val_accuracy: 0.1797\n",
      "Epoch 66/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1880 - accuracy: 0.5128 - val_loss: 1.4036 - val_accuracy: 0.1875\n",
      "Epoch 67/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1880 - accuracy: 0.5117 - val_loss: 1.3996 - val_accuracy: 0.1797\n",
      "Epoch 68/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1870 - accuracy: 0.5145 - val_loss: 1.4040 - val_accuracy: 0.1797\n",
      "Epoch 69/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1869 - accuracy: 0.5128 - val_loss: 1.4036 - val_accuracy: 0.1875\n",
      "Epoch 70/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1863 - accuracy: 0.5128 - val_loss: 1.3958 - val_accuracy: 0.1875\n",
      "Epoch 71/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1844 - accuracy: 0.5134 - val_loss: 1.4004 - val_accuracy: 0.1875\n",
      "Epoch 72/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1832 - accuracy: 0.5140 - val_loss: 1.3950 - val_accuracy: 0.1875\n",
      "Epoch 73/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1836 - accuracy: 0.5128 - val_loss: 1.3971 - val_accuracy: 0.1875\n",
      "Epoch 74/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1837 - accuracy: 0.5140 - val_loss: 1.3952 - val_accuracy: 0.1875\n",
      "Epoch 75/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1828 - accuracy: 0.5128 - val_loss: 1.3926 - val_accuracy: 0.1875\n",
      "Epoch 76/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1807 - accuracy: 0.5112 - val_loss: 1.3917 - val_accuracy: 0.1875\n",
      "Epoch 77/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1812 - accuracy: 0.5140 - val_loss: 1.3943 - val_accuracy: 0.1875\n",
      "Epoch 78/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1814 - accuracy: 0.5134 - val_loss: 1.3765 - val_accuracy: 0.1875\n",
      "Epoch 79/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1791 - accuracy: 0.5128 - val_loss: 1.3958 - val_accuracy: 0.1875\n",
      "Epoch 80/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1786 - accuracy: 0.5112 - val_loss: 1.3892 - val_accuracy: 0.1875\n",
      "Epoch 81/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1782 - accuracy: 0.5140 - val_loss: 1.3906 - val_accuracy: 0.1875\n",
      "Epoch 82/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1775 - accuracy: 0.5134 - val_loss: 1.3848 - val_accuracy: 0.1875\n",
      "Epoch 83/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1788 - accuracy: 0.5089 - val_loss: 1.3913 - val_accuracy: 0.1875\n",
      "Epoch 84/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1768 - accuracy: 0.5123 - val_loss: 1.3827 - val_accuracy: 0.1875\n",
      "Epoch 85/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1761 - accuracy: 0.5128 - val_loss: 1.3810 - val_accuracy: 0.1875\n",
      "Epoch 86/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1756 - accuracy: 0.5117 - val_loss: 1.3790 - val_accuracy: 0.1875\n",
      "Epoch 87/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1755 - accuracy: 0.5128 - val_loss: 1.3853 - val_accuracy: 0.1875\n",
      "Epoch 88/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1755 - accuracy: 0.5140 - val_loss: 1.3719 - val_accuracy: 0.1875\n",
      "Epoch 89/300\n",
      "28/28 [==============================] - 2s 63ms/step - loss: 1.1762 - accuracy: 0.5134 - val_loss: 1.3787 - val_accuracy: 0.1875\n",
      "Epoch 90/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1745 - accuracy: 0.5123 - val_loss: 1.3800 - val_accuracy: 0.1875\n",
      "Epoch 91/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1741 - accuracy: 0.5140 - val_loss: 1.3756 - val_accuracy: 0.1875\n",
      "Epoch 92/300\n",
      "28/28 [==============================] - 1s 29ms/step - loss: 1.1758 - accuracy: 0.5123 - val_loss: 1.3756 - val_accuracy: 0.1875\n",
      "Epoch 93/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1753 - accuracy: 0.5128 - val_loss: 1.3726 - val_accuracy: 0.1875\n",
      "Epoch 94/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1744 - accuracy: 0.5134 - val_loss: 1.3709 - val_accuracy: 0.1875\n",
      "Epoch 95/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1746 - accuracy: 0.5100 - val_loss: 1.3725 - val_accuracy: 0.1875\n",
      "Epoch 96/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1737 - accuracy: 0.5112 - val_loss: 1.3731 - val_accuracy: 0.1875\n",
      "Epoch 97/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1731 - accuracy: 0.5145 - val_loss: 1.3674 - val_accuracy: 0.1875\n",
      "Epoch 98/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1733 - accuracy: 0.5140 - val_loss: 1.3687 - val_accuracy: 0.1875\n",
      "Epoch 99/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1739 - accuracy: 0.5128 - val_loss: 1.3758 - val_accuracy: 0.1875\n",
      "Epoch 100/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1731 - accuracy: 0.5123 - val_loss: 1.3702 - val_accuracy: 0.1875\n",
      "Epoch 101/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1735 - accuracy: 0.5134 - val_loss: 1.3652 - val_accuracy: 0.1875\n",
      "Epoch 102/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1733 - accuracy: 0.5140 - val_loss: 1.3772 - val_accuracy: 0.1875\n",
      "Epoch 103/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1735 - accuracy: 0.5112 - val_loss: 1.3698 - val_accuracy: 0.1875\n",
      "Epoch 104/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1729 - accuracy: 0.5112 - val_loss: 1.3705 - val_accuracy: 0.1875\n",
      "Epoch 105/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1727 - accuracy: 0.5128 - val_loss: 1.3640 - val_accuracy: 0.1875\n",
      "Epoch 106/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1727 - accuracy: 0.5128 - val_loss: 1.3646 - val_accuracy: 0.1875\n",
      "Epoch 107/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1727 - accuracy: 0.5128 - val_loss: 1.3697 - val_accuracy: 0.1875\n",
      "Epoch 108/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1726 - accuracy: 0.5123 - val_loss: 1.3636 - val_accuracy: 0.1875\n",
      "Epoch 109/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1720 - accuracy: 0.5112 - val_loss: 1.3671 - val_accuracy: 0.1875\n",
      "Epoch 110/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1727 - accuracy: 0.5134 - val_loss: 1.3711 - val_accuracy: 0.1875\n",
      "Epoch 111/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1721 - accuracy: 0.5128 - val_loss: 1.3688 - val_accuracy: 0.1875\n",
      "Epoch 112/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1716 - accuracy: 0.5145 - val_loss: 1.3623 - val_accuracy: 0.1875\n",
      "Epoch 113/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1721 - accuracy: 0.5140 - val_loss: 1.3710 - val_accuracy: 0.1875\n",
      "Epoch 114/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1720 - accuracy: 0.5134 - val_loss: 1.3709 - val_accuracy: 0.1875\n",
      "Epoch 115/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1716 - accuracy: 0.5123 - val_loss: 1.3642 - val_accuracy: 0.1875\n",
      "Epoch 116/300\n",
      "28/28 [==============================] - 1s 29ms/step - loss: 1.1719 - accuracy: 0.5162 - val_loss: 1.3645 - val_accuracy: 0.1875\n",
      "Epoch 117/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1731 - accuracy: 0.5151 - val_loss: 1.3698 - val_accuracy: 0.1875\n",
      "Epoch 118/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1714 - accuracy: 0.5134 - val_loss: 1.3652 - val_accuracy: 0.1875\n",
      "Epoch 119/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1722 - accuracy: 0.5128 - val_loss: 1.3692 - val_accuracy: 0.1875\n",
      "Epoch 120/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1717 - accuracy: 0.5128 - val_loss: 1.3576 - val_accuracy: 0.1875\n",
      "Epoch 121/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1719 - accuracy: 0.5106 - val_loss: 1.3653 - val_accuracy: 0.1875\n",
      "Epoch 122/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1710 - accuracy: 0.5145 - val_loss: 1.3650 - val_accuracy: 0.1875\n",
      "Epoch 123/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1707 - accuracy: 0.5128 - val_loss: 1.3652 - val_accuracy: 0.1875\n",
      "Epoch 124/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1711 - accuracy: 0.5123 - val_loss: 1.3616 - val_accuracy: 0.1875\n",
      "Epoch 125/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1704 - accuracy: 0.5117 - val_loss: 1.3631 - val_accuracy: 0.1875\n",
      "Epoch 126/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1713 - accuracy: 0.5134 - val_loss: 1.3584 - val_accuracy: 0.1875\n",
      "Epoch 127/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1707 - accuracy: 0.5128 - val_loss: 1.3702 - val_accuracy: 0.1875\n",
      "Epoch 128/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1702 - accuracy: 0.5128 - val_loss: 1.3631 - val_accuracy: 0.1875\n",
      "Epoch 129/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1707 - accuracy: 0.5117 - val_loss: 1.3624 - val_accuracy: 0.1875\n",
      "Epoch 130/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1712 - accuracy: 0.5134 - val_loss: 1.3737 - val_accuracy: 0.1875\n",
      "Epoch 131/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1704 - accuracy: 0.5140 - val_loss: 1.3647 - val_accuracy: 0.1875\n",
      "Epoch 132/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1703 - accuracy: 0.5112 - val_loss: 1.3652 - val_accuracy: 0.1875\n",
      "Epoch 133/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1703 - accuracy: 0.5140 - val_loss: 1.3613 - val_accuracy: 0.1875\n",
      "Epoch 134/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1701 - accuracy: 0.5128 - val_loss: 1.3670 - val_accuracy: 0.1875\n",
      "Epoch 135/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1704 - accuracy: 0.5156 - val_loss: 1.3673 - val_accuracy: 0.1875\n",
      "Epoch 136/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1704 - accuracy: 0.5128 - val_loss: 1.3631 - val_accuracy: 0.1875\n",
      "Epoch 137/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1710 - accuracy: 0.5123 - val_loss: 1.3690 - val_accuracy: 0.1875\n",
      "Epoch 138/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1699 - accuracy: 0.5134 - val_loss: 1.3643 - val_accuracy: 0.1875\n",
      "Epoch 139/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1692 - accuracy: 0.5117 - val_loss: 1.3655 - val_accuracy: 0.1875\n",
      "Epoch 140/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1696 - accuracy: 0.5140 - val_loss: 1.3661 - val_accuracy: 0.1875\n",
      "Epoch 141/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1700 - accuracy: 0.5117 - val_loss: 1.3643 - val_accuracy: 0.1875\n",
      "Epoch 142/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1691 - accuracy: 0.5140 - val_loss: 1.3690 - val_accuracy: 0.1875\n",
      "Epoch 143/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1698 - accuracy: 0.5128 - val_loss: 1.3641 - val_accuracy: 0.1875\n",
      "Epoch 144/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1716 - accuracy: 0.5128 - val_loss: 1.3705 - val_accuracy: 0.1875\n",
      "Epoch 145/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1697 - accuracy: 0.5128 - val_loss: 1.3625 - val_accuracy: 0.1875\n",
      "Epoch 146/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1698 - accuracy: 0.5112 - val_loss: 1.3668 - val_accuracy: 0.1875\n",
      "Epoch 147/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1697 - accuracy: 0.5123 - val_loss: 1.3653 - val_accuracy: 0.1875\n",
      "Epoch 148/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1693 - accuracy: 0.5123 - val_loss: 1.3716 - val_accuracy: 0.1875\n",
      "Epoch 149/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1702 - accuracy: 0.5151 - val_loss: 1.3716 - val_accuracy: 0.1875\n",
      "Epoch 150/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1696 - accuracy: 0.5123 - val_loss: 1.3669 - val_accuracy: 0.1875\n",
      "Epoch 151/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1688 - accuracy: 0.5128 - val_loss: 1.3657 - val_accuracy: 0.1875\n",
      "Epoch 152/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1694 - accuracy: 0.5112 - val_loss: 1.3635 - val_accuracy: 0.1875\n",
      "Epoch 153/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1695 - accuracy: 0.5123 - val_loss: 1.3728 - val_accuracy: 0.1875\n",
      "Epoch 154/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1682 - accuracy: 0.5117 - val_loss: 1.3613 - val_accuracy: 0.1875\n",
      "Epoch 155/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1688 - accuracy: 0.5134 - val_loss: 1.3733 - val_accuracy: 0.1875\n",
      "Epoch 156/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1685 - accuracy: 0.5145 - val_loss: 1.3649 - val_accuracy: 0.1875\n",
      "Epoch 157/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1685 - accuracy: 0.5117 - val_loss: 1.3690 - val_accuracy: 0.1875\n",
      "Epoch 158/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1683 - accuracy: 0.5117 - val_loss: 1.3696 - val_accuracy: 0.1875\n",
      "Epoch 159/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1688 - accuracy: 0.5128 - val_loss: 1.3666 - val_accuracy: 0.1875\n",
      "Epoch 160/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1689 - accuracy: 0.5123 - val_loss: 1.3695 - val_accuracy: 0.1875\n",
      "Epoch 161/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1693 - accuracy: 0.5128 - val_loss: 1.3653 - val_accuracy: 0.1875\n",
      "Epoch 162/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1698 - accuracy: 0.5128 - val_loss: 1.3708 - val_accuracy: 0.1875\n",
      "Epoch 163/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1704 - accuracy: 0.5117 - val_loss: 1.3661 - val_accuracy: 0.1875\n",
      "Epoch 164/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1695 - accuracy: 0.5106 - val_loss: 1.3681 - val_accuracy: 0.1875\n",
      "Epoch 165/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1684 - accuracy: 0.5134 - val_loss: 1.3740 - val_accuracy: 0.1875\n",
      "Epoch 166/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1700 - accuracy: 0.5151 - val_loss: 1.3726 - val_accuracy: 0.1875\n",
      "Epoch 167/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1691 - accuracy: 0.5123 - val_loss: 1.3706 - val_accuracy: 0.1875\n",
      "Epoch 168/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1682 - accuracy: 0.5117 - val_loss: 1.3642 - val_accuracy: 0.1875\n",
      "Epoch 169/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1686 - accuracy: 0.5123 - val_loss: 1.3681 - val_accuracy: 0.1875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1684 - accuracy: 0.5134 - val_loss: 1.3745 - val_accuracy: 0.1875\n",
      "Epoch 171/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1684 - accuracy: 0.5128 - val_loss: 1.3727 - val_accuracy: 0.1875\n",
      "Epoch 172/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1689 - accuracy: 0.5100 - val_loss: 1.3675 - val_accuracy: 0.1875\n",
      "Epoch 173/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1678 - accuracy: 0.5134 - val_loss: 1.3684 - val_accuracy: 0.1875\n",
      "Epoch 174/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1679 - accuracy: 0.5117 - val_loss: 1.3724 - val_accuracy: 0.1875\n",
      "Epoch 175/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1679 - accuracy: 0.5123 - val_loss: 1.3687 - val_accuracy: 0.1875\n",
      "Epoch 176/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1681 - accuracy: 0.5123 - val_loss: 1.3688 - val_accuracy: 0.1875\n",
      "Epoch 177/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1683 - accuracy: 0.5117 - val_loss: 1.3662 - val_accuracy: 0.1875\n",
      "Epoch 178/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1682 - accuracy: 0.5117 - val_loss: 1.3762 - val_accuracy: 0.1875\n",
      "Epoch 179/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1685 - accuracy: 0.5117 - val_loss: 1.3727 - val_accuracy: 0.1875\n",
      "Epoch 180/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1675 - accuracy: 0.5128 - val_loss: 1.3760 - val_accuracy: 0.1875\n",
      "Epoch 181/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1684 - accuracy: 0.5117 - val_loss: 1.3633 - val_accuracy: 0.1875\n",
      "Epoch 182/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1689 - accuracy: 0.5106 - val_loss: 1.3647 - val_accuracy: 0.1875\n",
      "Epoch 183/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1679 - accuracy: 0.5140 - val_loss: 1.3654 - val_accuracy: 0.1875\n",
      "Epoch 184/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1685 - accuracy: 0.5117 - val_loss: 1.3625 - val_accuracy: 0.1875\n",
      "Epoch 185/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1673 - accuracy: 0.5123 - val_loss: 1.3671 - val_accuracy: 0.1875\n",
      "Epoch 186/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1682 - accuracy: 0.5100 - val_loss: 1.3627 - val_accuracy: 0.1875\n",
      "Epoch 187/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1679 - accuracy: 0.5140 - val_loss: 1.3728 - val_accuracy: 0.1875\n",
      "Epoch 188/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1683 - accuracy: 0.5112 - val_loss: 1.3792 - val_accuracy: 0.1875\n",
      "Epoch 189/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1685 - accuracy: 0.5095 - val_loss: 1.3692 - val_accuracy: 0.1875\n",
      "Epoch 190/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1683 - accuracy: 0.5123 - val_loss: 1.3692 - val_accuracy: 0.1875\n",
      "Epoch 191/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1682 - accuracy: 0.5128 - val_loss: 1.3662 - val_accuracy: 0.1875\n",
      "Epoch 192/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1677 - accuracy: 0.5117 - val_loss: 1.3687 - val_accuracy: 0.1875\n",
      "Epoch 193/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1677 - accuracy: 0.5123 - val_loss: 1.3673 - val_accuracy: 0.1875\n",
      "Epoch 194/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1681 - accuracy: 0.5123 - val_loss: 1.3650 - val_accuracy: 0.1875\n",
      "Epoch 195/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1676 - accuracy: 0.5117 - val_loss: 1.3660 - val_accuracy: 0.1875\n",
      "Epoch 196/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1680 - accuracy: 0.5156 - val_loss: 1.3654 - val_accuracy: 0.1875\n",
      "Epoch 197/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1672 - accuracy: 0.5134 - val_loss: 1.3728 - val_accuracy: 0.1875\n",
      "Epoch 198/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1674 - accuracy: 0.5123 - val_loss: 1.3627 - val_accuracy: 0.1875\n",
      "Epoch 199/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1674 - accuracy: 0.5145 - val_loss: 1.3710 - val_accuracy: 0.1875\n",
      "Epoch 200/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1674 - accuracy: 0.5117 - val_loss: 1.3676 - val_accuracy: 0.1875\n",
      "Epoch 201/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1672 - accuracy: 0.5112 - val_loss: 1.3656 - val_accuracy: 0.1875\n",
      "Epoch 202/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1682 - accuracy: 0.5140 - val_loss: 1.3682 - val_accuracy: 0.1875\n",
      "Epoch 203/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1682 - accuracy: 0.5128 - val_loss: 1.3629 - val_accuracy: 0.1875\n",
      "Epoch 204/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1679 - accuracy: 0.5140 - val_loss: 1.3632 - val_accuracy: 0.1875\n",
      "Epoch 205/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1679 - accuracy: 0.5095 - val_loss: 1.3602 - val_accuracy: 0.1875\n",
      "Epoch 206/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1678 - accuracy: 0.5134 - val_loss: 1.3657 - val_accuracy: 0.1875\n",
      "Epoch 207/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1672 - accuracy: 0.5117 - val_loss: 1.3641 - val_accuracy: 0.1875\n",
      "Epoch 208/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1675 - accuracy: 0.5123 - val_loss: 1.3610 - val_accuracy: 0.1875\n",
      "Epoch 209/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1671 - accuracy: 0.5112 - val_loss: 1.3650 - val_accuracy: 0.1875\n",
      "Epoch 210/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1677 - accuracy: 0.5128 - val_loss: 1.3651 - val_accuracy: 0.1875\n",
      "Epoch 211/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1677 - accuracy: 0.5123 - val_loss: 1.3707 - val_accuracy: 0.1875\n",
      "Epoch 212/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1673 - accuracy: 0.5117 - val_loss: 1.3609 - val_accuracy: 0.1875\n",
      "Epoch 213/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1683 - accuracy: 0.5112 - val_loss: 1.3569 - val_accuracy: 0.1875\n",
      "Epoch 214/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1671 - accuracy: 0.5112 - val_loss: 1.3661 - val_accuracy: 0.1875\n",
      "Epoch 215/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1673 - accuracy: 0.5106 - val_loss: 1.3641 - val_accuracy: 0.1875\n",
      "Epoch 216/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1680 - accuracy: 0.5095 - val_loss: 1.3584 - val_accuracy: 0.1875\n",
      "Epoch 217/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1678 - accuracy: 0.5123 - val_loss: 1.3625 - val_accuracy: 0.1875\n",
      "Epoch 218/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1676 - accuracy: 0.5106 - val_loss: 1.3571 - val_accuracy: 0.1875\n",
      "Epoch 219/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1669 - accuracy: 0.5128 - val_loss: 1.3653 - val_accuracy: 0.1875\n",
      "Epoch 220/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1674 - accuracy: 0.5128 - val_loss: 1.3660 - val_accuracy: 0.1875\n",
      "Epoch 221/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1672 - accuracy: 0.5112 - val_loss: 1.3578 - val_accuracy: 0.1875\n",
      "Epoch 222/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1668 - accuracy: 0.5145 - val_loss: 1.3575 - val_accuracy: 0.1875\n",
      "Epoch 223/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1671 - accuracy: 0.5095 - val_loss: 1.3608 - val_accuracy: 0.1875\n",
      "Epoch 224/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1674 - accuracy: 0.5128 - val_loss: 1.3639 - val_accuracy: 0.1875\n",
      "Epoch 225/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1674 - accuracy: 0.5117 - val_loss: 1.3620 - val_accuracy: 0.1875\n",
      "Epoch 226/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1680 - accuracy: 0.5123 - val_loss: 1.3680 - val_accuracy: 0.1875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1674 - accuracy: 0.5117 - val_loss: 1.3606 - val_accuracy: 0.1875\n",
      "Epoch 228/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1666 - accuracy: 0.5134 - val_loss: 1.3622 - val_accuracy: 0.1875\n",
      "Epoch 229/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1671 - accuracy: 0.5123 - val_loss: 1.3632 - val_accuracy: 0.1953\n",
      "Epoch 230/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1667 - accuracy: 0.5123 - val_loss: 1.3642 - val_accuracy: 0.1875\n",
      "Epoch 231/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1674 - accuracy: 0.5117 - val_loss: 1.3644 - val_accuracy: 0.1875\n",
      "Epoch 232/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1669 - accuracy: 0.5106 - val_loss: 1.3611 - val_accuracy: 0.1875\n",
      "Epoch 233/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1665 - accuracy: 0.5100 - val_loss: 1.3584 - val_accuracy: 0.1953\n",
      "Epoch 234/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1668 - accuracy: 0.5100 - val_loss: 1.3617 - val_accuracy: 0.1875\n",
      "Epoch 235/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1673 - accuracy: 0.5117 - val_loss: 1.3591 - val_accuracy: 0.1875\n",
      "Epoch 236/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1670 - accuracy: 0.5117 - val_loss: 1.3662 - val_accuracy: 0.1875\n",
      "Epoch 237/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1667 - accuracy: 0.5117 - val_loss: 1.3583 - val_accuracy: 0.1875\n",
      "Epoch 238/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1690 - accuracy: 0.5123 - val_loss: 1.3556 - val_accuracy: 0.1875\n",
      "Epoch 239/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1680 - accuracy: 0.5128 - val_loss: 1.3568 - val_accuracy: 0.1875\n",
      "Epoch 240/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1679 - accuracy: 0.5123 - val_loss: 1.3698 - val_accuracy: 0.1953\n",
      "Epoch 241/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1666 - accuracy: 0.5095 - val_loss: 1.3565 - val_accuracy: 0.1875\n",
      "Epoch 242/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1672 - accuracy: 0.5117 - val_loss: 1.3595 - val_accuracy: 0.1875\n",
      "Epoch 243/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1664 - accuracy: 0.5106 - val_loss: 1.3620 - val_accuracy: 0.1875\n",
      "Epoch 244/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1665 - accuracy: 0.5128 - val_loss: 1.3614 - val_accuracy: 0.1875\n",
      "Epoch 245/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1665 - accuracy: 0.5095 - val_loss: 1.3609 - val_accuracy: 0.1875\n",
      "Epoch 246/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1672 - accuracy: 0.5134 - val_loss: 1.3588 - val_accuracy: 0.1875\n",
      "Epoch 247/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1667 - accuracy: 0.5134 - val_loss: 1.3631 - val_accuracy: 0.1953\n",
      "Epoch 248/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1663 - accuracy: 0.5106 - val_loss: 1.3601 - val_accuracy: 0.1875\n",
      "Epoch 249/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1665 - accuracy: 0.5134 - val_loss: 1.3571 - val_accuracy: 0.1953\n",
      "Epoch 250/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1660 - accuracy: 0.5117 - val_loss: 1.3632 - val_accuracy: 0.1875\n",
      "Epoch 251/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1662 - accuracy: 0.5128 - val_loss: 1.3603 - val_accuracy: 0.1953\n",
      "Epoch 252/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1674 - accuracy: 0.5123 - val_loss: 1.3616 - val_accuracy: 0.1953\n",
      "Epoch 253/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1670 - accuracy: 0.5123 - val_loss: 1.3580 - val_accuracy: 0.1875\n",
      "Epoch 254/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1665 - accuracy: 0.5123 - val_loss: 1.3574 - val_accuracy: 0.1875\n",
      "Epoch 255/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1672 - accuracy: 0.5123 - val_loss: 1.3592 - val_accuracy: 0.1953\n",
      "Epoch 256/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1664 - accuracy: 0.5117 - val_loss: 1.3575 - val_accuracy: 0.1953\n",
      "Epoch 257/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1660 - accuracy: 0.5112 - val_loss: 1.3542 - val_accuracy: 0.1875\n",
      "Epoch 258/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1669 - accuracy: 0.5112 - val_loss: 1.3668 - val_accuracy: 0.1875\n",
      "Epoch 259/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1678 - accuracy: 0.5112 - val_loss: 1.3566 - val_accuracy: 0.1875\n",
      "Epoch 260/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1670 - accuracy: 0.5140 - val_loss: 1.3584 - val_accuracy: 0.1875\n",
      "Epoch 261/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1668 - accuracy: 0.5100 - val_loss: 1.3637 - val_accuracy: 0.1875\n",
      "Epoch 262/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1666 - accuracy: 0.5106 - val_loss: 1.3648 - val_accuracy: 0.1875\n",
      "Epoch 263/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1666 - accuracy: 0.5128 - val_loss: 1.3650 - val_accuracy: 0.1875\n",
      "Epoch 264/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1676 - accuracy: 0.5112 - val_loss: 1.3510 - val_accuracy: 0.1875\n",
      "Epoch 265/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1657 - accuracy: 0.5128 - val_loss: 1.3667 - val_accuracy: 0.1875\n",
      "Epoch 266/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1660 - accuracy: 0.5117 - val_loss: 1.3532 - val_accuracy: 0.1953\n",
      "Epoch 267/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1663 - accuracy: 0.5112 - val_loss: 1.3584 - val_accuracy: 0.1875\n",
      "Epoch 268/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1660 - accuracy: 0.5140 - val_loss: 1.3559 - val_accuracy: 0.1953\n",
      "Epoch 269/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1663 - accuracy: 0.5123 - val_loss: 1.3624 - val_accuracy: 0.1953\n",
      "Epoch 270/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1667 - accuracy: 0.5128 - val_loss: 1.3591 - val_accuracy: 0.1875\n",
      "Epoch 271/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1667 - accuracy: 0.5134 - val_loss: 1.3567 - val_accuracy: 0.1875\n",
      "Epoch 272/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1671 - accuracy: 0.5134 - val_loss: 1.3681 - val_accuracy: 0.1875\n",
      "Epoch 273/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1663 - accuracy: 0.5128 - val_loss: 1.3576 - val_accuracy: 0.1953\n",
      "Epoch 274/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1667 - accuracy: 0.5100 - val_loss: 1.3519 - val_accuracy: 0.1953\n",
      "Epoch 275/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1668 - accuracy: 0.5123 - val_loss: 1.3553 - val_accuracy: 0.1875\n",
      "Epoch 276/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1664 - accuracy: 0.5123 - val_loss: 1.3560 - val_accuracy: 0.1875\n",
      "Epoch 277/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1663 - accuracy: 0.5123 - val_loss: 1.3594 - val_accuracy: 0.1875\n",
      "Epoch 278/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1664 - accuracy: 0.5140 - val_loss: 1.3597 - val_accuracy: 0.1953\n",
      "Epoch 279/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1670 - accuracy: 0.5095 - val_loss: 1.3650 - val_accuracy: 0.1875\n",
      "Epoch 280/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1663 - accuracy: 0.5128 - val_loss: 1.3578 - val_accuracy: 0.1953\n",
      "Epoch 281/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1659 - accuracy: 0.5112 - val_loss: 1.3598 - val_accuracy: 0.1875\n",
      "Epoch 282/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1658 - accuracy: 0.5128 - val_loss: 1.3550 - val_accuracy: 0.1875\n",
      "Epoch 283/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1665 - accuracy: 0.5123 - val_loss: 1.3582 - val_accuracy: 0.1875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 284/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1663 - accuracy: 0.5145 - val_loss: 1.3565 - val_accuracy: 0.1953\n",
      "Epoch 285/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1664 - accuracy: 0.5123 - val_loss: 1.3601 - val_accuracy: 0.1875\n",
      "Epoch 286/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1656 - accuracy: 0.5117 - val_loss: 1.3583 - val_accuracy: 0.1953\n",
      "Epoch 287/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1659 - accuracy: 0.5117 - val_loss: 1.3566 - val_accuracy: 0.1875\n",
      "Epoch 288/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1673 - accuracy: 0.5145 - val_loss: 1.3542 - val_accuracy: 0.1875\n",
      "Epoch 289/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1671 - accuracy: 0.5128 - val_loss: 1.3565 - val_accuracy: 0.1875\n",
      "Epoch 290/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1663 - accuracy: 0.5095 - val_loss: 1.3609 - val_accuracy: 0.1953\n",
      "Epoch 291/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1655 - accuracy: 0.5123 - val_loss: 1.3474 - val_accuracy: 0.1875\n",
      "Epoch 292/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1665 - accuracy: 0.5106 - val_loss: 1.3572 - val_accuracy: 0.1875\n",
      "Epoch 293/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1659 - accuracy: 0.5123 - val_loss: 1.3579 - val_accuracy: 0.1875\n",
      "Epoch 294/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1657 - accuracy: 0.5128 - val_loss: 1.3517 - val_accuracy: 0.1953\n",
      "Epoch 295/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1657 - accuracy: 0.5117 - val_loss: 1.3497 - val_accuracy: 0.1953\n",
      "Epoch 296/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1661 - accuracy: 0.5128 - val_loss: 1.3625 - val_accuracy: 0.1875\n",
      "Epoch 297/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1663 - accuracy: 0.5140 - val_loss: 1.3554 - val_accuracy: 0.1953\n",
      "Epoch 298/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1658 - accuracy: 0.5123 - val_loss: 1.3519 - val_accuracy: 0.1953\n",
      "Epoch 299/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1665 - accuracy: 0.5106 - val_loss: 1.3452 - val_accuracy: 0.1953\n",
      "Epoch 300/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1664 - accuracy: 0.5173 - val_loss: 1.3524 - val_accuracy: 0.1875\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "56/56 [==============================] - 0s 1ms/step\n",
      "Recall: [0.94 0.   0.61]\n",
      "Precision: [0.42 0.   0.72]\n",
      "F1-score: [0.58 0.   0.66]\n",
      "Accuracy: 0.51\n",
      "Brier score:0.1978864719557754\n",
      "Brier climat:0.21031001984126987\n",
      "Brier skill score:0.059072543927631505\n",
      "Recall: [0.94 0.   0.61]\n",
      "Precision: [0.42 0.   0.72]\n",
      "F1-score: [0.58 0.   0.66]\n",
      "Accuracy: 0.51\n",
      "Brier score:0.21394958564664762\n",
      "Brier climat:0.21510168650793648\n",
      "Brier skill score:0.005356075445026143\n",
      "Recall: [0.94 0.   0.61]\n",
      "Precision: [0.42 0.   0.72]\n",
      "F1-score: [0.58 0.   0.66]\n",
      "Accuracy: 0.51\n",
      "Brier score:0.15446502506337598\n",
      "Brier climat:0.23117559523809525\n",
      "Brier skill score:0.3318281503534686\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "Recall: [0.42 0.   0.57]\n",
      "Precision: [0.24 0.   0.13]\n",
      "F1-score: [0.31 0.   0.21]\n",
      "Accuracy: 0.19\n",
      "Brier score:0.2678352851973663\n",
      "Brier climat:0.2090451388888889\n",
      "Brier skill score:-0.2812318268722116\n",
      "Recall: [0.42 0.   0.57]\n",
      "Precision: [0.24 0.   0.13]\n",
      "F1-score: [0.31 0.   0.21]\n",
      "Accuracy: 0.19\n",
      "Brier score:0.37550377257127343\n",
      "Brier climat:0.3083680555555556\n",
      "Brier skill score:-0.21771294336816505\n",
      "Recall: [0.42 0.   0.57]\n",
      "Precision: [0.24 0.   0.13]\n",
      "F1-score: [0.31 0.   0.21]\n",
      "Accuracy: 0.19\n",
      "Brier score:0.18382674551251668\n",
      "Brier climat:0.15026041666666667\n",
      "Brier skill score:-0.22338769977134154\n",
      "******** 10\n",
      "validation years [1991, 1992]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "{0: 2.0, 1: 1.0, 2: 1.0}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 60)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 183       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183\n",
      "Trainable params: 183\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.6428 - accuracy: 0.4487WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "28/28 [==============================] - 4s 107ms/step - loss: 1.6428 - accuracy: 0.4487 - val_loss: 1.5297 - val_accuracy: 0.2656\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.5203 - accuracy: 0.4710 - val_loss: 1.5564 - val_accuracy: 0.1484\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4754 - accuracy: 0.4738 - val_loss: 1.5602 - val_accuracy: 0.1094\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.4462 - accuracy: 0.4721 - val_loss: 1.5524 - val_accuracy: 0.1094\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4241 - accuracy: 0.4738 - val_loss: 1.5487 - val_accuracy: 0.0938\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.4044 - accuracy: 0.4794 - val_loss: 1.5359 - val_accuracy: 0.0859\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3856 - accuracy: 0.4821 - val_loss: 1.5299 - val_accuracy: 0.0469\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3691 - accuracy: 0.4911 - val_loss: 1.5232 - val_accuracy: 0.0469\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3538 - accuracy: 0.4983 - val_loss: 1.5104 - val_accuracy: 0.0469\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3410 - accuracy: 0.5011 - val_loss: 1.5009 - val_accuracy: 0.0469\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3289 - accuracy: 0.5089 - val_loss: 1.4950 - val_accuracy: 0.0625\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3180 - accuracy: 0.5095 - val_loss: 1.4863 - val_accuracy: 0.0703\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3088 - accuracy: 0.5061 - val_loss: 1.4845 - val_accuracy: 0.0781\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2999 - accuracy: 0.5067 - val_loss: 1.4682 - val_accuracy: 0.1016\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2925 - accuracy: 0.5106 - val_loss: 1.4694 - val_accuracy: 0.1016\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2849 - accuracy: 0.5067 - val_loss: 1.4598 - val_accuracy: 0.1172\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2791 - accuracy: 0.5056 - val_loss: 1.4572 - val_accuracy: 0.1172\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2729 - accuracy: 0.5078 - val_loss: 1.4513 - val_accuracy: 0.1172\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2681 - accuracy: 0.5067 - val_loss: 1.4538 - val_accuracy: 0.1172\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2637 - accuracy: 0.5073 - val_loss: 1.4428 - val_accuracy: 0.1250\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2586 - accuracy: 0.5106 - val_loss: 1.4336 - val_accuracy: 0.1250\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2557 - accuracy: 0.5078 - val_loss: 1.4332 - val_accuracy: 0.1250\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2511 - accuracy: 0.5106 - val_loss: 1.4300 - val_accuracy: 0.1250\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2479 - accuracy: 0.5123 - val_loss: 1.4226 - val_accuracy: 0.1328\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2456 - accuracy: 0.5117 - val_loss: 1.4105 - val_accuracy: 0.1328\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2414 - accuracy: 0.5089 - val_loss: 1.4189 - val_accuracy: 0.1328\n",
      "Epoch 27/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2382 - accuracy: 0.5100 - val_loss: 1.4128 - val_accuracy: 0.1328\n",
      "Epoch 28/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2350 - accuracy: 0.5100 - val_loss: 1.4188 - val_accuracy: 0.1328\n",
      "Epoch 29/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2330 - accuracy: 0.5095 - val_loss: 1.4141 - val_accuracy: 0.1328\n",
      "Epoch 30/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2303 - accuracy: 0.5106 - val_loss: 1.4101 - val_accuracy: 0.1328\n",
      "Epoch 31/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2285 - accuracy: 0.5112 - val_loss: 1.4064 - val_accuracy: 0.1328\n",
      "Epoch 32/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2267 - accuracy: 0.5128 - val_loss: 1.3979 - val_accuracy: 0.1328\n",
      "Epoch 33/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2245 - accuracy: 0.5140 - val_loss: 1.4041 - val_accuracy: 0.1328\n",
      "Epoch 34/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2221 - accuracy: 0.5123 - val_loss: 1.3982 - val_accuracy: 0.1328\n",
      "Epoch 35/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2202 - accuracy: 0.5112 - val_loss: 1.3984 - val_accuracy: 0.1328\n",
      "Epoch 36/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2183 - accuracy: 0.5117 - val_loss: 1.3951 - val_accuracy: 0.1328\n",
      "Epoch 37/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2164 - accuracy: 0.5112 - val_loss: 1.3916 - val_accuracy: 0.1406\n",
      "Epoch 38/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2148 - accuracy: 0.5123 - val_loss: 1.3936 - val_accuracy: 0.1328\n",
      "Epoch 39/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2139 - accuracy: 0.5117 - val_loss: 1.3938 - val_accuracy: 0.1406\n",
      "Epoch 40/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2115 - accuracy: 0.5123 - val_loss: 1.3963 - val_accuracy: 0.1328\n",
      "Epoch 41/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2092 - accuracy: 0.5123 - val_loss: 1.3851 - val_accuracy: 0.1406\n",
      "Epoch 42/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2080 - accuracy: 0.5112 - val_loss: 1.3869 - val_accuracy: 0.1406\n",
      "Epoch 43/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2066 - accuracy: 0.5134 - val_loss: 1.3852 - val_accuracy: 0.1406\n",
      "Epoch 44/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2059 - accuracy: 0.5117 - val_loss: 1.3813 - val_accuracy: 0.1406\n",
      "Epoch 45/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2034 - accuracy: 0.5117 - val_loss: 1.3842 - val_accuracy: 0.1406\n",
      "Epoch 46/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2019 - accuracy: 0.5112 - val_loss: 1.3822 - val_accuracy: 0.1406\n",
      "Epoch 47/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2006 - accuracy: 0.5112 - val_loss: 1.3809 - val_accuracy: 0.1406\n",
      "Epoch 48/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1998 - accuracy: 0.5117 - val_loss: 1.3821 - val_accuracy: 0.1484\n",
      "Epoch 49/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1988 - accuracy: 0.5117 - val_loss: 1.3850 - val_accuracy: 0.1484\n",
      "Epoch 50/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1970 - accuracy: 0.5106 - val_loss: 1.3787 - val_accuracy: 0.1406\n",
      "Epoch 51/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1953 - accuracy: 0.5134 - val_loss: 1.3794 - val_accuracy: 0.1406\n",
      "Epoch 52/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1949 - accuracy: 0.5123 - val_loss: 1.3817 - val_accuracy: 0.1406\n",
      "Epoch 53/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1931 - accuracy: 0.5134 - val_loss: 1.3791 - val_accuracy: 0.1406\n",
      "Epoch 54/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1925 - accuracy: 0.5134 - val_loss: 1.3770 - val_accuracy: 0.1484\n",
      "Epoch 55/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1918 - accuracy: 0.5123 - val_loss: 1.3818 - val_accuracy: 0.1406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1907 - accuracy: 0.5140 - val_loss: 1.3748 - val_accuracy: 0.1406\n",
      "Epoch 57/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1894 - accuracy: 0.5123 - val_loss: 1.3760 - val_accuracy: 0.1406\n",
      "Epoch 58/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1876 - accuracy: 0.5134 - val_loss: 1.3754 - val_accuracy: 0.1484\n",
      "Epoch 59/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1870 - accuracy: 0.5134 - val_loss: 1.3811 - val_accuracy: 0.1406\n",
      "Epoch 60/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1861 - accuracy: 0.5106 - val_loss: 1.3767 - val_accuracy: 0.1406\n",
      "Epoch 61/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1842 - accuracy: 0.5128 - val_loss: 1.3715 - val_accuracy: 0.1484\n",
      "Epoch 62/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1840 - accuracy: 0.5128 - val_loss: 1.3694 - val_accuracy: 0.1484\n",
      "Epoch 63/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1830 - accuracy: 0.5106 - val_loss: 1.3790 - val_accuracy: 0.1406\n",
      "Epoch 64/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1824 - accuracy: 0.5140 - val_loss: 1.3717 - val_accuracy: 0.1406\n",
      "Epoch 65/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1811 - accuracy: 0.5145 - val_loss: 1.3776 - val_accuracy: 0.1406\n",
      "Epoch 66/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1808 - accuracy: 0.5128 - val_loss: 1.3655 - val_accuracy: 0.1484\n",
      "Epoch 67/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1791 - accuracy: 0.5134 - val_loss: 1.3670 - val_accuracy: 0.1484\n",
      "Epoch 68/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1802 - accuracy: 0.5145 - val_loss: 1.3595 - val_accuracy: 0.1484\n",
      "Epoch 69/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1785 - accuracy: 0.5140 - val_loss: 1.3654 - val_accuracy: 0.1484\n",
      "Epoch 70/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1777 - accuracy: 0.5134 - val_loss: 1.3610 - val_accuracy: 0.1484\n",
      "Epoch 71/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1780 - accuracy: 0.5106 - val_loss: 1.3625 - val_accuracy: 0.1484\n",
      "Epoch 72/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1776 - accuracy: 0.5140 - val_loss: 1.3637 - val_accuracy: 0.1406\n",
      "Epoch 73/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1773 - accuracy: 0.5140 - val_loss: 1.3592 - val_accuracy: 0.1484\n",
      "Epoch 74/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1773 - accuracy: 0.5128 - val_loss: 1.3532 - val_accuracy: 0.1484\n",
      "Epoch 75/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1766 - accuracy: 0.5145 - val_loss: 1.3643 - val_accuracy: 0.1406\n",
      "Epoch 76/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1770 - accuracy: 0.5112 - val_loss: 1.3589 - val_accuracy: 0.1406\n",
      "Epoch 77/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1766 - accuracy: 0.5123 - val_loss: 1.3598 - val_accuracy: 0.1406\n",
      "Epoch 78/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1767 - accuracy: 0.5134 - val_loss: 1.3551 - val_accuracy: 0.1484\n",
      "Epoch 79/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1758 - accuracy: 0.5123 - val_loss: 1.3555 - val_accuracy: 0.1406\n",
      "Epoch 80/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1765 - accuracy: 0.5134 - val_loss: 1.3598 - val_accuracy: 0.1406\n",
      "Epoch 81/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1767 - accuracy: 0.5128 - val_loss: 1.3544 - val_accuracy: 0.1484\n",
      "Epoch 82/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1762 - accuracy: 0.5112 - val_loss: 1.3484 - val_accuracy: 0.1484\n",
      "Epoch 83/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1758 - accuracy: 0.5117 - val_loss: 1.3532 - val_accuracy: 0.1484\n",
      "Epoch 84/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1748 - accuracy: 0.5140 - val_loss: 1.3467 - val_accuracy: 0.1484\n",
      "Epoch 85/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1748 - accuracy: 0.5123 - val_loss: 1.3497 - val_accuracy: 0.1484\n",
      "Epoch 86/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1752 - accuracy: 0.5123 - val_loss: 1.3473 - val_accuracy: 0.1484\n",
      "Epoch 87/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1755 - accuracy: 0.5128 - val_loss: 1.3583 - val_accuracy: 0.1406\n",
      "Epoch 88/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1749 - accuracy: 0.5128 - val_loss: 1.3494 - val_accuracy: 0.1484\n",
      "Epoch 89/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1750 - accuracy: 0.5123 - val_loss: 1.3613 - val_accuracy: 0.1406\n",
      "Epoch 90/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1748 - accuracy: 0.5128 - val_loss: 1.3471 - val_accuracy: 0.1484\n",
      "Epoch 91/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1744 - accuracy: 0.5117 - val_loss: 1.3507 - val_accuracy: 0.1484\n",
      "Epoch 92/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1742 - accuracy: 0.5128 - val_loss: 1.3488 - val_accuracy: 0.1484\n",
      "Epoch 93/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1742 - accuracy: 0.5100 - val_loss: 1.3439 - val_accuracy: 0.1484\n",
      "Epoch 94/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1743 - accuracy: 0.5123 - val_loss: 1.3448 - val_accuracy: 0.1484\n",
      "Epoch 95/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1739 - accuracy: 0.5112 - val_loss: 1.3473 - val_accuracy: 0.1484\n",
      "Epoch 96/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1741 - accuracy: 0.5117 - val_loss: 1.3464 - val_accuracy: 0.1484\n",
      "Epoch 97/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1738 - accuracy: 0.5117 - val_loss: 1.3437 - val_accuracy: 0.1484\n",
      "Epoch 98/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1730 - accuracy: 0.5145 - val_loss: 1.3488 - val_accuracy: 0.1406\n",
      "Epoch 99/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1727 - accuracy: 0.5128 - val_loss: 1.3493 - val_accuracy: 0.1484\n",
      "Epoch 100/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1732 - accuracy: 0.5128 - val_loss: 1.3461 - val_accuracy: 0.1484\n",
      "Epoch 101/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1731 - accuracy: 0.5106 - val_loss: 1.3507 - val_accuracy: 0.1406\n",
      "Epoch 102/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1731 - accuracy: 0.5134 - val_loss: 1.3437 - val_accuracy: 0.1484\n",
      "Epoch 103/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1725 - accuracy: 0.5117 - val_loss: 1.3478 - val_accuracy: 0.1484\n",
      "Epoch 104/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1724 - accuracy: 0.5117 - val_loss: 1.3427 - val_accuracy: 0.1484\n",
      "Epoch 105/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1736 - accuracy: 0.5134 - val_loss: 1.3503 - val_accuracy: 0.1406\n",
      "Epoch 106/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1722 - accuracy: 0.5123 - val_loss: 1.3400 - val_accuracy: 0.1484\n",
      "Epoch 107/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1726 - accuracy: 0.5145 - val_loss: 1.3416 - val_accuracy: 0.1484\n",
      "Epoch 108/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1722 - accuracy: 0.5128 - val_loss: 1.3497 - val_accuracy: 0.1484\n",
      "Epoch 109/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1723 - accuracy: 0.5100 - val_loss: 1.3515 - val_accuracy: 0.1406\n",
      "Epoch 110/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1719 - accuracy: 0.5123 - val_loss: 1.3411 - val_accuracy: 0.1484\n",
      "Epoch 111/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1719 - accuracy: 0.5128 - val_loss: 1.3470 - val_accuracy: 0.1484\n",
      "Epoch 112/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1722 - accuracy: 0.5117 - val_loss: 1.3482 - val_accuracy: 0.1484\n",
      "Epoch 113/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1716 - accuracy: 0.5123 - val_loss: 1.3507 - val_accuracy: 0.1406\n",
      "Epoch 114/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1727 - accuracy: 0.5128 - val_loss: 1.3490 - val_accuracy: 0.1484\n",
      "Epoch 115/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1721 - accuracy: 0.5123 - val_loss: 1.3439 - val_accuracy: 0.1484\n",
      "Epoch 116/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1716 - accuracy: 0.5128 - val_loss: 1.3501 - val_accuracy: 0.1484\n",
      "Epoch 117/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1714 - accuracy: 0.5117 - val_loss: 1.3413 - val_accuracy: 0.1406\n",
      "Epoch 118/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1715 - accuracy: 0.5145 - val_loss: 1.3439 - val_accuracy: 0.1406\n",
      "Epoch 119/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1712 - accuracy: 0.5128 - val_loss: 1.3428 - val_accuracy: 0.1406\n",
      "Epoch 120/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1707 - accuracy: 0.5123 - val_loss: 1.3412 - val_accuracy: 0.1484\n",
      "Epoch 121/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1711 - accuracy: 0.5134 - val_loss: 1.3480 - val_accuracy: 0.1406\n",
      "Epoch 122/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1709 - accuracy: 0.5123 - val_loss: 1.3395 - val_accuracy: 0.1484\n",
      "Epoch 123/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1706 - accuracy: 0.5128 - val_loss: 1.3412 - val_accuracy: 0.1484\n",
      "Epoch 124/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1712 - accuracy: 0.5123 - val_loss: 1.3427 - val_accuracy: 0.1406\n",
      "Epoch 125/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1712 - accuracy: 0.5123 - val_loss: 1.3467 - val_accuracy: 0.1484\n",
      "Epoch 126/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1708 - accuracy: 0.5123 - val_loss: 1.3483 - val_accuracy: 0.1406\n",
      "Epoch 127/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1707 - accuracy: 0.5123 - val_loss: 1.3360 - val_accuracy: 0.1406\n",
      "Epoch 128/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1710 - accuracy: 0.5123 - val_loss: 1.3449 - val_accuracy: 0.1406\n",
      "Epoch 129/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1708 - accuracy: 0.5128 - val_loss: 1.3430 - val_accuracy: 0.1484\n",
      "Epoch 130/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1705 - accuracy: 0.5128 - val_loss: 1.3403 - val_accuracy: 0.1484\n",
      "Epoch 131/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1703 - accuracy: 0.5123 - val_loss: 1.3454 - val_accuracy: 0.1406\n",
      "Epoch 132/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1699 - accuracy: 0.5134 - val_loss: 1.3426 - val_accuracy: 0.1406\n",
      "Epoch 133/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1710 - accuracy: 0.5140 - val_loss: 1.3525 - val_accuracy: 0.1406\n",
      "Epoch 134/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1715 - accuracy: 0.5128 - val_loss: 1.3436 - val_accuracy: 0.1406\n",
      "Epoch 135/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1701 - accuracy: 0.5123 - val_loss: 1.3418 - val_accuracy: 0.1484\n",
      "Epoch 136/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1699 - accuracy: 0.5123 - val_loss: 1.3373 - val_accuracy: 0.1484\n",
      "Epoch 137/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1700 - accuracy: 0.5128 - val_loss: 1.3493 - val_accuracy: 0.1406\n",
      "Epoch 138/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1707 - accuracy: 0.5128 - val_loss: 1.3398 - val_accuracy: 0.1406\n",
      "Epoch 139/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1703 - accuracy: 0.5145 - val_loss: 1.3468 - val_accuracy: 0.1406\n",
      "Epoch 140/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1703 - accuracy: 0.5123 - val_loss: 1.3428 - val_accuracy: 0.1406\n",
      "Epoch 141/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1703 - accuracy: 0.5134 - val_loss: 1.3420 - val_accuracy: 0.1406\n",
      "Epoch 142/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1700 - accuracy: 0.5123 - val_loss: 1.3448 - val_accuracy: 0.1406\n",
      "Epoch 143/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1700 - accuracy: 0.5151 - val_loss: 1.3409 - val_accuracy: 0.1406\n",
      "Epoch 144/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1699 - accuracy: 0.5128 - val_loss: 1.3431 - val_accuracy: 0.1406\n",
      "Epoch 145/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1702 - accuracy: 0.5151 - val_loss: 1.3457 - val_accuracy: 0.1406\n",
      "Epoch 146/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1701 - accuracy: 0.5117 - val_loss: 1.3475 - val_accuracy: 0.1406\n",
      "Epoch 147/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1690 - accuracy: 0.5134 - val_loss: 1.3415 - val_accuracy: 0.1406\n",
      "Epoch 148/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1697 - accuracy: 0.5134 - val_loss: 1.3371 - val_accuracy: 0.1406\n",
      "Epoch 149/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1694 - accuracy: 0.5128 - val_loss: 1.3467 - val_accuracy: 0.1406\n",
      "Epoch 150/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1701 - accuracy: 0.5117 - val_loss: 1.3398 - val_accuracy: 0.1484\n",
      "Epoch 151/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1702 - accuracy: 0.5151 - val_loss: 1.3397 - val_accuracy: 0.1406\n",
      "Epoch 152/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1697 - accuracy: 0.5123 - val_loss: 1.3419 - val_accuracy: 0.1406\n",
      "Epoch 153/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1693 - accuracy: 0.5145 - val_loss: 1.3398 - val_accuracy: 0.1406\n",
      "Epoch 154/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1693 - accuracy: 0.5123 - val_loss: 1.3403 - val_accuracy: 0.1406\n",
      "Epoch 155/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1687 - accuracy: 0.5134 - val_loss: 1.3402 - val_accuracy: 0.1406\n",
      "Epoch 156/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1697 - accuracy: 0.5145 - val_loss: 1.3409 - val_accuracy: 0.1406\n",
      "Epoch 157/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1702 - accuracy: 0.5117 - val_loss: 1.3408 - val_accuracy: 0.1406\n",
      "Epoch 158/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1699 - accuracy: 0.5128 - val_loss: 1.3437 - val_accuracy: 0.1406\n",
      "Epoch 159/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1721 - accuracy: 0.5134 - val_loss: 1.3441 - val_accuracy: 0.1406\n",
      "Epoch 160/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1689 - accuracy: 0.5140 - val_loss: 1.3386 - val_accuracy: 0.1406\n",
      "Epoch 161/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1697 - accuracy: 0.5134 - val_loss: 1.3454 - val_accuracy: 0.1406\n",
      "Epoch 162/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1693 - accuracy: 0.5140 - val_loss: 1.3384 - val_accuracy: 0.1406\n",
      "Epoch 163/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1700 - accuracy: 0.5134 - val_loss: 1.3453 - val_accuracy: 0.1406\n",
      "Epoch 164/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1695 - accuracy: 0.5140 - val_loss: 1.3377 - val_accuracy: 0.1406\n",
      "Epoch 165/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1690 - accuracy: 0.5117 - val_loss: 1.3435 - val_accuracy: 0.1406\n",
      "Epoch 166/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1697 - accuracy: 0.5123 - val_loss: 1.3397 - val_accuracy: 0.1406\n",
      "Epoch 167/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1693 - accuracy: 0.5128 - val_loss: 1.3393 - val_accuracy: 0.1406\n",
      "Epoch 168/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1686 - accuracy: 0.5134 - val_loss: 1.3382 - val_accuracy: 0.1406\n",
      "Epoch 169/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1694 - accuracy: 0.5140 - val_loss: 1.3430 - val_accuracy: 0.1406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1687 - accuracy: 0.5128 - val_loss: 1.3429 - val_accuracy: 0.1406\n",
      "Epoch 171/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1693 - accuracy: 0.5128 - val_loss: 1.3416 - val_accuracy: 0.1406\n",
      "Epoch 172/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1691 - accuracy: 0.5123 - val_loss: 1.3430 - val_accuracy: 0.1406\n",
      "Epoch 173/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1693 - accuracy: 0.5151 - val_loss: 1.3407 - val_accuracy: 0.1406\n",
      "Epoch 174/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1694 - accuracy: 0.5123 - val_loss: 1.3359 - val_accuracy: 0.1406\n",
      "Epoch 175/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1693 - accuracy: 0.5128 - val_loss: 1.3358 - val_accuracy: 0.1406\n",
      "Epoch 176/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1691 - accuracy: 0.5134 - val_loss: 1.3395 - val_accuracy: 0.1406\n",
      "Epoch 177/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1692 - accuracy: 0.5134 - val_loss: 1.3377 - val_accuracy: 0.1406\n",
      "Epoch 178/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1689 - accuracy: 0.5145 - val_loss: 1.3371 - val_accuracy: 0.1406\n",
      "Epoch 179/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1698 - accuracy: 0.5134 - val_loss: 1.3410 - val_accuracy: 0.1406\n",
      "Epoch 180/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1687 - accuracy: 0.5140 - val_loss: 1.3346 - val_accuracy: 0.1406\n",
      "Epoch 181/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1689 - accuracy: 0.5140 - val_loss: 1.3336 - val_accuracy: 0.1406\n",
      "Epoch 182/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1692 - accuracy: 0.5128 - val_loss: 1.3389 - val_accuracy: 0.1406\n",
      "Epoch 183/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1686 - accuracy: 0.5128 - val_loss: 1.3322 - val_accuracy: 0.1406\n",
      "Epoch 184/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1696 - accuracy: 0.5134 - val_loss: 1.3389 - val_accuracy: 0.1406\n",
      "Epoch 185/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1685 - accuracy: 0.5140 - val_loss: 1.3341 - val_accuracy: 0.1406\n",
      "Epoch 186/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1691 - accuracy: 0.5145 - val_loss: 1.3389 - val_accuracy: 0.1406\n",
      "Epoch 187/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1684 - accuracy: 0.5140 - val_loss: 1.3368 - val_accuracy: 0.1406\n",
      "Epoch 188/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1683 - accuracy: 0.5134 - val_loss: 1.3342 - val_accuracy: 0.1406\n",
      "Epoch 189/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1692 - accuracy: 0.5151 - val_loss: 1.3404 - val_accuracy: 0.1406\n",
      "Epoch 190/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1688 - accuracy: 0.5128 - val_loss: 1.3349 - val_accuracy: 0.1406\n",
      "Epoch 191/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1689 - accuracy: 0.5134 - val_loss: 1.3359 - val_accuracy: 0.1406\n",
      "Epoch 192/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1682 - accuracy: 0.5145 - val_loss: 1.3307 - val_accuracy: 0.1406\n",
      "Epoch 193/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1688 - accuracy: 0.5140 - val_loss: 1.3317 - val_accuracy: 0.1406\n",
      "Epoch 194/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1686 - accuracy: 0.5117 - val_loss: 1.3323 - val_accuracy: 0.1406\n",
      "Epoch 195/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1695 - accuracy: 0.5140 - val_loss: 1.3366 - val_accuracy: 0.1406\n",
      "Epoch 196/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1688 - accuracy: 0.5128 - val_loss: 1.3344 - val_accuracy: 0.1406\n",
      "Epoch 197/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1683 - accuracy: 0.5140 - val_loss: 1.3372 - val_accuracy: 0.1406\n",
      "Epoch 198/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1685 - accuracy: 0.5156 - val_loss: 1.3400 - val_accuracy: 0.1406\n",
      "Epoch 199/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1688 - accuracy: 0.5151 - val_loss: 1.3339 - val_accuracy: 0.1406\n",
      "Epoch 200/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1684 - accuracy: 0.5145 - val_loss: 1.3378 - val_accuracy: 0.1406\n",
      "Epoch 201/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1681 - accuracy: 0.5134 - val_loss: 1.3342 - val_accuracy: 0.1406\n",
      "Epoch 202/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1682 - accuracy: 0.5145 - val_loss: 1.3397 - val_accuracy: 0.1406\n",
      "Epoch 203/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1681 - accuracy: 0.5140 - val_loss: 1.3342 - val_accuracy: 0.1406\n",
      "Epoch 204/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1692 - accuracy: 0.5140 - val_loss: 1.3335 - val_accuracy: 0.1406\n",
      "Epoch 205/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1680 - accuracy: 0.5145 - val_loss: 1.3318 - val_accuracy: 0.1406\n",
      "Epoch 206/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1680 - accuracy: 0.5145 - val_loss: 1.3421 - val_accuracy: 0.1406\n",
      "Epoch 207/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1685 - accuracy: 0.5156 - val_loss: 1.3313 - val_accuracy: 0.1406\n",
      "Epoch 208/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1683 - accuracy: 0.5140 - val_loss: 1.3373 - val_accuracy: 0.1406\n",
      "Epoch 209/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1687 - accuracy: 0.5134 - val_loss: 1.3382 - val_accuracy: 0.1406\n",
      "Epoch 210/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1689 - accuracy: 0.5151 - val_loss: 1.3377 - val_accuracy: 0.1406\n",
      "Epoch 211/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1688 - accuracy: 0.5140 - val_loss: 1.3313 - val_accuracy: 0.1406\n",
      "Epoch 212/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1699 - accuracy: 0.5151 - val_loss: 1.3298 - val_accuracy: 0.1406\n",
      "Epoch 213/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1681 - accuracy: 0.5134 - val_loss: 1.3340 - val_accuracy: 0.1406\n",
      "Epoch 214/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1683 - accuracy: 0.5140 - val_loss: 1.3294 - val_accuracy: 0.1406\n",
      "Epoch 215/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1684 - accuracy: 0.5117 - val_loss: 1.3412 - val_accuracy: 0.1406\n",
      "Epoch 216/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1679 - accuracy: 0.5140 - val_loss: 1.3343 - val_accuracy: 0.1406\n",
      "Epoch 217/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1682 - accuracy: 0.5162 - val_loss: 1.3310 - val_accuracy: 0.1484\n",
      "Epoch 218/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1680 - accuracy: 0.5140 - val_loss: 1.3340 - val_accuracy: 0.1406\n",
      "Epoch 219/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1682 - accuracy: 0.5134 - val_loss: 1.3277 - val_accuracy: 0.1484\n",
      "Epoch 220/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1683 - accuracy: 0.5151 - val_loss: 1.3381 - val_accuracy: 0.1406\n",
      "Epoch 221/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1681 - accuracy: 0.5134 - val_loss: 1.3353 - val_accuracy: 0.1484\n",
      "Epoch 222/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1682 - accuracy: 0.5145 - val_loss: 1.3347 - val_accuracy: 0.1406\n",
      "Epoch 223/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1680 - accuracy: 0.5140 - val_loss: 1.3339 - val_accuracy: 0.1406\n",
      "Epoch 224/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1677 - accuracy: 0.5151 - val_loss: 1.3323 - val_accuracy: 0.1484\n",
      "Epoch 225/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1673 - accuracy: 0.5140 - val_loss: 1.3330 - val_accuracy: 0.1406\n",
      "Epoch 226/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1681 - accuracy: 0.5140 - val_loss: 1.3377 - val_accuracy: 0.1406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1681 - accuracy: 0.5145 - val_loss: 1.3310 - val_accuracy: 0.1484\n",
      "Epoch 228/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1676 - accuracy: 0.5140 - val_loss: 1.3290 - val_accuracy: 0.1406\n",
      "Epoch 229/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1677 - accuracy: 0.5156 - val_loss: 1.3273 - val_accuracy: 0.1484\n",
      "Epoch 230/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1679 - accuracy: 0.5140 - val_loss: 1.3333 - val_accuracy: 0.1406\n",
      "Epoch 231/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1686 - accuracy: 0.5151 - val_loss: 1.3345 - val_accuracy: 0.1406\n",
      "Epoch 232/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1679 - accuracy: 0.5145 - val_loss: 1.3350 - val_accuracy: 0.1406\n",
      "Epoch 233/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1675 - accuracy: 0.5151 - val_loss: 1.3329 - val_accuracy: 0.1406\n",
      "Epoch 234/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1674 - accuracy: 0.5134 - val_loss: 1.3352 - val_accuracy: 0.1484\n",
      "Epoch 235/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1675 - accuracy: 0.5140 - val_loss: 1.3371 - val_accuracy: 0.1406\n",
      "Epoch 236/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1679 - accuracy: 0.5151 - val_loss: 1.3320 - val_accuracy: 0.1406\n",
      "Epoch 237/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1681 - accuracy: 0.5140 - val_loss: 1.3301 - val_accuracy: 0.1484\n",
      "Epoch 238/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1676 - accuracy: 0.5151 - val_loss: 1.3362 - val_accuracy: 0.1484\n",
      "Epoch 239/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1682 - accuracy: 0.5145 - val_loss: 1.3301 - val_accuracy: 0.1406\n",
      "Epoch 240/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1677 - accuracy: 0.5140 - val_loss: 1.3353 - val_accuracy: 0.1484\n",
      "Epoch 241/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1674 - accuracy: 0.5140 - val_loss: 1.3357 - val_accuracy: 0.1406\n",
      "Epoch 242/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1671 - accuracy: 0.5140 - val_loss: 1.3312 - val_accuracy: 0.1484\n",
      "Epoch 243/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1678 - accuracy: 0.5140 - val_loss: 1.3315 - val_accuracy: 0.1484\n",
      "Epoch 244/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1673 - accuracy: 0.5134 - val_loss: 1.3355 - val_accuracy: 0.1406\n",
      "Epoch 245/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1674 - accuracy: 0.5145 - val_loss: 1.3301 - val_accuracy: 0.1484\n",
      "Epoch 246/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1677 - accuracy: 0.5151 - val_loss: 1.3358 - val_accuracy: 0.1484\n",
      "Epoch 247/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1675 - accuracy: 0.5151 - val_loss: 1.3294 - val_accuracy: 0.1484\n",
      "Epoch 248/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1674 - accuracy: 0.5151 - val_loss: 1.3347 - val_accuracy: 0.1406\n",
      "Epoch 249/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1675 - accuracy: 0.5140 - val_loss: 1.3320 - val_accuracy: 0.1406\n",
      "Epoch 250/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1677 - accuracy: 0.5140 - val_loss: 1.3288 - val_accuracy: 0.1484\n",
      "Epoch 251/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1677 - accuracy: 0.5151 - val_loss: 1.3291 - val_accuracy: 0.1484\n",
      "Epoch 252/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1685 - accuracy: 0.5145 - val_loss: 1.3392 - val_accuracy: 0.1406\n",
      "Epoch 253/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1699 - accuracy: 0.5140 - val_loss: 1.3286 - val_accuracy: 0.1484\n",
      "Epoch 254/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1687 - accuracy: 0.5134 - val_loss: 1.3322 - val_accuracy: 0.1484\n",
      "Epoch 255/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1676 - accuracy: 0.5140 - val_loss: 1.3384 - val_accuracy: 0.1406\n",
      "Epoch 256/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1674 - accuracy: 0.5145 - val_loss: 1.3306 - val_accuracy: 0.1484\n",
      "Epoch 257/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1694 - accuracy: 0.5134 - val_loss: 1.3281 - val_accuracy: 0.1484\n",
      "Epoch 258/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1682 - accuracy: 0.5140 - val_loss: 1.3375 - val_accuracy: 0.1484\n",
      "Epoch 259/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1675 - accuracy: 0.5156 - val_loss: 1.3379 - val_accuracy: 0.1484\n",
      "Epoch 260/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1681 - accuracy: 0.5140 - val_loss: 1.3290 - val_accuracy: 0.1484\n",
      "Epoch 261/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1683 - accuracy: 0.5162 - val_loss: 1.3314 - val_accuracy: 0.1484\n",
      "Epoch 262/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1674 - accuracy: 0.5140 - val_loss: 1.3325 - val_accuracy: 0.1406\n",
      "Epoch 263/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1685 - accuracy: 0.5140 - val_loss: 1.3368 - val_accuracy: 0.1484\n",
      "Epoch 264/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1673 - accuracy: 0.5156 - val_loss: 1.3312 - val_accuracy: 0.1484\n",
      "Epoch 265/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1685 - accuracy: 0.5145 - val_loss: 1.3369 - val_accuracy: 0.1484\n",
      "Epoch 266/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1681 - accuracy: 0.5140 - val_loss: 1.3307 - val_accuracy: 0.1484\n",
      "Epoch 267/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1678 - accuracy: 0.5173 - val_loss: 1.3380 - val_accuracy: 0.1406\n",
      "Epoch 268/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1677 - accuracy: 0.5151 - val_loss: 1.3350 - val_accuracy: 0.1484\n",
      "Epoch 269/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1677 - accuracy: 0.5162 - val_loss: 1.3306 - val_accuracy: 0.1484\n",
      "Epoch 270/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1670 - accuracy: 0.5145 - val_loss: 1.3328 - val_accuracy: 0.1484\n",
      "Epoch 271/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1677 - accuracy: 0.5179 - val_loss: 1.3368 - val_accuracy: 0.1406\n",
      "Epoch 272/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1681 - accuracy: 0.5140 - val_loss: 1.3352 - val_accuracy: 0.1406\n",
      "Epoch 273/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1676 - accuracy: 0.5140 - val_loss: 1.3312 - val_accuracy: 0.1484\n",
      "Epoch 274/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1671 - accuracy: 0.5140 - val_loss: 1.3316 - val_accuracy: 0.1484\n",
      "Epoch 275/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1675 - accuracy: 0.5156 - val_loss: 1.3293 - val_accuracy: 0.1484\n",
      "Epoch 276/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1680 - accuracy: 0.5151 - val_loss: 1.3367 - val_accuracy: 0.1484\n",
      "Epoch 277/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1672 - accuracy: 0.5167 - val_loss: 1.3276 - val_accuracy: 0.1484\n",
      "Epoch 278/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1674 - accuracy: 0.5151 - val_loss: 1.3331 - val_accuracy: 0.1484\n",
      "Epoch 279/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1683 - accuracy: 0.5140 - val_loss: 1.3384 - val_accuracy: 0.1406\n",
      "Epoch 280/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1678 - accuracy: 0.5145 - val_loss: 1.3294 - val_accuracy: 0.1484\n",
      "Epoch 281/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1674 - accuracy: 0.5145 - val_loss: 1.3390 - val_accuracy: 0.1406\n",
      "Epoch 282/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1681 - accuracy: 0.5151 - val_loss: 1.3315 - val_accuracy: 0.1484\n",
      "Epoch 283/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1679 - accuracy: 0.5151 - val_loss: 1.3296 - val_accuracy: 0.1484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 284/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1673 - accuracy: 0.5145 - val_loss: 1.3305 - val_accuracy: 0.1484\n",
      "Epoch 285/300\n",
      "28/28 [==============================] - 1s 29ms/step - loss: 1.1676 - accuracy: 0.5140 - val_loss: 1.3277 - val_accuracy: 0.1484\n",
      "Epoch 286/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1675 - accuracy: 0.5156 - val_loss: 1.3289 - val_accuracy: 0.1484\n",
      "Epoch 287/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1678 - accuracy: 0.5134 - val_loss: 1.3350 - val_accuracy: 0.1484\n",
      "Epoch 288/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1677 - accuracy: 0.5140 - val_loss: 1.3317 - val_accuracy: 0.1484\n",
      "Epoch 289/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1674 - accuracy: 0.5173 - val_loss: 1.3321 - val_accuracy: 0.1484\n",
      "Epoch 290/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1675 - accuracy: 0.5162 - val_loss: 1.3312 - val_accuracy: 0.1484\n",
      "Epoch 291/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1678 - accuracy: 0.5162 - val_loss: 1.3301 - val_accuracy: 0.1484\n",
      "Epoch 292/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1675 - accuracy: 0.5134 - val_loss: 1.3357 - val_accuracy: 0.1406\n",
      "Epoch 293/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1671 - accuracy: 0.5151 - val_loss: 1.3312 - val_accuracy: 0.1484\n",
      "Epoch 294/300\n",
      "28/28 [==============================] - 1s 29ms/step - loss: 1.1672 - accuracy: 0.5162 - val_loss: 1.3331 - val_accuracy: 0.1484\n",
      "Epoch 295/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1670 - accuracy: 0.5162 - val_loss: 1.3318 - val_accuracy: 0.1484\n",
      "Epoch 296/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1671 - accuracy: 0.5145 - val_loss: 1.3347 - val_accuracy: 0.1484\n",
      "Epoch 297/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1.1677 - accuracy: 0.5145 - val_loss: 1.3332 - val_accuracy: 0.1484\n",
      "Epoch 298/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1678 - accuracy: 0.5167 - val_loss: 1.3364 - val_accuracy: 0.1484\n",
      "Epoch 299/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1677 - accuracy: 0.5162 - val_loss: 1.3316 - val_accuracy: 0.1484\n",
      "Epoch 300/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1671 - accuracy: 0.5156 - val_loss: 1.3307 - val_accuracy: 0.1484\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "56/56 [==============================] - 0s 1ms/step\n",
      "Recall: [0.94 0.   0.61]\n",
      "Precision: [0.42 0.   0.71]\n",
      "F1-score: [0.58 0.   0.66]\n",
      "Accuracy: 0.51\n",
      "Brier score:0.19528589902414353\n",
      "Brier climat:0.20986359126984128\n",
      "Brier skill score:0.06946270268935706\n",
      "Recall: [0.94 0.   0.61]\n",
      "Precision: [0.42 0.   0.71]\n",
      "F1-score: [0.58 0.   0.66]\n",
      "Accuracy: 0.51\n",
      "Brier score:0.21381925324172205\n",
      "Brier climat:0.21525049603174604\n",
      "Brier skill score:0.006649196245349875\n",
      "Recall: [0.94 0.   0.61]\n",
      "Precision: [0.42 0.   0.71]\n",
      "F1-score: [0.58 0.   0.66]\n",
      "Accuracy: 0.51\n",
      "Brier score:0.15574367193636132\n",
      "Brier climat:0.23020833333333335\n",
      "Brier skill score:0.3234664024483852\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "Recall: [0.5 0.  0. ]\n",
      "Precision: [0.22 0.   0.  ]\n",
      "F1-score: [0.31 0.   0.  ]\n",
      "Accuracy: 0.15\n",
      "Brier score:0.2912656695414905\n",
      "Brier climat:0.2152951388888889\n",
      "Brier skill score:-0.3528669111837637\n",
      "Recall: [0.5 0.  0. ]\n",
      "Precision: [0.22 0.   0.  ]\n",
      "F1-score: [0.31 0.   0.  ]\n",
      "Accuracy: 0.15\n",
      "Brier score:0.36091212647972726\n",
      "Brier climat:0.30628472222222225\n",
      "Brier skill score:-0.17835497592292748\n",
      "Recall: [0.5 0.  0. ]\n",
      "Precision: [0.22 0.   0.  ]\n",
      "F1-score: [0.31 0.   0.  ]\n",
      "Accuracy: 0.15\n",
      "Brier score:0.17335586110698564\n",
      "Brier climat:0.16380208333333332\n",
      "Brier skill score:-0.058325129810532506\n",
      "******** 11\n",
      "validation years [1992, 1993]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "{0: 2.0, 1: 1.0, 2: 1.0}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 60)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 183       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183\n",
      "Trainable params: 183\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/28 [===========================>..] - ETA: 0s - loss: 1.8882 - accuracy: 0.3310WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "28/28 [==============================] - 4s 107ms/step - loss: 1.8832 - accuracy: 0.3309 - val_loss: 1.3078 - val_accuracy: 0.4375\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.6418 - accuracy: 0.4235 - val_loss: 1.3431 - val_accuracy: 0.4141\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.5374 - accuracy: 0.4626 - val_loss: 1.3731 - val_accuracy: 0.2656\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4843 - accuracy: 0.4794 - val_loss: 1.3941 - val_accuracy: 0.2500\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4512 - accuracy: 0.4905 - val_loss: 1.4083 - val_accuracy: 0.2031\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.4278 - accuracy: 0.4939 - val_loss: 1.4168 - val_accuracy: 0.2031\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4108 - accuracy: 0.5022 - val_loss: 1.4154 - val_accuracy: 0.2109\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3970 - accuracy: 0.5045 - val_loss: 1.4239 - val_accuracy: 0.1953\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3856 - accuracy: 0.5017 - val_loss: 1.4221 - val_accuracy: 0.1797\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3746 - accuracy: 0.5028 - val_loss: 1.4236 - val_accuracy: 0.1719\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3648 - accuracy: 0.5089 - val_loss: 1.4138 - val_accuracy: 0.1797\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3557 - accuracy: 0.5073 - val_loss: 1.4127 - val_accuracy: 0.1797\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3470 - accuracy: 0.5089 - val_loss: 1.4093 - val_accuracy: 0.1797\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3398 - accuracy: 0.5061 - val_loss: 1.4082 - val_accuracy: 0.1797\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3325 - accuracy: 0.5084 - val_loss: 1.4054 - val_accuracy: 0.1797\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3257 - accuracy: 0.5078 - val_loss: 1.4094 - val_accuracy: 0.1797\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3203 - accuracy: 0.5095 - val_loss: 1.4065 - val_accuracy: 0.1797\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3149 - accuracy: 0.5106 - val_loss: 1.4085 - val_accuracy: 0.1797\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3089 - accuracy: 0.5123 - val_loss: 1.4042 - val_accuracy: 0.1797\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.3044 - accuracy: 0.5145 - val_loss: 1.4051 - val_accuracy: 0.1797\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2990 - accuracy: 0.5145 - val_loss: 1.4054 - val_accuracy: 0.1797\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2950 - accuracy: 0.5156 - val_loss: 1.4044 - val_accuracy: 0.1797\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2903 - accuracy: 0.5190 - val_loss: 1.4028 - val_accuracy: 0.1797\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2865 - accuracy: 0.5179 - val_loss: 1.4036 - val_accuracy: 0.1797\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2813 - accuracy: 0.5173 - val_loss: 1.3991 - val_accuracy: 0.1797\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2774 - accuracy: 0.5173 - val_loss: 1.4026 - val_accuracy: 0.1797\n",
      "Epoch 27/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2743 - accuracy: 0.5140 - val_loss: 1.3978 - val_accuracy: 0.1797\n",
      "Epoch 28/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2704 - accuracy: 0.5162 - val_loss: 1.3936 - val_accuracy: 0.1797\n",
      "Epoch 29/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2670 - accuracy: 0.5140 - val_loss: 1.3971 - val_accuracy: 0.1797\n",
      "Epoch 30/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2632 - accuracy: 0.5123 - val_loss: 1.3944 - val_accuracy: 0.1797\n",
      "Epoch 31/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2606 - accuracy: 0.5145 - val_loss: 1.3962 - val_accuracy: 0.1797\n",
      "Epoch 32/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2573 - accuracy: 0.5151 - val_loss: 1.3936 - val_accuracy: 0.1797\n",
      "Epoch 33/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2551 - accuracy: 0.5151 - val_loss: 1.3942 - val_accuracy: 0.1797\n",
      "Epoch 34/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2518 - accuracy: 0.5145 - val_loss: 1.3876 - val_accuracy: 0.1797\n",
      "Epoch 35/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2488 - accuracy: 0.5134 - val_loss: 1.3844 - val_accuracy: 0.1797\n",
      "Epoch 36/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2463 - accuracy: 0.5151 - val_loss: 1.3827 - val_accuracy: 0.1797\n",
      "Epoch 37/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2444 - accuracy: 0.5145 - val_loss: 1.3826 - val_accuracy: 0.1797\n",
      "Epoch 38/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2420 - accuracy: 0.5140 - val_loss: 1.3834 - val_accuracy: 0.1797\n",
      "Epoch 39/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2397 - accuracy: 0.5156 - val_loss: 1.3819 - val_accuracy: 0.1797\n",
      "Epoch 40/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2371 - accuracy: 0.5140 - val_loss: 1.3782 - val_accuracy: 0.1797\n",
      "Epoch 41/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2355 - accuracy: 0.5151 - val_loss: 1.3773 - val_accuracy: 0.1797\n",
      "Epoch 42/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2340 - accuracy: 0.5167 - val_loss: 1.3776 - val_accuracy: 0.1797\n",
      "Epoch 43/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2308 - accuracy: 0.5128 - val_loss: 1.3705 - val_accuracy: 0.1797\n",
      "Epoch 44/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2289 - accuracy: 0.5151 - val_loss: 1.3699 - val_accuracy: 0.1797\n",
      "Epoch 45/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2276 - accuracy: 0.5167 - val_loss: 1.3666 - val_accuracy: 0.1797\n",
      "Epoch 46/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2249 - accuracy: 0.5156 - val_loss: 1.3681 - val_accuracy: 0.1797\n",
      "Epoch 47/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2238 - accuracy: 0.5167 - val_loss: 1.3660 - val_accuracy: 0.1797\n",
      "Epoch 48/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2218 - accuracy: 0.5162 - val_loss: 1.3631 - val_accuracy: 0.1797\n",
      "Epoch 49/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2202 - accuracy: 0.5156 - val_loss: 1.3590 - val_accuracy: 0.1797\n",
      "Epoch 50/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2183 - accuracy: 0.5162 - val_loss: 1.3589 - val_accuracy: 0.1797\n",
      "Epoch 51/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2174 - accuracy: 0.5167 - val_loss: 1.3630 - val_accuracy: 0.1797\n",
      "Epoch 52/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2154 - accuracy: 0.5162 - val_loss: 1.3526 - val_accuracy: 0.1797\n",
      "Epoch 53/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2142 - accuracy: 0.5151 - val_loss: 1.3598 - val_accuracy: 0.1797\n",
      "Epoch 54/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2135 - accuracy: 0.5140 - val_loss: 1.3568 - val_accuracy: 0.1797\n",
      "Epoch 55/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2114 - accuracy: 0.5162 - val_loss: 1.3567 - val_accuracy: 0.1797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2098 - accuracy: 0.5145 - val_loss: 1.3634 - val_accuracy: 0.1797\n",
      "Epoch 57/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2080 - accuracy: 0.5156 - val_loss: 1.3538 - val_accuracy: 0.1797\n",
      "Epoch 58/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2070 - accuracy: 0.5167 - val_loss: 1.3550 - val_accuracy: 0.1797\n",
      "Epoch 59/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2051 - accuracy: 0.5156 - val_loss: 1.3538 - val_accuracy: 0.1797\n",
      "Epoch 60/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2046 - accuracy: 0.5162 - val_loss: 1.3555 - val_accuracy: 0.1797\n",
      "Epoch 61/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2029 - accuracy: 0.5173 - val_loss: 1.3511 - val_accuracy: 0.1797\n",
      "Epoch 62/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2013 - accuracy: 0.5156 - val_loss: 1.3540 - val_accuracy: 0.1797\n",
      "Epoch 63/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2003 - accuracy: 0.5167 - val_loss: 1.3521 - val_accuracy: 0.1797\n",
      "Epoch 64/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1997 - accuracy: 0.5134 - val_loss: 1.3515 - val_accuracy: 0.1797\n",
      "Epoch 65/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1989 - accuracy: 0.5145 - val_loss: 1.3514 - val_accuracy: 0.1797\n",
      "Epoch 66/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1983 - accuracy: 0.5167 - val_loss: 1.3515 - val_accuracy: 0.1797\n",
      "Epoch 67/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1972 - accuracy: 0.5151 - val_loss: 1.3469 - val_accuracy: 0.1797\n",
      "Epoch 68/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1961 - accuracy: 0.5140 - val_loss: 1.3454 - val_accuracy: 0.1797\n",
      "Epoch 69/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1952 - accuracy: 0.5173 - val_loss: 1.3472 - val_accuracy: 0.1797\n",
      "Epoch 70/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1941 - accuracy: 0.5173 - val_loss: 1.3450 - val_accuracy: 0.1797\n",
      "Epoch 71/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1935 - accuracy: 0.5156 - val_loss: 1.3438 - val_accuracy: 0.1797\n",
      "Epoch 72/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1938 - accuracy: 0.5140 - val_loss: 1.3472 - val_accuracy: 0.1797\n",
      "Epoch 73/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1929 - accuracy: 0.5151 - val_loss: 1.3472 - val_accuracy: 0.1797\n",
      "Epoch 74/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1923 - accuracy: 0.5162 - val_loss: 1.3418 - val_accuracy: 0.1797\n",
      "Epoch 75/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1908 - accuracy: 0.5156 - val_loss: 1.3437 - val_accuracy: 0.1797\n",
      "Epoch 76/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1913 - accuracy: 0.5123 - val_loss: 1.3427 - val_accuracy: 0.1797\n",
      "Epoch 77/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1893 - accuracy: 0.5140 - val_loss: 1.3433 - val_accuracy: 0.1797\n",
      "Epoch 78/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1894 - accuracy: 0.5145 - val_loss: 1.3447 - val_accuracy: 0.1797\n",
      "Epoch 79/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1882 - accuracy: 0.5140 - val_loss: 1.3447 - val_accuracy: 0.1797\n",
      "Epoch 80/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1886 - accuracy: 0.5128 - val_loss: 1.3447 - val_accuracy: 0.1797\n",
      "Epoch 81/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1871 - accuracy: 0.5140 - val_loss: 1.3427 - val_accuracy: 0.1797\n",
      "Epoch 82/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1865 - accuracy: 0.5134 - val_loss: 1.3465 - val_accuracy: 0.1797\n",
      "Epoch 83/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1865 - accuracy: 0.5134 - val_loss: 1.3409 - val_accuracy: 0.1797\n",
      "Epoch 84/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1862 - accuracy: 0.5145 - val_loss: 1.3462 - val_accuracy: 0.1797\n",
      "Epoch 85/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1846 - accuracy: 0.5134 - val_loss: 1.3422 - val_accuracy: 0.1797\n",
      "Epoch 86/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1846 - accuracy: 0.5123 - val_loss: 1.3421 - val_accuracy: 0.1797\n",
      "Epoch 87/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1848 - accuracy: 0.5151 - val_loss: 1.3476 - val_accuracy: 0.1797\n",
      "Epoch 88/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1844 - accuracy: 0.5151 - val_loss: 1.3453 - val_accuracy: 0.1797\n",
      "Epoch 89/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1843 - accuracy: 0.5145 - val_loss: 1.3421 - val_accuracy: 0.1797\n",
      "Epoch 90/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1832 - accuracy: 0.5140 - val_loss: 1.3409 - val_accuracy: 0.1797\n",
      "Epoch 91/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1833 - accuracy: 0.5140 - val_loss: 1.3487 - val_accuracy: 0.1797\n",
      "Epoch 92/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1836 - accuracy: 0.5145 - val_loss: 1.3433 - val_accuracy: 0.1797\n",
      "Epoch 93/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1836 - accuracy: 0.5117 - val_loss: 1.3418 - val_accuracy: 0.1797\n",
      "Epoch 94/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1826 - accuracy: 0.5140 - val_loss: 1.3470 - val_accuracy: 0.1797\n",
      "Epoch 95/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1820 - accuracy: 0.5145 - val_loss: 1.3415 - val_accuracy: 0.1797\n",
      "Epoch 96/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1817 - accuracy: 0.5134 - val_loss: 1.3406 - val_accuracy: 0.1797\n",
      "Epoch 97/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1811 - accuracy: 0.5151 - val_loss: 1.3455 - val_accuracy: 0.1797\n",
      "Epoch 98/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1809 - accuracy: 0.5128 - val_loss: 1.3401 - val_accuracy: 0.1797\n",
      "Epoch 99/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1809 - accuracy: 0.5112 - val_loss: 1.3383 - val_accuracy: 0.1797\n",
      "Epoch 100/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1807 - accuracy: 0.5145 - val_loss: 1.3384 - val_accuracy: 0.1797\n",
      "Epoch 101/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1800 - accuracy: 0.5134 - val_loss: 1.3401 - val_accuracy: 0.1797\n",
      "Epoch 102/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1802 - accuracy: 0.5145 - val_loss: 1.3395 - val_accuracy: 0.1797\n",
      "Epoch 103/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1797 - accuracy: 0.5140 - val_loss: 1.3382 - val_accuracy: 0.1797\n",
      "Epoch 104/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1799 - accuracy: 0.5134 - val_loss: 1.3400 - val_accuracy: 0.1797\n",
      "Epoch 105/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1794 - accuracy: 0.5123 - val_loss: 1.3401 - val_accuracy: 0.1797\n",
      "Epoch 106/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1792 - accuracy: 0.5156 - val_loss: 1.3404 - val_accuracy: 0.1797\n",
      "Epoch 107/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1798 - accuracy: 0.5128 - val_loss: 1.3385 - val_accuracy: 0.1797\n",
      "Epoch 108/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1794 - accuracy: 0.5145 - val_loss: 1.3382 - val_accuracy: 0.1797\n",
      "Epoch 109/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1785 - accuracy: 0.5167 - val_loss: 1.3392 - val_accuracy: 0.1797\n",
      "Epoch 110/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1784 - accuracy: 0.5123 - val_loss: 1.3347 - val_accuracy: 0.1797\n",
      "Epoch 111/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1787 - accuracy: 0.5151 - val_loss: 1.3364 - val_accuracy: 0.1797\n",
      "Epoch 112/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1783 - accuracy: 0.5128 - val_loss: 1.3376 - val_accuracy: 0.1797\n",
      "Epoch 113/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1781 - accuracy: 0.5167 - val_loss: 1.3387 - val_accuracy: 0.1797\n",
      "Epoch 114/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1774 - accuracy: 0.5145 - val_loss: 1.3366 - val_accuracy: 0.1797\n",
      "Epoch 115/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1776 - accuracy: 0.5128 - val_loss: 1.3379 - val_accuracy: 0.1797\n",
      "Epoch 116/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1771 - accuracy: 0.5145 - val_loss: 1.3383 - val_accuracy: 0.1797\n",
      "Epoch 117/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1787 - accuracy: 0.5140 - val_loss: 1.3452 - val_accuracy: 0.1797\n",
      "Epoch 118/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1786 - accuracy: 0.5128 - val_loss: 1.3405 - val_accuracy: 0.1797\n",
      "Epoch 119/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1761 - accuracy: 0.5128 - val_loss: 1.3328 - val_accuracy: 0.1797\n",
      "Epoch 120/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1762 - accuracy: 0.5145 - val_loss: 1.3390 - val_accuracy: 0.1797\n",
      "Epoch 121/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1764 - accuracy: 0.5140 - val_loss: 1.3325 - val_accuracy: 0.1797\n",
      "Epoch 122/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1771 - accuracy: 0.5134 - val_loss: 1.3388 - val_accuracy: 0.1797\n",
      "Epoch 123/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1781 - accuracy: 0.5134 - val_loss: 1.3354 - val_accuracy: 0.1797\n",
      "Epoch 124/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1759 - accuracy: 0.5134 - val_loss: 1.3354 - val_accuracy: 0.1797\n",
      "Epoch 125/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1757 - accuracy: 0.5140 - val_loss: 1.3424 - val_accuracy: 0.1797\n",
      "Epoch 126/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1758 - accuracy: 0.5134 - val_loss: 1.3345 - val_accuracy: 0.1797\n",
      "Epoch 127/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1770 - accuracy: 0.5128 - val_loss: 1.3351 - val_accuracy: 0.1797\n",
      "Epoch 128/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1759 - accuracy: 0.5123 - val_loss: 1.3372 - val_accuracy: 0.1797\n",
      "Epoch 129/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1751 - accuracy: 0.5134 - val_loss: 1.3347 - val_accuracy: 0.1797\n",
      "Epoch 130/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1747 - accuracy: 0.5140 - val_loss: 1.3397 - val_accuracy: 0.1797\n",
      "Epoch 131/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1746 - accuracy: 0.5134 - val_loss: 1.3347 - val_accuracy: 0.1797\n",
      "Epoch 132/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1754 - accuracy: 0.5140 - val_loss: 1.3325 - val_accuracy: 0.1797\n",
      "Epoch 133/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1749 - accuracy: 0.5128 - val_loss: 1.3360 - val_accuracy: 0.1797\n",
      "Epoch 134/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1755 - accuracy: 0.5145 - val_loss: 1.3395 - val_accuracy: 0.1797\n",
      "Epoch 135/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1752 - accuracy: 0.5128 - val_loss: 1.3345 - val_accuracy: 0.1875\n",
      "Epoch 136/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1750 - accuracy: 0.5140 - val_loss: 1.3377 - val_accuracy: 0.1797\n",
      "Epoch 137/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1743 - accuracy: 0.5140 - val_loss: 1.3364 - val_accuracy: 0.1875\n",
      "Epoch 138/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1745 - accuracy: 0.5134 - val_loss: 1.3386 - val_accuracy: 0.1797\n",
      "Epoch 139/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1746 - accuracy: 0.5123 - val_loss: 1.3437 - val_accuracy: 0.1797\n",
      "Epoch 140/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1744 - accuracy: 0.5134 - val_loss: 1.3382 - val_accuracy: 0.1797\n",
      "Epoch 141/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1737 - accuracy: 0.5140 - val_loss: 1.3357 - val_accuracy: 0.1797\n",
      "Epoch 142/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1737 - accuracy: 0.5140 - val_loss: 1.3367 - val_accuracy: 0.1797\n",
      "Epoch 143/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1740 - accuracy: 0.5123 - val_loss: 1.3376 - val_accuracy: 0.1797\n",
      "Epoch 144/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1752 - accuracy: 0.5145 - val_loss: 1.3367 - val_accuracy: 0.1797\n",
      "Epoch 145/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1740 - accuracy: 0.5140 - val_loss: 1.3367 - val_accuracy: 0.1797\n",
      "Epoch 146/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1740 - accuracy: 0.5145 - val_loss: 1.3387 - val_accuracy: 0.1797\n",
      "Epoch 147/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1735 - accuracy: 0.5134 - val_loss: 1.3330 - val_accuracy: 0.1875\n",
      "Epoch 148/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1738 - accuracy: 0.5117 - val_loss: 1.3364 - val_accuracy: 0.1875\n",
      "Epoch 149/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1734 - accuracy: 0.5145 - val_loss: 1.3424 - val_accuracy: 0.1797\n",
      "Epoch 150/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1738 - accuracy: 0.5140 - val_loss: 1.3384 - val_accuracy: 0.1797\n",
      "Epoch 151/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1735 - accuracy: 0.5145 - val_loss: 1.3416 - val_accuracy: 0.1797\n",
      "Epoch 152/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1746 - accuracy: 0.5128 - val_loss: 1.3393 - val_accuracy: 0.1797\n",
      "Epoch 153/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1735 - accuracy: 0.5151 - val_loss: 1.3372 - val_accuracy: 0.1875\n",
      "Epoch 154/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1730 - accuracy: 0.5140 - val_loss: 1.3370 - val_accuracy: 0.1875\n",
      "Epoch 155/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1728 - accuracy: 0.5151 - val_loss: 1.3388 - val_accuracy: 0.1797\n",
      "Epoch 156/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1734 - accuracy: 0.5145 - val_loss: 1.3400 - val_accuracy: 0.1797\n",
      "Epoch 157/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1733 - accuracy: 0.5151 - val_loss: 1.3441 - val_accuracy: 0.1797\n",
      "Epoch 158/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1727 - accuracy: 0.5123 - val_loss: 1.3389 - val_accuracy: 0.1797\n",
      "Epoch 159/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1729 - accuracy: 0.5151 - val_loss: 1.3398 - val_accuracy: 0.1797\n",
      "Epoch 160/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1725 - accuracy: 0.5151 - val_loss: 1.3395 - val_accuracy: 0.1797\n",
      "Epoch 161/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1730 - accuracy: 0.5140 - val_loss: 1.3406 - val_accuracy: 0.1797\n",
      "Epoch 162/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1730 - accuracy: 0.5162 - val_loss: 1.3394 - val_accuracy: 0.1797\n",
      "Epoch 163/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1730 - accuracy: 0.5123 - val_loss: 1.3379 - val_accuracy: 0.1875\n",
      "Epoch 164/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1725 - accuracy: 0.5145 - val_loss: 1.3400 - val_accuracy: 0.1797\n",
      "Epoch 165/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1727 - accuracy: 0.5140 - val_loss: 1.3400 - val_accuracy: 0.1797\n",
      "Epoch 166/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1726 - accuracy: 0.5151 - val_loss: 1.3436 - val_accuracy: 0.1797\n",
      "Epoch 167/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1724 - accuracy: 0.5151 - val_loss: 1.3376 - val_accuracy: 0.1797\n",
      "Epoch 168/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1724 - accuracy: 0.5128 - val_loss: 1.3407 - val_accuracy: 0.1797\n",
      "Epoch 169/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1738 - accuracy: 0.5156 - val_loss: 1.3435 - val_accuracy: 0.1797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1725 - accuracy: 0.5151 - val_loss: 1.3427 - val_accuracy: 0.1797\n",
      "Epoch 171/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1733 - accuracy: 0.5140 - val_loss: 1.3397 - val_accuracy: 0.1797\n",
      "Epoch 172/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1725 - accuracy: 0.5123 - val_loss: 1.3367 - val_accuracy: 0.1875\n",
      "Epoch 173/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1730 - accuracy: 0.5156 - val_loss: 1.3402 - val_accuracy: 0.1797\n",
      "Epoch 174/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1730 - accuracy: 0.5145 - val_loss: 1.3408 - val_accuracy: 0.1797\n",
      "Epoch 175/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1723 - accuracy: 0.5140 - val_loss: 1.3418 - val_accuracy: 0.1797\n",
      "Epoch 176/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1724 - accuracy: 0.5145 - val_loss: 1.3376 - val_accuracy: 0.1875\n",
      "Epoch 177/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1719 - accuracy: 0.5128 - val_loss: 1.3418 - val_accuracy: 0.1797\n",
      "Epoch 178/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1725 - accuracy: 0.5140 - val_loss: 1.3403 - val_accuracy: 0.1797\n",
      "Epoch 179/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1722 - accuracy: 0.5145 - val_loss: 1.3412 - val_accuracy: 0.1875\n",
      "Epoch 180/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1726 - accuracy: 0.5151 - val_loss: 1.3445 - val_accuracy: 0.1797\n",
      "Epoch 181/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1720 - accuracy: 0.5151 - val_loss: 1.3434 - val_accuracy: 0.1797\n",
      "Epoch 182/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1720 - accuracy: 0.5145 - val_loss: 1.3377 - val_accuracy: 0.1797\n",
      "Epoch 183/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1727 - accuracy: 0.5156 - val_loss: 1.3382 - val_accuracy: 0.1797\n",
      "Epoch 184/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1722 - accuracy: 0.5140 - val_loss: 1.3422 - val_accuracy: 0.1797\n",
      "Epoch 185/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1717 - accuracy: 0.5151 - val_loss: 1.3426 - val_accuracy: 0.1797\n",
      "Epoch 186/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1715 - accuracy: 0.5134 - val_loss: 1.3374 - val_accuracy: 0.1875\n",
      "Epoch 187/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1730 - accuracy: 0.5106 - val_loss: 1.3361 - val_accuracy: 0.1875\n",
      "Epoch 188/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1716 - accuracy: 0.5156 - val_loss: 1.3397 - val_accuracy: 0.1797\n",
      "Epoch 189/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1714 - accuracy: 0.5140 - val_loss: 1.3362 - val_accuracy: 0.1797\n",
      "Epoch 190/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1726 - accuracy: 0.5140 - val_loss: 1.3423 - val_accuracy: 0.1875\n",
      "Epoch 191/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1715 - accuracy: 0.5156 - val_loss: 1.3422 - val_accuracy: 0.1875\n",
      "Epoch 192/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1715 - accuracy: 0.5145 - val_loss: 1.3449 - val_accuracy: 0.1797\n",
      "Epoch 193/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1722 - accuracy: 0.5123 - val_loss: 1.3365 - val_accuracy: 0.1875\n",
      "Epoch 194/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1717 - accuracy: 0.5145 - val_loss: 1.3415 - val_accuracy: 0.1875\n",
      "Epoch 195/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1710 - accuracy: 0.5156 - val_loss: 1.3376 - val_accuracy: 0.1875\n",
      "Epoch 196/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1716 - accuracy: 0.5151 - val_loss: 1.3448 - val_accuracy: 0.1797\n",
      "Epoch 197/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1718 - accuracy: 0.5145 - val_loss: 1.3359 - val_accuracy: 0.1875\n",
      "Epoch 198/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1715 - accuracy: 0.5151 - val_loss: 1.3393 - val_accuracy: 0.1875\n",
      "Epoch 199/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1715 - accuracy: 0.5134 - val_loss: 1.3396 - val_accuracy: 0.1797\n",
      "Epoch 200/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1718 - accuracy: 0.5140 - val_loss: 1.3417 - val_accuracy: 0.1875\n",
      "Epoch 201/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1712 - accuracy: 0.5162 - val_loss: 1.3374 - val_accuracy: 0.1797\n",
      "Epoch 202/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1716 - accuracy: 0.5167 - val_loss: 1.3410 - val_accuracy: 0.1875\n",
      "Epoch 203/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1718 - accuracy: 0.5151 - val_loss: 1.3371 - val_accuracy: 0.1875\n",
      "Epoch 204/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1708 - accuracy: 0.5151 - val_loss: 1.3393 - val_accuracy: 0.1875\n",
      "Epoch 205/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1711 - accuracy: 0.5140 - val_loss: 1.3376 - val_accuracy: 0.1875\n",
      "Epoch 206/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1706 - accuracy: 0.5156 - val_loss: 1.3372 - val_accuracy: 0.1875\n",
      "Epoch 207/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1710 - accuracy: 0.5151 - val_loss: 1.3389 - val_accuracy: 0.1875\n",
      "Epoch 208/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1708 - accuracy: 0.5145 - val_loss: 1.3409 - val_accuracy: 0.1875\n",
      "Epoch 209/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1708 - accuracy: 0.5151 - val_loss: 1.3383 - val_accuracy: 0.1875\n",
      "Epoch 210/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1715 - accuracy: 0.5140 - val_loss: 1.3412 - val_accuracy: 0.1875\n",
      "Epoch 211/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1711 - accuracy: 0.5156 - val_loss: 1.3381 - val_accuracy: 0.1875\n",
      "Epoch 212/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1709 - accuracy: 0.5151 - val_loss: 1.3345 - val_accuracy: 0.1875\n",
      "Epoch 213/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1708 - accuracy: 0.5145 - val_loss: 1.3417 - val_accuracy: 0.1797\n",
      "Epoch 214/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1701 - accuracy: 0.5162 - val_loss: 1.3351 - val_accuracy: 0.1875\n",
      "Epoch 215/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1708 - accuracy: 0.5140 - val_loss: 1.3420 - val_accuracy: 0.1875\n",
      "Epoch 216/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1706 - accuracy: 0.5145 - val_loss: 1.3388 - val_accuracy: 0.1875\n",
      "Epoch 217/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1705 - accuracy: 0.5145 - val_loss: 1.3355 - val_accuracy: 0.1875\n",
      "Epoch 218/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1704 - accuracy: 0.5145 - val_loss: 1.3455 - val_accuracy: 0.1797\n",
      "Epoch 219/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1706 - accuracy: 0.5112 - val_loss: 1.3383 - val_accuracy: 0.1875\n",
      "Epoch 220/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1714 - accuracy: 0.5151 - val_loss: 1.3388 - val_accuracy: 0.1875\n",
      "Epoch 221/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1710 - accuracy: 0.5145 - val_loss: 1.3389 - val_accuracy: 0.1875\n",
      "Epoch 222/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1707 - accuracy: 0.5145 - val_loss: 1.3338 - val_accuracy: 0.1875\n",
      "Epoch 223/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1703 - accuracy: 0.5156 - val_loss: 1.3359 - val_accuracy: 0.1875\n",
      "Epoch 224/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1706 - accuracy: 0.5145 - val_loss: 1.3331 - val_accuracy: 0.1797\n",
      "Epoch 225/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1706 - accuracy: 0.5140 - val_loss: 1.3337 - val_accuracy: 0.1875\n",
      "Epoch 226/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1710 - accuracy: 0.5156 - val_loss: 1.3379 - val_accuracy: 0.1875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1702 - accuracy: 0.5162 - val_loss: 1.3375 - val_accuracy: 0.1875\n",
      "Epoch 228/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1708 - accuracy: 0.5140 - val_loss: 1.3480 - val_accuracy: 0.1875\n",
      "Epoch 229/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1701 - accuracy: 0.5145 - val_loss: 1.3398 - val_accuracy: 0.1875\n",
      "Epoch 230/300\n",
      "28/28 [==============================] - 2s 56ms/step - loss: 1.1700 - accuracy: 0.5140 - val_loss: 1.3410 - val_accuracy: 0.1875\n",
      "Epoch 231/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1709 - accuracy: 0.5145 - val_loss: 1.3451 - val_accuracy: 0.1875\n",
      "Epoch 232/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1700 - accuracy: 0.5151 - val_loss: 1.3374 - val_accuracy: 0.1875\n",
      "Epoch 233/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1702 - accuracy: 0.5162 - val_loss: 1.3376 - val_accuracy: 0.1875\n",
      "Epoch 234/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1698 - accuracy: 0.5162 - val_loss: 1.3402 - val_accuracy: 0.1875\n",
      "Epoch 235/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1705 - accuracy: 0.5156 - val_loss: 1.3419 - val_accuracy: 0.1797\n",
      "Epoch 236/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1695 - accuracy: 0.5156 - val_loss: 1.3399 - val_accuracy: 0.1875\n",
      "Epoch 237/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1698 - accuracy: 0.5151 - val_loss: 1.3386 - val_accuracy: 0.1875\n",
      "Epoch 238/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1709 - accuracy: 0.5128 - val_loss: 1.3411 - val_accuracy: 0.1797\n",
      "Epoch 239/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1694 - accuracy: 0.5134 - val_loss: 1.3369 - val_accuracy: 0.1875\n",
      "Epoch 240/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1697 - accuracy: 0.5151 - val_loss: 1.3404 - val_accuracy: 0.1875\n",
      "Epoch 241/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1715 - accuracy: 0.5112 - val_loss: 1.3399 - val_accuracy: 0.1797\n",
      "Epoch 242/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1703 - accuracy: 0.5151 - val_loss: 1.3410 - val_accuracy: 0.1875\n",
      "Epoch 243/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1709 - accuracy: 0.5167 - val_loss: 1.3435 - val_accuracy: 0.1875\n",
      "Epoch 244/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1704 - accuracy: 0.5140 - val_loss: 1.3424 - val_accuracy: 0.1875\n",
      "Epoch 245/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1701 - accuracy: 0.5140 - val_loss: 1.3377 - val_accuracy: 0.1875\n",
      "Epoch 246/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1695 - accuracy: 0.5167 - val_loss: 1.3385 - val_accuracy: 0.1875\n",
      "Epoch 247/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1706 - accuracy: 0.5128 - val_loss: 1.3375 - val_accuracy: 0.1875\n",
      "Epoch 248/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1706 - accuracy: 0.5140 - val_loss: 1.3420 - val_accuracy: 0.1797\n",
      "Epoch 249/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1706 - accuracy: 0.5156 - val_loss: 1.3433 - val_accuracy: 0.1875\n",
      "Epoch 250/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1704 - accuracy: 0.5123 - val_loss: 1.3392 - val_accuracy: 0.1875\n",
      "Epoch 251/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1696 - accuracy: 0.5140 - val_loss: 1.3406 - val_accuracy: 0.1875\n",
      "Epoch 252/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1698 - accuracy: 0.5156 - val_loss: 1.3396 - val_accuracy: 0.1875\n",
      "Epoch 253/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1697 - accuracy: 0.5167 - val_loss: 1.3381 - val_accuracy: 0.1875\n",
      "Epoch 254/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1696 - accuracy: 0.5140 - val_loss: 1.3457 - val_accuracy: 0.1875\n",
      "Epoch 255/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1698 - accuracy: 0.5112 - val_loss: 1.3396 - val_accuracy: 0.1875\n",
      "Epoch 256/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1697 - accuracy: 0.5151 - val_loss: 1.3376 - val_accuracy: 0.1875\n",
      "Epoch 257/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1704 - accuracy: 0.5134 - val_loss: 1.3409 - val_accuracy: 0.1875\n",
      "Epoch 258/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1698 - accuracy: 0.5156 - val_loss: 1.3408 - val_accuracy: 0.1875\n",
      "Epoch 259/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1697 - accuracy: 0.5140 - val_loss: 1.3407 - val_accuracy: 0.1875\n",
      "Epoch 260/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1701 - accuracy: 0.5134 - val_loss: 1.3406 - val_accuracy: 0.1875\n",
      "Epoch 261/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1697 - accuracy: 0.5151 - val_loss: 1.3362 - val_accuracy: 0.1875\n",
      "Epoch 262/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1695 - accuracy: 0.5156 - val_loss: 1.3400 - val_accuracy: 0.1875\n",
      "Epoch 263/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1697 - accuracy: 0.5128 - val_loss: 1.3368 - val_accuracy: 0.1875\n",
      "Epoch 264/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1698 - accuracy: 0.5145 - val_loss: 1.3391 - val_accuracy: 0.1875\n",
      "Epoch 265/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1701 - accuracy: 0.5145 - val_loss: 1.3425 - val_accuracy: 0.1875\n",
      "Epoch 266/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1693 - accuracy: 0.5156 - val_loss: 1.3384 - val_accuracy: 0.1875\n",
      "Epoch 267/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1692 - accuracy: 0.5151 - val_loss: 1.3415 - val_accuracy: 0.1875\n",
      "Epoch 268/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1693 - accuracy: 0.5117 - val_loss: 1.3371 - val_accuracy: 0.1875\n",
      "Epoch 269/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1706 - accuracy: 0.5140 - val_loss: 1.3493 - val_accuracy: 0.1875\n",
      "Epoch 270/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1722 - accuracy: 0.5134 - val_loss: 1.3368 - val_accuracy: 0.1875\n",
      "Epoch 271/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1693 - accuracy: 0.5156 - val_loss: 1.3369 - val_accuracy: 0.1875\n",
      "Epoch 272/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1690 - accuracy: 0.5123 - val_loss: 1.3414 - val_accuracy: 0.1875\n",
      "Epoch 273/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1690 - accuracy: 0.5162 - val_loss: 1.3376 - val_accuracy: 0.1875\n",
      "Epoch 274/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1690 - accuracy: 0.5162 - val_loss: 1.3416 - val_accuracy: 0.1875\n",
      "Epoch 275/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1700 - accuracy: 0.5145 - val_loss: 1.3411 - val_accuracy: 0.1875\n",
      "Epoch 276/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1708 - accuracy: 0.5173 - val_loss: 1.3403 - val_accuracy: 0.1875\n",
      "Epoch 277/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1700 - accuracy: 0.5106 - val_loss: 1.3401 - val_accuracy: 0.1875\n",
      "Epoch 278/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1698 - accuracy: 0.5145 - val_loss: 1.3369 - val_accuracy: 0.1875\n",
      "Epoch 279/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1701 - accuracy: 0.5162 - val_loss: 1.3388 - val_accuracy: 0.1875\n",
      "Epoch 280/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1694 - accuracy: 0.5167 - val_loss: 1.3415 - val_accuracy: 0.1875\n",
      "Epoch 281/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1696 - accuracy: 0.5151 - val_loss: 1.3391 - val_accuracy: 0.1875\n",
      "Epoch 282/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1691 - accuracy: 0.5145 - val_loss: 1.3401 - val_accuracy: 0.1875\n",
      "Epoch 283/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1691 - accuracy: 0.5162 - val_loss: 1.3463 - val_accuracy: 0.1875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 284/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1693 - accuracy: 0.5140 - val_loss: 1.3441 - val_accuracy: 0.1875\n",
      "Epoch 285/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1692 - accuracy: 0.5162 - val_loss: 1.3435 - val_accuracy: 0.1875\n",
      "Epoch 286/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1693 - accuracy: 0.5151 - val_loss: 1.3450 - val_accuracy: 0.1875\n",
      "Epoch 287/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1692 - accuracy: 0.5140 - val_loss: 1.3433 - val_accuracy: 0.1875\n",
      "Epoch 288/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1697 - accuracy: 0.5156 - val_loss: 1.3429 - val_accuracy: 0.1875\n",
      "Epoch 289/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1693 - accuracy: 0.5167 - val_loss: 1.3396 - val_accuracy: 0.1875\n",
      "Epoch 290/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1685 - accuracy: 0.5167 - val_loss: 1.3429 - val_accuracy: 0.1875\n",
      "Epoch 291/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1693 - accuracy: 0.5128 - val_loss: 1.3363 - val_accuracy: 0.1875\n",
      "Epoch 292/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1694 - accuracy: 0.5151 - val_loss: 1.3427 - val_accuracy: 0.1875\n",
      "Epoch 293/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1694 - accuracy: 0.5173 - val_loss: 1.3449 - val_accuracy: 0.1875\n",
      "Epoch 294/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1694 - accuracy: 0.5145 - val_loss: 1.3471 - val_accuracy: 0.1797\n",
      "Epoch 295/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1688 - accuracy: 0.5184 - val_loss: 1.3423 - val_accuracy: 0.1875\n",
      "Epoch 296/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1685 - accuracy: 0.5162 - val_loss: 1.3452 - val_accuracy: 0.1875\n",
      "Epoch 297/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1687 - accuracy: 0.5123 - val_loss: 1.3449 - val_accuracy: 0.1875\n",
      "Epoch 298/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1693 - accuracy: 0.5156 - val_loss: 1.3407 - val_accuracy: 0.1875\n",
      "Epoch 299/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1700 - accuracy: 0.5156 - val_loss: 1.3406 - val_accuracy: 0.1875\n",
      "Epoch 300/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1697 - accuracy: 0.5145 - val_loss: 1.3428 - val_accuracy: 0.1875\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "56/56 [==============================] - 0s 1ms/step\n",
      "Recall: [0.94 0.   0.64]\n",
      "Precision: [0.43 0.   0.73]\n",
      "F1-score: [0.58 0.   0.68]\n",
      "Accuracy: 0.52\n",
      "Brier score:0.1955777460998736\n",
      "Brier climat:0.21202132936507936\n",
      "Brier skill score:0.07755626905296664\n",
      "Recall: [0.94 0.   0.64]\n",
      "Precision: [0.43 0.   0.73]\n",
      "F1-score: [0.58 0.   0.68]\n",
      "Accuracy: 0.52\n",
      "Brier score:0.21747277016023453\n",
      "Brier climat:0.2182638888888889\n",
      "Brier skill score:0.0036245974204972553\n",
      "Recall: [0.94 0.   0.64]\n",
      "Precision: [0.43 0.   0.73]\n",
      "F1-score: [0.58 0.   0.68]\n",
      "Accuracy: 0.52\n",
      "Brier score:0.1485810318619241\n",
      "Brier climat:0.22704613095238096\n",
      "Brier skill score:0.34559099845182384\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "4/4 [==============================] - 0s 998us/step\n",
      "Recall: [0.79 0.   0.05]\n",
      "Precision: [0.19 0.   0.18]\n",
      "F1-score: [0.3  0.   0.08]\n",
      "Accuracy: 0.19\n",
      "Brier score:0.30954787753076596\n",
      "Brier climat:0.18508680555555557\n",
      "Brier skill score:-0.6724470261487776\n",
      "Recall: [0.79 0.   0.05]\n",
      "Precision: [0.19 0.   0.18]\n",
      "F1-score: [0.3  0.   0.08]\n",
      "Accuracy: 0.19\n",
      "Brier score:0.2945956751477682\n",
      "Brier climat:0.2640972222222222\n",
      "Brier skill score:-0.11548191483772352\n",
      "Recall: [0.79 0.   0.05]\n",
      "Precision: [0.19 0.   0.18]\n",
      "F1-score: [0.3  0.   0.08]\n",
      "Accuracy: 0.19\n",
      "Brier score:0.23142764411054975\n",
      "Brier climat:0.20807291666666666\n",
      "Brier skill score:-0.11224299547498262\n",
      "******** 12\n",
      "validation years [1993, 1994]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "{0: 2.0, 1: 1.0, 2: 1.0}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 60)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 183       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183\n",
      "Trainable params: 183\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.7097 - accuracy: 0.4403WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "28/28 [==============================] - 4s 112ms/step - loss: 1.7097 - accuracy: 0.4403 - val_loss: 1.3275 - val_accuracy: 0.3359\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.5884 - accuracy: 0.4693 - val_loss: 1.2297 - val_accuracy: 0.5156\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.5353 - accuracy: 0.4788 - val_loss: 1.1820 - val_accuracy: 0.6094\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.5014 - accuracy: 0.4967 - val_loss: 1.1584 - val_accuracy: 0.6016\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4793 - accuracy: 0.5006 - val_loss: 1.1408 - val_accuracy: 0.5938\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.4602 - accuracy: 0.5006 - val_loss: 1.1279 - val_accuracy: 0.5859\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4442 - accuracy: 0.5028 - val_loss: 1.1219 - val_accuracy: 0.5781\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4314 - accuracy: 0.5000 - val_loss: 1.1155 - val_accuracy: 0.5703\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4191 - accuracy: 0.4961 - val_loss: 1.1060 - val_accuracy: 0.5703\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.4071 - accuracy: 0.4978 - val_loss: 1.1025 - val_accuracy: 0.5625\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3968 - accuracy: 0.4944 - val_loss: 1.0937 - val_accuracy: 0.5547\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3873 - accuracy: 0.4994 - val_loss: 1.0906 - val_accuracy: 0.5547\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3774 - accuracy: 0.4933 - val_loss: 1.0853 - val_accuracy: 0.5469\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3699 - accuracy: 0.4961 - val_loss: 1.0807 - val_accuracy: 0.5469\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3630 - accuracy: 0.4905 - val_loss: 1.0790 - val_accuracy: 0.5391\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3554 - accuracy: 0.4900 - val_loss: 1.0728 - val_accuracy: 0.5391\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3483 - accuracy: 0.4900 - val_loss: 1.0708 - val_accuracy: 0.5391\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3426 - accuracy: 0.4883 - val_loss: 1.0704 - val_accuracy: 0.5391\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.3363 - accuracy: 0.4911 - val_loss: 1.0688 - val_accuracy: 0.5391\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3304 - accuracy: 0.4877 - val_loss: 1.0656 - val_accuracy: 0.5469\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3241 - accuracy: 0.4888 - val_loss: 1.0601 - val_accuracy: 0.5547\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3187 - accuracy: 0.4900 - val_loss: 1.0558 - val_accuracy: 0.5547\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3139 - accuracy: 0.4900 - val_loss: 1.0588 - val_accuracy: 0.5469\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3090 - accuracy: 0.4933 - val_loss: 1.0525 - val_accuracy: 0.5469\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3054 - accuracy: 0.4944 - val_loss: 1.0476 - val_accuracy: 0.5469\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2987 - accuracy: 0.4933 - val_loss: 1.0464 - val_accuracy: 0.5469\n",
      "Epoch 27/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2949 - accuracy: 0.4944 - val_loss: 1.0466 - val_accuracy: 0.5469\n",
      "Epoch 28/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2896 - accuracy: 0.4944 - val_loss: 1.0416 - val_accuracy: 0.5469\n",
      "Epoch 29/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2862 - accuracy: 0.4933 - val_loss: 1.0375 - val_accuracy: 0.5469\n",
      "Epoch 30/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2824 - accuracy: 0.4911 - val_loss: 1.0341 - val_accuracy: 0.5469\n",
      "Epoch 31/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2770 - accuracy: 0.4950 - val_loss: 1.0367 - val_accuracy: 0.5469\n",
      "Epoch 32/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2735 - accuracy: 0.4939 - val_loss: 1.0334 - val_accuracy: 0.5469\n",
      "Epoch 33/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2695 - accuracy: 0.4933 - val_loss: 1.0294 - val_accuracy: 0.5469\n",
      "Epoch 34/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2655 - accuracy: 0.4939 - val_loss: 1.0256 - val_accuracy: 0.5469\n",
      "Epoch 35/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2619 - accuracy: 0.4950 - val_loss: 1.0249 - val_accuracy: 0.5469\n",
      "Epoch 36/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2591 - accuracy: 0.4927 - val_loss: 1.0216 - val_accuracy: 0.5469\n",
      "Epoch 37/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2557 - accuracy: 0.4967 - val_loss: 1.0243 - val_accuracy: 0.5391\n",
      "Epoch 38/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2526 - accuracy: 0.4939 - val_loss: 1.0205 - val_accuracy: 0.5469\n",
      "Epoch 39/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2503 - accuracy: 0.4933 - val_loss: 1.0197 - val_accuracy: 0.5391\n",
      "Epoch 40/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2468 - accuracy: 0.4927 - val_loss: 1.0192 - val_accuracy: 0.5469\n",
      "Epoch 41/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2444 - accuracy: 0.4939 - val_loss: 1.0166 - val_accuracy: 0.5469\n",
      "Epoch 42/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2415 - accuracy: 0.4916 - val_loss: 1.0175 - val_accuracy: 0.5391\n",
      "Epoch 43/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2402 - accuracy: 0.4905 - val_loss: 1.0126 - val_accuracy: 0.5469\n",
      "Epoch 44/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2370 - accuracy: 0.4905 - val_loss: 1.0147 - val_accuracy: 0.5391\n",
      "Epoch 45/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2355 - accuracy: 0.4911 - val_loss: 1.0171 - val_accuracy: 0.5391\n",
      "Epoch 46/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2332 - accuracy: 0.4916 - val_loss: 1.0114 - val_accuracy: 0.5391\n",
      "Epoch 47/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2321 - accuracy: 0.4927 - val_loss: 1.0137 - val_accuracy: 0.5391\n",
      "Epoch 48/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2301 - accuracy: 0.4911 - val_loss: 1.0129 - val_accuracy: 0.5391\n",
      "Epoch 49/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2298 - accuracy: 0.4922 - val_loss: 1.0070 - val_accuracy: 0.5469\n",
      "Epoch 50/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2284 - accuracy: 0.4922 - val_loss: 1.0092 - val_accuracy: 0.5469\n",
      "Epoch 51/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2264 - accuracy: 0.4911 - val_loss: 1.0099 - val_accuracy: 0.5469\n",
      "Epoch 52/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2251 - accuracy: 0.4939 - val_loss: 1.0123 - val_accuracy: 0.5391\n",
      "Epoch 53/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2249 - accuracy: 0.4939 - val_loss: 1.0122 - val_accuracy: 0.5391\n",
      "Epoch 54/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2238 - accuracy: 0.4978 - val_loss: 1.0088 - val_accuracy: 0.5391\n",
      "Epoch 55/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2225 - accuracy: 0.4961 - val_loss: 1.0123 - val_accuracy: 0.5391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2218 - accuracy: 0.4950 - val_loss: 1.0114 - val_accuracy: 0.5391\n",
      "Epoch 57/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2209 - accuracy: 0.4922 - val_loss: 1.0068 - val_accuracy: 0.5469\n",
      "Epoch 58/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2197 - accuracy: 0.4972 - val_loss: 1.0111 - val_accuracy: 0.5391\n",
      "Epoch 59/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2197 - accuracy: 0.4961 - val_loss: 1.0104 - val_accuracy: 0.5469\n",
      "Epoch 60/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2186 - accuracy: 0.4972 - val_loss: 1.0107 - val_accuracy: 0.5391\n",
      "Epoch 61/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2177 - accuracy: 0.4939 - val_loss: 1.0121 - val_accuracy: 0.5391\n",
      "Epoch 62/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2169 - accuracy: 0.4961 - val_loss: 1.0090 - val_accuracy: 0.5469\n",
      "Epoch 63/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2162 - accuracy: 0.4922 - val_loss: 1.0114 - val_accuracy: 0.5391\n",
      "Epoch 64/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2149 - accuracy: 0.4961 - val_loss: 1.0099 - val_accuracy: 0.5469\n",
      "Epoch 65/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2150 - accuracy: 0.4967 - val_loss: 1.0136 - val_accuracy: 0.5391\n",
      "Epoch 66/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2143 - accuracy: 0.4916 - val_loss: 1.0086 - val_accuracy: 0.5469\n",
      "Epoch 67/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2132 - accuracy: 0.4961 - val_loss: 1.0138 - val_accuracy: 0.5391\n",
      "Epoch 68/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2127 - accuracy: 0.4955 - val_loss: 1.0082 - val_accuracy: 0.5469\n",
      "Epoch 69/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2118 - accuracy: 0.4961 - val_loss: 1.0116 - val_accuracy: 0.5391\n",
      "Epoch 70/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2115 - accuracy: 0.4950 - val_loss: 1.0094 - val_accuracy: 0.5391\n",
      "Epoch 71/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2112 - accuracy: 0.4967 - val_loss: 1.0080 - val_accuracy: 0.5469\n",
      "Epoch 72/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2104 - accuracy: 0.4922 - val_loss: 1.0092 - val_accuracy: 0.5391\n",
      "Epoch 73/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2102 - accuracy: 0.4961 - val_loss: 1.0103 - val_accuracy: 0.5391\n",
      "Epoch 74/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2099 - accuracy: 0.4955 - val_loss: 1.0107 - val_accuracy: 0.5391\n",
      "Epoch 75/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2086 - accuracy: 0.4961 - val_loss: 1.0101 - val_accuracy: 0.5391\n",
      "Epoch 76/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2086 - accuracy: 0.4950 - val_loss: 1.0073 - val_accuracy: 0.5469\n",
      "Epoch 77/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2093 - accuracy: 0.4950 - val_loss: 1.0045 - val_accuracy: 0.5469\n",
      "Epoch 78/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2078 - accuracy: 0.4927 - val_loss: 1.0096 - val_accuracy: 0.5391\n",
      "Epoch 79/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2070 - accuracy: 0.4967 - val_loss: 1.0059 - val_accuracy: 0.5469\n",
      "Epoch 80/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2064 - accuracy: 0.4950 - val_loss: 1.0072 - val_accuracy: 0.5469\n",
      "Epoch 81/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2062 - accuracy: 0.4950 - val_loss: 1.0056 - val_accuracy: 0.5469\n",
      "Epoch 82/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2061 - accuracy: 0.4950 - val_loss: 1.0049 - val_accuracy: 0.5469\n",
      "Epoch 83/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2067 - accuracy: 0.4933 - val_loss: 1.0056 - val_accuracy: 0.5469\n",
      "Epoch 84/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2053 - accuracy: 0.4955 - val_loss: 1.0079 - val_accuracy: 0.5391\n",
      "Epoch 85/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2049 - accuracy: 0.4955 - val_loss: 1.0059 - val_accuracy: 0.5469\n",
      "Epoch 86/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2045 - accuracy: 0.4944 - val_loss: 1.0073 - val_accuracy: 0.5391\n",
      "Epoch 87/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2048 - accuracy: 0.4950 - val_loss: 1.0089 - val_accuracy: 0.5391\n",
      "Epoch 88/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2041 - accuracy: 0.4955 - val_loss: 1.0037 - val_accuracy: 0.5469\n",
      "Epoch 89/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2044 - accuracy: 0.4922 - val_loss: 1.0025 - val_accuracy: 0.5469\n",
      "Epoch 90/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2043 - accuracy: 0.4967 - val_loss: 1.0033 - val_accuracy: 0.5469\n",
      "Epoch 91/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2036 - accuracy: 0.4939 - val_loss: 1.0019 - val_accuracy: 0.5469\n",
      "Epoch 92/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2042 - accuracy: 0.4933 - val_loss: 1.0037 - val_accuracy: 0.5469\n",
      "Epoch 93/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2031 - accuracy: 0.4955 - val_loss: 1.0049 - val_accuracy: 0.5469\n",
      "Epoch 94/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2027 - accuracy: 0.4927 - val_loss: 1.0040 - val_accuracy: 0.5469\n",
      "Epoch 95/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2026 - accuracy: 0.4944 - val_loss: 1.0024 - val_accuracy: 0.5469\n",
      "Epoch 96/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2026 - accuracy: 0.4939 - val_loss: 1.0054 - val_accuracy: 0.5391\n",
      "Epoch 97/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2032 - accuracy: 0.4927 - val_loss: 1.0059 - val_accuracy: 0.5391\n",
      "Epoch 98/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2025 - accuracy: 0.4939 - val_loss: 1.0047 - val_accuracy: 0.5391\n",
      "Epoch 99/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2027 - accuracy: 0.4944 - val_loss: 1.0064 - val_accuracy: 0.5391\n",
      "Epoch 100/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2026 - accuracy: 0.4939 - val_loss: 1.0023 - val_accuracy: 0.5469\n",
      "Epoch 101/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2028 - accuracy: 0.4933 - val_loss: 1.0019 - val_accuracy: 0.5469\n",
      "Epoch 102/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2026 - accuracy: 0.4927 - val_loss: 1.0050 - val_accuracy: 0.5391\n",
      "Epoch 103/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2020 - accuracy: 0.4922 - val_loss: 1.0050 - val_accuracy: 0.5391\n",
      "Epoch 104/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2019 - accuracy: 0.4927 - val_loss: 1.0060 - val_accuracy: 0.5391\n",
      "Epoch 105/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2023 - accuracy: 0.4922 - val_loss: 1.0084 - val_accuracy: 0.5391\n",
      "Epoch 106/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2035 - accuracy: 0.4916 - val_loss: 1.0055 - val_accuracy: 0.5391\n",
      "Epoch 107/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2024 - accuracy: 0.4944 - val_loss: 1.0043 - val_accuracy: 0.5469\n",
      "Epoch 108/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2013 - accuracy: 0.4916 - val_loss: 1.0012 - val_accuracy: 0.5469\n",
      "Epoch 109/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2019 - accuracy: 0.4939 - val_loss: 1.0042 - val_accuracy: 0.5391\n",
      "Epoch 110/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2018 - accuracy: 0.4905 - val_loss: 1.0061 - val_accuracy: 0.5391\n",
      "Epoch 111/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2011 - accuracy: 0.4939 - val_loss: 1.0041 - val_accuracy: 0.5391\n",
      "Epoch 112/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2008 - accuracy: 0.4933 - val_loss: 1.0013 - val_accuracy: 0.5469\n",
      "Epoch 113/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2003 - accuracy: 0.4927 - val_loss: 1.0016 - val_accuracy: 0.5391\n",
      "Epoch 114/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2009 - accuracy: 0.4922 - val_loss: 1.0002 - val_accuracy: 0.5469\n",
      "Epoch 115/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2014 - accuracy: 0.4933 - val_loss: 1.0065 - val_accuracy: 0.5391\n",
      "Epoch 116/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2006 - accuracy: 0.4922 - val_loss: 1.0034 - val_accuracy: 0.5391\n",
      "Epoch 117/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2011 - accuracy: 0.4933 - val_loss: 1.0023 - val_accuracy: 0.5391\n",
      "Epoch 118/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2004 - accuracy: 0.4939 - val_loss: 1.0062 - val_accuracy: 0.5391\n",
      "Epoch 119/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1999 - accuracy: 0.4944 - val_loss: 0.9985 - val_accuracy: 0.5469\n",
      "Epoch 120/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1996 - accuracy: 0.4927 - val_loss: 1.0030 - val_accuracy: 0.5391\n",
      "Epoch 121/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1998 - accuracy: 0.4905 - val_loss: 1.0036 - val_accuracy: 0.5391\n",
      "Epoch 122/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1992 - accuracy: 0.4922 - val_loss: 1.0024 - val_accuracy: 0.5391\n",
      "Epoch 123/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1997 - accuracy: 0.4933 - val_loss: 0.9989 - val_accuracy: 0.5469\n",
      "Epoch 124/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2007 - accuracy: 0.4922 - val_loss: 1.0001 - val_accuracy: 0.5391\n",
      "Epoch 125/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1999 - accuracy: 0.4900 - val_loss: 1.0028 - val_accuracy: 0.5391\n",
      "Epoch 126/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2002 - accuracy: 0.4933 - val_loss: 1.0022 - val_accuracy: 0.5391\n",
      "Epoch 127/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1995 - accuracy: 0.4950 - val_loss: 1.0026 - val_accuracy: 0.5391\n",
      "Epoch 128/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1997 - accuracy: 0.4939 - val_loss: 1.0021 - val_accuracy: 0.5391\n",
      "Epoch 129/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1998 - accuracy: 0.4939 - val_loss: 1.0021 - val_accuracy: 0.5391\n",
      "Epoch 130/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1989 - accuracy: 0.4905 - val_loss: 1.0004 - val_accuracy: 0.5469\n",
      "Epoch 131/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1988 - accuracy: 0.4950 - val_loss: 1.0018 - val_accuracy: 0.5391\n",
      "Epoch 132/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1989 - accuracy: 0.4927 - val_loss: 0.9998 - val_accuracy: 0.5391\n",
      "Epoch 133/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1988 - accuracy: 0.4939 - val_loss: 1.0034 - val_accuracy: 0.5391\n",
      "Epoch 134/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1987 - accuracy: 0.4911 - val_loss: 1.0044 - val_accuracy: 0.5391\n",
      "Epoch 135/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1992 - accuracy: 0.4950 - val_loss: 1.0008 - val_accuracy: 0.5391\n",
      "Epoch 136/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1994 - accuracy: 0.4911 - val_loss: 1.0052 - val_accuracy: 0.5391\n",
      "Epoch 137/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1983 - accuracy: 0.4944 - val_loss: 1.0040 - val_accuracy: 0.5391\n",
      "Epoch 138/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1988 - accuracy: 0.4944 - val_loss: 1.0026 - val_accuracy: 0.5391\n",
      "Epoch 139/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1991 - accuracy: 0.4877 - val_loss: 1.0067 - val_accuracy: 0.5391\n",
      "Epoch 140/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1991 - accuracy: 0.4944 - val_loss: 1.0066 - val_accuracy: 0.5391\n",
      "Epoch 141/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1983 - accuracy: 0.4955 - val_loss: 1.0023 - val_accuracy: 0.5391\n",
      "Epoch 142/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1986 - accuracy: 0.4939 - val_loss: 1.0037 - val_accuracy: 0.5469\n",
      "Epoch 143/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1986 - accuracy: 0.4927 - val_loss: 1.0048 - val_accuracy: 0.5391\n",
      "Epoch 144/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1987 - accuracy: 0.4916 - val_loss: 1.0047 - val_accuracy: 0.5391\n",
      "Epoch 145/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1988 - accuracy: 0.4933 - val_loss: 1.0080 - val_accuracy: 0.5391\n",
      "Epoch 146/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1985 - accuracy: 0.4933 - val_loss: 1.0049 - val_accuracy: 0.5391\n",
      "Epoch 147/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1984 - accuracy: 0.4922 - val_loss: 1.0090 - val_accuracy: 0.5391\n",
      "Epoch 148/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1988 - accuracy: 0.4939 - val_loss: 1.0084 - val_accuracy: 0.5391\n",
      "Epoch 149/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1981 - accuracy: 0.4927 - val_loss: 1.0047 - val_accuracy: 0.5391\n",
      "Epoch 150/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1979 - accuracy: 0.4916 - val_loss: 1.0092 - val_accuracy: 0.5391\n",
      "Epoch 151/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1985 - accuracy: 0.4955 - val_loss: 1.0067 - val_accuracy: 0.5391\n",
      "Epoch 152/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1994 - accuracy: 0.4927 - val_loss: 1.0065 - val_accuracy: 0.5391\n",
      "Epoch 153/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1984 - accuracy: 0.4939 - val_loss: 1.0069 - val_accuracy: 0.5391\n",
      "Epoch 154/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1985 - accuracy: 0.4922 - val_loss: 1.0057 - val_accuracy: 0.5391\n",
      "Epoch 155/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1978 - accuracy: 0.4922 - val_loss: 1.0073 - val_accuracy: 0.5391\n",
      "Epoch 156/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1978 - accuracy: 0.4939 - val_loss: 1.0091 - val_accuracy: 0.5391\n",
      "Epoch 157/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1971 - accuracy: 0.4939 - val_loss: 1.0075 - val_accuracy: 0.5391\n",
      "Epoch 158/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1978 - accuracy: 0.4922 - val_loss: 1.0095 - val_accuracy: 0.5391\n",
      "Epoch 159/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1984 - accuracy: 0.4944 - val_loss: 1.0042 - val_accuracy: 0.5391\n",
      "Epoch 160/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1983 - accuracy: 0.4905 - val_loss: 1.0089 - val_accuracy: 0.5391\n",
      "Epoch 161/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1978 - accuracy: 0.4939 - val_loss: 1.0064 - val_accuracy: 0.5391\n",
      "Epoch 162/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1986 - accuracy: 0.4905 - val_loss: 1.0089 - val_accuracy: 0.5391\n",
      "Epoch 163/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1982 - accuracy: 0.4950 - val_loss: 1.0061 - val_accuracy: 0.5391\n",
      "Epoch 164/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1985 - accuracy: 0.4900 - val_loss: 1.0094 - val_accuracy: 0.5391\n",
      "Epoch 165/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1970 - accuracy: 0.4933 - val_loss: 1.0101 - val_accuracy: 0.5391\n",
      "Epoch 166/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1970 - accuracy: 0.4939 - val_loss: 1.0076 - val_accuracy: 0.5391\n",
      "Epoch 167/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1974 - accuracy: 0.4944 - val_loss: 1.0095 - val_accuracy: 0.5391\n",
      "Epoch 168/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1974 - accuracy: 0.4933 - val_loss: 1.0090 - val_accuracy: 0.5391\n",
      "Epoch 169/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1973 - accuracy: 0.4950 - val_loss: 1.0052 - val_accuracy: 0.5391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1976 - accuracy: 0.4922 - val_loss: 1.0105 - val_accuracy: 0.5391\n",
      "Epoch 171/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1976 - accuracy: 0.4944 - val_loss: 1.0082 - val_accuracy: 0.5391\n",
      "Epoch 172/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1973 - accuracy: 0.4933 - val_loss: 1.0073 - val_accuracy: 0.5391\n",
      "Epoch 173/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1979 - accuracy: 0.4933 - val_loss: 1.0086 - val_accuracy: 0.5391\n",
      "Epoch 174/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1969 - accuracy: 0.4939 - val_loss: 1.0127 - val_accuracy: 0.5391\n",
      "Epoch 175/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1982 - accuracy: 0.4950 - val_loss: 1.0138 - val_accuracy: 0.5391\n",
      "Epoch 176/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1982 - accuracy: 0.4955 - val_loss: 1.0095 - val_accuracy: 0.5391\n",
      "Epoch 177/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1970 - accuracy: 0.4961 - val_loss: 1.0076 - val_accuracy: 0.5391\n",
      "Epoch 178/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1966 - accuracy: 0.4939 - val_loss: 1.0085 - val_accuracy: 0.5391\n",
      "Epoch 179/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1973 - accuracy: 0.4933 - val_loss: 1.0077 - val_accuracy: 0.5391\n",
      "Epoch 180/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1979 - accuracy: 0.4950 - val_loss: 1.0087 - val_accuracy: 0.5391\n",
      "Epoch 181/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1972 - accuracy: 0.4939 - val_loss: 1.0110 - val_accuracy: 0.5391\n",
      "Epoch 182/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1972 - accuracy: 0.4950 - val_loss: 1.0067 - val_accuracy: 0.5391\n",
      "Epoch 183/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1973 - accuracy: 0.4955 - val_loss: 1.0088 - val_accuracy: 0.5391\n",
      "Epoch 184/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1972 - accuracy: 0.4961 - val_loss: 1.0078 - val_accuracy: 0.5391\n",
      "Epoch 185/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1969 - accuracy: 0.4933 - val_loss: 1.0106 - val_accuracy: 0.5391\n",
      "Epoch 186/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1975 - accuracy: 0.4950 - val_loss: 1.0102 - val_accuracy: 0.5391\n",
      "Epoch 187/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1971 - accuracy: 0.4950 - val_loss: 1.0080 - val_accuracy: 0.5391\n",
      "Epoch 188/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1970 - accuracy: 0.4961 - val_loss: 1.0092 - val_accuracy: 0.5391\n",
      "Epoch 189/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1972 - accuracy: 0.4933 - val_loss: 1.0081 - val_accuracy: 0.5391\n",
      "Epoch 190/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1969 - accuracy: 0.4944 - val_loss: 1.0086 - val_accuracy: 0.5469\n",
      "Epoch 191/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1965 - accuracy: 0.4927 - val_loss: 1.0099 - val_accuracy: 0.5391\n",
      "Epoch 192/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1963 - accuracy: 0.4955 - val_loss: 1.0083 - val_accuracy: 0.5391\n",
      "Epoch 193/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1964 - accuracy: 0.4916 - val_loss: 1.0116 - val_accuracy: 0.5391\n",
      "Epoch 194/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1975 - accuracy: 0.4939 - val_loss: 1.0109 - val_accuracy: 0.5391\n",
      "Epoch 195/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1982 - accuracy: 0.4961 - val_loss: 1.0082 - val_accuracy: 0.5391\n",
      "Epoch 196/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1964 - accuracy: 0.4922 - val_loss: 1.0092 - val_accuracy: 0.5391\n",
      "Epoch 197/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1969 - accuracy: 0.4944 - val_loss: 1.0095 - val_accuracy: 0.5391\n",
      "Epoch 198/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1963 - accuracy: 0.4955 - val_loss: 1.0053 - val_accuracy: 0.5469\n",
      "Epoch 199/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1964 - accuracy: 0.4922 - val_loss: 1.0114 - val_accuracy: 0.5391\n",
      "Epoch 200/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1965 - accuracy: 0.4955 - val_loss: 1.0109 - val_accuracy: 0.5391\n",
      "Epoch 201/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1961 - accuracy: 0.4939 - val_loss: 1.0093 - val_accuracy: 0.5391\n",
      "Epoch 202/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1969 - accuracy: 0.4927 - val_loss: 1.0101 - val_accuracy: 0.5391\n",
      "Epoch 203/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1967 - accuracy: 0.4944 - val_loss: 1.0130 - val_accuracy: 0.5469\n",
      "Epoch 204/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1966 - accuracy: 0.4955 - val_loss: 1.0070 - val_accuracy: 0.5391\n",
      "Epoch 205/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1970 - accuracy: 0.4944 - val_loss: 1.0065 - val_accuracy: 0.5469\n",
      "Epoch 206/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1962 - accuracy: 0.4933 - val_loss: 1.0097 - val_accuracy: 0.5469\n",
      "Epoch 207/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1966 - accuracy: 0.4950 - val_loss: 1.0112 - val_accuracy: 0.5391\n",
      "Epoch 208/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1965 - accuracy: 0.4944 - val_loss: 1.0097 - val_accuracy: 0.5391\n",
      "Epoch 209/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1967 - accuracy: 0.4944 - val_loss: 1.0085 - val_accuracy: 0.5391\n",
      "Epoch 210/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1960 - accuracy: 0.4955 - val_loss: 1.0087 - val_accuracy: 0.5469\n",
      "Epoch 211/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1959 - accuracy: 0.4927 - val_loss: 1.0087 - val_accuracy: 0.5391\n",
      "Epoch 212/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1957 - accuracy: 0.4950 - val_loss: 1.0078 - val_accuracy: 0.5391\n",
      "Epoch 213/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1960 - accuracy: 0.4955 - val_loss: 1.0112 - val_accuracy: 0.5391\n",
      "Epoch 214/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1963 - accuracy: 0.4933 - val_loss: 1.0082 - val_accuracy: 0.5391\n",
      "Epoch 215/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1953 - accuracy: 0.4933 - val_loss: 1.0094 - val_accuracy: 0.5391\n",
      "Epoch 216/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1965 - accuracy: 0.4950 - val_loss: 1.0051 - val_accuracy: 0.5391\n",
      "Epoch 217/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1956 - accuracy: 0.4927 - val_loss: 1.0103 - val_accuracy: 0.5391\n",
      "Epoch 218/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1954 - accuracy: 0.4955 - val_loss: 1.0098 - val_accuracy: 0.5391\n",
      "Epoch 219/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1952 - accuracy: 0.4944 - val_loss: 1.0100 - val_accuracy: 0.5469\n",
      "Epoch 220/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1953 - accuracy: 0.4927 - val_loss: 1.0111 - val_accuracy: 0.5469\n",
      "Epoch 221/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1957 - accuracy: 0.4961 - val_loss: 1.0113 - val_accuracy: 0.5469\n",
      "Epoch 222/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1954 - accuracy: 0.4939 - val_loss: 1.0089 - val_accuracy: 0.5469\n",
      "Epoch 223/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1961 - accuracy: 0.4939 - val_loss: 1.0099 - val_accuracy: 0.5391\n",
      "Epoch 224/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1958 - accuracy: 0.4955 - val_loss: 1.0078 - val_accuracy: 0.5391\n",
      "Epoch 225/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1953 - accuracy: 0.4950 - val_loss: 1.0085 - val_accuracy: 0.5391\n",
      "Epoch 226/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1961 - accuracy: 0.4933 - val_loss: 1.0077 - val_accuracy: 0.5391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1962 - accuracy: 0.4911 - val_loss: 1.0123 - val_accuracy: 0.5391\n",
      "Epoch 228/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1958 - accuracy: 0.4967 - val_loss: 1.0095 - val_accuracy: 0.5391\n",
      "Epoch 229/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1954 - accuracy: 0.4955 - val_loss: 1.0083 - val_accuracy: 0.5391\n",
      "Epoch 230/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1959 - accuracy: 0.4927 - val_loss: 1.0111 - val_accuracy: 0.5391\n",
      "Epoch 231/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1952 - accuracy: 0.4955 - val_loss: 1.0099 - val_accuracy: 0.5469\n",
      "Epoch 232/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1951 - accuracy: 0.4939 - val_loss: 1.0082 - val_accuracy: 0.5391\n",
      "Epoch 233/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1955 - accuracy: 0.4939 - val_loss: 1.0086 - val_accuracy: 0.5469\n",
      "Epoch 234/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1963 - accuracy: 0.4967 - val_loss: 1.0047 - val_accuracy: 0.5469\n",
      "Epoch 235/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1977 - accuracy: 0.4939 - val_loss: 1.0121 - val_accuracy: 0.5469\n",
      "Epoch 236/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1952 - accuracy: 0.4933 - val_loss: 1.0065 - val_accuracy: 0.5391\n",
      "Epoch 237/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1960 - accuracy: 0.4955 - val_loss: 1.0093 - val_accuracy: 0.5469\n",
      "Epoch 238/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1960 - accuracy: 0.4944 - val_loss: 1.0077 - val_accuracy: 0.5391\n",
      "Epoch 239/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1956 - accuracy: 0.4944 - val_loss: 1.0066 - val_accuracy: 0.5391\n",
      "Epoch 240/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1952 - accuracy: 0.4944 - val_loss: 1.0125 - val_accuracy: 0.5469\n",
      "Epoch 241/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1957 - accuracy: 0.4944 - val_loss: 1.0083 - val_accuracy: 0.5391\n",
      "Epoch 242/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1963 - accuracy: 0.4961 - val_loss: 1.0093 - val_accuracy: 0.5469\n",
      "Epoch 243/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1966 - accuracy: 0.4955 - val_loss: 1.0117 - val_accuracy: 0.5391\n",
      "Epoch 244/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1952 - accuracy: 0.4927 - val_loss: 1.0096 - val_accuracy: 0.5391\n",
      "Epoch 245/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1956 - accuracy: 0.4950 - val_loss: 1.0097 - val_accuracy: 0.5391\n",
      "Epoch 246/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1956 - accuracy: 0.4933 - val_loss: 1.0096 - val_accuracy: 0.5469\n",
      "Epoch 247/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1948 - accuracy: 0.4950 - val_loss: 1.0126 - val_accuracy: 0.5469\n",
      "Epoch 248/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1951 - accuracy: 0.4955 - val_loss: 1.0096 - val_accuracy: 0.5469\n",
      "Epoch 249/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1951 - accuracy: 0.4939 - val_loss: 1.0050 - val_accuracy: 0.5391\n",
      "Epoch 250/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1953 - accuracy: 0.4972 - val_loss: 1.0109 - val_accuracy: 0.5469\n",
      "Epoch 251/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1948 - accuracy: 0.4933 - val_loss: 1.0083 - val_accuracy: 0.5469\n",
      "Epoch 252/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1945 - accuracy: 0.4967 - val_loss: 1.0092 - val_accuracy: 0.5469\n",
      "Epoch 253/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1953 - accuracy: 0.4955 - val_loss: 1.0066 - val_accuracy: 0.5391\n",
      "Epoch 254/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1950 - accuracy: 0.4944 - val_loss: 1.0078 - val_accuracy: 0.5391\n",
      "Epoch 255/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1948 - accuracy: 0.4950 - val_loss: 1.0090 - val_accuracy: 0.5469\n",
      "Epoch 256/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1950 - accuracy: 0.4939 - val_loss: 1.0086 - val_accuracy: 0.5469\n",
      "Epoch 257/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1946 - accuracy: 0.4933 - val_loss: 1.0090 - val_accuracy: 0.5469\n",
      "Epoch 258/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1950 - accuracy: 0.4955 - val_loss: 1.0083 - val_accuracy: 0.5469\n",
      "Epoch 259/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1950 - accuracy: 0.4939 - val_loss: 1.0109 - val_accuracy: 0.5469\n",
      "Epoch 260/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1944 - accuracy: 0.4961 - val_loss: 1.0076 - val_accuracy: 0.5469\n",
      "Epoch 261/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1947 - accuracy: 0.4944 - val_loss: 1.0101 - val_accuracy: 0.5391\n",
      "Epoch 262/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1946 - accuracy: 0.4939 - val_loss: 1.0098 - val_accuracy: 0.5469\n",
      "Epoch 263/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1947 - accuracy: 0.4944 - val_loss: 1.0065 - val_accuracy: 0.5469\n",
      "Epoch 264/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1943 - accuracy: 0.4950 - val_loss: 1.0095 - val_accuracy: 0.5469\n",
      "Epoch 265/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1942 - accuracy: 0.4950 - val_loss: 1.0087 - val_accuracy: 0.5391\n",
      "Epoch 266/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1942 - accuracy: 0.4939 - val_loss: 1.0106 - val_accuracy: 0.5469\n",
      "Epoch 267/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1946 - accuracy: 0.4961 - val_loss: 1.0110 - val_accuracy: 0.5469\n",
      "Epoch 268/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1950 - accuracy: 0.4939 - val_loss: 1.0107 - val_accuracy: 0.5469\n",
      "Epoch 269/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1951 - accuracy: 0.4967 - val_loss: 1.0087 - val_accuracy: 0.5391\n",
      "Epoch 270/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1951 - accuracy: 0.4944 - val_loss: 1.0109 - val_accuracy: 0.5391\n",
      "Epoch 271/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1953 - accuracy: 0.4933 - val_loss: 1.0071 - val_accuracy: 0.5469\n",
      "Epoch 272/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1945 - accuracy: 0.4939 - val_loss: 1.0103 - val_accuracy: 0.5469\n",
      "Epoch 273/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1953 - accuracy: 0.4939 - val_loss: 1.0098 - val_accuracy: 0.5469\n",
      "Epoch 274/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1946 - accuracy: 0.4961 - val_loss: 1.0123 - val_accuracy: 0.5469\n",
      "Epoch 275/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1957 - accuracy: 0.4939 - val_loss: 1.0072 - val_accuracy: 0.5391\n",
      "Epoch 276/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1947 - accuracy: 0.4961 - val_loss: 1.0100 - val_accuracy: 0.5391\n",
      "Epoch 277/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1956 - accuracy: 0.4972 - val_loss: 1.0084 - val_accuracy: 0.5391\n",
      "Epoch 278/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1943 - accuracy: 0.4933 - val_loss: 1.0105 - val_accuracy: 0.5469\n",
      "Epoch 279/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1947 - accuracy: 0.4933 - val_loss: 1.0123 - val_accuracy: 0.5391\n",
      "Epoch 280/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1949 - accuracy: 0.4978 - val_loss: 1.0102 - val_accuracy: 0.5469\n",
      "Epoch 281/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1942 - accuracy: 0.4944 - val_loss: 1.0098 - val_accuracy: 0.5469\n",
      "Epoch 282/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1943 - accuracy: 0.4950 - val_loss: 1.0131 - val_accuracy: 0.5469\n",
      "Epoch 283/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1946 - accuracy: 0.4967 - val_loss: 1.0125 - val_accuracy: 0.5469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 284/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1946 - accuracy: 0.4955 - val_loss: 1.0069 - val_accuracy: 0.5391\n",
      "Epoch 285/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1938 - accuracy: 0.4939 - val_loss: 1.0106 - val_accuracy: 0.5391\n",
      "Epoch 286/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1942 - accuracy: 0.4983 - val_loss: 1.0081 - val_accuracy: 0.5469\n",
      "Epoch 287/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1954 - accuracy: 0.4933 - val_loss: 1.0119 - val_accuracy: 0.5469\n",
      "Epoch 288/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1945 - accuracy: 0.4950 - val_loss: 1.0102 - val_accuracy: 0.5469\n",
      "Epoch 289/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1938 - accuracy: 0.4978 - val_loss: 1.0085 - val_accuracy: 0.5391\n",
      "Epoch 290/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1949 - accuracy: 0.4972 - val_loss: 1.0084 - val_accuracy: 0.5469\n",
      "Epoch 291/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1940 - accuracy: 0.4955 - val_loss: 1.0090 - val_accuracy: 0.5469\n",
      "Epoch 292/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1939 - accuracy: 0.4967 - val_loss: 1.0086 - val_accuracy: 0.5391\n",
      "Epoch 293/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1943 - accuracy: 0.4967 - val_loss: 1.0065 - val_accuracy: 0.5391\n",
      "Epoch 294/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1939 - accuracy: 0.4955 - val_loss: 1.0104 - val_accuracy: 0.5469\n",
      "Epoch 295/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1939 - accuracy: 0.4989 - val_loss: 1.0109 - val_accuracy: 0.5391\n",
      "Epoch 296/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1934 - accuracy: 0.4961 - val_loss: 1.0082 - val_accuracy: 0.5469\n",
      "Epoch 297/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1945 - accuracy: 0.4967 - val_loss: 1.0118 - val_accuracy: 0.5469\n",
      "Epoch 298/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1938 - accuracy: 0.4944 - val_loss: 1.0093 - val_accuracy: 0.5469\n",
      "Epoch 299/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1939 - accuracy: 0.4944 - val_loss: 1.0112 - val_accuracy: 0.5469\n",
      "Epoch 300/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1944 - accuracy: 0.4961 - val_loss: 1.0090 - val_accuracy: 0.5391\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "56/56 [==============================] - 0s 1ms/step\n",
      "Recall: [0.94 0.   0.59]\n",
      "Precision: [0.41 0.   0.71]\n",
      "F1-score: [0.57 0.   0.65]\n",
      "Accuracy: 0.49\n",
      "Brier score:0.20429388013226557\n",
      "Brier climat:0.21295138888888893\n",
      "Brier skill score:0.040654859316933356\n",
      "Recall: [0.94 0.   0.59]\n",
      "Precision: [0.41 0.   0.71]\n",
      "F1-score: [0.57 0.   0.65]\n",
      "Accuracy: 0.49\n",
      "Brier score:0.22418705608317224\n",
      "Brier climat:0.22380704365079365\n",
      "Brier skill score:-0.0016979467052500308\n",
      "Recall: [0.94 0.   0.59]\n",
      "Precision: [0.41 0.   0.71]\n",
      "F1-score: [0.57 0.   0.65]\n",
      "Accuracy: 0.49\n",
      "Brier score:0.14806524670496055\n",
      "Brier climat:0.2211681547619048\n",
      "Brier skill score:0.33053089462921126\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "Recall: [0.71 0.   0.68]\n",
      "Precision: [0.31 0.   0.71]\n",
      "F1-score: [0.43 0.   0.69]\n",
      "Accuracy: 0.54\n",
      "Brier score:0.15802147508889058\n",
      "Brier climat:0.17206597222222225\n",
      "Brier skill score:0.08162274592673835\n",
      "Recall: [0.71 0.   0.68]\n",
      "Precision: [0.31 0.   0.71]\n",
      "F1-score: [0.43 0.   0.69]\n",
      "Accuracy: 0.54\n",
      "Brier score:0.171131435038092\n",
      "Brier climat:0.18649305555555556\n",
      "Brier skill score:0.08237100556748289\n",
      "Recall: [0.71 0.   0.68]\n",
      "Precision: [0.31 0.   0.71]\n",
      "F1-score: [0.43 0.   0.69]\n",
      "Accuracy: 0.54\n",
      "Brier score:0.24049625514894274\n",
      "Brier climat:0.2903645833333333\n",
      "Brier skill score:0.17174383876956034\n",
      "******** 13\n",
      "validation years [1994, 1995]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "{0: 2.0, 1: 1.0, 2: 1.0}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 60)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 183       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183\n",
      "Trainable params: 183\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 2.0578 - accuracy: 0.2963WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "28/28 [==============================] - 4s 108ms/step - loss: 2.0472 - accuracy: 0.3019 - val_loss: 1.1802 - val_accuracy: 0.6172\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.7827 - accuracy: 0.4079 - val_loss: 1.0699 - val_accuracy: 0.7109\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.6441 - accuracy: 0.4369 - val_loss: 1.0387 - val_accuracy: 0.7031\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.5609 - accuracy: 0.4693 - val_loss: 1.0282 - val_accuracy: 0.7109\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.5109 - accuracy: 0.4883 - val_loss: 1.0102 - val_accuracy: 0.6953\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.4806 - accuracy: 0.4827 - val_loss: 1.0088 - val_accuracy: 0.6953\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4614 - accuracy: 0.4794 - val_loss: 0.9990 - val_accuracy: 0.6875\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4476 - accuracy: 0.4849 - val_loss: 1.0012 - val_accuracy: 0.6875\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4352 - accuracy: 0.4894 - val_loss: 0.9911 - val_accuracy: 0.6719\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.4244 - accuracy: 0.4883 - val_loss: 0.9912 - val_accuracy: 0.6719\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4165 - accuracy: 0.4849 - val_loss: 0.9856 - val_accuracy: 0.6641\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.4078 - accuracy: 0.4821 - val_loss: 0.9862 - val_accuracy: 0.6641\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.4017 - accuracy: 0.4771 - val_loss: 0.9800 - val_accuracy: 0.6641\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3922 - accuracy: 0.4788 - val_loss: 0.9801 - val_accuracy: 0.6562\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3855 - accuracy: 0.4777 - val_loss: 0.9785 - val_accuracy: 0.6562\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3786 - accuracy: 0.4810 - val_loss: 0.9717 - val_accuracy: 0.6562\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3716 - accuracy: 0.4821 - val_loss: 0.9709 - val_accuracy: 0.6562\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3651 - accuracy: 0.4816 - val_loss: 0.9649 - val_accuracy: 0.6562\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3588 - accuracy: 0.4788 - val_loss: 0.9654 - val_accuracy: 0.6562\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3519 - accuracy: 0.4799 - val_loss: 0.9617 - val_accuracy: 0.6562\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3465 - accuracy: 0.4788 - val_loss: 0.9575 - val_accuracy: 0.6562\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3408 - accuracy: 0.4799 - val_loss: 0.9556 - val_accuracy: 0.6562\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3362 - accuracy: 0.4799 - val_loss: 0.9531 - val_accuracy: 0.6562\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3312 - accuracy: 0.4827 - val_loss: 0.9513 - val_accuracy: 0.6562\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3260 - accuracy: 0.4844 - val_loss: 0.9533 - val_accuracy: 0.6484\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3216 - accuracy: 0.4821 - val_loss: 0.9524 - val_accuracy: 0.6484\n",
      "Epoch 27/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3168 - accuracy: 0.4816 - val_loss: 0.9404 - val_accuracy: 0.6562\n",
      "Epoch 28/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3128 - accuracy: 0.4821 - val_loss: 0.9423 - val_accuracy: 0.6562\n",
      "Epoch 29/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3093 - accuracy: 0.4838 - val_loss: 0.9424 - val_accuracy: 0.6562\n",
      "Epoch 30/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.3040 - accuracy: 0.4821 - val_loss: 0.9382 - val_accuracy: 0.6562\n",
      "Epoch 31/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2999 - accuracy: 0.4827 - val_loss: 0.9414 - val_accuracy: 0.6484\n",
      "Epoch 32/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2957 - accuracy: 0.4810 - val_loss: 0.9396 - val_accuracy: 0.6484\n",
      "Epoch 33/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2924 - accuracy: 0.4810 - val_loss: 0.9353 - val_accuracy: 0.6484\n",
      "Epoch 34/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2881 - accuracy: 0.4827 - val_loss: 0.9355 - val_accuracy: 0.6484\n",
      "Epoch 35/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2851 - accuracy: 0.4805 - val_loss: 0.9308 - val_accuracy: 0.6484\n",
      "Epoch 36/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2815 - accuracy: 0.4799 - val_loss: 0.9297 - val_accuracy: 0.6484\n",
      "Epoch 37/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2782 - accuracy: 0.4782 - val_loss: 0.9218 - val_accuracy: 0.6484\n",
      "Epoch 38/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2752 - accuracy: 0.4794 - val_loss: 0.9260 - val_accuracy: 0.6484\n",
      "Epoch 39/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2722 - accuracy: 0.4782 - val_loss: 0.9232 - val_accuracy: 0.6484\n",
      "Epoch 40/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2693 - accuracy: 0.4794 - val_loss: 0.9208 - val_accuracy: 0.6484\n",
      "Epoch 41/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2672 - accuracy: 0.4788 - val_loss: 0.9178 - val_accuracy: 0.6484\n",
      "Epoch 42/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2648 - accuracy: 0.4799 - val_loss: 0.9168 - val_accuracy: 0.6484\n",
      "Epoch 43/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2635 - accuracy: 0.4777 - val_loss: 0.9223 - val_accuracy: 0.6484\n",
      "Epoch 44/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2603 - accuracy: 0.4799 - val_loss: 0.9181 - val_accuracy: 0.6484\n",
      "Epoch 45/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2590 - accuracy: 0.4799 - val_loss: 0.9159 - val_accuracy: 0.6484\n",
      "Epoch 46/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2568 - accuracy: 0.4777 - val_loss: 0.9143 - val_accuracy: 0.6484\n",
      "Epoch 47/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2538 - accuracy: 0.4771 - val_loss: 0.9179 - val_accuracy: 0.6484\n",
      "Epoch 48/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2512 - accuracy: 0.4766 - val_loss: 0.9129 - val_accuracy: 0.6484\n",
      "Epoch 49/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2499 - accuracy: 0.4777 - val_loss: 0.9143 - val_accuracy: 0.6484\n",
      "Epoch 50/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2478 - accuracy: 0.4760 - val_loss: 0.9122 - val_accuracy: 0.6484\n",
      "Epoch 51/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2463 - accuracy: 0.4777 - val_loss: 0.9096 - val_accuracy: 0.6484\n",
      "Epoch 52/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2449 - accuracy: 0.4777 - val_loss: 0.9076 - val_accuracy: 0.6484\n",
      "Epoch 53/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2431 - accuracy: 0.4782 - val_loss: 0.9063 - val_accuracy: 0.6484\n",
      "Epoch 54/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2416 - accuracy: 0.4777 - val_loss: 0.9037 - val_accuracy: 0.6484\n",
      "Epoch 55/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2401 - accuracy: 0.4788 - val_loss: 0.9003 - val_accuracy: 0.6484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2389 - accuracy: 0.4782 - val_loss: 0.9054 - val_accuracy: 0.6484\n",
      "Epoch 57/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2381 - accuracy: 0.4794 - val_loss: 0.8978 - val_accuracy: 0.6484\n",
      "Epoch 58/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2363 - accuracy: 0.4816 - val_loss: 0.8954 - val_accuracy: 0.6484\n",
      "Epoch 59/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2348 - accuracy: 0.4794 - val_loss: 0.8973 - val_accuracy: 0.6484\n",
      "Epoch 60/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2341 - accuracy: 0.4810 - val_loss: 0.8930 - val_accuracy: 0.6484\n",
      "Epoch 61/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2336 - accuracy: 0.4794 - val_loss: 0.8939 - val_accuracy: 0.6484\n",
      "Epoch 62/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2320 - accuracy: 0.4810 - val_loss: 0.8932 - val_accuracy: 0.6484\n",
      "Epoch 63/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2319 - accuracy: 0.4816 - val_loss: 0.8929 - val_accuracy: 0.6484\n",
      "Epoch 64/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2308 - accuracy: 0.4782 - val_loss: 0.8943 - val_accuracy: 0.6484\n",
      "Epoch 65/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2297 - accuracy: 0.4782 - val_loss: 0.8946 - val_accuracy: 0.6484\n",
      "Epoch 66/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2288 - accuracy: 0.4816 - val_loss: 0.8895 - val_accuracy: 0.6484\n",
      "Epoch 67/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2275 - accuracy: 0.4788 - val_loss: 0.8917 - val_accuracy: 0.6484\n",
      "Epoch 68/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2272 - accuracy: 0.4805 - val_loss: 0.8915 - val_accuracy: 0.6484\n",
      "Epoch 69/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2276 - accuracy: 0.4794 - val_loss: 0.8884 - val_accuracy: 0.6484\n",
      "Epoch 70/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2260 - accuracy: 0.4782 - val_loss: 0.8889 - val_accuracy: 0.6484\n",
      "Epoch 71/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2246 - accuracy: 0.4788 - val_loss: 0.8937 - val_accuracy: 0.6484\n",
      "Epoch 72/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2243 - accuracy: 0.4782 - val_loss: 0.8910 - val_accuracy: 0.6484\n",
      "Epoch 73/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2244 - accuracy: 0.4794 - val_loss: 0.8910 - val_accuracy: 0.6484\n",
      "Epoch 74/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2243 - accuracy: 0.4782 - val_loss: 0.8915 - val_accuracy: 0.6484\n",
      "Epoch 75/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2236 - accuracy: 0.4799 - val_loss: 0.8944 - val_accuracy: 0.6484\n",
      "Epoch 76/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2234 - accuracy: 0.4794 - val_loss: 0.8940 - val_accuracy: 0.6484\n",
      "Epoch 77/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2220 - accuracy: 0.4782 - val_loss: 0.8898 - val_accuracy: 0.6484\n",
      "Epoch 78/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2222 - accuracy: 0.4799 - val_loss: 0.8857 - val_accuracy: 0.6484\n",
      "Epoch 79/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2210 - accuracy: 0.4805 - val_loss: 0.8912 - val_accuracy: 0.6484\n",
      "Epoch 80/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2214 - accuracy: 0.4805 - val_loss: 0.8908 - val_accuracy: 0.6484\n",
      "Epoch 81/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2204 - accuracy: 0.4805 - val_loss: 0.8874 - val_accuracy: 0.6484\n",
      "Epoch 82/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2203 - accuracy: 0.4788 - val_loss: 0.8903 - val_accuracy: 0.6484\n",
      "Epoch 83/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2195 - accuracy: 0.4799 - val_loss: 0.8903 - val_accuracy: 0.6484\n",
      "Epoch 84/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2203 - accuracy: 0.4805 - val_loss: 0.8861 - val_accuracy: 0.6484\n",
      "Epoch 85/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2189 - accuracy: 0.4805 - val_loss: 0.8975 - val_accuracy: 0.6484\n",
      "Epoch 86/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2182 - accuracy: 0.4805 - val_loss: 0.8855 - val_accuracy: 0.6484\n",
      "Epoch 87/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2177 - accuracy: 0.4799 - val_loss: 0.8925 - val_accuracy: 0.6484\n",
      "Epoch 88/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2170 - accuracy: 0.4805 - val_loss: 0.8865 - val_accuracy: 0.6484\n",
      "Epoch 89/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2174 - accuracy: 0.4805 - val_loss: 0.8871 - val_accuracy: 0.6484\n",
      "Epoch 90/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2167 - accuracy: 0.4794 - val_loss: 0.8914 - val_accuracy: 0.6484\n",
      "Epoch 91/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2162 - accuracy: 0.4827 - val_loss: 0.8859 - val_accuracy: 0.6484\n",
      "Epoch 92/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2160 - accuracy: 0.4838 - val_loss: 0.8880 - val_accuracy: 0.6484\n",
      "Epoch 93/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2158 - accuracy: 0.4794 - val_loss: 0.8870 - val_accuracy: 0.6484\n",
      "Epoch 94/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2158 - accuracy: 0.4810 - val_loss: 0.8868 - val_accuracy: 0.6484\n",
      "Epoch 95/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2158 - accuracy: 0.4821 - val_loss: 0.8857 - val_accuracy: 0.6484\n",
      "Epoch 96/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2152 - accuracy: 0.4816 - val_loss: 0.8870 - val_accuracy: 0.6484\n",
      "Epoch 97/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2141 - accuracy: 0.4833 - val_loss: 0.8881 - val_accuracy: 0.6484\n",
      "Epoch 98/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2137 - accuracy: 0.4833 - val_loss: 0.8831 - val_accuracy: 0.6484\n",
      "Epoch 99/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2132 - accuracy: 0.4810 - val_loss: 0.8895 - val_accuracy: 0.6484\n",
      "Epoch 100/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2130 - accuracy: 0.4821 - val_loss: 0.8829 - val_accuracy: 0.6484\n",
      "Epoch 101/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2125 - accuracy: 0.4810 - val_loss: 0.8852 - val_accuracy: 0.6484\n",
      "Epoch 102/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2136 - accuracy: 0.4827 - val_loss: 0.8837 - val_accuracy: 0.6484\n",
      "Epoch 103/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2132 - accuracy: 0.4821 - val_loss: 0.8868 - val_accuracy: 0.6484\n",
      "Epoch 104/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2119 - accuracy: 0.4833 - val_loss: 0.8833 - val_accuracy: 0.6484\n",
      "Epoch 105/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2119 - accuracy: 0.4799 - val_loss: 0.8840 - val_accuracy: 0.6484\n",
      "Epoch 106/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2116 - accuracy: 0.4827 - val_loss: 0.8868 - val_accuracy: 0.6484\n",
      "Epoch 107/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2112 - accuracy: 0.4799 - val_loss: 0.8888 - val_accuracy: 0.6484\n",
      "Epoch 108/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2112 - accuracy: 0.4821 - val_loss: 0.8848 - val_accuracy: 0.6484\n",
      "Epoch 109/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2118 - accuracy: 0.4833 - val_loss: 0.8840 - val_accuracy: 0.6484\n",
      "Epoch 110/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2109 - accuracy: 0.4827 - val_loss: 0.8817 - val_accuracy: 0.6484\n",
      "Epoch 111/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2108 - accuracy: 0.4816 - val_loss: 0.8836 - val_accuracy: 0.6484\n",
      "Epoch 112/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2107 - accuracy: 0.4821 - val_loss: 0.8825 - val_accuracy: 0.6484\n",
      "Epoch 113/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2109 - accuracy: 0.4794 - val_loss: 0.8847 - val_accuracy: 0.6484\n",
      "Epoch 114/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2095 - accuracy: 0.4827 - val_loss: 0.8840 - val_accuracy: 0.6484\n",
      "Epoch 115/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2094 - accuracy: 0.4821 - val_loss: 0.8833 - val_accuracy: 0.6484\n",
      "Epoch 116/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2098 - accuracy: 0.4838 - val_loss: 0.8870 - val_accuracy: 0.6484\n",
      "Epoch 117/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2105 - accuracy: 0.4816 - val_loss: 0.8866 - val_accuracy: 0.6484\n",
      "Epoch 118/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2092 - accuracy: 0.4827 - val_loss: 0.8838 - val_accuracy: 0.6484\n",
      "Epoch 119/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2093 - accuracy: 0.4827 - val_loss: 0.8798 - val_accuracy: 0.6484\n",
      "Epoch 120/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2086 - accuracy: 0.4827 - val_loss: 0.8846 - val_accuracy: 0.6484\n",
      "Epoch 121/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2083 - accuracy: 0.4827 - val_loss: 0.8816 - val_accuracy: 0.6484\n",
      "Epoch 122/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2083 - accuracy: 0.4816 - val_loss: 0.8836 - val_accuracy: 0.6484\n",
      "Epoch 123/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2085 - accuracy: 0.4816 - val_loss: 0.8829 - val_accuracy: 0.6484\n",
      "Epoch 124/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2084 - accuracy: 0.4810 - val_loss: 0.8835 - val_accuracy: 0.6484\n",
      "Epoch 125/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2081 - accuracy: 0.4833 - val_loss: 0.8808 - val_accuracy: 0.6484\n",
      "Epoch 126/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2077 - accuracy: 0.4827 - val_loss: 0.8826 - val_accuracy: 0.6484\n",
      "Epoch 127/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2078 - accuracy: 0.4833 - val_loss: 0.8789 - val_accuracy: 0.6484\n",
      "Epoch 128/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2078 - accuracy: 0.4794 - val_loss: 0.8859 - val_accuracy: 0.6484\n",
      "Epoch 129/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2084 - accuracy: 0.4827 - val_loss: 0.8799 - val_accuracy: 0.6484\n",
      "Epoch 130/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2073 - accuracy: 0.4833 - val_loss: 0.8806 - val_accuracy: 0.6484\n",
      "Epoch 131/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2077 - accuracy: 0.4805 - val_loss: 0.8796 - val_accuracy: 0.6484\n",
      "Epoch 132/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2076 - accuracy: 0.4827 - val_loss: 0.8812 - val_accuracy: 0.6484\n",
      "Epoch 133/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2075 - accuracy: 0.4827 - val_loss: 0.8811 - val_accuracy: 0.6484\n",
      "Epoch 134/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2073 - accuracy: 0.4833 - val_loss: 0.8822 - val_accuracy: 0.6484\n",
      "Epoch 135/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2072 - accuracy: 0.4788 - val_loss: 0.8794 - val_accuracy: 0.6484\n",
      "Epoch 136/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2072 - accuracy: 0.4844 - val_loss: 0.8815 - val_accuracy: 0.6484\n",
      "Epoch 137/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2068 - accuracy: 0.4821 - val_loss: 0.8805 - val_accuracy: 0.6484\n",
      "Epoch 138/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2070 - accuracy: 0.4821 - val_loss: 0.8798 - val_accuracy: 0.6484\n",
      "Epoch 139/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2066 - accuracy: 0.4816 - val_loss: 0.8809 - val_accuracy: 0.6484\n",
      "Epoch 140/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2065 - accuracy: 0.4833 - val_loss: 0.8813 - val_accuracy: 0.6484\n",
      "Epoch 141/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2067 - accuracy: 0.4827 - val_loss: 0.8800 - val_accuracy: 0.6484\n",
      "Epoch 142/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2066 - accuracy: 0.4821 - val_loss: 0.8862 - val_accuracy: 0.6484\n",
      "Epoch 143/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2062 - accuracy: 0.4816 - val_loss: 0.8813 - val_accuracy: 0.6484\n",
      "Epoch 144/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2062 - accuracy: 0.4816 - val_loss: 0.8816 - val_accuracy: 0.6484\n",
      "Epoch 145/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2062 - accuracy: 0.4821 - val_loss: 0.8814 - val_accuracy: 0.6484\n",
      "Epoch 146/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2059 - accuracy: 0.4821 - val_loss: 0.8789 - val_accuracy: 0.6484\n",
      "Epoch 147/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2067 - accuracy: 0.4821 - val_loss: 0.8840 - val_accuracy: 0.6484\n",
      "Epoch 148/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2068 - accuracy: 0.4833 - val_loss: 0.8750 - val_accuracy: 0.6484\n",
      "Epoch 149/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2063 - accuracy: 0.4821 - val_loss: 0.8791 - val_accuracy: 0.6484\n",
      "Epoch 150/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2061 - accuracy: 0.4821 - val_loss: 0.8792 - val_accuracy: 0.6484\n",
      "Epoch 151/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2056 - accuracy: 0.4833 - val_loss: 0.8830 - val_accuracy: 0.6484\n",
      "Epoch 152/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2057 - accuracy: 0.4827 - val_loss: 0.8800 - val_accuracy: 0.6484\n",
      "Epoch 153/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2071 - accuracy: 0.4805 - val_loss: 0.8819 - val_accuracy: 0.6484\n",
      "Epoch 154/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2066 - accuracy: 0.4816 - val_loss: 0.8807 - val_accuracy: 0.6484\n",
      "Epoch 155/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2055 - accuracy: 0.4838 - val_loss: 0.8830 - val_accuracy: 0.6484\n",
      "Epoch 156/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2056 - accuracy: 0.4827 - val_loss: 0.8784 - val_accuracy: 0.6484\n",
      "Epoch 157/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2056 - accuracy: 0.4816 - val_loss: 0.8819 - val_accuracy: 0.6484\n",
      "Epoch 158/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2063 - accuracy: 0.4816 - val_loss: 0.8849 - val_accuracy: 0.6484\n",
      "Epoch 159/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2054 - accuracy: 0.4844 - val_loss: 0.8804 - val_accuracy: 0.6484\n",
      "Epoch 160/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2062 - accuracy: 0.4844 - val_loss: 0.8818 - val_accuracy: 0.6484\n",
      "Epoch 161/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2051 - accuracy: 0.4827 - val_loss: 0.8804 - val_accuracy: 0.6484\n",
      "Epoch 162/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2056 - accuracy: 0.4816 - val_loss: 0.8792 - val_accuracy: 0.6484\n",
      "Epoch 163/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2057 - accuracy: 0.4821 - val_loss: 0.8799 - val_accuracy: 0.6484\n",
      "Epoch 164/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2058 - accuracy: 0.4833 - val_loss: 0.8809 - val_accuracy: 0.6484\n",
      "Epoch 165/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2071 - accuracy: 0.4821 - val_loss: 0.8821 - val_accuracy: 0.6484\n",
      "Epoch 166/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2064 - accuracy: 0.4827 - val_loss: 0.8860 - val_accuracy: 0.6484\n",
      "Epoch 167/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2056 - accuracy: 0.4821 - val_loss: 0.8833 - val_accuracy: 0.6484\n",
      "Epoch 168/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2048 - accuracy: 0.4838 - val_loss: 0.8808 - val_accuracy: 0.6484\n",
      "Epoch 169/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2052 - accuracy: 0.4833 - val_loss: 0.8861 - val_accuracy: 0.6484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2058 - accuracy: 0.4833 - val_loss: 0.8789 - val_accuracy: 0.6484\n",
      "Epoch 171/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2060 - accuracy: 0.4844 - val_loss: 0.8799 - val_accuracy: 0.6484\n",
      "Epoch 172/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2053 - accuracy: 0.4816 - val_loss: 0.8810 - val_accuracy: 0.6484\n",
      "Epoch 173/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2052 - accuracy: 0.4838 - val_loss: 0.8815 - val_accuracy: 0.6484\n",
      "Epoch 174/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2056 - accuracy: 0.4827 - val_loss: 0.8803 - val_accuracy: 0.6484\n",
      "Epoch 175/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2056 - accuracy: 0.4838 - val_loss: 0.8855 - val_accuracy: 0.6484\n",
      "Epoch 176/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2050 - accuracy: 0.4827 - val_loss: 0.8835 - val_accuracy: 0.6484\n",
      "Epoch 177/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2054 - accuracy: 0.4810 - val_loss: 0.8837 - val_accuracy: 0.6484\n",
      "Epoch 178/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2054 - accuracy: 0.4838 - val_loss: 0.8863 - val_accuracy: 0.6484\n",
      "Epoch 179/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2054 - accuracy: 0.4833 - val_loss: 0.8859 - val_accuracy: 0.6484\n",
      "Epoch 180/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2050 - accuracy: 0.4833 - val_loss: 0.8846 - val_accuracy: 0.6484\n",
      "Epoch 181/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2053 - accuracy: 0.4821 - val_loss: 0.8858 - val_accuracy: 0.6484\n",
      "Epoch 182/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2052 - accuracy: 0.4838 - val_loss: 0.8856 - val_accuracy: 0.6484\n",
      "Epoch 183/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2074 - accuracy: 0.4827 - val_loss: 0.8868 - val_accuracy: 0.6484\n",
      "Epoch 184/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2051 - accuracy: 0.4838 - val_loss: 0.8812 - val_accuracy: 0.6484\n",
      "Epoch 185/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2058 - accuracy: 0.4833 - val_loss: 0.8832 - val_accuracy: 0.6484\n",
      "Epoch 186/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2049 - accuracy: 0.4838 - val_loss: 0.8833 - val_accuracy: 0.6484\n",
      "Epoch 187/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2045 - accuracy: 0.4827 - val_loss: 0.8801 - val_accuracy: 0.6484\n",
      "Epoch 188/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2051 - accuracy: 0.4838 - val_loss: 0.8807 - val_accuracy: 0.6484\n",
      "Epoch 189/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2046 - accuracy: 0.4838 - val_loss: 0.8847 - val_accuracy: 0.6484\n",
      "Epoch 190/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2050 - accuracy: 0.4833 - val_loss: 0.8846 - val_accuracy: 0.6484\n",
      "Epoch 191/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2046 - accuracy: 0.4849 - val_loss: 0.8796 - val_accuracy: 0.6484\n",
      "Epoch 192/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2055 - accuracy: 0.4844 - val_loss: 0.8810 - val_accuracy: 0.6484\n",
      "Epoch 193/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2054 - accuracy: 0.4821 - val_loss: 0.8858 - val_accuracy: 0.6484\n",
      "Epoch 194/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2054 - accuracy: 0.4838 - val_loss: 0.8756 - val_accuracy: 0.6484\n",
      "Epoch 195/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2056 - accuracy: 0.4794 - val_loss: 0.8812 - val_accuracy: 0.6484\n",
      "Epoch 196/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2047 - accuracy: 0.4833 - val_loss: 0.8870 - val_accuracy: 0.6484\n",
      "Epoch 197/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2054 - accuracy: 0.4833 - val_loss: 0.8878 - val_accuracy: 0.6484\n",
      "Epoch 198/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2038 - accuracy: 0.4827 - val_loss: 0.8835 - val_accuracy: 0.6484\n",
      "Epoch 199/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2051 - accuracy: 0.4833 - val_loss: 0.8829 - val_accuracy: 0.6484\n",
      "Epoch 200/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2044 - accuracy: 0.4833 - val_loss: 0.8844 - val_accuracy: 0.6484\n",
      "Epoch 201/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2051 - accuracy: 0.4827 - val_loss: 0.8874 - val_accuracy: 0.6484\n",
      "Epoch 202/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2052 - accuracy: 0.4810 - val_loss: 0.8821 - val_accuracy: 0.6484\n",
      "Epoch 203/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2043 - accuracy: 0.4833 - val_loss: 0.8832 - val_accuracy: 0.6484\n",
      "Epoch 204/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2057 - accuracy: 0.4827 - val_loss: 0.8859 - val_accuracy: 0.6484\n",
      "Epoch 205/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2055 - accuracy: 0.4827 - val_loss: 0.8834 - val_accuracy: 0.6484\n",
      "Epoch 206/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2045 - accuracy: 0.4838 - val_loss: 0.8815 - val_accuracy: 0.6484\n",
      "Epoch 207/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2054 - accuracy: 0.4827 - val_loss: 0.8807 - val_accuracy: 0.6484\n",
      "Epoch 208/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2056 - accuracy: 0.4816 - val_loss: 0.8888 - val_accuracy: 0.6484\n",
      "Epoch 209/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2043 - accuracy: 0.4838 - val_loss: 0.8854 - val_accuracy: 0.6484\n",
      "Epoch 210/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2049 - accuracy: 0.4827 - val_loss: 0.8828 - val_accuracy: 0.6484\n",
      "Epoch 211/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2049 - accuracy: 0.4827 - val_loss: 0.8851 - val_accuracy: 0.6484\n",
      "Epoch 212/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2045 - accuracy: 0.4844 - val_loss: 0.8751 - val_accuracy: 0.6484\n",
      "Epoch 213/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2050 - accuracy: 0.4855 - val_loss: 0.8849 - val_accuracy: 0.6484\n",
      "Epoch 214/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2042 - accuracy: 0.4844 - val_loss: 0.8857 - val_accuracy: 0.6484\n",
      "Epoch 215/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2042 - accuracy: 0.4821 - val_loss: 0.8868 - val_accuracy: 0.6484\n",
      "Epoch 216/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2055 - accuracy: 0.4838 - val_loss: 0.8886 - val_accuracy: 0.6484\n",
      "Epoch 217/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2051 - accuracy: 0.4838 - val_loss: 0.8838 - val_accuracy: 0.6484\n",
      "Epoch 218/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2036 - accuracy: 0.4844 - val_loss: 0.8888 - val_accuracy: 0.6484\n",
      "Epoch 219/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2042 - accuracy: 0.4827 - val_loss: 0.8820 - val_accuracy: 0.6484\n",
      "Epoch 220/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2039 - accuracy: 0.4827 - val_loss: 0.8831 - val_accuracy: 0.6484\n",
      "Epoch 221/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2041 - accuracy: 0.4838 - val_loss: 0.8859 - val_accuracy: 0.6484\n",
      "Epoch 222/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2039 - accuracy: 0.4855 - val_loss: 0.8859 - val_accuracy: 0.6484\n",
      "Epoch 223/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2039 - accuracy: 0.4849 - val_loss: 0.8843 - val_accuracy: 0.6484\n",
      "Epoch 224/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2041 - accuracy: 0.4855 - val_loss: 0.8826 - val_accuracy: 0.6484\n",
      "Epoch 225/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2039 - accuracy: 0.4821 - val_loss: 0.8842 - val_accuracy: 0.6484\n",
      "Epoch 226/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2042 - accuracy: 0.4833 - val_loss: 0.8835 - val_accuracy: 0.6484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2042 - accuracy: 0.4821 - val_loss: 0.8850 - val_accuracy: 0.6484\n",
      "Epoch 228/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2039 - accuracy: 0.4844 - val_loss: 0.8838 - val_accuracy: 0.6484\n",
      "Epoch 229/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2041 - accuracy: 0.4838 - val_loss: 0.8824 - val_accuracy: 0.6484\n",
      "Epoch 230/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2046 - accuracy: 0.4838 - val_loss: 0.8817 - val_accuracy: 0.6484\n",
      "Epoch 231/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2053 - accuracy: 0.4827 - val_loss: 0.8848 - val_accuracy: 0.6484\n",
      "Epoch 232/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2038 - accuracy: 0.4849 - val_loss: 0.8841 - val_accuracy: 0.6484\n",
      "Epoch 233/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2043 - accuracy: 0.4827 - val_loss: 0.8874 - val_accuracy: 0.6484\n",
      "Epoch 234/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2049 - accuracy: 0.4827 - val_loss: 0.8853 - val_accuracy: 0.6484\n",
      "Epoch 235/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2039 - accuracy: 0.4855 - val_loss: 0.8856 - val_accuracy: 0.6484\n",
      "Epoch 236/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2044 - accuracy: 0.4805 - val_loss: 0.8839 - val_accuracy: 0.6484\n",
      "Epoch 237/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2045 - accuracy: 0.4833 - val_loss: 0.8802 - val_accuracy: 0.6484\n",
      "Epoch 238/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2040 - accuracy: 0.4838 - val_loss: 0.8865 - val_accuracy: 0.6484\n",
      "Epoch 239/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2034 - accuracy: 0.4844 - val_loss: 0.8851 - val_accuracy: 0.6484\n",
      "Epoch 240/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2042 - accuracy: 0.4838 - val_loss: 0.8860 - val_accuracy: 0.6484\n",
      "Epoch 241/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2035 - accuracy: 0.4849 - val_loss: 0.8822 - val_accuracy: 0.6484\n",
      "Epoch 242/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2034 - accuracy: 0.4849 - val_loss: 0.8864 - val_accuracy: 0.6484\n",
      "Epoch 243/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2033 - accuracy: 0.4849 - val_loss: 0.8850 - val_accuracy: 0.6484\n",
      "Epoch 244/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2036 - accuracy: 0.4849 - val_loss: 0.8831 - val_accuracy: 0.6484\n",
      "Epoch 245/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2039 - accuracy: 0.4838 - val_loss: 0.8830 - val_accuracy: 0.6484\n",
      "Epoch 246/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2035 - accuracy: 0.4816 - val_loss: 0.8859 - val_accuracy: 0.6484\n",
      "Epoch 247/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2046 - accuracy: 0.4833 - val_loss: 0.8880 - val_accuracy: 0.6484\n",
      "Epoch 248/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2047 - accuracy: 0.4833 - val_loss: 0.8807 - val_accuracy: 0.6484\n",
      "Epoch 249/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2039 - accuracy: 0.4844 - val_loss: 0.8833 - val_accuracy: 0.6484\n",
      "Epoch 250/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2043 - accuracy: 0.4860 - val_loss: 0.8856 - val_accuracy: 0.6484\n",
      "Epoch 251/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2049 - accuracy: 0.4827 - val_loss: 0.8860 - val_accuracy: 0.6484\n",
      "Epoch 252/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2032 - accuracy: 0.4827 - val_loss: 0.8822 - val_accuracy: 0.6484\n",
      "Epoch 253/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2031 - accuracy: 0.4827 - val_loss: 0.8892 - val_accuracy: 0.6484\n",
      "Epoch 254/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2032 - accuracy: 0.4844 - val_loss: 0.8841 - val_accuracy: 0.6484\n",
      "Epoch 255/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2031 - accuracy: 0.4844 - val_loss: 0.8840 - val_accuracy: 0.6484\n",
      "Epoch 256/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2035 - accuracy: 0.4844 - val_loss: 0.8876 - val_accuracy: 0.6484\n",
      "Epoch 257/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2037 - accuracy: 0.4833 - val_loss: 0.8856 - val_accuracy: 0.6484\n",
      "Epoch 258/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2036 - accuracy: 0.4849 - val_loss: 0.8885 - val_accuracy: 0.6484\n",
      "Epoch 259/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2035 - accuracy: 0.4844 - val_loss: 0.8882 - val_accuracy: 0.6484\n",
      "Epoch 260/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2049 - accuracy: 0.4844 - val_loss: 0.8839 - val_accuracy: 0.6484\n",
      "Epoch 261/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2037 - accuracy: 0.4838 - val_loss: 0.8839 - val_accuracy: 0.6484\n",
      "Epoch 262/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2036 - accuracy: 0.4855 - val_loss: 0.8858 - val_accuracy: 0.6484\n",
      "Epoch 263/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2031 - accuracy: 0.4833 - val_loss: 0.8825 - val_accuracy: 0.6484\n",
      "Epoch 264/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2040 - accuracy: 0.4838 - val_loss: 0.8840 - val_accuracy: 0.6484\n",
      "Epoch 265/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2035 - accuracy: 0.4855 - val_loss: 0.8855 - val_accuracy: 0.6484\n",
      "Epoch 266/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2041 - accuracy: 0.4827 - val_loss: 0.8859 - val_accuracy: 0.6484\n",
      "Epoch 267/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2037 - accuracy: 0.4849 - val_loss: 0.8804 - val_accuracy: 0.6484\n",
      "Epoch 268/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2034 - accuracy: 0.4838 - val_loss: 0.8874 - val_accuracy: 0.6484\n",
      "Epoch 269/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2034 - accuracy: 0.4849 - val_loss: 0.8865 - val_accuracy: 0.6484\n",
      "Epoch 270/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2034 - accuracy: 0.4855 - val_loss: 0.8851 - val_accuracy: 0.6484\n",
      "Epoch 271/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2038 - accuracy: 0.4849 - val_loss: 0.8821 - val_accuracy: 0.6484\n",
      "Epoch 272/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2030 - accuracy: 0.4844 - val_loss: 0.8847 - val_accuracy: 0.6484\n",
      "Epoch 273/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2032 - accuracy: 0.4810 - val_loss: 0.8874 - val_accuracy: 0.6484\n",
      "Epoch 274/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2033 - accuracy: 0.4872 - val_loss: 0.8825 - val_accuracy: 0.6484\n",
      "Epoch 275/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2039 - accuracy: 0.4838 - val_loss: 0.8818 - val_accuracy: 0.6484\n",
      "Epoch 276/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2029 - accuracy: 0.4855 - val_loss: 0.8859 - val_accuracy: 0.6484\n",
      "Epoch 277/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2034 - accuracy: 0.4838 - val_loss: 0.8890 - val_accuracy: 0.6484\n",
      "Epoch 278/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2030 - accuracy: 0.4855 - val_loss: 0.8829 - val_accuracy: 0.6484\n",
      "Epoch 279/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2036 - accuracy: 0.4844 - val_loss: 0.8838 - val_accuracy: 0.6484\n",
      "Epoch 280/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2026 - accuracy: 0.4844 - val_loss: 0.8859 - val_accuracy: 0.6484\n",
      "Epoch 281/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2037 - accuracy: 0.4827 - val_loss: 0.8807 - val_accuracy: 0.6484\n",
      "Epoch 282/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2034 - accuracy: 0.4860 - val_loss: 0.8848 - val_accuracy: 0.6484\n",
      "Epoch 283/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2032 - accuracy: 0.4866 - val_loss: 0.8903 - val_accuracy: 0.6484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 284/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2041 - accuracy: 0.4838 - val_loss: 0.8902 - val_accuracy: 0.6484\n",
      "Epoch 285/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2028 - accuracy: 0.4833 - val_loss: 0.8835 - val_accuracy: 0.6484\n",
      "Epoch 286/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2029 - accuracy: 0.4838 - val_loss: 0.8880 - val_accuracy: 0.6484\n",
      "Epoch 287/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2044 - accuracy: 0.4838 - val_loss: 0.8901 - val_accuracy: 0.6484\n",
      "Epoch 288/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2035 - accuracy: 0.4833 - val_loss: 0.8849 - val_accuracy: 0.6484\n",
      "Epoch 289/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2031 - accuracy: 0.4849 - val_loss: 0.8860 - val_accuracy: 0.6484\n",
      "Epoch 290/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2048 - accuracy: 0.4838 - val_loss: 0.8819 - val_accuracy: 0.6484\n",
      "Epoch 291/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2037 - accuracy: 0.4849 - val_loss: 0.8830 - val_accuracy: 0.6484\n",
      "Epoch 292/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2027 - accuracy: 0.4844 - val_loss: 0.8878 - val_accuracy: 0.6484\n",
      "Epoch 293/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2028 - accuracy: 0.4855 - val_loss: 0.8840 - val_accuracy: 0.6484\n",
      "Epoch 294/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2028 - accuracy: 0.4849 - val_loss: 0.8860 - val_accuracy: 0.6484\n",
      "Epoch 295/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2029 - accuracy: 0.4855 - val_loss: 0.8853 - val_accuracy: 0.6484\n",
      "Epoch 296/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2030 - accuracy: 0.4838 - val_loss: 0.8868 - val_accuracy: 0.6484\n",
      "Epoch 297/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.2025 - accuracy: 0.4849 - val_loss: 0.8855 - val_accuracy: 0.6484\n",
      "Epoch 298/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.2026 - accuracy: 0.4844 - val_loss: 0.8861 - val_accuracy: 0.6484\n",
      "Epoch 299/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.2028 - accuracy: 0.4833 - val_loss: 0.8864 - val_accuracy: 0.6484\n",
      "Epoch 300/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.2025 - accuracy: 0.4849 - val_loss: 0.8859 - val_accuracy: 0.6484\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "56/56 [==============================] - 0s 1ms/step\n",
      "Recall: [0.93 0.   0.58]\n",
      "Precision: [0.4  0.   0.71]\n",
      "F1-score: [0.56 0.   0.64]\n",
      "Accuracy: 0.49\n",
      "Brier score:0.20561098409888717\n",
      "Brier climat:0.21168650793650795\n",
      "Brier skill score:0.028700571882658843\n",
      "Recall: [0.93 0.   0.58]\n",
      "Precision: [0.4  0.   0.71]\n",
      "F1-score: [0.56 0.   0.64]\n",
      "Accuracy: 0.49\n",
      "Brier score:0.22439341110851357\n",
      "Brier climat:0.22451388888888887\n",
      "Brier skill score:0.0005366161575639516\n",
      "Recall: [0.93 0.   0.58]\n",
      "Precision: [0.4  0.   0.71]\n",
      "F1-score: [0.56 0.   0.64]\n",
      "Accuracy: 0.49\n",
      "Brier score:0.1523204928779365\n",
      "Brier climat:0.22165178571428573\n",
      "Brier skill score:0.3127937481438515\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "4/4 [==============================] - 0s 990us/step\n",
      "Recall: [1.  0.  0.7]\n",
      "Precision: [0.49 0.   0.82]\n",
      "F1-score: [0.66 0.   0.76]\n",
      "Accuracy: 0.65\n",
      "Brier score:0.12716785342146067\n",
      "Brier climat:0.18977430555555558\n",
      "Brier skill score:0.3298995190672277\n",
      "Recall: [1.  0.  0.7]\n",
      "Precision: [0.49 0.   0.82]\n",
      "F1-score: [0.66 0.   0.76]\n",
      "Accuracy: 0.65\n",
      "Brier score:0.15979139951381716\n",
      "Brier climat:0.17659722222222224\n",
      "Brier skill score:0.09516470585962766\n",
      "Recall: [1.  0.  0.7]\n",
      "Precision: [0.49 0.   0.82]\n",
      "F1-score: [0.66 0.   0.76]\n",
      "Accuracy: 0.65\n",
      "Brier score:0.19328296779833676\n",
      "Brier climat:0.28359375\n",
      "Brier skill score:0.3184512430251486\n",
      "******** 14\n",
      "validation years [1995, 1996]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "{0: 2.0, 1: 1.0, 2: 1.0}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 60)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 183       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183\n",
      "Trainable params: 183\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 1.8059 - accuracy: 0.3987WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "28/28 [==============================] - 4s 107ms/step - loss: 1.8036 - accuracy: 0.3996 - val_loss: 1.5031 - val_accuracy: 0.5391\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.6664 - accuracy: 0.4107 - val_loss: 1.4140 - val_accuracy: 0.5391\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.5807 - accuracy: 0.4219 - val_loss: 1.3551 - val_accuracy: 0.5312\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.5271 - accuracy: 0.4364 - val_loss: 1.3179 - val_accuracy: 0.5312\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4931 - accuracy: 0.4548 - val_loss: 1.2906 - val_accuracy: 0.5312\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.4681 - accuracy: 0.4609 - val_loss: 1.2706 - val_accuracy: 0.5312\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4493 - accuracy: 0.4570 - val_loss: 1.2553 - val_accuracy: 0.5312\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4330 - accuracy: 0.4715 - val_loss: 1.2378 - val_accuracy: 0.5312\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.4179 - accuracy: 0.4682 - val_loss: 1.2256 - val_accuracy: 0.5312\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.4047 - accuracy: 0.4682 - val_loss: 1.2203 - val_accuracy: 0.5312\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3928 - accuracy: 0.4766 - val_loss: 1.2053 - val_accuracy: 0.5312\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3810 - accuracy: 0.4821 - val_loss: 1.1943 - val_accuracy: 0.5312\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3703 - accuracy: 0.4816 - val_loss: 1.1901 - val_accuracy: 0.5312\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3610 - accuracy: 0.4833 - val_loss: 1.1822 - val_accuracy: 0.5312\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3511 - accuracy: 0.4849 - val_loss: 1.1681 - val_accuracy: 0.5312\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3427 - accuracy: 0.4805 - val_loss: 1.1682 - val_accuracy: 0.5312\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3342 - accuracy: 0.4810 - val_loss: 1.1579 - val_accuracy: 0.5312\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3269 - accuracy: 0.4810 - val_loss: 1.1509 - val_accuracy: 0.5312\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3189 - accuracy: 0.4827 - val_loss: 1.1440 - val_accuracy: 0.5312\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3123 - accuracy: 0.4816 - val_loss: 1.1359 - val_accuracy: 0.5312\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3073 - accuracy: 0.4816 - val_loss: 1.1296 - val_accuracy: 0.5312\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3015 - accuracy: 0.4838 - val_loss: 1.1339 - val_accuracy: 0.5312\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2962 - accuracy: 0.4844 - val_loss: 1.1277 - val_accuracy: 0.5312\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2917 - accuracy: 0.4838 - val_loss: 1.1230 - val_accuracy: 0.5312\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2873 - accuracy: 0.4838 - val_loss: 1.1177 - val_accuracy: 0.5312\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2838 - accuracy: 0.4838 - val_loss: 1.1148 - val_accuracy: 0.5312\n",
      "Epoch 27/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2792 - accuracy: 0.4855 - val_loss: 1.1087 - val_accuracy: 0.5312\n",
      "Epoch 28/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2755 - accuracy: 0.4849 - val_loss: 1.1094 - val_accuracy: 0.5312\n",
      "Epoch 29/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2720 - accuracy: 0.4872 - val_loss: 1.1061 - val_accuracy: 0.5312\n",
      "Epoch 30/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2706 - accuracy: 0.4838 - val_loss: 1.1032 - val_accuracy: 0.5312\n",
      "Epoch 31/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2667 - accuracy: 0.4883 - val_loss: 1.0960 - val_accuracy: 0.5312\n",
      "Epoch 32/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2644 - accuracy: 0.4877 - val_loss: 1.0972 - val_accuracy: 0.5312\n",
      "Epoch 33/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2612 - accuracy: 0.4883 - val_loss: 1.0869 - val_accuracy: 0.5312\n",
      "Epoch 34/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2587 - accuracy: 0.4894 - val_loss: 1.0930 - val_accuracy: 0.5312\n",
      "Epoch 35/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2571 - accuracy: 0.4877 - val_loss: 1.0874 - val_accuracy: 0.5312\n",
      "Epoch 36/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2542 - accuracy: 0.4900 - val_loss: 1.0829 - val_accuracy: 0.5312\n",
      "Epoch 37/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2521 - accuracy: 0.4900 - val_loss: 1.0780 - val_accuracy: 0.5312\n",
      "Epoch 38/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2507 - accuracy: 0.4900 - val_loss: 1.0815 - val_accuracy: 0.5312\n",
      "Epoch 39/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2493 - accuracy: 0.4888 - val_loss: 1.0795 - val_accuracy: 0.5312\n",
      "Epoch 40/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2480 - accuracy: 0.4905 - val_loss: 1.0776 - val_accuracy: 0.5312\n",
      "Epoch 41/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2459 - accuracy: 0.4894 - val_loss: 1.0797 - val_accuracy: 0.5312\n",
      "Epoch 42/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2438 - accuracy: 0.4894 - val_loss: 1.0718 - val_accuracy: 0.5312\n",
      "Epoch 43/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2425 - accuracy: 0.4888 - val_loss: 1.0714 - val_accuracy: 0.5312\n",
      "Epoch 44/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2406 - accuracy: 0.4900 - val_loss: 1.0753 - val_accuracy: 0.5312\n",
      "Epoch 45/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2395 - accuracy: 0.4905 - val_loss: 1.0722 - val_accuracy: 0.5312\n",
      "Epoch 46/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2378 - accuracy: 0.4905 - val_loss: 1.0687 - val_accuracy: 0.5312\n",
      "Epoch 47/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2368 - accuracy: 0.4883 - val_loss: 1.0702 - val_accuracy: 0.5312\n",
      "Epoch 48/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2354 - accuracy: 0.4911 - val_loss: 1.0682 - val_accuracy: 0.5312\n",
      "Epoch 49/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2339 - accuracy: 0.4888 - val_loss: 1.0637 - val_accuracy: 0.5312\n",
      "Epoch 50/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2330 - accuracy: 0.4900 - val_loss: 1.0650 - val_accuracy: 0.5312\n",
      "Epoch 51/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2321 - accuracy: 0.4900 - val_loss: 1.0610 - val_accuracy: 0.5312\n",
      "Epoch 52/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2304 - accuracy: 0.4894 - val_loss: 1.0658 - val_accuracy: 0.5312\n",
      "Epoch 53/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2291 - accuracy: 0.4911 - val_loss: 1.0601 - val_accuracy: 0.5312\n",
      "Epoch 54/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2274 - accuracy: 0.4916 - val_loss: 1.0603 - val_accuracy: 0.5312\n",
      "Epoch 55/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2268 - accuracy: 0.4900 - val_loss: 1.0597 - val_accuracy: 0.5312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2251 - accuracy: 0.4911 - val_loss: 1.0571 - val_accuracy: 0.5312\n",
      "Epoch 57/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2242 - accuracy: 0.4905 - val_loss: 1.0526 - val_accuracy: 0.5312\n",
      "Epoch 58/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2232 - accuracy: 0.4911 - val_loss: 1.0548 - val_accuracy: 0.5312\n",
      "Epoch 59/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2223 - accuracy: 0.4911 - val_loss: 1.0514 - val_accuracy: 0.5312\n",
      "Epoch 60/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2216 - accuracy: 0.4911 - val_loss: 1.0507 - val_accuracy: 0.5312\n",
      "Epoch 61/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2196 - accuracy: 0.4922 - val_loss: 1.0495 - val_accuracy: 0.5312\n",
      "Epoch 62/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2192 - accuracy: 0.4922 - val_loss: 1.0471 - val_accuracy: 0.5312\n",
      "Epoch 63/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2176 - accuracy: 0.4911 - val_loss: 1.0494 - val_accuracy: 0.5312\n",
      "Epoch 64/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2176 - accuracy: 0.4927 - val_loss: 1.0447 - val_accuracy: 0.5312\n",
      "Epoch 65/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2162 - accuracy: 0.4922 - val_loss: 1.0444 - val_accuracy: 0.5312\n",
      "Epoch 66/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2158 - accuracy: 0.4916 - val_loss: 1.0464 - val_accuracy: 0.5312\n",
      "Epoch 67/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2147 - accuracy: 0.4922 - val_loss: 1.0376 - val_accuracy: 0.5312\n",
      "Epoch 68/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2127 - accuracy: 0.4922 - val_loss: 1.0417 - val_accuracy: 0.5312\n",
      "Epoch 69/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2130 - accuracy: 0.4905 - val_loss: 1.0479 - val_accuracy: 0.5312\n",
      "Epoch 70/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2118 - accuracy: 0.4922 - val_loss: 1.0390 - val_accuracy: 0.5312\n",
      "Epoch 71/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2110 - accuracy: 0.4894 - val_loss: 1.0383 - val_accuracy: 0.5312\n",
      "Epoch 72/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2099 - accuracy: 0.4905 - val_loss: 1.0394 - val_accuracy: 0.5312\n",
      "Epoch 73/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2092 - accuracy: 0.4916 - val_loss: 1.0359 - val_accuracy: 0.5312\n",
      "Epoch 74/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2088 - accuracy: 0.4905 - val_loss: 1.0394 - val_accuracy: 0.5312\n",
      "Epoch 75/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2075 - accuracy: 0.4916 - val_loss: 1.0346 - val_accuracy: 0.5312\n",
      "Epoch 76/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2064 - accuracy: 0.4911 - val_loss: 1.0336 - val_accuracy: 0.5312\n",
      "Epoch 77/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2057 - accuracy: 0.4922 - val_loss: 1.0354 - val_accuracy: 0.5312\n",
      "Epoch 78/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2055 - accuracy: 0.4911 - val_loss: 1.0353 - val_accuracy: 0.5312\n",
      "Epoch 79/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2044 - accuracy: 0.4911 - val_loss: 1.0320 - val_accuracy: 0.5312\n",
      "Epoch 80/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2044 - accuracy: 0.4900 - val_loss: 1.0337 - val_accuracy: 0.5312\n",
      "Epoch 81/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2033 - accuracy: 0.4911 - val_loss: 1.0298 - val_accuracy: 0.5312\n",
      "Epoch 82/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2037 - accuracy: 0.4894 - val_loss: 1.0256 - val_accuracy: 0.5312\n",
      "Epoch 83/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2027 - accuracy: 0.4894 - val_loss: 1.0330 - val_accuracy: 0.5312\n",
      "Epoch 84/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2011 - accuracy: 0.4911 - val_loss: 1.0199 - val_accuracy: 0.5312\n",
      "Epoch 85/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2000 - accuracy: 0.4883 - val_loss: 1.0256 - val_accuracy: 0.5312\n",
      "Epoch 86/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1998 - accuracy: 0.4911 - val_loss: 1.0283 - val_accuracy: 0.5312\n",
      "Epoch 87/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1994 - accuracy: 0.4900 - val_loss: 1.0242 - val_accuracy: 0.5312\n",
      "Epoch 88/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1988 - accuracy: 0.4911 - val_loss: 1.0270 - val_accuracy: 0.5312\n",
      "Epoch 89/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1981 - accuracy: 0.4894 - val_loss: 1.0235 - val_accuracy: 0.5312\n",
      "Epoch 90/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1978 - accuracy: 0.4911 - val_loss: 1.0219 - val_accuracy: 0.5312\n",
      "Epoch 91/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1972 - accuracy: 0.4888 - val_loss: 1.0185 - val_accuracy: 0.5312\n",
      "Epoch 92/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1977 - accuracy: 0.4894 - val_loss: 1.0238 - val_accuracy: 0.5312\n",
      "Epoch 93/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1978 - accuracy: 0.4860 - val_loss: 1.0178 - val_accuracy: 0.5312\n",
      "Epoch 94/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1972 - accuracy: 0.4894 - val_loss: 1.0298 - val_accuracy: 0.5312\n",
      "Epoch 95/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1961 - accuracy: 0.4900 - val_loss: 1.0219 - val_accuracy: 0.5312\n",
      "Epoch 96/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1957 - accuracy: 0.4883 - val_loss: 1.0176 - val_accuracy: 0.5312\n",
      "Epoch 97/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1959 - accuracy: 0.4872 - val_loss: 1.0245 - val_accuracy: 0.5312\n",
      "Epoch 98/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1952 - accuracy: 0.4866 - val_loss: 1.0184 - val_accuracy: 0.5312\n",
      "Epoch 99/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1953 - accuracy: 0.4888 - val_loss: 1.0220 - val_accuracy: 0.5312\n",
      "Epoch 100/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1958 - accuracy: 0.4888 - val_loss: 1.0229 - val_accuracy: 0.5312\n",
      "Epoch 101/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1949 - accuracy: 0.4866 - val_loss: 1.0224 - val_accuracy: 0.5312\n",
      "Epoch 102/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1943 - accuracy: 0.4888 - val_loss: 1.0218 - val_accuracy: 0.5312\n",
      "Epoch 103/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1951 - accuracy: 0.4827 - val_loss: 1.0228 - val_accuracy: 0.5312\n",
      "Epoch 104/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1950 - accuracy: 0.4888 - val_loss: 1.0180 - val_accuracy: 0.5312\n",
      "Epoch 105/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1941 - accuracy: 0.4866 - val_loss: 1.0209 - val_accuracy: 0.5312\n",
      "Epoch 106/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1940 - accuracy: 0.4872 - val_loss: 1.0184 - val_accuracy: 0.5312\n",
      "Epoch 107/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1938 - accuracy: 0.4860 - val_loss: 1.0200 - val_accuracy: 0.5312\n",
      "Epoch 108/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1932 - accuracy: 0.4894 - val_loss: 1.0173 - val_accuracy: 0.5312\n",
      "Epoch 109/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1934 - accuracy: 0.4866 - val_loss: 1.0177 - val_accuracy: 0.5312\n",
      "Epoch 110/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1939 - accuracy: 0.4844 - val_loss: 1.0171 - val_accuracy: 0.5312\n",
      "Epoch 111/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1928 - accuracy: 0.4860 - val_loss: 1.0155 - val_accuracy: 0.5312\n",
      "Epoch 112/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1928 - accuracy: 0.4877 - val_loss: 1.0225 - val_accuracy: 0.5312\n",
      "Epoch 113/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1921 - accuracy: 0.4872 - val_loss: 1.0231 - val_accuracy: 0.5312\n",
      "Epoch 114/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1924 - accuracy: 0.4894 - val_loss: 1.0200 - val_accuracy: 0.5312\n",
      "Epoch 115/300\n",
      "28/28 [==============================] - 1s 49ms/step - loss: 1.1934 - accuracy: 0.4888 - val_loss: 1.0236 - val_accuracy: 0.5312\n",
      "Epoch 116/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1919 - accuracy: 0.4860 - val_loss: 1.0190 - val_accuracy: 0.5312\n",
      "Epoch 117/300\n",
      "28/28 [==============================] - 0s 17ms/step - loss: 1.1924 - accuracy: 0.4844 - val_loss: 1.0218 - val_accuracy: 0.5312\n",
      "Epoch 118/300\n",
      "28/28 [==============================] - 1s 33ms/step - loss: 1.1922 - accuracy: 0.4877 - val_loss: 1.0189 - val_accuracy: 0.5312\n",
      "Epoch 119/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1918 - accuracy: 0.4872 - val_loss: 1.0227 - val_accuracy: 0.5312\n",
      "Epoch 120/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1915 - accuracy: 0.4883 - val_loss: 1.0200 - val_accuracy: 0.5312\n",
      "Epoch 121/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1922 - accuracy: 0.4866 - val_loss: 1.0138 - val_accuracy: 0.5312\n",
      "Epoch 122/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1914 - accuracy: 0.4860 - val_loss: 1.0191 - val_accuracy: 0.5312\n",
      "Epoch 123/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1915 - accuracy: 0.4872 - val_loss: 1.0155 - val_accuracy: 0.5312\n",
      "Epoch 124/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1909 - accuracy: 0.4877 - val_loss: 1.0164 - val_accuracy: 0.5312\n",
      "Epoch 125/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1907 - accuracy: 0.4855 - val_loss: 1.0170 - val_accuracy: 0.5312\n",
      "Epoch 126/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1912 - accuracy: 0.4866 - val_loss: 1.0191 - val_accuracy: 0.5312\n",
      "Epoch 127/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1906 - accuracy: 0.4866 - val_loss: 1.0161 - val_accuracy: 0.5312\n",
      "Epoch 128/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1908 - accuracy: 0.4855 - val_loss: 1.0176 - val_accuracy: 0.5312\n",
      "Epoch 129/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1906 - accuracy: 0.4888 - val_loss: 1.0175 - val_accuracy: 0.5312\n",
      "Epoch 130/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1911 - accuracy: 0.4860 - val_loss: 1.0158 - val_accuracy: 0.5312\n",
      "Epoch 131/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1903 - accuracy: 0.4866 - val_loss: 1.0149 - val_accuracy: 0.5312\n",
      "Epoch 132/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1904 - accuracy: 0.4860 - val_loss: 1.0157 - val_accuracy: 0.5312\n",
      "Epoch 133/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1907 - accuracy: 0.4855 - val_loss: 1.0194 - val_accuracy: 0.5312\n",
      "Epoch 134/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1906 - accuracy: 0.4883 - val_loss: 1.0165 - val_accuracy: 0.5312\n",
      "Epoch 135/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1910 - accuracy: 0.4877 - val_loss: 1.0115 - val_accuracy: 0.5312\n",
      "Epoch 136/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1907 - accuracy: 0.4866 - val_loss: 1.0151 - val_accuracy: 0.5312\n",
      "Epoch 137/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1908 - accuracy: 0.4855 - val_loss: 1.0157 - val_accuracy: 0.5312\n",
      "Epoch 138/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1925 - accuracy: 0.4900 - val_loss: 1.0167 - val_accuracy: 0.5312\n",
      "Epoch 139/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1901 - accuracy: 0.4877 - val_loss: 1.0192 - val_accuracy: 0.5312\n",
      "Epoch 140/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1899 - accuracy: 0.4866 - val_loss: 1.0138 - val_accuracy: 0.5312\n",
      "Epoch 141/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1905 - accuracy: 0.4844 - val_loss: 1.0169 - val_accuracy: 0.5312\n",
      "Epoch 142/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1913 - accuracy: 0.4877 - val_loss: 1.0151 - val_accuracy: 0.5312\n",
      "Epoch 143/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1893 - accuracy: 0.4866 - val_loss: 1.0128 - val_accuracy: 0.5312\n",
      "Epoch 144/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1897 - accuracy: 0.4872 - val_loss: 1.0217 - val_accuracy: 0.5312\n",
      "Epoch 145/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1913 - accuracy: 0.4872 - val_loss: 1.0159 - val_accuracy: 0.5312\n",
      "Epoch 146/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1895 - accuracy: 0.4872 - val_loss: 1.0192 - val_accuracy: 0.5312\n",
      "Epoch 147/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1898 - accuracy: 0.4872 - val_loss: 1.0139 - val_accuracy: 0.5312\n",
      "Epoch 148/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1896 - accuracy: 0.4872 - val_loss: 1.0189 - val_accuracy: 0.5312\n",
      "Epoch 149/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1899 - accuracy: 0.4849 - val_loss: 1.0131 - val_accuracy: 0.5312\n",
      "Epoch 150/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1893 - accuracy: 0.4883 - val_loss: 1.0121 - val_accuracy: 0.5312\n",
      "Epoch 151/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1906 - accuracy: 0.4838 - val_loss: 1.0167 - val_accuracy: 0.5312\n",
      "Epoch 152/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1895 - accuracy: 0.4860 - val_loss: 1.0164 - val_accuracy: 0.5312\n",
      "Epoch 153/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1893 - accuracy: 0.4860 - val_loss: 1.0169 - val_accuracy: 0.5312\n",
      "Epoch 154/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1888 - accuracy: 0.4849 - val_loss: 1.0104 - val_accuracy: 0.5312\n",
      "Epoch 155/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1902 - accuracy: 0.4872 - val_loss: 1.0182 - val_accuracy: 0.5312\n",
      "Epoch 156/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1897 - accuracy: 0.4877 - val_loss: 1.0171 - val_accuracy: 0.5312\n",
      "Epoch 157/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1891 - accuracy: 0.4872 - val_loss: 1.0165 - val_accuracy: 0.5312\n",
      "Epoch 158/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1888 - accuracy: 0.4866 - val_loss: 1.0147 - val_accuracy: 0.5312\n",
      "Epoch 159/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1889 - accuracy: 0.4860 - val_loss: 1.0157 - val_accuracy: 0.5312\n",
      "Epoch 160/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1900 - accuracy: 0.4866 - val_loss: 1.0146 - val_accuracy: 0.5312\n",
      "Epoch 161/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1891 - accuracy: 0.4866 - val_loss: 1.0147 - val_accuracy: 0.5312\n",
      "Epoch 162/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1903 - accuracy: 0.4855 - val_loss: 1.0186 - val_accuracy: 0.5312\n",
      "Epoch 163/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1887 - accuracy: 0.4866 - val_loss: 1.0129 - val_accuracy: 0.5312\n",
      "Epoch 164/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1892 - accuracy: 0.4866 - val_loss: 1.0150 - val_accuracy: 0.5312\n",
      "Epoch 165/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1891 - accuracy: 0.4883 - val_loss: 1.0146 - val_accuracy: 0.5312\n",
      "Epoch 166/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1888 - accuracy: 0.4866 - val_loss: 1.0132 - val_accuracy: 0.5312\n",
      "Epoch 167/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1891 - accuracy: 0.4866 - val_loss: 1.0153 - val_accuracy: 0.5312\n",
      "Epoch 168/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1887 - accuracy: 0.4877 - val_loss: 1.0138 - val_accuracy: 0.5312\n",
      "Epoch 169/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1887 - accuracy: 0.4877 - val_loss: 1.0123 - val_accuracy: 0.5312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1888 - accuracy: 0.4866 - val_loss: 1.0193 - val_accuracy: 0.5312\n",
      "Epoch 171/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1885 - accuracy: 0.4872 - val_loss: 1.0115 - val_accuracy: 0.5312\n",
      "Epoch 172/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1886 - accuracy: 0.4855 - val_loss: 1.0166 - val_accuracy: 0.5312\n",
      "Epoch 173/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1898 - accuracy: 0.4888 - val_loss: 1.0161 - val_accuracy: 0.5312\n",
      "Epoch 174/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1894 - accuracy: 0.4877 - val_loss: 1.0230 - val_accuracy: 0.5312\n",
      "Epoch 175/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1891 - accuracy: 0.4877 - val_loss: 1.0135 - val_accuracy: 0.5312\n",
      "Epoch 176/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1887 - accuracy: 0.4860 - val_loss: 1.0176 - val_accuracy: 0.5312\n",
      "Epoch 177/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1891 - accuracy: 0.4877 - val_loss: 1.0155 - val_accuracy: 0.5312\n",
      "Epoch 178/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1885 - accuracy: 0.4872 - val_loss: 1.0143 - val_accuracy: 0.5312\n",
      "Epoch 179/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1894 - accuracy: 0.4866 - val_loss: 1.0149 - val_accuracy: 0.5312\n",
      "Epoch 180/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1898 - accuracy: 0.4872 - val_loss: 1.0172 - val_accuracy: 0.5312\n",
      "Epoch 181/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1890 - accuracy: 0.4872 - val_loss: 1.0103 - val_accuracy: 0.5312\n",
      "Epoch 182/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1884 - accuracy: 0.4872 - val_loss: 1.0184 - val_accuracy: 0.5312\n",
      "Epoch 183/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1890 - accuracy: 0.4872 - val_loss: 1.0140 - val_accuracy: 0.5312\n",
      "Epoch 184/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1886 - accuracy: 0.4866 - val_loss: 1.0133 - val_accuracy: 0.5312\n",
      "Epoch 185/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1889 - accuracy: 0.4860 - val_loss: 1.0146 - val_accuracy: 0.5312\n",
      "Epoch 186/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1885 - accuracy: 0.4883 - val_loss: 1.0116 - val_accuracy: 0.5312\n",
      "Epoch 187/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1882 - accuracy: 0.4866 - val_loss: 1.0161 - val_accuracy: 0.5312\n",
      "Epoch 188/300\n",
      "28/28 [==============================] - 1s 29ms/step - loss: 1.1881 - accuracy: 0.4883 - val_loss: 1.0094 - val_accuracy: 0.5312\n",
      "Epoch 189/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1881 - accuracy: 0.4872 - val_loss: 1.0122 - val_accuracy: 0.5312\n",
      "Epoch 190/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1882 - accuracy: 0.4866 - val_loss: 1.0174 - val_accuracy: 0.5312\n",
      "Epoch 191/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1887 - accuracy: 0.4866 - val_loss: 1.0139 - val_accuracy: 0.5312\n",
      "Epoch 192/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1890 - accuracy: 0.4866 - val_loss: 1.0183 - val_accuracy: 0.5312\n",
      "Epoch 193/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1889 - accuracy: 0.4872 - val_loss: 1.0180 - val_accuracy: 0.5312\n",
      "Epoch 194/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1877 - accuracy: 0.4877 - val_loss: 1.0163 - val_accuracy: 0.5312\n",
      "Epoch 195/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1885 - accuracy: 0.4866 - val_loss: 1.0126 - val_accuracy: 0.5312\n",
      "Epoch 196/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1885 - accuracy: 0.4877 - val_loss: 1.0112 - val_accuracy: 0.5312\n",
      "Epoch 197/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1882 - accuracy: 0.4877 - val_loss: 1.0156 - val_accuracy: 0.5312\n",
      "Epoch 198/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1884 - accuracy: 0.4877 - val_loss: 1.0130 - val_accuracy: 0.5312\n",
      "Epoch 199/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1890 - accuracy: 0.4872 - val_loss: 1.0141 - val_accuracy: 0.5312\n",
      "Epoch 200/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1881 - accuracy: 0.4860 - val_loss: 1.0178 - val_accuracy: 0.5312\n",
      "Epoch 201/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1887 - accuracy: 0.4866 - val_loss: 1.0145 - val_accuracy: 0.5312\n",
      "Epoch 202/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1882 - accuracy: 0.4866 - val_loss: 1.0158 - val_accuracy: 0.5312\n",
      "Epoch 203/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1883 - accuracy: 0.4872 - val_loss: 1.0162 - val_accuracy: 0.5312\n",
      "Epoch 204/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1883 - accuracy: 0.4877 - val_loss: 1.0092 - val_accuracy: 0.5312\n",
      "Epoch 205/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1882 - accuracy: 0.4866 - val_loss: 1.0128 - val_accuracy: 0.5312\n",
      "Epoch 206/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1884 - accuracy: 0.4877 - val_loss: 1.0152 - val_accuracy: 0.5312\n",
      "Epoch 207/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1886 - accuracy: 0.4888 - val_loss: 1.0150 - val_accuracy: 0.5312\n",
      "Epoch 208/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1882 - accuracy: 0.4883 - val_loss: 1.0111 - val_accuracy: 0.5312\n",
      "Epoch 209/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1884 - accuracy: 0.4894 - val_loss: 1.0129 - val_accuracy: 0.5312\n",
      "Epoch 210/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1878 - accuracy: 0.4872 - val_loss: 1.0139 - val_accuracy: 0.5312\n",
      "Epoch 211/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1881 - accuracy: 0.4877 - val_loss: 1.0127 - val_accuracy: 0.5312\n",
      "Epoch 212/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1879 - accuracy: 0.4883 - val_loss: 1.0173 - val_accuracy: 0.5312\n",
      "Epoch 213/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1879 - accuracy: 0.4877 - val_loss: 1.0118 - val_accuracy: 0.5312\n",
      "Epoch 214/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1878 - accuracy: 0.4883 - val_loss: 1.0205 - val_accuracy: 0.5312\n",
      "Epoch 215/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1888 - accuracy: 0.4883 - val_loss: 1.0139 - val_accuracy: 0.5312\n",
      "Epoch 216/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1878 - accuracy: 0.4872 - val_loss: 1.0145 - val_accuracy: 0.5312\n",
      "Epoch 217/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1877 - accuracy: 0.4877 - val_loss: 1.0175 - val_accuracy: 0.5312\n",
      "Epoch 218/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1872 - accuracy: 0.4883 - val_loss: 1.0142 - val_accuracy: 0.5312\n",
      "Epoch 219/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1891 - accuracy: 0.4883 - val_loss: 1.0136 - val_accuracy: 0.5312\n",
      "Epoch 220/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1878 - accuracy: 0.4872 - val_loss: 1.0179 - val_accuracy: 0.5312\n",
      "Epoch 221/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1875 - accuracy: 0.4888 - val_loss: 1.0147 - val_accuracy: 0.5312\n",
      "Epoch 222/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1881 - accuracy: 0.4877 - val_loss: 1.0113 - val_accuracy: 0.5312\n",
      "Epoch 223/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1881 - accuracy: 0.4900 - val_loss: 1.0191 - val_accuracy: 0.5312\n",
      "Epoch 224/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1875 - accuracy: 0.4883 - val_loss: 1.0140 - val_accuracy: 0.5312\n",
      "Epoch 225/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1878 - accuracy: 0.4872 - val_loss: 1.0190 - val_accuracy: 0.5312\n",
      "Epoch 226/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1876 - accuracy: 0.4894 - val_loss: 1.0088 - val_accuracy: 0.5312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1876 - accuracy: 0.4877 - val_loss: 1.0100 - val_accuracy: 0.5312\n",
      "Epoch 228/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1875 - accuracy: 0.4883 - val_loss: 1.0131 - val_accuracy: 0.5312\n",
      "Epoch 229/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1877 - accuracy: 0.4877 - val_loss: 1.0143 - val_accuracy: 0.5312\n",
      "Epoch 230/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1875 - accuracy: 0.4866 - val_loss: 1.0082 - val_accuracy: 0.5391\n",
      "Epoch 231/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1875 - accuracy: 0.4855 - val_loss: 1.0115 - val_accuracy: 0.5312\n",
      "Epoch 232/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1876 - accuracy: 0.4877 - val_loss: 1.0155 - val_accuracy: 0.5312\n",
      "Epoch 233/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1871 - accuracy: 0.4872 - val_loss: 1.0153 - val_accuracy: 0.5312\n",
      "Epoch 234/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1878 - accuracy: 0.4866 - val_loss: 1.0181 - val_accuracy: 0.5312\n",
      "Epoch 235/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1871 - accuracy: 0.4860 - val_loss: 1.0122 - val_accuracy: 0.5312\n",
      "Epoch 236/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1870 - accuracy: 0.4872 - val_loss: 1.0147 - val_accuracy: 0.5312\n",
      "Epoch 237/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1877 - accuracy: 0.4872 - val_loss: 1.0126 - val_accuracy: 0.5312\n",
      "Epoch 238/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1880 - accuracy: 0.4894 - val_loss: 1.0132 - val_accuracy: 0.5312\n",
      "Epoch 239/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1873 - accuracy: 0.4877 - val_loss: 1.0099 - val_accuracy: 0.5312\n",
      "Epoch 240/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1872 - accuracy: 0.4872 - val_loss: 1.0123 - val_accuracy: 0.5312\n",
      "Epoch 241/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1875 - accuracy: 0.4877 - val_loss: 1.0133 - val_accuracy: 0.5312\n",
      "Epoch 242/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1872 - accuracy: 0.4883 - val_loss: 1.0137 - val_accuracy: 0.5312\n",
      "Epoch 243/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1871 - accuracy: 0.4872 - val_loss: 1.0114 - val_accuracy: 0.5312\n",
      "Epoch 244/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1878 - accuracy: 0.4894 - val_loss: 1.0183 - val_accuracy: 0.5312\n",
      "Epoch 245/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1876 - accuracy: 0.4866 - val_loss: 1.0113 - val_accuracy: 0.5312\n",
      "Epoch 246/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1869 - accuracy: 0.4872 - val_loss: 1.0113 - val_accuracy: 0.5312\n",
      "Epoch 247/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1868 - accuracy: 0.4877 - val_loss: 1.0165 - val_accuracy: 0.5312\n",
      "Epoch 248/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1887 - accuracy: 0.4872 - val_loss: 1.0163 - val_accuracy: 0.5312\n",
      "Epoch 249/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1882 - accuracy: 0.4900 - val_loss: 1.0112 - val_accuracy: 0.5312\n",
      "Epoch 250/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1875 - accuracy: 0.4872 - val_loss: 1.0135 - val_accuracy: 0.5312\n",
      "Epoch 251/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1878 - accuracy: 0.4877 - val_loss: 1.0128 - val_accuracy: 0.5312\n",
      "Epoch 252/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1867 - accuracy: 0.4877 - val_loss: 1.0114 - val_accuracy: 0.5312\n",
      "Epoch 253/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1877 - accuracy: 0.4866 - val_loss: 1.0086 - val_accuracy: 0.5312\n",
      "Epoch 254/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1873 - accuracy: 0.4872 - val_loss: 1.0194 - val_accuracy: 0.5312\n",
      "Epoch 255/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1869 - accuracy: 0.4888 - val_loss: 1.0128 - val_accuracy: 0.5312\n",
      "Epoch 256/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1871 - accuracy: 0.4860 - val_loss: 1.0070 - val_accuracy: 0.5312\n",
      "Epoch 257/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1870 - accuracy: 0.4883 - val_loss: 1.0125 - val_accuracy: 0.5312\n",
      "Epoch 258/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1882 - accuracy: 0.4888 - val_loss: 1.0181 - val_accuracy: 0.5312\n",
      "Epoch 259/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1882 - accuracy: 0.4883 - val_loss: 1.0139 - val_accuracy: 0.5312\n",
      "Epoch 260/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1868 - accuracy: 0.4883 - val_loss: 1.0164 - val_accuracy: 0.5312\n",
      "Epoch 261/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1870 - accuracy: 0.4866 - val_loss: 1.0126 - val_accuracy: 0.5312\n",
      "Epoch 262/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1867 - accuracy: 0.4872 - val_loss: 1.0103 - val_accuracy: 0.5312\n",
      "Epoch 263/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1874 - accuracy: 0.4872 - val_loss: 1.0124 - val_accuracy: 0.5312\n",
      "Epoch 264/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1869 - accuracy: 0.4877 - val_loss: 1.0111 - val_accuracy: 0.5312\n",
      "Epoch 265/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1871 - accuracy: 0.4866 - val_loss: 1.0180 - val_accuracy: 0.5312\n",
      "Epoch 266/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1870 - accuracy: 0.4855 - val_loss: 1.0154 - val_accuracy: 0.5312\n",
      "Epoch 267/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1871 - accuracy: 0.4872 - val_loss: 1.0139 - val_accuracy: 0.5312\n",
      "Epoch 268/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1878 - accuracy: 0.4860 - val_loss: 1.0132 - val_accuracy: 0.5312\n",
      "Epoch 269/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1869 - accuracy: 0.4888 - val_loss: 1.0133 - val_accuracy: 0.5312\n",
      "Epoch 270/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1875 - accuracy: 0.4855 - val_loss: 1.0066 - val_accuracy: 0.5312\n",
      "Epoch 271/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1868 - accuracy: 0.4872 - val_loss: 1.0123 - val_accuracy: 0.5312\n",
      "Epoch 272/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1873 - accuracy: 0.4888 - val_loss: 1.0092 - val_accuracy: 0.5312\n",
      "Epoch 273/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1869 - accuracy: 0.4866 - val_loss: 1.0129 - val_accuracy: 0.5312\n",
      "Epoch 274/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1867 - accuracy: 0.4866 - val_loss: 1.0095 - val_accuracy: 0.5312\n",
      "Epoch 275/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1868 - accuracy: 0.4872 - val_loss: 1.0100 - val_accuracy: 0.5312\n",
      "Epoch 276/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1869 - accuracy: 0.4877 - val_loss: 1.0114 - val_accuracy: 0.5312\n",
      "Epoch 277/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1869 - accuracy: 0.4866 - val_loss: 1.0141 - val_accuracy: 0.5312\n",
      "Epoch 278/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1871 - accuracy: 0.4866 - val_loss: 1.0104 - val_accuracy: 0.5312\n",
      "Epoch 279/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1869 - accuracy: 0.4888 - val_loss: 1.0090 - val_accuracy: 0.5312\n",
      "Epoch 280/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1868 - accuracy: 0.4872 - val_loss: 1.0109 - val_accuracy: 0.5312\n",
      "Epoch 281/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1868 - accuracy: 0.4866 - val_loss: 1.0108 - val_accuracy: 0.5312\n",
      "Epoch 282/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1870 - accuracy: 0.4866 - val_loss: 1.0073 - val_accuracy: 0.5312\n",
      "Epoch 283/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1875 - accuracy: 0.4883 - val_loss: 1.0099 - val_accuracy: 0.5312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 284/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1869 - accuracy: 0.4866 - val_loss: 1.0103 - val_accuracy: 0.5312\n",
      "Epoch 285/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1870 - accuracy: 0.4883 - val_loss: 1.0105 - val_accuracy: 0.5312\n",
      "Epoch 286/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1865 - accuracy: 0.4855 - val_loss: 1.0142 - val_accuracy: 0.5312\n",
      "Epoch 287/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1871 - accuracy: 0.4900 - val_loss: 1.0162 - val_accuracy: 0.5312\n",
      "Epoch 288/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1870 - accuracy: 0.4872 - val_loss: 1.0125 - val_accuracy: 0.5312\n",
      "Epoch 289/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1865 - accuracy: 0.4888 - val_loss: 1.0106 - val_accuracy: 0.5312\n",
      "Epoch 290/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1864 - accuracy: 0.4872 - val_loss: 1.0145 - val_accuracy: 0.5312\n",
      "Epoch 291/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1863 - accuracy: 0.4883 - val_loss: 1.0081 - val_accuracy: 0.5312\n",
      "Epoch 292/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1865 - accuracy: 0.4872 - val_loss: 1.0105 - val_accuracy: 0.5312\n",
      "Epoch 293/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1871 - accuracy: 0.4849 - val_loss: 1.0178 - val_accuracy: 0.5312\n",
      "Epoch 294/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1865 - accuracy: 0.4872 - val_loss: 1.0126 - val_accuracy: 0.5312\n",
      "Epoch 295/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1866 - accuracy: 0.4866 - val_loss: 1.0118 - val_accuracy: 0.5312\n",
      "Epoch 296/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1868 - accuracy: 0.4872 - val_loss: 1.0125 - val_accuracy: 0.5312\n",
      "Epoch 297/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1.1875 - accuracy: 0.4866 - val_loss: 1.0114 - val_accuracy: 0.5312\n",
      "Epoch 298/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1867 - accuracy: 0.4866 - val_loss: 1.0156 - val_accuracy: 0.5312\n",
      "Epoch 299/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1867 - accuracy: 0.4883 - val_loss: 1.0101 - val_accuracy: 0.5312\n",
      "Epoch 300/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1863 - accuracy: 0.4883 - val_loss: 1.0106 - val_accuracy: 0.5312\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "56/56 [==============================] - 0s 1ms/step\n",
      "Recall: [0.93 0.   0.6 ]\n",
      "Precision: [0.39 0.5  0.72]\n",
      "F1-score: [0.55 0.01 0.66]\n",
      "Accuracy: 0.49\n",
      "Brier score:0.1970258422493765\n",
      "Brier climat:0.20394841269841274\n",
      "Brier skill score:0.03394275227467913\n",
      "Recall: [0.93 0.   0.6 ]\n",
      "Precision: [0.39 0.5  0.72]\n",
      "F1-score: [0.55 0.01 0.66]\n",
      "Accuracy: 0.49\n",
      "Brier score:0.2204379317342194\n",
      "Brier climat:0.22395585317460315\n",
      "Brier skill score:0.015708102246566713\n",
      "Recall: [0.93 0.   0.6 ]\n",
      "Precision: [0.39 0.5  0.72]\n",
      "F1-score: [0.55 0.01 0.66]\n",
      "Accuracy: 0.49\n",
      "Brier score:0.15459485865225897\n",
      "Brier climat:0.22782738095238095\n",
      "Brier skill score:0.32143863478564316\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "Recall: [1. 0. 0.]\n",
      "Precision: [0.53 0.   0.  ]\n",
      "F1-score: [0.69 0.   0.  ]\n",
      "Accuracy: 0.53\n",
      "Brier score:0.2141150241008196\n",
      "Brier climat:0.2981076388888889\n",
      "Brier skill score:0.28175264176767767\n",
      "Recall: [1. 0. 0.]\n",
      "Precision: [0.53 0.   0.  ]\n",
      "F1-score: [0.69 0.   0.  ]\n",
      "Accuracy: 0.53\n",
      "Brier score:0.18016487138590942\n",
      "Brier climat:0.18440972222222224\n",
      "Brier skill score:0.023018584840107215\n",
      "Recall: [1. 0. 0.]\n",
      "Precision: [0.53 0.   0.  ]\n",
      "F1-score: [0.69 0.   0.  ]\n",
      "Accuracy: 0.53\n",
      "Brier score:0.17499602989070825\n",
      "Brier climat:0.1971354166666667\n",
      "Brier skill score:0.11230547585162542\n",
      "******** 15\n",
      "validation years [1996, 1997]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "{0: 2.0, 1: 1.0, 2: 1.0}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 60)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 183       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183\n",
      "Trainable params: 183\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - ETA: 0s - loss: 1.8918 - accuracy: 0.3415WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "28/28 [==============================] - 4s 110ms/step - loss: 1.8918 - accuracy: 0.3415 - val_loss: 1.1507 - val_accuracy: 0.6484\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.7178 - accuracy: 0.3945 - val_loss: 0.9526 - val_accuracy: 0.7500\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.6162 - accuracy: 0.4286 - val_loss: 0.8544 - val_accuracy: 0.7656\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.5538 - accuracy: 0.4576 - val_loss: 0.7978 - val_accuracy: 0.7812\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.5146 - accuracy: 0.4710 - val_loss: 0.7614 - val_accuracy: 0.8047\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4877 - accuracy: 0.4743 - val_loss: 0.7429 - val_accuracy: 0.8203\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.4682 - accuracy: 0.4777 - val_loss: 0.7260 - val_accuracy: 0.7891\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.4515 - accuracy: 0.4805 - val_loss: 0.7100 - val_accuracy: 0.7812\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.4377 - accuracy: 0.4821 - val_loss: 0.7036 - val_accuracy: 0.8047\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4260 - accuracy: 0.4749 - val_loss: 0.6947 - val_accuracy: 0.7969\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.4149 - accuracy: 0.4743 - val_loss: 0.6921 - val_accuracy: 0.7969\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4050 - accuracy: 0.4743 - val_loss: 0.6863 - val_accuracy: 0.7969\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3961 - accuracy: 0.4738 - val_loss: 0.6809 - val_accuracy: 0.7969\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3876 - accuracy: 0.4693 - val_loss: 0.6787 - val_accuracy: 0.7891\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3789 - accuracy: 0.4682 - val_loss: 0.6722 - val_accuracy: 0.7969\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3716 - accuracy: 0.4704 - val_loss: 0.6691 - val_accuracy: 0.7891\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3640 - accuracy: 0.4704 - val_loss: 0.6624 - val_accuracy: 0.7891\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3579 - accuracy: 0.4648 - val_loss: 0.6596 - val_accuracy: 0.7891\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3522 - accuracy: 0.4693 - val_loss: 0.6528 - val_accuracy: 0.7969\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3464 - accuracy: 0.4665 - val_loss: 0.6500 - val_accuracy: 0.7891\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3426 - accuracy: 0.4648 - val_loss: 0.6446 - val_accuracy: 0.7969\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3366 - accuracy: 0.4671 - val_loss: 0.6413 - val_accuracy: 0.7969\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3322 - accuracy: 0.4654 - val_loss: 0.6390 - val_accuracy: 0.7812\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3283 - accuracy: 0.4660 - val_loss: 0.6367 - val_accuracy: 0.7812\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3250 - accuracy: 0.4660 - val_loss: 0.6277 - val_accuracy: 0.7969\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3212 - accuracy: 0.4671 - val_loss: 0.6263 - val_accuracy: 0.7891\n",
      "Epoch 27/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3178 - accuracy: 0.4665 - val_loss: 0.6207 - val_accuracy: 0.7969\n",
      "Epoch 28/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3146 - accuracy: 0.4671 - val_loss: 0.6197 - val_accuracy: 0.7969\n",
      "Epoch 29/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3109 - accuracy: 0.4671 - val_loss: 0.6152 - val_accuracy: 0.7969\n",
      "Epoch 30/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3083 - accuracy: 0.4654 - val_loss: 0.6124 - val_accuracy: 0.7969\n",
      "Epoch 31/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3061 - accuracy: 0.4665 - val_loss: 0.6119 - val_accuracy: 0.7969\n",
      "Epoch 32/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3036 - accuracy: 0.4665 - val_loss: 0.6077 - val_accuracy: 0.7969\n",
      "Epoch 33/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3005 - accuracy: 0.4671 - val_loss: 0.6077 - val_accuracy: 0.7969\n",
      "Epoch 34/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2984 - accuracy: 0.4665 - val_loss: 0.6049 - val_accuracy: 0.7969\n",
      "Epoch 35/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2974 - accuracy: 0.4665 - val_loss: 0.6035 - val_accuracy: 0.7969\n",
      "Epoch 36/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2939 - accuracy: 0.4676 - val_loss: 0.6008 - val_accuracy: 0.7969\n",
      "Epoch 37/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2911 - accuracy: 0.4693 - val_loss: 0.6042 - val_accuracy: 0.7891\n",
      "Epoch 38/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2895 - accuracy: 0.4715 - val_loss: 0.6019 - val_accuracy: 0.7812\n",
      "Epoch 39/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2876 - accuracy: 0.4710 - val_loss: 0.6000 - val_accuracy: 0.7969\n",
      "Epoch 40/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2857 - accuracy: 0.4688 - val_loss: 0.5981 - val_accuracy: 0.7734\n",
      "Epoch 41/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2843 - accuracy: 0.4715 - val_loss: 0.5970 - val_accuracy: 0.7891\n",
      "Epoch 42/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2815 - accuracy: 0.4727 - val_loss: 0.5952 - val_accuracy: 0.7734\n",
      "Epoch 43/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2798 - accuracy: 0.4721 - val_loss: 0.5919 - val_accuracy: 0.7812\n",
      "Epoch 44/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2792 - accuracy: 0.4727 - val_loss: 0.5962 - val_accuracy: 0.7734\n",
      "Epoch 45/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2770 - accuracy: 0.4715 - val_loss: 0.5901 - val_accuracy: 0.7734\n",
      "Epoch 46/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2750 - accuracy: 0.4727 - val_loss: 0.5918 - val_accuracy: 0.7734\n",
      "Epoch 47/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2741 - accuracy: 0.4688 - val_loss: 0.5883 - val_accuracy: 0.7734\n",
      "Epoch 48/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2724 - accuracy: 0.4721 - val_loss: 0.5921 - val_accuracy: 0.7734\n",
      "Epoch 49/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2716 - accuracy: 0.4693 - val_loss: 0.5893 - val_accuracy: 0.7734\n",
      "Epoch 50/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2696 - accuracy: 0.4704 - val_loss: 0.5865 - val_accuracy: 0.7734\n",
      "Epoch 51/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2681 - accuracy: 0.4704 - val_loss: 0.5889 - val_accuracy: 0.7734\n",
      "Epoch 52/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2663 - accuracy: 0.4699 - val_loss: 0.5859 - val_accuracy: 0.7734\n",
      "Epoch 53/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2652 - accuracy: 0.4704 - val_loss: 0.5862 - val_accuracy: 0.7734\n",
      "Epoch 54/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2643 - accuracy: 0.4699 - val_loss: 0.5819 - val_accuracy: 0.7734\n",
      "Epoch 55/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2638 - accuracy: 0.4699 - val_loss: 0.5825 - val_accuracy: 0.7734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2621 - accuracy: 0.4693 - val_loss: 0.5829 - val_accuracy: 0.7734\n",
      "Epoch 57/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2615 - accuracy: 0.4676 - val_loss: 0.5794 - val_accuracy: 0.7734\n",
      "Epoch 58/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2603 - accuracy: 0.4688 - val_loss: 0.5825 - val_accuracy: 0.7734\n",
      "Epoch 59/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2599 - accuracy: 0.4688 - val_loss: 0.5809 - val_accuracy: 0.7734\n",
      "Epoch 60/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2585 - accuracy: 0.4671 - val_loss: 0.5814 - val_accuracy: 0.7734\n",
      "Epoch 61/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2578 - accuracy: 0.4693 - val_loss: 0.5770 - val_accuracy: 0.7734\n",
      "Epoch 62/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2576 - accuracy: 0.4682 - val_loss: 0.5771 - val_accuracy: 0.7734\n",
      "Epoch 63/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2565 - accuracy: 0.4648 - val_loss: 0.5745 - val_accuracy: 0.7734\n",
      "Epoch 64/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2553 - accuracy: 0.4688 - val_loss: 0.5763 - val_accuracy: 0.7734\n",
      "Epoch 65/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2550 - accuracy: 0.4676 - val_loss: 0.5782 - val_accuracy: 0.7734\n",
      "Epoch 66/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2539 - accuracy: 0.4665 - val_loss: 0.5756 - val_accuracy: 0.7734\n",
      "Epoch 67/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2532 - accuracy: 0.4648 - val_loss: 0.5760 - val_accuracy: 0.7734\n",
      "Epoch 68/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2544 - accuracy: 0.4704 - val_loss: 0.5712 - val_accuracy: 0.7734\n",
      "Epoch 69/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2528 - accuracy: 0.4654 - val_loss: 0.5768 - val_accuracy: 0.7734\n",
      "Epoch 70/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2518 - accuracy: 0.4676 - val_loss: 0.5754 - val_accuracy: 0.7734\n",
      "Epoch 71/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2514 - accuracy: 0.4688 - val_loss: 0.5756 - val_accuracy: 0.7734\n",
      "Epoch 72/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2506 - accuracy: 0.4660 - val_loss: 0.5719 - val_accuracy: 0.7734\n",
      "Epoch 73/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2494 - accuracy: 0.4665 - val_loss: 0.5724 - val_accuracy: 0.7734\n",
      "Epoch 74/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2498 - accuracy: 0.4671 - val_loss: 0.5757 - val_accuracy: 0.7734\n",
      "Epoch 75/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2487 - accuracy: 0.4648 - val_loss: 0.5707 - val_accuracy: 0.7734\n",
      "Epoch 76/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2483 - accuracy: 0.4671 - val_loss: 0.5710 - val_accuracy: 0.7734\n",
      "Epoch 77/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2473 - accuracy: 0.4643 - val_loss: 0.5724 - val_accuracy: 0.7734\n",
      "Epoch 78/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2462 - accuracy: 0.4660 - val_loss: 0.5690 - val_accuracy: 0.7734\n",
      "Epoch 79/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2467 - accuracy: 0.4676 - val_loss: 0.5684 - val_accuracy: 0.7734\n",
      "Epoch 80/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2454 - accuracy: 0.4665 - val_loss: 0.5733 - val_accuracy: 0.7734\n",
      "Epoch 81/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2456 - accuracy: 0.4682 - val_loss: 0.5666 - val_accuracy: 0.7734\n",
      "Epoch 82/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2443 - accuracy: 0.4671 - val_loss: 0.5669 - val_accuracy: 0.7812\n",
      "Epoch 83/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2432 - accuracy: 0.4654 - val_loss: 0.5681 - val_accuracy: 0.7734\n",
      "Epoch 84/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2438 - accuracy: 0.4671 - val_loss: 0.5669 - val_accuracy: 0.7734\n",
      "Epoch 85/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2430 - accuracy: 0.4660 - val_loss: 0.5673 - val_accuracy: 0.7734\n",
      "Epoch 86/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2426 - accuracy: 0.4665 - val_loss: 0.5682 - val_accuracy: 0.7734\n",
      "Epoch 87/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2421 - accuracy: 0.4665 - val_loss: 0.5655 - val_accuracy: 0.7734\n",
      "Epoch 88/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2422 - accuracy: 0.4688 - val_loss: 0.5678 - val_accuracy: 0.7891\n",
      "Epoch 89/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2426 - accuracy: 0.4654 - val_loss: 0.5715 - val_accuracy: 0.7734\n",
      "Epoch 90/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2413 - accuracy: 0.4688 - val_loss: 0.5659 - val_accuracy: 0.7812\n",
      "Epoch 91/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2416 - accuracy: 0.4693 - val_loss: 0.5682 - val_accuracy: 0.7734\n",
      "Epoch 92/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2409 - accuracy: 0.4671 - val_loss: 0.5668 - val_accuracy: 0.7812\n",
      "Epoch 93/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2411 - accuracy: 0.4676 - val_loss: 0.5671 - val_accuracy: 0.7734\n",
      "Epoch 94/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2410 - accuracy: 0.4671 - val_loss: 0.5672 - val_accuracy: 0.7891\n",
      "Epoch 95/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2408 - accuracy: 0.4704 - val_loss: 0.5654 - val_accuracy: 0.7734\n",
      "Epoch 96/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2408 - accuracy: 0.4665 - val_loss: 0.5645 - val_accuracy: 0.7734\n",
      "Epoch 97/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2402 - accuracy: 0.4676 - val_loss: 0.5661 - val_accuracy: 0.7812\n",
      "Epoch 98/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2398 - accuracy: 0.4676 - val_loss: 0.5658 - val_accuracy: 0.7812\n",
      "Epoch 99/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2402 - accuracy: 0.4660 - val_loss: 0.5676 - val_accuracy: 0.7812\n",
      "Epoch 100/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2397 - accuracy: 0.4665 - val_loss: 0.5664 - val_accuracy: 0.7734\n",
      "Epoch 101/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2391 - accuracy: 0.4699 - val_loss: 0.5671 - val_accuracy: 0.7734\n",
      "Epoch 102/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2387 - accuracy: 0.4654 - val_loss: 0.5655 - val_accuracy: 0.7812\n",
      "Epoch 103/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2386 - accuracy: 0.4671 - val_loss: 0.5663 - val_accuracy: 0.7812\n",
      "Epoch 104/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2392 - accuracy: 0.4676 - val_loss: 0.5643 - val_accuracy: 0.7734\n",
      "Epoch 105/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2384 - accuracy: 0.4688 - val_loss: 0.5666 - val_accuracy: 0.7734\n",
      "Epoch 106/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2386 - accuracy: 0.4676 - val_loss: 0.5659 - val_accuracy: 0.7891\n",
      "Epoch 107/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2379 - accuracy: 0.4671 - val_loss: 0.5679 - val_accuracy: 0.7734\n",
      "Epoch 108/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2383 - accuracy: 0.4676 - val_loss: 0.5651 - val_accuracy: 0.7734\n",
      "Epoch 109/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2382 - accuracy: 0.4688 - val_loss: 0.5624 - val_accuracy: 0.7812\n",
      "Epoch 110/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2376 - accuracy: 0.4676 - val_loss: 0.5632 - val_accuracy: 0.7812\n",
      "Epoch 111/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2377 - accuracy: 0.4660 - val_loss: 0.5661 - val_accuracy: 0.7812\n",
      "Epoch 112/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2373 - accuracy: 0.4682 - val_loss: 0.5618 - val_accuracy: 0.7734\n",
      "Epoch 113/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2371 - accuracy: 0.4676 - val_loss: 0.5629 - val_accuracy: 0.7891\n",
      "Epoch 114/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2366 - accuracy: 0.4665 - val_loss: 0.5626 - val_accuracy: 0.7812\n",
      "Epoch 115/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2379 - accuracy: 0.4660 - val_loss: 0.5639 - val_accuracy: 0.7891\n",
      "Epoch 116/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2370 - accuracy: 0.4648 - val_loss: 0.5607 - val_accuracy: 0.7891\n",
      "Epoch 117/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2359 - accuracy: 0.4665 - val_loss: 0.5633 - val_accuracy: 0.7891\n",
      "Epoch 118/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2363 - accuracy: 0.4632 - val_loss: 0.5643 - val_accuracy: 0.7812\n",
      "Epoch 119/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2361 - accuracy: 0.4665 - val_loss: 0.5648 - val_accuracy: 0.7734\n",
      "Epoch 120/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2361 - accuracy: 0.4660 - val_loss: 0.5612 - val_accuracy: 0.7891\n",
      "Epoch 121/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2365 - accuracy: 0.4665 - val_loss: 0.5611 - val_accuracy: 0.7891\n",
      "Epoch 122/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2363 - accuracy: 0.4682 - val_loss: 0.5638 - val_accuracy: 0.7812\n",
      "Epoch 123/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2360 - accuracy: 0.4665 - val_loss: 0.5629 - val_accuracy: 0.7734\n",
      "Epoch 124/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2356 - accuracy: 0.4654 - val_loss: 0.5623 - val_accuracy: 0.7891\n",
      "Epoch 125/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2354 - accuracy: 0.4648 - val_loss: 0.5598 - val_accuracy: 0.7812\n",
      "Epoch 126/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2350 - accuracy: 0.4671 - val_loss: 0.5622 - val_accuracy: 0.7812\n",
      "Epoch 127/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2353 - accuracy: 0.4648 - val_loss: 0.5596 - val_accuracy: 0.7812\n",
      "Epoch 128/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2356 - accuracy: 0.4660 - val_loss: 0.5601 - val_accuracy: 0.7891\n",
      "Epoch 129/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2357 - accuracy: 0.4665 - val_loss: 0.5631 - val_accuracy: 0.7812\n",
      "Epoch 130/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2355 - accuracy: 0.4626 - val_loss: 0.5618 - val_accuracy: 0.7891\n",
      "Epoch 131/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2350 - accuracy: 0.4626 - val_loss: 0.5648 - val_accuracy: 0.7969\n",
      "Epoch 132/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2343 - accuracy: 0.4648 - val_loss: 0.5615 - val_accuracy: 0.7891\n",
      "Epoch 133/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2357 - accuracy: 0.4643 - val_loss: 0.5587 - val_accuracy: 0.7891\n",
      "Epoch 134/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2342 - accuracy: 0.4648 - val_loss: 0.5627 - val_accuracy: 0.7891\n",
      "Epoch 135/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2342 - accuracy: 0.4637 - val_loss: 0.5613 - val_accuracy: 0.7891\n",
      "Epoch 136/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2345 - accuracy: 0.4643 - val_loss: 0.5601 - val_accuracy: 0.7891\n",
      "Epoch 137/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2339 - accuracy: 0.4643 - val_loss: 0.5588 - val_accuracy: 0.7891\n",
      "Epoch 138/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2345 - accuracy: 0.4660 - val_loss: 0.5634 - val_accuracy: 0.7891\n",
      "Epoch 139/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2339 - accuracy: 0.4643 - val_loss: 0.5594 - val_accuracy: 0.7891\n",
      "Epoch 140/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2340 - accuracy: 0.4643 - val_loss: 0.5624 - val_accuracy: 0.7891\n",
      "Epoch 141/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2352 - accuracy: 0.4637 - val_loss: 0.5591 - val_accuracy: 0.7812\n",
      "Epoch 142/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2341 - accuracy: 0.4665 - val_loss: 0.5591 - val_accuracy: 0.7969\n",
      "Epoch 143/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2339 - accuracy: 0.4660 - val_loss: 0.5594 - val_accuracy: 0.7891\n",
      "Epoch 144/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2341 - accuracy: 0.4671 - val_loss: 0.5601 - val_accuracy: 0.7812\n",
      "Epoch 145/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2348 - accuracy: 0.4665 - val_loss: 0.5628 - val_accuracy: 0.7891\n",
      "Epoch 146/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2363 - accuracy: 0.4660 - val_loss: 0.5591 - val_accuracy: 0.7891\n",
      "Epoch 147/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2334 - accuracy: 0.4643 - val_loss: 0.5617 - val_accuracy: 0.7891\n",
      "Epoch 148/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2342 - accuracy: 0.4654 - val_loss: 0.5609 - val_accuracy: 0.7891\n",
      "Epoch 149/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2341 - accuracy: 0.4688 - val_loss: 0.5611 - val_accuracy: 0.7891\n",
      "Epoch 150/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2333 - accuracy: 0.4648 - val_loss: 0.5617 - val_accuracy: 0.7891\n",
      "Epoch 151/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2353 - accuracy: 0.4671 - val_loss: 0.5608 - val_accuracy: 0.7734\n",
      "Epoch 152/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2353 - accuracy: 0.4699 - val_loss: 0.5608 - val_accuracy: 0.7969\n",
      "Epoch 153/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2330 - accuracy: 0.4643 - val_loss: 0.5584 - val_accuracy: 0.7969\n",
      "Epoch 154/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2339 - accuracy: 0.4665 - val_loss: 0.5608 - val_accuracy: 0.7891\n",
      "Epoch 155/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2335 - accuracy: 0.4676 - val_loss: 0.5589 - val_accuracy: 0.7969\n",
      "Epoch 156/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2345 - accuracy: 0.4648 - val_loss: 0.5623 - val_accuracy: 0.7891\n",
      "Epoch 157/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2326 - accuracy: 0.4660 - val_loss: 0.5617 - val_accuracy: 0.7812\n",
      "Epoch 158/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2333 - accuracy: 0.4665 - val_loss: 0.5583 - val_accuracy: 0.7891\n",
      "Epoch 159/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2333 - accuracy: 0.4660 - val_loss: 0.5594 - val_accuracy: 0.7891\n",
      "Epoch 160/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2342 - accuracy: 0.4682 - val_loss: 0.5629 - val_accuracy: 0.7891\n",
      "Epoch 161/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2330 - accuracy: 0.4671 - val_loss: 0.5615 - val_accuracy: 0.7891\n",
      "Epoch 162/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2331 - accuracy: 0.4665 - val_loss: 0.5623 - val_accuracy: 0.7891\n",
      "Epoch 163/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2336 - accuracy: 0.4660 - val_loss: 0.5610 - val_accuracy: 0.7891\n",
      "Epoch 164/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2333 - accuracy: 0.4704 - val_loss: 0.5591 - val_accuracy: 0.7969\n",
      "Epoch 165/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2335 - accuracy: 0.4693 - val_loss: 0.5577 - val_accuracy: 0.7969\n",
      "Epoch 166/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2325 - accuracy: 0.4665 - val_loss: 0.5612 - val_accuracy: 0.7734\n",
      "Epoch 167/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2335 - accuracy: 0.4693 - val_loss: 0.5592 - val_accuracy: 0.7891\n",
      "Epoch 168/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2331 - accuracy: 0.4660 - val_loss: 0.5605 - val_accuracy: 0.7891\n",
      "Epoch 169/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2324 - accuracy: 0.4660 - val_loss: 0.5603 - val_accuracy: 0.7891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2330 - accuracy: 0.4654 - val_loss: 0.5621 - val_accuracy: 0.7891\n",
      "Epoch 171/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2325 - accuracy: 0.4665 - val_loss: 0.5609 - val_accuracy: 0.7891\n",
      "Epoch 172/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2329 - accuracy: 0.4654 - val_loss: 0.5627 - val_accuracy: 0.7891\n",
      "Epoch 173/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2326 - accuracy: 0.4671 - val_loss: 0.5589 - val_accuracy: 0.7891\n",
      "Epoch 174/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2330 - accuracy: 0.4710 - val_loss: 0.5578 - val_accuracy: 0.7891\n",
      "Epoch 175/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2325 - accuracy: 0.4654 - val_loss: 0.5612 - val_accuracy: 0.7891\n",
      "Epoch 176/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2327 - accuracy: 0.4643 - val_loss: 0.5622 - val_accuracy: 0.7891\n",
      "Epoch 177/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2325 - accuracy: 0.4671 - val_loss: 0.5616 - val_accuracy: 0.7891\n",
      "Epoch 178/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2329 - accuracy: 0.4665 - val_loss: 0.5624 - val_accuracy: 0.7812\n",
      "Epoch 179/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2331 - accuracy: 0.4660 - val_loss: 0.5593 - val_accuracy: 0.7891\n",
      "Epoch 180/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2326 - accuracy: 0.4637 - val_loss: 0.5597 - val_accuracy: 0.7891\n",
      "Epoch 181/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2329 - accuracy: 0.4660 - val_loss: 0.5635 - val_accuracy: 0.7734\n",
      "Epoch 182/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2322 - accuracy: 0.4665 - val_loss: 0.5597 - val_accuracy: 0.7891\n",
      "Epoch 183/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2330 - accuracy: 0.4671 - val_loss: 0.5604 - val_accuracy: 0.7891\n",
      "Epoch 184/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2333 - accuracy: 0.4682 - val_loss: 0.5630 - val_accuracy: 0.7734\n",
      "Epoch 185/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2332 - accuracy: 0.4665 - val_loss: 0.5617 - val_accuracy: 0.7734\n",
      "Epoch 186/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2319 - accuracy: 0.4671 - val_loss: 0.5615 - val_accuracy: 0.7734\n",
      "Epoch 187/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2324 - accuracy: 0.4676 - val_loss: 0.5599 - val_accuracy: 0.7891\n",
      "Epoch 188/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2325 - accuracy: 0.4665 - val_loss: 0.5586 - val_accuracy: 0.7891\n",
      "Epoch 189/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2332 - accuracy: 0.4676 - val_loss: 0.5596 - val_accuracy: 0.7812\n",
      "Epoch 190/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2325 - accuracy: 0.4665 - val_loss: 0.5614 - val_accuracy: 0.7734\n",
      "Epoch 191/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2324 - accuracy: 0.4660 - val_loss: 0.5600 - val_accuracy: 0.7891\n",
      "Epoch 192/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2324 - accuracy: 0.4693 - val_loss: 0.5604 - val_accuracy: 0.7734\n",
      "Epoch 193/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2322 - accuracy: 0.4660 - val_loss: 0.5597 - val_accuracy: 0.7734\n",
      "Epoch 194/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2326 - accuracy: 0.4665 - val_loss: 0.5628 - val_accuracy: 0.7734\n",
      "Epoch 195/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2328 - accuracy: 0.4637 - val_loss: 0.5633 - val_accuracy: 0.7734\n",
      "Epoch 196/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2319 - accuracy: 0.4660 - val_loss: 0.5597 - val_accuracy: 0.7812\n",
      "Epoch 197/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2321 - accuracy: 0.4660 - val_loss: 0.5619 - val_accuracy: 0.7734\n",
      "Epoch 198/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2320 - accuracy: 0.4682 - val_loss: 0.5602 - val_accuracy: 0.7812\n",
      "Epoch 199/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2329 - accuracy: 0.4660 - val_loss: 0.5607 - val_accuracy: 0.7734\n",
      "Epoch 200/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2324 - accuracy: 0.4660 - val_loss: 0.5592 - val_accuracy: 0.7734\n",
      "Epoch 201/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2316 - accuracy: 0.4654 - val_loss: 0.5626 - val_accuracy: 0.7891\n",
      "Epoch 202/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2319 - accuracy: 0.4654 - val_loss: 0.5604 - val_accuracy: 0.7734\n",
      "Epoch 203/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2321 - accuracy: 0.4637 - val_loss: 0.5598 - val_accuracy: 0.7734\n",
      "Epoch 204/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2323 - accuracy: 0.4665 - val_loss: 0.5610 - val_accuracy: 0.7734\n",
      "Epoch 205/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2321 - accuracy: 0.4665 - val_loss: 0.5599 - val_accuracy: 0.7891\n",
      "Epoch 206/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2320 - accuracy: 0.4643 - val_loss: 0.5591 - val_accuracy: 0.7891\n",
      "Epoch 207/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2320 - accuracy: 0.4660 - val_loss: 0.5618 - val_accuracy: 0.7734\n",
      "Epoch 208/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2321 - accuracy: 0.4671 - val_loss: 0.5573 - val_accuracy: 0.7891\n",
      "Epoch 209/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2314 - accuracy: 0.4648 - val_loss: 0.5619 - val_accuracy: 0.7734\n",
      "Epoch 210/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2320 - accuracy: 0.4665 - val_loss: 0.5633 - val_accuracy: 0.7891\n",
      "Epoch 211/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2317 - accuracy: 0.4654 - val_loss: 0.5613 - val_accuracy: 0.7734\n",
      "Epoch 212/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2317 - accuracy: 0.4665 - val_loss: 0.5604 - val_accuracy: 0.7734\n",
      "Epoch 213/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2311 - accuracy: 0.4660 - val_loss: 0.5591 - val_accuracy: 0.7812\n",
      "Epoch 214/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2314 - accuracy: 0.4648 - val_loss: 0.5613 - val_accuracy: 0.7734\n",
      "Epoch 215/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2320 - accuracy: 0.4665 - val_loss: 0.5626 - val_accuracy: 0.7734\n",
      "Epoch 216/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2318 - accuracy: 0.4682 - val_loss: 0.5592 - val_accuracy: 0.7734\n",
      "Epoch 217/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2317 - accuracy: 0.4648 - val_loss: 0.5619 - val_accuracy: 0.7734\n",
      "Epoch 218/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2321 - accuracy: 0.4654 - val_loss: 0.5576 - val_accuracy: 0.7891\n",
      "Epoch 219/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2328 - accuracy: 0.4660 - val_loss: 0.5605 - val_accuracy: 0.7734\n",
      "Epoch 220/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2316 - accuracy: 0.4654 - val_loss: 0.5590 - val_accuracy: 0.7734\n",
      "Epoch 221/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2312 - accuracy: 0.4660 - val_loss: 0.5607 - val_accuracy: 0.7734\n",
      "Epoch 222/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2316 - accuracy: 0.4654 - val_loss: 0.5602 - val_accuracy: 0.7891\n",
      "Epoch 223/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2316 - accuracy: 0.4671 - val_loss: 0.5601 - val_accuracy: 0.7734\n",
      "Epoch 224/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2321 - accuracy: 0.4660 - val_loss: 0.5595 - val_accuracy: 0.7734\n",
      "Epoch 225/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2313 - accuracy: 0.4643 - val_loss: 0.5616 - val_accuracy: 0.7891\n",
      "Epoch 226/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2316 - accuracy: 0.4648 - val_loss: 0.5606 - val_accuracy: 0.7734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2314 - accuracy: 0.4654 - val_loss: 0.5611 - val_accuracy: 0.7891\n",
      "Epoch 228/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2321 - accuracy: 0.4660 - val_loss: 0.5608 - val_accuracy: 0.7891\n",
      "Epoch 229/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2316 - accuracy: 0.4665 - val_loss: 0.5594 - val_accuracy: 0.7891\n",
      "Epoch 230/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2313 - accuracy: 0.4654 - val_loss: 0.5596 - val_accuracy: 0.7734\n",
      "Epoch 231/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2319 - accuracy: 0.4660 - val_loss: 0.5591 - val_accuracy: 0.7891\n",
      "Epoch 232/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2312 - accuracy: 0.4654 - val_loss: 0.5628 - val_accuracy: 0.7734\n",
      "Epoch 233/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2311 - accuracy: 0.4660 - val_loss: 0.5592 - val_accuracy: 0.7891\n",
      "Epoch 234/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2321 - accuracy: 0.4654 - val_loss: 0.5620 - val_accuracy: 0.7891\n",
      "Epoch 235/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2315 - accuracy: 0.4654 - val_loss: 0.5597 - val_accuracy: 0.7734\n",
      "Epoch 236/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2310 - accuracy: 0.4676 - val_loss: 0.5578 - val_accuracy: 0.7891\n",
      "Epoch 237/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2309 - accuracy: 0.4654 - val_loss: 0.5590 - val_accuracy: 0.7734\n",
      "Epoch 238/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2310 - accuracy: 0.4654 - val_loss: 0.5630 - val_accuracy: 0.7734\n",
      "Epoch 239/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2318 - accuracy: 0.4671 - val_loss: 0.5563 - val_accuracy: 0.7812\n",
      "Epoch 240/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2305 - accuracy: 0.4671 - val_loss: 0.5618 - val_accuracy: 0.7891\n",
      "Epoch 241/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2313 - accuracy: 0.4654 - val_loss: 0.5606 - val_accuracy: 0.7734\n",
      "Epoch 242/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2311 - accuracy: 0.4654 - val_loss: 0.5580 - val_accuracy: 0.7734\n",
      "Epoch 243/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2315 - accuracy: 0.4676 - val_loss: 0.5575 - val_accuracy: 0.7891\n",
      "Epoch 244/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2313 - accuracy: 0.4648 - val_loss: 0.5626 - val_accuracy: 0.7734\n",
      "Epoch 245/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2311 - accuracy: 0.4654 - val_loss: 0.5608 - val_accuracy: 0.7734\n",
      "Epoch 246/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2314 - accuracy: 0.4699 - val_loss: 0.5581 - val_accuracy: 0.7734\n",
      "Epoch 247/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2318 - accuracy: 0.4676 - val_loss: 0.5610 - val_accuracy: 0.7734\n",
      "Epoch 248/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2312 - accuracy: 0.4665 - val_loss: 0.5600 - val_accuracy: 0.7734\n",
      "Epoch 249/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2317 - accuracy: 0.4648 - val_loss: 0.5619 - val_accuracy: 0.7734\n",
      "Epoch 250/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2306 - accuracy: 0.4660 - val_loss: 0.5601 - val_accuracy: 0.7734\n",
      "Epoch 251/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2324 - accuracy: 0.4654 - val_loss: 0.5582 - val_accuracy: 0.7812\n",
      "Epoch 252/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2315 - accuracy: 0.4665 - val_loss: 0.5587 - val_accuracy: 0.7891\n",
      "Epoch 253/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2305 - accuracy: 0.4648 - val_loss: 0.5628 - val_accuracy: 0.7734\n",
      "Epoch 254/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2304 - accuracy: 0.4648 - val_loss: 0.5593 - val_accuracy: 0.7891\n",
      "Epoch 255/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2312 - accuracy: 0.4665 - val_loss: 0.5579 - val_accuracy: 0.7812\n",
      "Epoch 256/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2310 - accuracy: 0.4648 - val_loss: 0.5613 - val_accuracy: 0.7734\n",
      "Epoch 257/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2319 - accuracy: 0.4654 - val_loss: 0.5617 - val_accuracy: 0.7734\n",
      "Epoch 258/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2316 - accuracy: 0.4665 - val_loss: 0.5585 - val_accuracy: 0.7812\n",
      "Epoch 259/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2310 - accuracy: 0.4665 - val_loss: 0.5622 - val_accuracy: 0.7891\n",
      "Epoch 260/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2314 - accuracy: 0.4660 - val_loss: 0.5573 - val_accuracy: 0.7891\n",
      "Epoch 261/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2310 - accuracy: 0.4660 - val_loss: 0.5603 - val_accuracy: 0.7734\n",
      "Epoch 262/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2313 - accuracy: 0.4665 - val_loss: 0.5615 - val_accuracy: 0.7734\n",
      "Epoch 263/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2308 - accuracy: 0.4688 - val_loss: 0.5582 - val_accuracy: 0.7891\n",
      "Epoch 264/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2313 - accuracy: 0.4654 - val_loss: 0.5561 - val_accuracy: 0.7891\n",
      "Epoch 265/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2307 - accuracy: 0.4648 - val_loss: 0.5608 - val_accuracy: 0.7734\n",
      "Epoch 266/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2305 - accuracy: 0.4676 - val_loss: 0.5583 - val_accuracy: 0.7891\n",
      "Epoch 267/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2322 - accuracy: 0.4648 - val_loss: 0.5609 - val_accuracy: 0.7891\n",
      "Epoch 268/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2311 - accuracy: 0.4688 - val_loss: 0.5582 - val_accuracy: 0.7734\n",
      "Epoch 269/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2305 - accuracy: 0.4665 - val_loss: 0.5596 - val_accuracy: 0.7734\n",
      "Epoch 270/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2317 - accuracy: 0.4665 - val_loss: 0.5603 - val_accuracy: 0.7891\n",
      "Epoch 271/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2314 - accuracy: 0.4648 - val_loss: 0.5581 - val_accuracy: 0.7812\n",
      "Epoch 272/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2310 - accuracy: 0.4660 - val_loss: 0.5574 - val_accuracy: 0.7891\n",
      "Epoch 273/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2313 - accuracy: 0.4671 - val_loss: 0.5595 - val_accuracy: 0.7734\n",
      "Epoch 274/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2310 - accuracy: 0.4648 - val_loss: 0.5571 - val_accuracy: 0.7891\n",
      "Epoch 275/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2310 - accuracy: 0.4654 - val_loss: 0.5590 - val_accuracy: 0.7891\n",
      "Epoch 276/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2303 - accuracy: 0.4648 - val_loss: 0.5591 - val_accuracy: 0.7734\n",
      "Epoch 277/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2305 - accuracy: 0.4665 - val_loss: 0.5571 - val_accuracy: 0.7891\n",
      "Epoch 278/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2308 - accuracy: 0.4660 - val_loss: 0.5587 - val_accuracy: 0.7734\n",
      "Epoch 279/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2309 - accuracy: 0.4643 - val_loss: 0.5591 - val_accuracy: 0.7734\n",
      "Epoch 280/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2307 - accuracy: 0.4660 - val_loss: 0.5606 - val_accuracy: 0.7891\n",
      "Epoch 281/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2305 - accuracy: 0.4676 - val_loss: 0.5564 - val_accuracy: 0.7891\n",
      "Epoch 282/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2301 - accuracy: 0.4654 - val_loss: 0.5622 - val_accuracy: 0.7734\n",
      "Epoch 283/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2308 - accuracy: 0.4654 - val_loss: 0.5583 - val_accuracy: 0.7734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 284/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2309 - accuracy: 0.4660 - val_loss: 0.5593 - val_accuracy: 0.7734\n",
      "Epoch 285/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2312 - accuracy: 0.4660 - val_loss: 0.5591 - val_accuracy: 0.7734\n",
      "Epoch 286/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2306 - accuracy: 0.4637 - val_loss: 0.5585 - val_accuracy: 0.7734\n",
      "Epoch 287/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2312 - accuracy: 0.4660 - val_loss: 0.5580 - val_accuracy: 0.7734\n",
      "Epoch 288/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2305 - accuracy: 0.4665 - val_loss: 0.5601 - val_accuracy: 0.7734\n",
      "Epoch 289/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2305 - accuracy: 0.4660 - val_loss: 0.5611 - val_accuracy: 0.7734\n",
      "Epoch 290/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2307 - accuracy: 0.4676 - val_loss: 0.5570 - val_accuracy: 0.7891\n",
      "Epoch 291/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2306 - accuracy: 0.4671 - val_loss: 0.5612 - val_accuracy: 0.7734\n",
      "Epoch 292/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2301 - accuracy: 0.4660 - val_loss: 0.5589 - val_accuracy: 0.7734\n",
      "Epoch 293/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2302 - accuracy: 0.4665 - val_loss: 0.5583 - val_accuracy: 0.7891\n",
      "Epoch 294/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2311 - accuracy: 0.4654 - val_loss: 0.5584 - val_accuracy: 0.7891\n",
      "Epoch 295/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2309 - accuracy: 0.4648 - val_loss: 0.5582 - val_accuracy: 0.7734\n",
      "Epoch 296/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2302 - accuracy: 0.4660 - val_loss: 0.5569 - val_accuracy: 0.7891\n",
      "Epoch 297/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.2301 - accuracy: 0.4671 - val_loss: 0.5571 - val_accuracy: 0.7891\n",
      "Epoch 298/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.2305 - accuracy: 0.4665 - val_loss: 0.5586 - val_accuracy: 0.7734\n",
      "Epoch 299/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.2311 - accuracy: 0.4699 - val_loss: 0.5560 - val_accuracy: 0.7891\n",
      "Epoch 300/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.2299 - accuracy: 0.4648 - val_loss: 0.5582 - val_accuracy: 0.7812\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "56/56 [==============================] - 0s 1ms/step\n",
      "Recall: [0.93 0.   0.53]\n",
      "Precision: [0.39 0.6  0.68]\n",
      "F1-score: [0.55 0.01 0.6 ]\n",
      "Accuracy: 0.47\n",
      "Brier score:0.20643405781537166\n",
      "Brier climat:0.21053323412698413\n",
      "Brier skill score:0.019470447640300037\n",
      "Recall: [0.93 0.   0.53]\n",
      "Precision: [0.39 0.6  0.68]\n",
      "F1-score: [0.55 0.01 0.6 ]\n",
      "Accuracy: 0.47\n",
      "Brier score:0.22820445388492308\n",
      "Brier climat:0.2258531746031746\n",
      "Brier skill score:-0.010410654115797557\n",
      "Recall: [0.93 0.   0.53]\n",
      "Precision: [0.39 0.6  0.68]\n",
      "F1-score: [0.55 0.01 0.6 ]\n",
      "Accuracy: 0.47\n",
      "Brier score:0.16111353091663255\n",
      "Brier climat:0.21982886904761906\n",
      "Brier skill score:0.26709566575747457\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "Recall: [1.   0.   0.83]\n",
      "Precision: [0.56 0.   1.  ]\n",
      "F1-score: [0.71 0.   0.91]\n",
      "Accuracy: 0.78\n",
      "Brier score:0.10379839518653489\n",
      "Brier climat:0.2059201388888889\n",
      "Brier skill score:0.4959288792897387\n",
      "Recall: [1.   0.   0.83]\n",
      "Precision: [0.56 0.   1.  ]\n",
      "F1-score: [0.71 0.   0.91]\n",
      "Accuracy: 0.78\n",
      "Brier score:0.09442778577392494\n",
      "Brier climat:0.15784722222222225\n",
      "Brier skill score:0.401777336056085\n",
      "Recall: [1.   0.   0.83]\n",
      "Precision: [0.56 0.   1.  ]\n",
      "F1-score: [0.71 0.   0.91]\n",
      "Accuracy: 0.78\n",
      "Brier score:0.07798806297471397\n",
      "Brier climat:0.30911458333333336\n",
      "Brier skill score:0.7477050026765782\n",
      "******** 16\n",
      "validation years [1997, 1998]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "{0: 2.0, 1: 1.0, 2: 1.0}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 60)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 183       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183\n",
      "Trainable params: 183\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - ETA: 0s - loss: 2.0101 - accuracy: 0.3192WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "28/28 [==============================] - 4s 106ms/step - loss: 2.0101 - accuracy: 0.3192 - val_loss: 2.0712 - val_accuracy: 0.3125\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.7510 - accuracy: 0.3410 - val_loss: 1.2744 - val_accuracy: 0.3906\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.6239 - accuracy: 0.3929 - val_loss: 0.9778 - val_accuracy: 0.7734\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.5625 - accuracy: 0.4386 - val_loss: 0.8793 - val_accuracy: 0.7734\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.5287 - accuracy: 0.4581 - val_loss: 0.8223 - val_accuracy: 0.7734\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.5057 - accuracy: 0.4587 - val_loss: 0.7797 - val_accuracy: 0.7734\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4887 - accuracy: 0.4609 - val_loss: 0.7590 - val_accuracy: 0.7734\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.4752 - accuracy: 0.4660 - val_loss: 0.7429 - val_accuracy: 0.7734\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4629 - accuracy: 0.4676 - val_loss: 0.7274 - val_accuracy: 0.7656\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4509 - accuracy: 0.4654 - val_loss: 0.7138 - val_accuracy: 0.7734\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4403 - accuracy: 0.4688 - val_loss: 0.6997 - val_accuracy: 0.7734\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.4299 - accuracy: 0.4727 - val_loss: 0.6983 - val_accuracy: 0.7734\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4216 - accuracy: 0.4777 - val_loss: 0.6868 - val_accuracy: 0.7578\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.4123 - accuracy: 0.4732 - val_loss: 0.6854 - val_accuracy: 0.7656\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4046 - accuracy: 0.4749 - val_loss: 0.6675 - val_accuracy: 0.7812\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3958 - accuracy: 0.4766 - val_loss: 0.6738 - val_accuracy: 0.7734\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3884 - accuracy: 0.4771 - val_loss: 0.6659 - val_accuracy: 0.7734\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3809 - accuracy: 0.4760 - val_loss: 0.6622 - val_accuracy: 0.7812\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3742 - accuracy: 0.4805 - val_loss: 0.6509 - val_accuracy: 0.7812\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3676 - accuracy: 0.4788 - val_loss: 0.6455 - val_accuracy: 0.7734\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3619 - accuracy: 0.4782 - val_loss: 0.6376 - val_accuracy: 0.7812\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3567 - accuracy: 0.4782 - val_loss: 0.6367 - val_accuracy: 0.7812\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3509 - accuracy: 0.4771 - val_loss: 0.6384 - val_accuracy: 0.7812\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3467 - accuracy: 0.4766 - val_loss: 0.6265 - val_accuracy: 0.8047\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3422 - accuracy: 0.4721 - val_loss: 0.6243 - val_accuracy: 0.8125\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.3374 - accuracy: 0.4732 - val_loss: 0.6182 - val_accuracy: 0.8125\n",
      "Epoch 27/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3335 - accuracy: 0.4727 - val_loss: 0.6169 - val_accuracy: 0.8125\n",
      "Epoch 28/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3298 - accuracy: 0.4766 - val_loss: 0.6243 - val_accuracy: 0.7969\n",
      "Epoch 29/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3261 - accuracy: 0.4710 - val_loss: 0.6169 - val_accuracy: 0.8125\n",
      "Epoch 30/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3222 - accuracy: 0.4732 - val_loss: 0.6086 - val_accuracy: 0.8125\n",
      "Epoch 31/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3187 - accuracy: 0.4715 - val_loss: 0.6084 - val_accuracy: 0.8125\n",
      "Epoch 32/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3154 - accuracy: 0.4710 - val_loss: 0.6135 - val_accuracy: 0.8125\n",
      "Epoch 33/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3119 - accuracy: 0.4727 - val_loss: 0.6008 - val_accuracy: 0.8125\n",
      "Epoch 34/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3083 - accuracy: 0.4760 - val_loss: 0.5974 - val_accuracy: 0.8125\n",
      "Epoch 35/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3049 - accuracy: 0.4727 - val_loss: 0.5971 - val_accuracy: 0.8125\n",
      "Epoch 36/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3019 - accuracy: 0.4715 - val_loss: 0.5879 - val_accuracy: 0.8125\n",
      "Epoch 37/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2997 - accuracy: 0.4704 - val_loss: 0.5834 - val_accuracy: 0.8125\n",
      "Epoch 38/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2963 - accuracy: 0.4727 - val_loss: 0.5844 - val_accuracy: 0.8125\n",
      "Epoch 39/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2935 - accuracy: 0.4732 - val_loss: 0.5821 - val_accuracy: 0.8125\n",
      "Epoch 40/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2915 - accuracy: 0.4699 - val_loss: 0.5806 - val_accuracy: 0.8125\n",
      "Epoch 41/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2887 - accuracy: 0.4704 - val_loss: 0.5817 - val_accuracy: 0.8125\n",
      "Epoch 42/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2869 - accuracy: 0.4710 - val_loss: 0.5764 - val_accuracy: 0.8125\n",
      "Epoch 43/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2844 - accuracy: 0.4710 - val_loss: 0.5709 - val_accuracy: 0.8125\n",
      "Epoch 44/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2826 - accuracy: 0.4704 - val_loss: 0.5689 - val_accuracy: 0.8125\n",
      "Epoch 45/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2808 - accuracy: 0.4721 - val_loss: 0.5566 - val_accuracy: 0.8125\n",
      "Epoch 46/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2782 - accuracy: 0.4715 - val_loss: 0.5740 - val_accuracy: 0.8125\n",
      "Epoch 47/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2758 - accuracy: 0.4710 - val_loss: 0.5654 - val_accuracy: 0.8125\n",
      "Epoch 48/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2740 - accuracy: 0.4721 - val_loss: 0.5607 - val_accuracy: 0.8125\n",
      "Epoch 49/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2720 - accuracy: 0.4738 - val_loss: 0.5604 - val_accuracy: 0.8125\n",
      "Epoch 50/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2702 - accuracy: 0.4732 - val_loss: 0.5581 - val_accuracy: 0.8125\n",
      "Epoch 51/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2690 - accuracy: 0.4715 - val_loss: 0.5626 - val_accuracy: 0.8125\n",
      "Epoch 52/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2672 - accuracy: 0.4727 - val_loss: 0.5546 - val_accuracy: 0.8125\n",
      "Epoch 53/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2664 - accuracy: 0.4749 - val_loss: 0.5618 - val_accuracy: 0.8125\n",
      "Epoch 54/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2642 - accuracy: 0.4738 - val_loss: 0.5565 - val_accuracy: 0.8125\n",
      "Epoch 55/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2630 - accuracy: 0.4738 - val_loss: 0.5556 - val_accuracy: 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2616 - accuracy: 0.4738 - val_loss: 0.5582 - val_accuracy: 0.8125\n",
      "Epoch 57/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2611 - accuracy: 0.4732 - val_loss: 0.5566 - val_accuracy: 0.8125\n",
      "Epoch 58/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2591 - accuracy: 0.4732 - val_loss: 0.5476 - val_accuracy: 0.8125\n",
      "Epoch 59/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2580 - accuracy: 0.4760 - val_loss: 0.5515 - val_accuracy: 0.8125\n",
      "Epoch 60/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2563 - accuracy: 0.4749 - val_loss: 0.5499 - val_accuracy: 0.8125\n",
      "Epoch 61/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2560 - accuracy: 0.4727 - val_loss: 0.5431 - val_accuracy: 0.8125\n",
      "Epoch 62/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2549 - accuracy: 0.4749 - val_loss: 0.5448 - val_accuracy: 0.8125\n",
      "Epoch 63/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2541 - accuracy: 0.4743 - val_loss: 0.5471 - val_accuracy: 0.8125\n",
      "Epoch 64/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2521 - accuracy: 0.4760 - val_loss: 0.5587 - val_accuracy: 0.8125\n",
      "Epoch 65/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2517 - accuracy: 0.4760 - val_loss: 0.5438 - val_accuracy: 0.8125\n",
      "Epoch 66/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2510 - accuracy: 0.4754 - val_loss: 0.5524 - val_accuracy: 0.8125\n",
      "Epoch 67/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2502 - accuracy: 0.4721 - val_loss: 0.5426 - val_accuracy: 0.8125\n",
      "Epoch 68/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2490 - accuracy: 0.4743 - val_loss: 0.5418 - val_accuracy: 0.8125\n",
      "Epoch 69/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2477 - accuracy: 0.4749 - val_loss: 0.5480 - val_accuracy: 0.8125\n",
      "Epoch 70/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2476 - accuracy: 0.4749 - val_loss: 0.5394 - val_accuracy: 0.8125\n",
      "Epoch 71/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2469 - accuracy: 0.4754 - val_loss: 0.5563 - val_accuracy: 0.8125\n",
      "Epoch 72/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2467 - accuracy: 0.4743 - val_loss: 0.5480 - val_accuracy: 0.8125\n",
      "Epoch 73/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2459 - accuracy: 0.4754 - val_loss: 0.5551 - val_accuracy: 0.8125\n",
      "Epoch 74/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2449 - accuracy: 0.4749 - val_loss: 0.5519 - val_accuracy: 0.8125\n",
      "Epoch 75/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2449 - accuracy: 0.4777 - val_loss: 0.5454 - val_accuracy: 0.8125\n",
      "Epoch 76/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2435 - accuracy: 0.4771 - val_loss: 0.5485 - val_accuracy: 0.8125\n",
      "Epoch 77/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2439 - accuracy: 0.4794 - val_loss: 0.5555 - val_accuracy: 0.8125\n",
      "Epoch 78/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2428 - accuracy: 0.4771 - val_loss: 0.5527 - val_accuracy: 0.8125\n",
      "Epoch 79/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2418 - accuracy: 0.4782 - val_loss: 0.5512 - val_accuracy: 0.8125\n",
      "Epoch 80/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2421 - accuracy: 0.4799 - val_loss: 0.5465 - val_accuracy: 0.8125\n",
      "Epoch 81/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2414 - accuracy: 0.4754 - val_loss: 0.5508 - val_accuracy: 0.8125\n",
      "Epoch 82/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2412 - accuracy: 0.4782 - val_loss: 0.5446 - val_accuracy: 0.8125\n",
      "Epoch 83/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2403 - accuracy: 0.4782 - val_loss: 0.5508 - val_accuracy: 0.8125\n",
      "Epoch 84/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2404 - accuracy: 0.4816 - val_loss: 0.5497 - val_accuracy: 0.8125\n",
      "Epoch 85/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2398 - accuracy: 0.4799 - val_loss: 0.5533 - val_accuracy: 0.8125\n",
      "Epoch 86/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2392 - accuracy: 0.4794 - val_loss: 0.5534 - val_accuracy: 0.8125\n",
      "Epoch 87/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2390 - accuracy: 0.4782 - val_loss: 0.5550 - val_accuracy: 0.8125\n",
      "Epoch 88/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2380 - accuracy: 0.4805 - val_loss: 0.5481 - val_accuracy: 0.8125\n",
      "Epoch 89/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2390 - accuracy: 0.4788 - val_loss: 0.5501 - val_accuracy: 0.8125\n",
      "Epoch 90/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2373 - accuracy: 0.4788 - val_loss: 0.5507 - val_accuracy: 0.8125\n",
      "Epoch 91/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2384 - accuracy: 0.4788 - val_loss: 0.5434 - val_accuracy: 0.8125\n",
      "Epoch 92/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2377 - accuracy: 0.4810 - val_loss: 0.5447 - val_accuracy: 0.8125\n",
      "Epoch 93/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2378 - accuracy: 0.4794 - val_loss: 0.5474 - val_accuracy: 0.8125\n",
      "Epoch 94/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2380 - accuracy: 0.4821 - val_loss: 0.5515 - val_accuracy: 0.8125\n",
      "Epoch 95/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2380 - accuracy: 0.4788 - val_loss: 0.5445 - val_accuracy: 0.8125\n",
      "Epoch 96/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2367 - accuracy: 0.4799 - val_loss: 0.5484 - val_accuracy: 0.8125\n",
      "Epoch 97/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2361 - accuracy: 0.4810 - val_loss: 0.5499 - val_accuracy: 0.8125\n",
      "Epoch 98/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2361 - accuracy: 0.4805 - val_loss: 0.5526 - val_accuracy: 0.8125\n",
      "Epoch 99/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2364 - accuracy: 0.4805 - val_loss: 0.5534 - val_accuracy: 0.8125\n",
      "Epoch 100/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2361 - accuracy: 0.4821 - val_loss: 0.5472 - val_accuracy: 0.8125\n",
      "Epoch 101/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2354 - accuracy: 0.4821 - val_loss: 0.5462 - val_accuracy: 0.8125\n",
      "Epoch 102/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2351 - accuracy: 0.4838 - val_loss: 0.5478 - val_accuracy: 0.8125\n",
      "Epoch 103/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2347 - accuracy: 0.4821 - val_loss: 0.5473 - val_accuracy: 0.8125\n",
      "Epoch 104/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2344 - accuracy: 0.4816 - val_loss: 0.5450 - val_accuracy: 0.8125\n",
      "Epoch 105/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2359 - accuracy: 0.4816 - val_loss: 0.5526 - val_accuracy: 0.8125\n",
      "Epoch 106/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2341 - accuracy: 0.4816 - val_loss: 0.5412 - val_accuracy: 0.8125\n",
      "Epoch 107/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2338 - accuracy: 0.4816 - val_loss: 0.5459 - val_accuracy: 0.8125\n",
      "Epoch 108/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2349 - accuracy: 0.4805 - val_loss: 0.5428 - val_accuracy: 0.8125\n",
      "Epoch 109/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2340 - accuracy: 0.4838 - val_loss: 0.5415 - val_accuracy: 0.8125\n",
      "Epoch 110/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2341 - accuracy: 0.4816 - val_loss: 0.5476 - val_accuracy: 0.8125\n",
      "Epoch 111/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2338 - accuracy: 0.4810 - val_loss: 0.5372 - val_accuracy: 0.8125\n",
      "Epoch 112/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2337 - accuracy: 0.4821 - val_loss: 0.5442 - val_accuracy: 0.8125\n",
      "Epoch 113/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2335 - accuracy: 0.4799 - val_loss: 0.5462 - val_accuracy: 0.8125\n",
      "Epoch 114/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2334 - accuracy: 0.4827 - val_loss: 0.5489 - val_accuracy: 0.8125\n",
      "Epoch 115/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2332 - accuracy: 0.4816 - val_loss: 0.5452 - val_accuracy: 0.8125\n",
      "Epoch 116/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2329 - accuracy: 0.4827 - val_loss: 0.5424 - val_accuracy: 0.8125\n",
      "Epoch 117/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2327 - accuracy: 0.4805 - val_loss: 0.5430 - val_accuracy: 0.8125\n",
      "Epoch 118/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2323 - accuracy: 0.4821 - val_loss: 0.5459 - val_accuracy: 0.8125\n",
      "Epoch 119/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2331 - accuracy: 0.4821 - val_loss: 0.5522 - val_accuracy: 0.8125\n",
      "Epoch 120/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2325 - accuracy: 0.4838 - val_loss: 0.5396 - val_accuracy: 0.8125\n",
      "Epoch 121/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2329 - accuracy: 0.4827 - val_loss: 0.5400 - val_accuracy: 0.8125\n",
      "Epoch 122/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2331 - accuracy: 0.4805 - val_loss: 0.5425 - val_accuracy: 0.8125\n",
      "Epoch 123/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2321 - accuracy: 0.4816 - val_loss: 0.5455 - val_accuracy: 0.8125\n",
      "Epoch 124/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2328 - accuracy: 0.4827 - val_loss: 0.5427 - val_accuracy: 0.8125\n",
      "Epoch 125/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2333 - accuracy: 0.4788 - val_loss: 0.5509 - val_accuracy: 0.8125\n",
      "Epoch 126/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2316 - accuracy: 0.4805 - val_loss: 0.5398 - val_accuracy: 0.8125\n",
      "Epoch 127/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2326 - accuracy: 0.4777 - val_loss: 0.5477 - val_accuracy: 0.8125\n",
      "Epoch 128/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2326 - accuracy: 0.4827 - val_loss: 0.5497 - val_accuracy: 0.8125\n",
      "Epoch 129/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2318 - accuracy: 0.4827 - val_loss: 0.5458 - val_accuracy: 0.8125\n",
      "Epoch 130/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2316 - accuracy: 0.4821 - val_loss: 0.5456 - val_accuracy: 0.8125\n",
      "Epoch 131/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2315 - accuracy: 0.4821 - val_loss: 0.5408 - val_accuracy: 0.8125\n",
      "Epoch 132/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2314 - accuracy: 0.4799 - val_loss: 0.5431 - val_accuracy: 0.8125\n",
      "Epoch 133/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2312 - accuracy: 0.4821 - val_loss: 0.5446 - val_accuracy: 0.8125\n",
      "Epoch 134/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2313 - accuracy: 0.4810 - val_loss: 0.5392 - val_accuracy: 0.8125\n",
      "Epoch 135/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2313 - accuracy: 0.4844 - val_loss: 0.5336 - val_accuracy: 0.8125\n",
      "Epoch 136/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2320 - accuracy: 0.4805 - val_loss: 0.5436 - val_accuracy: 0.8125\n",
      "Epoch 137/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2313 - accuracy: 0.4799 - val_loss: 0.5451 - val_accuracy: 0.8125\n",
      "Epoch 138/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2308 - accuracy: 0.4810 - val_loss: 0.5407 - val_accuracy: 0.8125\n",
      "Epoch 139/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2317 - accuracy: 0.4805 - val_loss: 0.5419 - val_accuracy: 0.8125\n",
      "Epoch 140/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2309 - accuracy: 0.4816 - val_loss: 0.5425 - val_accuracy: 0.8125\n",
      "Epoch 141/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2307 - accuracy: 0.4816 - val_loss: 0.5407 - val_accuracy: 0.8125\n",
      "Epoch 142/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2305 - accuracy: 0.4821 - val_loss: 0.5401 - val_accuracy: 0.8125\n",
      "Epoch 143/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2308 - accuracy: 0.4821 - val_loss: 0.5401 - val_accuracy: 0.8125\n",
      "Epoch 144/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2306 - accuracy: 0.4816 - val_loss: 0.5351 - val_accuracy: 0.8125\n",
      "Epoch 145/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2304 - accuracy: 0.4827 - val_loss: 0.5354 - val_accuracy: 0.8125\n",
      "Epoch 146/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2305 - accuracy: 0.4816 - val_loss: 0.5387 - val_accuracy: 0.8125\n",
      "Epoch 147/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2309 - accuracy: 0.4833 - val_loss: 0.5399 - val_accuracy: 0.8125\n",
      "Epoch 148/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2303 - accuracy: 0.4816 - val_loss: 0.5379 - val_accuracy: 0.8125\n",
      "Epoch 149/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2308 - accuracy: 0.4833 - val_loss: 0.5336 - val_accuracy: 0.8125\n",
      "Epoch 150/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2305 - accuracy: 0.4821 - val_loss: 0.5390 - val_accuracy: 0.8125\n",
      "Epoch 151/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2304 - accuracy: 0.4805 - val_loss: 0.5414 - val_accuracy: 0.8125\n",
      "Epoch 152/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2301 - accuracy: 0.4838 - val_loss: 0.5364 - val_accuracy: 0.8125\n",
      "Epoch 153/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2301 - accuracy: 0.4810 - val_loss: 0.5420 - val_accuracy: 0.8125\n",
      "Epoch 154/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2304 - accuracy: 0.4821 - val_loss: 0.5349 - val_accuracy: 0.8125\n",
      "Epoch 155/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2299 - accuracy: 0.4827 - val_loss: 0.5389 - val_accuracy: 0.8125\n",
      "Epoch 156/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2301 - accuracy: 0.4821 - val_loss: 0.5434 - val_accuracy: 0.8125\n",
      "Epoch 157/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2299 - accuracy: 0.4821 - val_loss: 0.5464 - val_accuracy: 0.8125\n",
      "Epoch 158/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2300 - accuracy: 0.4827 - val_loss: 0.5392 - val_accuracy: 0.8125\n",
      "Epoch 159/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2299 - accuracy: 0.4810 - val_loss: 0.5402 - val_accuracy: 0.8125\n",
      "Epoch 160/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2297 - accuracy: 0.4816 - val_loss: 0.5426 - val_accuracy: 0.8125\n",
      "Epoch 161/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2297 - accuracy: 0.4833 - val_loss: 0.5394 - val_accuracy: 0.8125\n",
      "Epoch 162/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2295 - accuracy: 0.4816 - val_loss: 0.5412 - val_accuracy: 0.8125\n",
      "Epoch 163/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2298 - accuracy: 0.4833 - val_loss: 0.5377 - val_accuracy: 0.8125\n",
      "Epoch 164/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2298 - accuracy: 0.4821 - val_loss: 0.5391 - val_accuracy: 0.8125\n",
      "Epoch 165/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2295 - accuracy: 0.4816 - val_loss: 0.5398 - val_accuracy: 0.8125\n",
      "Epoch 166/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2299 - accuracy: 0.4816 - val_loss: 0.5378 - val_accuracy: 0.8125\n",
      "Epoch 167/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2295 - accuracy: 0.4810 - val_loss: 0.5378 - val_accuracy: 0.8125\n",
      "Epoch 168/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2304 - accuracy: 0.4799 - val_loss: 0.5362 - val_accuracy: 0.8125\n",
      "Epoch 169/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2288 - accuracy: 0.4827 - val_loss: 0.5388 - val_accuracy: 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2297 - accuracy: 0.4805 - val_loss: 0.5401 - val_accuracy: 0.8125\n",
      "Epoch 171/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2309 - accuracy: 0.4821 - val_loss: 0.5428 - val_accuracy: 0.8125\n",
      "Epoch 172/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2292 - accuracy: 0.4810 - val_loss: 0.5398 - val_accuracy: 0.8125\n",
      "Epoch 173/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2305 - accuracy: 0.4788 - val_loss: 0.5354 - val_accuracy: 0.8125\n",
      "Epoch 174/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2305 - accuracy: 0.4799 - val_loss: 0.5370 - val_accuracy: 0.8125\n",
      "Epoch 175/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2308 - accuracy: 0.4799 - val_loss: 0.5427 - val_accuracy: 0.8125\n",
      "Epoch 176/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2299 - accuracy: 0.4799 - val_loss: 0.5357 - val_accuracy: 0.8125\n",
      "Epoch 177/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2291 - accuracy: 0.4810 - val_loss: 0.5433 - val_accuracy: 0.8125\n",
      "Epoch 178/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2296 - accuracy: 0.4794 - val_loss: 0.5351 - val_accuracy: 0.8125\n",
      "Epoch 179/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2291 - accuracy: 0.4816 - val_loss: 0.5390 - val_accuracy: 0.8125\n",
      "Epoch 180/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2290 - accuracy: 0.4794 - val_loss: 0.5444 - val_accuracy: 0.8125\n",
      "Epoch 181/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2295 - accuracy: 0.4810 - val_loss: 0.5437 - val_accuracy: 0.8125\n",
      "Epoch 182/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2297 - accuracy: 0.4805 - val_loss: 0.5305 - val_accuracy: 0.8125\n",
      "Epoch 183/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2283 - accuracy: 0.4838 - val_loss: 0.5438 - val_accuracy: 0.8125\n",
      "Epoch 184/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2282 - accuracy: 0.4821 - val_loss: 0.5403 - val_accuracy: 0.8125\n",
      "Epoch 185/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2297 - accuracy: 0.4810 - val_loss: 0.5453 - val_accuracy: 0.8125\n",
      "Epoch 186/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2301 - accuracy: 0.4816 - val_loss: 0.5357 - val_accuracy: 0.8125\n",
      "Epoch 187/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2284 - accuracy: 0.4833 - val_loss: 0.5455 - val_accuracy: 0.8125\n",
      "Epoch 188/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2296 - accuracy: 0.4833 - val_loss: 0.5432 - val_accuracy: 0.8125\n",
      "Epoch 189/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2282 - accuracy: 0.4821 - val_loss: 0.5372 - val_accuracy: 0.8125\n",
      "Epoch 190/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2280 - accuracy: 0.4799 - val_loss: 0.5375 - val_accuracy: 0.8125\n",
      "Epoch 191/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2281 - accuracy: 0.4838 - val_loss: 0.5404 - val_accuracy: 0.8125\n",
      "Epoch 192/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2276 - accuracy: 0.4821 - val_loss: 0.5382 - val_accuracy: 0.8125\n",
      "Epoch 193/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2287 - accuracy: 0.4849 - val_loss: 0.5314 - val_accuracy: 0.8125\n",
      "Epoch 194/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2288 - accuracy: 0.4844 - val_loss: 0.5379 - val_accuracy: 0.8125\n",
      "Epoch 195/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2281 - accuracy: 0.4805 - val_loss: 0.5351 - val_accuracy: 0.8125\n",
      "Epoch 196/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2286 - accuracy: 0.4821 - val_loss: 0.5354 - val_accuracy: 0.8125\n",
      "Epoch 197/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2289 - accuracy: 0.4833 - val_loss: 0.5332 - val_accuracy: 0.8125\n",
      "Epoch 198/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2288 - accuracy: 0.4810 - val_loss: 0.5348 - val_accuracy: 0.8125\n",
      "Epoch 199/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2280 - accuracy: 0.4810 - val_loss: 0.5418 - val_accuracy: 0.8125\n",
      "Epoch 200/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2274 - accuracy: 0.4827 - val_loss: 0.5435 - val_accuracy: 0.8125\n",
      "Epoch 201/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2283 - accuracy: 0.4794 - val_loss: 0.5437 - val_accuracy: 0.8125\n",
      "Epoch 202/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2280 - accuracy: 0.4827 - val_loss: 0.5323 - val_accuracy: 0.8125\n",
      "Epoch 203/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2284 - accuracy: 0.4788 - val_loss: 0.5410 - val_accuracy: 0.8125\n",
      "Epoch 204/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2282 - accuracy: 0.4838 - val_loss: 0.5427 - val_accuracy: 0.8125\n",
      "Epoch 205/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2281 - accuracy: 0.4805 - val_loss: 0.5443 - val_accuracy: 0.8125\n",
      "Epoch 206/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2296 - accuracy: 0.4816 - val_loss: 0.5381 - val_accuracy: 0.8125\n",
      "Epoch 207/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2282 - accuracy: 0.4794 - val_loss: 0.5393 - val_accuracy: 0.8125\n",
      "Epoch 208/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2276 - accuracy: 0.4821 - val_loss: 0.5404 - val_accuracy: 0.8125\n",
      "Epoch 209/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2277 - accuracy: 0.4833 - val_loss: 0.5308 - val_accuracy: 0.8125\n",
      "Epoch 210/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2276 - accuracy: 0.4810 - val_loss: 0.5402 - val_accuracy: 0.8125\n",
      "Epoch 211/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2278 - accuracy: 0.4816 - val_loss: 0.5351 - val_accuracy: 0.8125\n",
      "Epoch 212/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2273 - accuracy: 0.4816 - val_loss: 0.5396 - val_accuracy: 0.8125\n",
      "Epoch 213/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2281 - accuracy: 0.4805 - val_loss: 0.5433 - val_accuracy: 0.8125\n",
      "Epoch 214/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2279 - accuracy: 0.4794 - val_loss: 0.5369 - val_accuracy: 0.8125\n",
      "Epoch 215/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2278 - accuracy: 0.4805 - val_loss: 0.5416 - val_accuracy: 0.8125\n",
      "Epoch 216/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2282 - accuracy: 0.4816 - val_loss: 0.5446 - val_accuracy: 0.8125\n",
      "Epoch 217/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2279 - accuracy: 0.4799 - val_loss: 0.5383 - val_accuracy: 0.8125\n",
      "Epoch 218/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2287 - accuracy: 0.4821 - val_loss: 0.5296 - val_accuracy: 0.8125\n",
      "Epoch 219/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2277 - accuracy: 0.4833 - val_loss: 0.5347 - val_accuracy: 0.8125\n",
      "Epoch 220/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2279 - accuracy: 0.4810 - val_loss: 0.5330 - val_accuracy: 0.8125\n",
      "Epoch 221/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2275 - accuracy: 0.4821 - val_loss: 0.5450 - val_accuracy: 0.8125\n",
      "Epoch 222/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2276 - accuracy: 0.4810 - val_loss: 0.5374 - val_accuracy: 0.8125\n",
      "Epoch 223/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2280 - accuracy: 0.4816 - val_loss: 0.5370 - val_accuracy: 0.8125\n",
      "Epoch 224/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2281 - accuracy: 0.4766 - val_loss: 0.5419 - val_accuracy: 0.8125\n",
      "Epoch 225/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2277 - accuracy: 0.4833 - val_loss: 0.5341 - val_accuracy: 0.8125\n",
      "Epoch 226/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2276 - accuracy: 0.4833 - val_loss: 0.5417 - val_accuracy: 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2276 - accuracy: 0.4810 - val_loss: 0.5328 - val_accuracy: 0.8125\n",
      "Epoch 228/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2277 - accuracy: 0.4833 - val_loss: 0.5382 - val_accuracy: 0.8125\n",
      "Epoch 229/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2272 - accuracy: 0.4805 - val_loss: 0.5409 - val_accuracy: 0.8125\n",
      "Epoch 230/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2280 - accuracy: 0.4805 - val_loss: 0.5388 - val_accuracy: 0.8125\n",
      "Epoch 231/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2272 - accuracy: 0.4833 - val_loss: 0.5429 - val_accuracy: 0.8125\n",
      "Epoch 232/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2274 - accuracy: 0.4805 - val_loss: 0.5427 - val_accuracy: 0.8125\n",
      "Epoch 233/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2273 - accuracy: 0.4816 - val_loss: 0.5351 - val_accuracy: 0.8125\n",
      "Epoch 234/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2272 - accuracy: 0.4805 - val_loss: 0.5380 - val_accuracy: 0.8125\n",
      "Epoch 235/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2265 - accuracy: 0.4788 - val_loss: 0.5368 - val_accuracy: 0.8125\n",
      "Epoch 236/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2274 - accuracy: 0.4805 - val_loss: 0.5328 - val_accuracy: 0.8125\n",
      "Epoch 237/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2269 - accuracy: 0.4827 - val_loss: 0.5388 - val_accuracy: 0.8125\n",
      "Epoch 238/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2270 - accuracy: 0.4816 - val_loss: 0.5342 - val_accuracy: 0.8125\n",
      "Epoch 239/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2272 - accuracy: 0.4799 - val_loss: 0.5386 - val_accuracy: 0.8125\n",
      "Epoch 240/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2267 - accuracy: 0.4799 - val_loss: 0.5406 - val_accuracy: 0.8125\n",
      "Epoch 241/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2264 - accuracy: 0.4799 - val_loss: 0.5421 - val_accuracy: 0.8125\n",
      "Epoch 242/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2267 - accuracy: 0.4810 - val_loss: 0.5346 - val_accuracy: 0.8125\n",
      "Epoch 243/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2268 - accuracy: 0.4805 - val_loss: 0.5364 - val_accuracy: 0.8125\n",
      "Epoch 244/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2280 - accuracy: 0.4827 - val_loss: 0.5388 - val_accuracy: 0.8125\n",
      "Epoch 245/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2275 - accuracy: 0.4805 - val_loss: 0.5357 - val_accuracy: 0.8125\n",
      "Epoch 246/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2266 - accuracy: 0.4827 - val_loss: 0.5388 - val_accuracy: 0.8125\n",
      "Epoch 247/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2271 - accuracy: 0.4816 - val_loss: 0.5360 - val_accuracy: 0.8125\n",
      "Epoch 248/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2271 - accuracy: 0.4827 - val_loss: 0.5375 - val_accuracy: 0.8125\n",
      "Epoch 249/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2276 - accuracy: 0.4816 - val_loss: 0.5464 - val_accuracy: 0.8125\n",
      "Epoch 250/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2267 - accuracy: 0.4816 - val_loss: 0.5416 - val_accuracy: 0.8125\n",
      "Epoch 251/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2276 - accuracy: 0.4788 - val_loss: 0.5348 - val_accuracy: 0.8125\n",
      "Epoch 252/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2265 - accuracy: 0.4821 - val_loss: 0.5458 - val_accuracy: 0.8125\n",
      "Epoch 253/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2265 - accuracy: 0.4810 - val_loss: 0.5400 - val_accuracy: 0.8125\n",
      "Epoch 254/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2274 - accuracy: 0.4816 - val_loss: 0.5471 - val_accuracy: 0.8125\n",
      "Epoch 255/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2287 - accuracy: 0.4827 - val_loss: 0.5393 - val_accuracy: 0.8125\n",
      "Epoch 256/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2271 - accuracy: 0.4788 - val_loss: 0.5427 - val_accuracy: 0.8125\n",
      "Epoch 257/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2269 - accuracy: 0.4821 - val_loss: 0.5418 - val_accuracy: 0.8125\n",
      "Epoch 258/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2267 - accuracy: 0.4805 - val_loss: 0.5395 - val_accuracy: 0.8125\n",
      "Epoch 259/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2267 - accuracy: 0.4805 - val_loss: 0.5333 - val_accuracy: 0.8125\n",
      "Epoch 260/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2267 - accuracy: 0.4827 - val_loss: 0.5415 - val_accuracy: 0.8125\n",
      "Epoch 261/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2266 - accuracy: 0.4788 - val_loss: 0.5396 - val_accuracy: 0.8125\n",
      "Epoch 262/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2266 - accuracy: 0.4827 - val_loss: 0.5415 - val_accuracy: 0.8125\n",
      "Epoch 263/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2272 - accuracy: 0.4821 - val_loss: 0.5346 - val_accuracy: 0.8125\n",
      "Epoch 264/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2281 - accuracy: 0.4821 - val_loss: 0.5411 - val_accuracy: 0.8125\n",
      "Epoch 265/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2267 - accuracy: 0.4821 - val_loss: 0.5415 - val_accuracy: 0.8125\n",
      "Epoch 266/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2264 - accuracy: 0.4805 - val_loss: 0.5323 - val_accuracy: 0.8125\n",
      "Epoch 267/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2273 - accuracy: 0.4844 - val_loss: 0.5345 - val_accuracy: 0.8125\n",
      "Epoch 268/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2269 - accuracy: 0.4799 - val_loss: 0.5345 - val_accuracy: 0.8125\n",
      "Epoch 269/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2265 - accuracy: 0.4810 - val_loss: 0.5385 - val_accuracy: 0.8125\n",
      "Epoch 270/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2265 - accuracy: 0.4827 - val_loss: 0.5408 - val_accuracy: 0.8125\n",
      "Epoch 271/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2276 - accuracy: 0.4788 - val_loss: 0.5430 - val_accuracy: 0.8125\n",
      "Epoch 272/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2263 - accuracy: 0.4810 - val_loss: 0.5312 - val_accuracy: 0.8125\n",
      "Epoch 273/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2275 - accuracy: 0.4816 - val_loss: 0.5378 - val_accuracy: 0.8125\n",
      "Epoch 274/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2274 - accuracy: 0.4844 - val_loss: 0.5400 - val_accuracy: 0.8125\n",
      "Epoch 275/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2260 - accuracy: 0.4833 - val_loss: 0.5391 - val_accuracy: 0.8125\n",
      "Epoch 276/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2268 - accuracy: 0.4821 - val_loss: 0.5400 - val_accuracy: 0.8125\n",
      "Epoch 277/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2260 - accuracy: 0.4810 - val_loss: 0.5439 - val_accuracy: 0.8125\n",
      "Epoch 278/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2261 - accuracy: 0.4805 - val_loss: 0.5410 - val_accuracy: 0.8125\n",
      "Epoch 279/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2259 - accuracy: 0.4849 - val_loss: 0.5361 - val_accuracy: 0.8125\n",
      "Epoch 280/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2264 - accuracy: 0.4810 - val_loss: 0.5463 - val_accuracy: 0.8125\n",
      "Epoch 281/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2261 - accuracy: 0.4816 - val_loss: 0.5358 - val_accuracy: 0.8125\n",
      "Epoch 282/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2263 - accuracy: 0.4799 - val_loss: 0.5395 - val_accuracy: 0.8125\n",
      "Epoch 283/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2257 - accuracy: 0.4821 - val_loss: 0.5428 - val_accuracy: 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 284/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2264 - accuracy: 0.4788 - val_loss: 0.5317 - val_accuracy: 0.8125\n",
      "Epoch 285/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2260 - accuracy: 0.4821 - val_loss: 0.5390 - val_accuracy: 0.8125\n",
      "Epoch 286/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2268 - accuracy: 0.4844 - val_loss: 0.5410 - val_accuracy: 0.8125\n",
      "Epoch 287/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2266 - accuracy: 0.4833 - val_loss: 0.5403 - val_accuracy: 0.8125\n",
      "Epoch 288/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2255 - accuracy: 0.4816 - val_loss: 0.5415 - val_accuracy: 0.8125\n",
      "Epoch 289/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2259 - accuracy: 0.4821 - val_loss: 0.5426 - val_accuracy: 0.8125\n",
      "Epoch 290/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2256 - accuracy: 0.4844 - val_loss: 0.5385 - val_accuracy: 0.8125\n",
      "Epoch 291/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2256 - accuracy: 0.4838 - val_loss: 0.5423 - val_accuracy: 0.8125\n",
      "Epoch 292/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2256 - accuracy: 0.4827 - val_loss: 0.5395 - val_accuracy: 0.8125\n",
      "Epoch 293/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2258 - accuracy: 0.4810 - val_loss: 0.5418 - val_accuracy: 0.8125\n",
      "Epoch 294/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2257 - accuracy: 0.4810 - val_loss: 0.5413 - val_accuracy: 0.8125\n",
      "Epoch 295/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2261 - accuracy: 0.4810 - val_loss: 0.5371 - val_accuracy: 0.8125\n",
      "Epoch 296/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2255 - accuracy: 0.4849 - val_loss: 0.5434 - val_accuracy: 0.8125\n",
      "Epoch 297/300\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1.2257 - accuracy: 0.4833 - val_loss: 0.5388 - val_accuracy: 0.8125\n",
      "Epoch 298/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.2256 - accuracy: 0.4827 - val_loss: 0.5367 - val_accuracy: 0.8125\n",
      "Epoch 299/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.2260 - accuracy: 0.4816 - val_loss: 0.5394 - val_accuracy: 0.8125\n",
      "Epoch 300/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.2262 - accuracy: 0.4816 - val_loss: 0.5397 - val_accuracy: 0.8125\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "56/56 [==============================] - 0s 1ms/step\n",
      "Recall: [0.92 0.01 0.58]\n",
      "Precision: [0.4  0.71 0.68]\n",
      "F1-score: [0.56 0.02 0.62]\n",
      "Accuracy: 0.48\n",
      "Brier score:0.20256077896516939\n",
      "Brier climat:0.20986359126984128\n",
      "Brier skill score:0.034797900200239984\n",
      "Recall: [0.92 0.01 0.58]\n",
      "Precision: [0.4  0.71 0.68]\n",
      "F1-score: [0.56 0.02 0.62]\n",
      "Accuracy: 0.48\n",
      "Brier score:0.2275965797859203\n",
      "Brier climat:0.22499751984126984\n",
      "Brier skill score:-0.011551504863182682\n",
      "Recall: [0.92 0.01 0.58]\n",
      "Precision: [0.4  0.71 0.68]\n",
      "F1-score: [0.56 0.02 0.62]\n",
      "Accuracy: 0.48\n",
      "Brier score:0.1637032708672407\n",
      "Brier climat:0.2223214285714286\n",
      "Brier skill score:0.2636640025248611\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "Recall: [1. 0. 1.]\n",
      "Precision: [0.62 0.   1.  ]\n",
      "F1-score: [0.77 0.   1.  ]\n",
      "Accuracy: 0.81\n",
      "Brier score:0.13926272806922618\n",
      "Brier climat:0.2152951388888889\n",
      "Brier skill score:0.35315433136138796\n",
      "Recall: [1. 0. 1.]\n",
      "Precision: [0.62 0.   1.  ]\n",
      "F1-score: [0.77 0.   1.  ]\n",
      "Accuracy: 0.81\n",
      "Brier score:0.12976110450456232\n",
      "Brier climat:0.1698263888888889\n",
      "Brier skill score:0.23591907386395528\n",
      "Recall: [1. 0. 1.]\n",
      "Precision: [0.62 0.   1.  ]\n",
      "F1-score: [0.77 0.   1.  ]\n",
      "Accuracy: 0.81\n",
      "Brier score:0.017339070891875745\n",
      "Brier climat:0.27421875\n",
      "Brier skill score:0.936769200166379\n",
      "******** 17\n",
      "validation years [1998, 1999]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "{0: 2.0, 1: 1.0, 2: 1.0}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 60)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 183       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183\n",
      "Trainable params: 183\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/28 [==========================>...] - ETA: 0s - loss: 1.7724 - accuracy: 0.3810WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "28/28 [==============================] - 4s 131ms/step - loss: 1.7799 - accuracy: 0.3845 - val_loss: 1.3117 - val_accuracy: 0.5391\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.6262 - accuracy: 0.4459 - val_loss: 1.2122 - val_accuracy: 0.5156\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.5352 - accuracy: 0.4760 - val_loss: 1.1484 - val_accuracy: 0.5078\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4831 - accuracy: 0.4805 - val_loss: 1.1112 - val_accuracy: 0.4609\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4503 - accuracy: 0.4933 - val_loss: 1.0894 - val_accuracy: 0.5078\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4296 - accuracy: 0.4866 - val_loss: 1.0744 - val_accuracy: 0.5391\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4134 - accuracy: 0.4894 - val_loss: 1.0666 - val_accuracy: 0.5625\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3999 - accuracy: 0.4849 - val_loss: 1.0604 - val_accuracy: 0.5625\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3887 - accuracy: 0.4883 - val_loss: 1.0503 - val_accuracy: 0.5625\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3784 - accuracy: 0.4900 - val_loss: 1.0442 - val_accuracy: 0.5625\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3697 - accuracy: 0.4888 - val_loss: 1.0370 - val_accuracy: 0.5781\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3605 - accuracy: 0.4888 - val_loss: 1.0369 - val_accuracy: 0.5625\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3517 - accuracy: 0.4883 - val_loss: 1.0341 - val_accuracy: 0.5625\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3443 - accuracy: 0.4905 - val_loss: 1.0280 - val_accuracy: 0.5625\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3380 - accuracy: 0.4939 - val_loss: 1.0251 - val_accuracy: 0.5625\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3309 - accuracy: 0.4905 - val_loss: 1.0182 - val_accuracy: 0.5703\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3253 - accuracy: 0.4872 - val_loss: 1.0205 - val_accuracy: 0.5469\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3192 - accuracy: 0.4877 - val_loss: 1.0149 - val_accuracy: 0.5547\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3140 - accuracy: 0.4860 - val_loss: 1.0175 - val_accuracy: 0.5312\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3091 - accuracy: 0.4866 - val_loss: 1.0126 - val_accuracy: 0.5469\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3039 - accuracy: 0.4860 - val_loss: 1.0099 - val_accuracy: 0.5469\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2992 - accuracy: 0.4866 - val_loss: 1.0091 - val_accuracy: 0.5391\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2944 - accuracy: 0.4877 - val_loss: 1.0115 - val_accuracy: 0.5234\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2904 - accuracy: 0.4888 - val_loss: 1.0075 - val_accuracy: 0.5234\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2855 - accuracy: 0.4883 - val_loss: 1.0045 - val_accuracy: 0.5234\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2817 - accuracy: 0.4883 - val_loss: 1.0020 - val_accuracy: 0.5234\n",
      "Epoch 27/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2777 - accuracy: 0.4950 - val_loss: 0.9978 - val_accuracy: 0.5234\n",
      "Epoch 28/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2742 - accuracy: 0.4927 - val_loss: 0.9982 - val_accuracy: 0.5234\n",
      "Epoch 29/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2709 - accuracy: 0.4933 - val_loss: 0.9967 - val_accuracy: 0.5234\n",
      "Epoch 30/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2672 - accuracy: 0.4933 - val_loss: 0.9960 - val_accuracy: 0.5234\n",
      "Epoch 31/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2650 - accuracy: 0.4911 - val_loss: 0.9892 - val_accuracy: 0.5391\n",
      "Epoch 32/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2616 - accuracy: 0.4944 - val_loss: 0.9923 - val_accuracy: 0.5234\n",
      "Epoch 33/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2595 - accuracy: 0.4944 - val_loss: 0.9921 - val_accuracy: 0.5234\n",
      "Epoch 34/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2566 - accuracy: 0.4939 - val_loss: 0.9883 - val_accuracy: 0.5234\n",
      "Epoch 35/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2537 - accuracy: 0.4961 - val_loss: 0.9908 - val_accuracy: 0.5234\n",
      "Epoch 36/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2518 - accuracy: 0.4961 - val_loss: 0.9890 - val_accuracy: 0.5234\n",
      "Epoch 37/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2497 - accuracy: 0.4967 - val_loss: 0.9838 - val_accuracy: 0.5234\n",
      "Epoch 38/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2467 - accuracy: 0.4950 - val_loss: 0.9842 - val_accuracy: 0.5234\n",
      "Epoch 39/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2447 - accuracy: 0.4944 - val_loss: 0.9837 - val_accuracy: 0.5234\n",
      "Epoch 40/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2429 - accuracy: 0.4972 - val_loss: 0.9850 - val_accuracy: 0.5234\n",
      "Epoch 41/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2402 - accuracy: 0.4939 - val_loss: 0.9797 - val_accuracy: 0.5234\n",
      "Epoch 42/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2393 - accuracy: 0.4933 - val_loss: 0.9849 - val_accuracy: 0.5234\n",
      "Epoch 43/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2371 - accuracy: 0.4933 - val_loss: 0.9806 - val_accuracy: 0.5234\n",
      "Epoch 44/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2353 - accuracy: 0.4944 - val_loss: 0.9810 - val_accuracy: 0.5234\n",
      "Epoch 45/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2337 - accuracy: 0.4922 - val_loss: 0.9799 - val_accuracy: 0.5234\n",
      "Epoch 46/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2318 - accuracy: 0.4950 - val_loss: 0.9799 - val_accuracy: 0.5234\n",
      "Epoch 47/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2305 - accuracy: 0.4950 - val_loss: 0.9807 - val_accuracy: 0.5234\n",
      "Epoch 48/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2293 - accuracy: 0.4961 - val_loss: 0.9778 - val_accuracy: 0.5234\n",
      "Epoch 49/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2283 - accuracy: 0.4927 - val_loss: 0.9787 - val_accuracy: 0.5234\n",
      "Epoch 50/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2267 - accuracy: 0.4944 - val_loss: 0.9774 - val_accuracy: 0.5234\n",
      "Epoch 51/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2256 - accuracy: 0.4933 - val_loss: 0.9816 - val_accuracy: 0.5234\n",
      "Epoch 52/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2244 - accuracy: 0.4950 - val_loss: 0.9737 - val_accuracy: 0.5234\n",
      "Epoch 53/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2229 - accuracy: 0.4939 - val_loss: 0.9754 - val_accuracy: 0.5234\n",
      "Epoch 54/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2216 - accuracy: 0.4961 - val_loss: 0.9719 - val_accuracy: 0.5234\n",
      "Epoch 55/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2208 - accuracy: 0.4933 - val_loss: 0.9739 - val_accuracy: 0.5234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2192 - accuracy: 0.4967 - val_loss: 0.9698 - val_accuracy: 0.5234\n",
      "Epoch 57/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2180 - accuracy: 0.4955 - val_loss: 0.9731 - val_accuracy: 0.5234\n",
      "Epoch 58/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2192 - accuracy: 0.4933 - val_loss: 0.9745 - val_accuracy: 0.5234\n",
      "Epoch 59/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2168 - accuracy: 0.4933 - val_loss: 0.9755 - val_accuracy: 0.5234\n",
      "Epoch 60/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2154 - accuracy: 0.4933 - val_loss: 0.9715 - val_accuracy: 0.5234\n",
      "Epoch 61/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2145 - accuracy: 0.4955 - val_loss: 0.9682 - val_accuracy: 0.5234\n",
      "Epoch 62/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2138 - accuracy: 0.4933 - val_loss: 0.9672 - val_accuracy: 0.5234\n",
      "Epoch 63/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2138 - accuracy: 0.4916 - val_loss: 0.9705 - val_accuracy: 0.5234\n",
      "Epoch 64/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2135 - accuracy: 0.4967 - val_loss: 0.9672 - val_accuracy: 0.5234\n",
      "Epoch 65/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2120 - accuracy: 0.4922 - val_loss: 0.9712 - val_accuracy: 0.5234\n",
      "Epoch 66/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2116 - accuracy: 0.4933 - val_loss: 0.9708 - val_accuracy: 0.5234\n",
      "Epoch 67/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2107 - accuracy: 0.4927 - val_loss: 0.9701 - val_accuracy: 0.5234\n",
      "Epoch 68/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2104 - accuracy: 0.4939 - val_loss: 0.9706 - val_accuracy: 0.5234\n",
      "Epoch 69/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2102 - accuracy: 0.4927 - val_loss: 0.9701 - val_accuracy: 0.5234\n",
      "Epoch 70/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2095 - accuracy: 0.4927 - val_loss: 0.9696 - val_accuracy: 0.5234\n",
      "Epoch 71/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2087 - accuracy: 0.4933 - val_loss: 0.9677 - val_accuracy: 0.5234\n",
      "Epoch 72/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2089 - accuracy: 0.4939 - val_loss: 0.9689 - val_accuracy: 0.5234\n",
      "Epoch 73/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2087 - accuracy: 0.4922 - val_loss: 0.9664 - val_accuracy: 0.5234\n",
      "Epoch 74/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2075 - accuracy: 0.4944 - val_loss: 0.9673 - val_accuracy: 0.5234\n",
      "Epoch 75/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2075 - accuracy: 0.4911 - val_loss: 0.9681 - val_accuracy: 0.5234\n",
      "Epoch 76/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2065 - accuracy: 0.4950 - val_loss: 0.9686 - val_accuracy: 0.5234\n",
      "Epoch 77/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2062 - accuracy: 0.4894 - val_loss: 0.9657 - val_accuracy: 0.5234\n",
      "Epoch 78/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2060 - accuracy: 0.4933 - val_loss: 0.9605 - val_accuracy: 0.5234\n",
      "Epoch 79/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2053 - accuracy: 0.4916 - val_loss: 0.9685 - val_accuracy: 0.5234\n",
      "Epoch 80/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2050 - accuracy: 0.4933 - val_loss: 0.9668 - val_accuracy: 0.5234\n",
      "Epoch 81/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2043 - accuracy: 0.4922 - val_loss: 0.9647 - val_accuracy: 0.5234\n",
      "Epoch 82/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2056 - accuracy: 0.4944 - val_loss: 0.9672 - val_accuracy: 0.5234\n",
      "Epoch 83/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2046 - accuracy: 0.4927 - val_loss: 0.9673 - val_accuracy: 0.5234\n",
      "Epoch 84/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2036 - accuracy: 0.4900 - val_loss: 0.9662 - val_accuracy: 0.5234\n",
      "Epoch 85/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2034 - accuracy: 0.4950 - val_loss: 0.9630 - val_accuracy: 0.5234\n",
      "Epoch 86/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2035 - accuracy: 0.4922 - val_loss: 0.9620 - val_accuracy: 0.5234\n",
      "Epoch 87/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2029 - accuracy: 0.4927 - val_loss: 0.9651 - val_accuracy: 0.5234\n",
      "Epoch 88/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2030 - accuracy: 0.4950 - val_loss: 0.9630 - val_accuracy: 0.5234\n",
      "Epoch 89/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2026 - accuracy: 0.4922 - val_loss: 0.9682 - val_accuracy: 0.5234\n",
      "Epoch 90/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2019 - accuracy: 0.4933 - val_loss: 0.9661 - val_accuracy: 0.5234\n",
      "Epoch 91/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2017 - accuracy: 0.4916 - val_loss: 0.9676 - val_accuracy: 0.5234\n",
      "Epoch 92/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2020 - accuracy: 0.4944 - val_loss: 0.9647 - val_accuracy: 0.5234\n",
      "Epoch 93/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2007 - accuracy: 0.4950 - val_loss: 0.9607 - val_accuracy: 0.5234\n",
      "Epoch 94/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2012 - accuracy: 0.4916 - val_loss: 0.9650 - val_accuracy: 0.5234\n",
      "Epoch 95/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2014 - accuracy: 0.4950 - val_loss: 0.9617 - val_accuracy: 0.5234\n",
      "Epoch 96/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2005 - accuracy: 0.4922 - val_loss: 0.9608 - val_accuracy: 0.5234\n",
      "Epoch 97/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2005 - accuracy: 0.4950 - val_loss: 0.9635 - val_accuracy: 0.5234\n",
      "Epoch 98/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2011 - accuracy: 0.4933 - val_loss: 0.9673 - val_accuracy: 0.5234\n",
      "Epoch 99/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2001 - accuracy: 0.4922 - val_loss: 0.9605 - val_accuracy: 0.5234\n",
      "Epoch 100/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1996 - accuracy: 0.4939 - val_loss: 0.9638 - val_accuracy: 0.5234\n",
      "Epoch 101/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1993 - accuracy: 0.4939 - val_loss: 0.9648 - val_accuracy: 0.5234\n",
      "Epoch 102/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1991 - accuracy: 0.4916 - val_loss: 0.9592 - val_accuracy: 0.5234\n",
      "Epoch 103/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1988 - accuracy: 0.4950 - val_loss: 0.9642 - val_accuracy: 0.5234\n",
      "Epoch 104/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1986 - accuracy: 0.4944 - val_loss: 0.9633 - val_accuracy: 0.5234\n",
      "Epoch 105/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2000 - accuracy: 0.4944 - val_loss: 0.9639 - val_accuracy: 0.5234\n",
      "Epoch 106/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1989 - accuracy: 0.4933 - val_loss: 0.9628 - val_accuracy: 0.5234\n",
      "Epoch 107/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1985 - accuracy: 0.4939 - val_loss: 0.9619 - val_accuracy: 0.5234\n",
      "Epoch 108/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1980 - accuracy: 0.4939 - val_loss: 0.9611 - val_accuracy: 0.5234\n",
      "Epoch 109/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1981 - accuracy: 0.4950 - val_loss: 0.9641 - val_accuracy: 0.5234\n",
      "Epoch 110/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1979 - accuracy: 0.4922 - val_loss: 0.9637 - val_accuracy: 0.5234\n",
      "Epoch 111/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1975 - accuracy: 0.4927 - val_loss: 0.9614 - val_accuracy: 0.5234\n",
      "Epoch 112/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1976 - accuracy: 0.4922 - val_loss: 0.9646 - val_accuracy: 0.5234\n",
      "Epoch 113/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1971 - accuracy: 0.4939 - val_loss: 0.9640 - val_accuracy: 0.5234\n",
      "Epoch 114/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1975 - accuracy: 0.4911 - val_loss: 0.9610 - val_accuracy: 0.5234\n",
      "Epoch 115/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1973 - accuracy: 0.4927 - val_loss: 0.9612 - val_accuracy: 0.5234\n",
      "Epoch 116/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1977 - accuracy: 0.4933 - val_loss: 0.9583 - val_accuracy: 0.5234\n",
      "Epoch 117/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1974 - accuracy: 0.4939 - val_loss: 0.9609 - val_accuracy: 0.5234\n",
      "Epoch 118/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1969 - accuracy: 0.4950 - val_loss: 0.9626 - val_accuracy: 0.5234\n",
      "Epoch 119/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1975 - accuracy: 0.4911 - val_loss: 0.9631 - val_accuracy: 0.5234\n",
      "Epoch 120/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1966 - accuracy: 0.4927 - val_loss: 0.9638 - val_accuracy: 0.5234\n",
      "Epoch 121/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1968 - accuracy: 0.4927 - val_loss: 0.9661 - val_accuracy: 0.5234\n",
      "Epoch 122/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1975 - accuracy: 0.4905 - val_loss: 0.9637 - val_accuracy: 0.5234\n",
      "Epoch 123/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1970 - accuracy: 0.4955 - val_loss: 0.9613 - val_accuracy: 0.5234\n",
      "Epoch 124/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1960 - accuracy: 0.4933 - val_loss: 0.9633 - val_accuracy: 0.5234\n",
      "Epoch 125/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1967 - accuracy: 0.4939 - val_loss: 0.9700 - val_accuracy: 0.5234\n",
      "Epoch 126/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1964 - accuracy: 0.4955 - val_loss: 0.9645 - val_accuracy: 0.5234\n",
      "Epoch 127/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1968 - accuracy: 0.4922 - val_loss: 0.9644 - val_accuracy: 0.5234\n",
      "Epoch 128/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1956 - accuracy: 0.4950 - val_loss: 0.9624 - val_accuracy: 0.5234\n",
      "Epoch 129/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1957 - accuracy: 0.4927 - val_loss: 0.9622 - val_accuracy: 0.5234\n",
      "Epoch 130/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1951 - accuracy: 0.4950 - val_loss: 0.9623 - val_accuracy: 0.5234\n",
      "Epoch 131/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1957 - accuracy: 0.4944 - val_loss: 0.9627 - val_accuracy: 0.5234\n",
      "Epoch 132/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1950 - accuracy: 0.4944 - val_loss: 0.9597 - val_accuracy: 0.5234\n",
      "Epoch 133/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1955 - accuracy: 0.4961 - val_loss: 0.9613 - val_accuracy: 0.5234\n",
      "Epoch 134/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1950 - accuracy: 0.4927 - val_loss: 0.9624 - val_accuracy: 0.5234\n",
      "Epoch 135/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1959 - accuracy: 0.4955 - val_loss: 0.9629 - val_accuracy: 0.5234\n",
      "Epoch 136/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1961 - accuracy: 0.4939 - val_loss: 0.9632 - val_accuracy: 0.5234\n",
      "Epoch 137/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1951 - accuracy: 0.4939 - val_loss: 0.9613 - val_accuracy: 0.5234\n",
      "Epoch 138/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1950 - accuracy: 0.4955 - val_loss: 0.9639 - val_accuracy: 0.5234\n",
      "Epoch 139/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1947 - accuracy: 0.4950 - val_loss: 0.9635 - val_accuracy: 0.5234\n",
      "Epoch 140/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1948 - accuracy: 0.4939 - val_loss: 0.9638 - val_accuracy: 0.5234\n",
      "Epoch 141/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1948 - accuracy: 0.4933 - val_loss: 0.9602 - val_accuracy: 0.5234\n",
      "Epoch 142/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1947 - accuracy: 0.4967 - val_loss: 0.9634 - val_accuracy: 0.5234\n",
      "Epoch 143/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1946 - accuracy: 0.4950 - val_loss: 0.9616 - val_accuracy: 0.5234\n",
      "Epoch 144/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1943 - accuracy: 0.4950 - val_loss: 0.9700 - val_accuracy: 0.5234\n",
      "Epoch 145/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1948 - accuracy: 0.4911 - val_loss: 0.9647 - val_accuracy: 0.5234\n",
      "Epoch 146/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1936 - accuracy: 0.4950 - val_loss: 0.9609 - val_accuracy: 0.5234\n",
      "Epoch 147/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1942 - accuracy: 0.4955 - val_loss: 0.9656 - val_accuracy: 0.5234\n",
      "Epoch 148/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1944 - accuracy: 0.4939 - val_loss: 0.9614 - val_accuracy: 0.5234\n",
      "Epoch 149/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1945 - accuracy: 0.4978 - val_loss: 0.9630 - val_accuracy: 0.5234\n",
      "Epoch 150/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1954 - accuracy: 0.4916 - val_loss: 0.9602 - val_accuracy: 0.5234\n",
      "Epoch 151/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1937 - accuracy: 0.4961 - val_loss: 0.9670 - val_accuracy: 0.5234\n",
      "Epoch 152/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1941 - accuracy: 0.4950 - val_loss: 0.9622 - val_accuracy: 0.5234\n",
      "Epoch 153/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1934 - accuracy: 0.4950 - val_loss: 0.9645 - val_accuracy: 0.5234\n",
      "Epoch 154/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1938 - accuracy: 0.4955 - val_loss: 0.9576 - val_accuracy: 0.5234\n",
      "Epoch 155/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1941 - accuracy: 0.4939 - val_loss: 0.9631 - val_accuracy: 0.5234\n",
      "Epoch 156/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1930 - accuracy: 0.4944 - val_loss: 0.9665 - val_accuracy: 0.5234\n",
      "Epoch 157/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1931 - accuracy: 0.4961 - val_loss: 0.9663 - val_accuracy: 0.5234\n",
      "Epoch 158/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1929 - accuracy: 0.4961 - val_loss: 0.9623 - val_accuracy: 0.5234\n",
      "Epoch 159/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1932 - accuracy: 0.4950 - val_loss: 0.9595 - val_accuracy: 0.5234\n",
      "Epoch 160/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1934 - accuracy: 0.4927 - val_loss: 0.9608 - val_accuracy: 0.5234\n",
      "Epoch 161/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1934 - accuracy: 0.4972 - val_loss: 0.9655 - val_accuracy: 0.5234\n",
      "Epoch 162/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1937 - accuracy: 0.4927 - val_loss: 0.9617 - val_accuracy: 0.5234\n",
      "Epoch 163/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1934 - accuracy: 0.4950 - val_loss: 0.9596 - val_accuracy: 0.5234\n",
      "Epoch 164/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1933 - accuracy: 0.4961 - val_loss: 0.9605 - val_accuracy: 0.5234\n",
      "Epoch 165/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1926 - accuracy: 0.4950 - val_loss: 0.9641 - val_accuracy: 0.5234\n",
      "Epoch 166/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1929 - accuracy: 0.4933 - val_loss: 0.9586 - val_accuracy: 0.5234\n",
      "Epoch 167/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1924 - accuracy: 0.4939 - val_loss: 0.9612 - val_accuracy: 0.5234\n",
      "Epoch 168/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1927 - accuracy: 0.4944 - val_loss: 0.9632 - val_accuracy: 0.5234\n",
      "Epoch 169/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1931 - accuracy: 0.4961 - val_loss: 0.9592 - val_accuracy: 0.5234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1926 - accuracy: 0.4955 - val_loss: 0.9628 - val_accuracy: 0.5234\n",
      "Epoch 171/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1925 - accuracy: 0.4955 - val_loss: 0.9597 - val_accuracy: 0.5234\n",
      "Epoch 172/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1927 - accuracy: 0.4972 - val_loss: 0.9628 - val_accuracy: 0.5234\n",
      "Epoch 173/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1933 - accuracy: 0.4972 - val_loss: 0.9616 - val_accuracy: 0.5234\n",
      "Epoch 174/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1923 - accuracy: 0.4933 - val_loss: 0.9610 - val_accuracy: 0.5234\n",
      "Epoch 175/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1925 - accuracy: 0.4978 - val_loss: 0.9632 - val_accuracy: 0.5234\n",
      "Epoch 176/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1926 - accuracy: 0.4933 - val_loss: 0.9647 - val_accuracy: 0.5234\n",
      "Epoch 177/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1922 - accuracy: 0.4950 - val_loss: 0.9587 - val_accuracy: 0.5234\n",
      "Epoch 178/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1917 - accuracy: 0.4950 - val_loss: 0.9599 - val_accuracy: 0.5234\n",
      "Epoch 179/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1926 - accuracy: 0.4961 - val_loss: 0.9556 - val_accuracy: 0.5234\n",
      "Epoch 180/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1925 - accuracy: 0.4950 - val_loss: 0.9591 - val_accuracy: 0.5234\n",
      "Epoch 181/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1918 - accuracy: 0.4955 - val_loss: 0.9574 - val_accuracy: 0.5234\n",
      "Epoch 182/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1917 - accuracy: 0.4944 - val_loss: 0.9606 - val_accuracy: 0.5234\n",
      "Epoch 183/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1917 - accuracy: 0.4967 - val_loss: 0.9610 - val_accuracy: 0.5234\n",
      "Epoch 184/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1923 - accuracy: 0.4967 - val_loss: 0.9594 - val_accuracy: 0.5234\n",
      "Epoch 185/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1924 - accuracy: 0.4967 - val_loss: 0.9600 - val_accuracy: 0.5234\n",
      "Epoch 186/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1916 - accuracy: 0.4944 - val_loss: 0.9618 - val_accuracy: 0.5234\n",
      "Epoch 187/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1914 - accuracy: 0.4955 - val_loss: 0.9620 - val_accuracy: 0.5234\n",
      "Epoch 188/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1921 - accuracy: 0.4939 - val_loss: 0.9670 - val_accuracy: 0.5234\n",
      "Epoch 189/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1921 - accuracy: 0.4950 - val_loss: 0.9626 - val_accuracy: 0.5234\n",
      "Epoch 190/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1915 - accuracy: 0.4944 - val_loss: 0.9615 - val_accuracy: 0.5234\n",
      "Epoch 191/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1913 - accuracy: 0.4961 - val_loss: 0.9590 - val_accuracy: 0.5234\n",
      "Epoch 192/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1913 - accuracy: 0.4955 - val_loss: 0.9605 - val_accuracy: 0.5234\n",
      "Epoch 193/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1912 - accuracy: 0.4939 - val_loss: 0.9602 - val_accuracy: 0.5234\n",
      "Epoch 194/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1923 - accuracy: 0.4955 - val_loss: 0.9591 - val_accuracy: 0.5234\n",
      "Epoch 195/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1911 - accuracy: 0.4950 - val_loss: 0.9590 - val_accuracy: 0.5234\n",
      "Epoch 196/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1912 - accuracy: 0.4950 - val_loss: 0.9588 - val_accuracy: 0.5234\n",
      "Epoch 197/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1915 - accuracy: 0.4961 - val_loss: 0.9612 - val_accuracy: 0.5234\n",
      "Epoch 198/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1914 - accuracy: 0.4922 - val_loss: 0.9596 - val_accuracy: 0.5234\n",
      "Epoch 199/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1922 - accuracy: 0.4944 - val_loss: 0.9621 - val_accuracy: 0.5234\n",
      "Epoch 200/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1915 - accuracy: 0.4927 - val_loss: 0.9622 - val_accuracy: 0.5234\n",
      "Epoch 201/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1915 - accuracy: 0.4955 - val_loss: 0.9574 - val_accuracy: 0.5234\n",
      "Epoch 202/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1912 - accuracy: 0.4944 - val_loss: 0.9612 - val_accuracy: 0.5234\n",
      "Epoch 203/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1913 - accuracy: 0.4922 - val_loss: 0.9618 - val_accuracy: 0.5234\n",
      "Epoch 204/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1907 - accuracy: 0.4933 - val_loss: 0.9606 - val_accuracy: 0.5234\n",
      "Epoch 205/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1910 - accuracy: 0.4944 - val_loss: 0.9533 - val_accuracy: 0.5234\n",
      "Epoch 206/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1924 - accuracy: 0.4967 - val_loss: 0.9578 - val_accuracy: 0.5234\n",
      "Epoch 207/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1911 - accuracy: 0.4911 - val_loss: 0.9604 - val_accuracy: 0.5234\n",
      "Epoch 208/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1910 - accuracy: 0.4955 - val_loss: 0.9593 - val_accuracy: 0.5234\n",
      "Epoch 209/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1924 - accuracy: 0.4933 - val_loss: 0.9618 - val_accuracy: 0.5234\n",
      "Epoch 210/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1918 - accuracy: 0.4939 - val_loss: 0.9589 - val_accuracy: 0.5234\n",
      "Epoch 211/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1915 - accuracy: 0.4911 - val_loss: 0.9626 - val_accuracy: 0.5234\n",
      "Epoch 212/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1907 - accuracy: 0.4955 - val_loss: 0.9575 - val_accuracy: 0.5234\n",
      "Epoch 213/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1907 - accuracy: 0.4950 - val_loss: 0.9559 - val_accuracy: 0.5234\n",
      "Epoch 214/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1908 - accuracy: 0.4972 - val_loss: 0.9571 - val_accuracy: 0.5234\n",
      "Epoch 215/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1916 - accuracy: 0.4927 - val_loss: 0.9590 - val_accuracy: 0.5234\n",
      "Epoch 216/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1918 - accuracy: 0.4950 - val_loss: 0.9608 - val_accuracy: 0.5234\n",
      "Epoch 217/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1913 - accuracy: 0.4933 - val_loss: 0.9620 - val_accuracy: 0.5234\n",
      "Epoch 218/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1917 - accuracy: 0.4955 - val_loss: 0.9593 - val_accuracy: 0.5234\n",
      "Epoch 219/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1910 - accuracy: 0.4939 - val_loss: 0.9613 - val_accuracy: 0.5234\n",
      "Epoch 220/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1907 - accuracy: 0.4950 - val_loss: 0.9605 - val_accuracy: 0.5234\n",
      "Epoch 221/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1905 - accuracy: 0.4922 - val_loss: 0.9644 - val_accuracy: 0.5234\n",
      "Epoch 222/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1903 - accuracy: 0.4933 - val_loss: 0.9582 - val_accuracy: 0.5234\n",
      "Epoch 223/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1907 - accuracy: 0.4927 - val_loss: 0.9616 - val_accuracy: 0.5234\n",
      "Epoch 224/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1900 - accuracy: 0.4944 - val_loss: 0.9558 - val_accuracy: 0.5234\n",
      "Epoch 225/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1905 - accuracy: 0.4950 - val_loss: 0.9590 - val_accuracy: 0.5234\n",
      "Epoch 226/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1905 - accuracy: 0.4950 - val_loss: 0.9580 - val_accuracy: 0.5234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1906 - accuracy: 0.4939 - val_loss: 0.9631 - val_accuracy: 0.5234\n",
      "Epoch 228/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1906 - accuracy: 0.4972 - val_loss: 0.9588 - val_accuracy: 0.5234\n",
      "Epoch 229/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1910 - accuracy: 0.4916 - val_loss: 0.9633 - val_accuracy: 0.5234\n",
      "Epoch 230/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1902 - accuracy: 0.4939 - val_loss: 0.9572 - val_accuracy: 0.5234\n",
      "Epoch 231/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1905 - accuracy: 0.4950 - val_loss: 0.9566 - val_accuracy: 0.5234\n",
      "Epoch 232/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1907 - accuracy: 0.4939 - val_loss: 0.9585 - val_accuracy: 0.5234\n",
      "Epoch 233/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1902 - accuracy: 0.4944 - val_loss: 0.9545 - val_accuracy: 0.5234\n",
      "Epoch 234/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1900 - accuracy: 0.4916 - val_loss: 0.9610 - val_accuracy: 0.5234\n",
      "Epoch 235/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1902 - accuracy: 0.4939 - val_loss: 0.9553 - val_accuracy: 0.5234\n",
      "Epoch 236/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1901 - accuracy: 0.4927 - val_loss: 0.9563 - val_accuracy: 0.5234\n",
      "Epoch 237/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1896 - accuracy: 0.4972 - val_loss: 0.9554 - val_accuracy: 0.5234\n",
      "Epoch 238/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1902 - accuracy: 0.4939 - val_loss: 0.9594 - val_accuracy: 0.5234\n",
      "Epoch 239/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1904 - accuracy: 0.4950 - val_loss: 0.9604 - val_accuracy: 0.5234\n",
      "Epoch 240/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1902 - accuracy: 0.4939 - val_loss: 0.9598 - val_accuracy: 0.5234\n",
      "Epoch 241/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1899 - accuracy: 0.4933 - val_loss: 0.9620 - val_accuracy: 0.5234\n",
      "Epoch 242/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1899 - accuracy: 0.4939 - val_loss: 0.9592 - val_accuracy: 0.5234\n",
      "Epoch 243/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1902 - accuracy: 0.4967 - val_loss: 0.9615 - val_accuracy: 0.5234\n",
      "Epoch 244/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1898 - accuracy: 0.4933 - val_loss: 0.9590 - val_accuracy: 0.5234\n",
      "Epoch 245/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1895 - accuracy: 0.4933 - val_loss: 0.9584 - val_accuracy: 0.5234\n",
      "Epoch 246/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1900 - accuracy: 0.4967 - val_loss: 0.9567 - val_accuracy: 0.5234\n",
      "Epoch 247/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1904 - accuracy: 0.4944 - val_loss: 0.9606 - val_accuracy: 0.5234\n",
      "Epoch 248/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1906 - accuracy: 0.4922 - val_loss: 0.9614 - val_accuracy: 0.5234\n",
      "Epoch 249/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1895 - accuracy: 0.4955 - val_loss: 0.9558 - val_accuracy: 0.5234\n",
      "Epoch 250/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1903 - accuracy: 0.4927 - val_loss: 0.9531 - val_accuracy: 0.5234\n",
      "Epoch 251/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1895 - accuracy: 0.4955 - val_loss: 0.9597 - val_accuracy: 0.5234\n",
      "Epoch 252/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1912 - accuracy: 0.4927 - val_loss: 0.9609 - val_accuracy: 0.5234\n",
      "Epoch 253/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1908 - accuracy: 0.4944 - val_loss: 0.9643 - val_accuracy: 0.5234\n",
      "Epoch 254/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1901 - accuracy: 0.4933 - val_loss: 0.9577 - val_accuracy: 0.5234\n",
      "Epoch 255/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1903 - accuracy: 0.4900 - val_loss: 0.9602 - val_accuracy: 0.5234\n",
      "Epoch 256/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1896 - accuracy: 0.4944 - val_loss: 0.9590 - val_accuracy: 0.5234\n",
      "Epoch 257/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1894 - accuracy: 0.4933 - val_loss: 0.9617 - val_accuracy: 0.5234\n",
      "Epoch 258/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1899 - accuracy: 0.4933 - val_loss: 0.9634 - val_accuracy: 0.5234\n",
      "Epoch 259/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1897 - accuracy: 0.4944 - val_loss: 0.9598 - val_accuracy: 0.5234\n",
      "Epoch 260/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1892 - accuracy: 0.4961 - val_loss: 0.9593 - val_accuracy: 0.5234\n",
      "Epoch 261/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1899 - accuracy: 0.4939 - val_loss: 0.9632 - val_accuracy: 0.5234\n",
      "Epoch 262/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1907 - accuracy: 0.4961 - val_loss: 0.9592 - val_accuracy: 0.5234\n",
      "Epoch 263/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1894 - accuracy: 0.4944 - val_loss: 0.9586 - val_accuracy: 0.5234\n",
      "Epoch 264/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1917 - accuracy: 0.4916 - val_loss: 0.9600 - val_accuracy: 0.5234\n",
      "Epoch 265/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1897 - accuracy: 0.4955 - val_loss: 0.9571 - val_accuracy: 0.5234\n",
      "Epoch 266/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1907 - accuracy: 0.4967 - val_loss: 0.9560 - val_accuracy: 0.5234\n",
      "Epoch 267/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1896 - accuracy: 0.4967 - val_loss: 0.9611 - val_accuracy: 0.5234\n",
      "Epoch 268/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1893 - accuracy: 0.4961 - val_loss: 0.9581 - val_accuracy: 0.5234\n",
      "Epoch 269/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1902 - accuracy: 0.4944 - val_loss: 0.9586 - val_accuracy: 0.5234\n",
      "Epoch 270/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1894 - accuracy: 0.4939 - val_loss: 0.9594 - val_accuracy: 0.5234\n",
      "Epoch 271/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1896 - accuracy: 0.4961 - val_loss: 0.9584 - val_accuracy: 0.5234\n",
      "Epoch 272/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1895 - accuracy: 0.4967 - val_loss: 0.9625 - val_accuracy: 0.5234\n",
      "Epoch 273/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1910 - accuracy: 0.4955 - val_loss: 0.9567 - val_accuracy: 0.5234\n",
      "Epoch 274/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1898 - accuracy: 0.4916 - val_loss: 0.9640 - val_accuracy: 0.5234\n",
      "Epoch 275/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1899 - accuracy: 0.4955 - val_loss: 0.9613 - val_accuracy: 0.5234\n",
      "Epoch 276/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1894 - accuracy: 0.4972 - val_loss: 0.9612 - val_accuracy: 0.5234\n",
      "Epoch 277/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1899 - accuracy: 0.4927 - val_loss: 0.9606 - val_accuracy: 0.5234\n",
      "Epoch 278/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1901 - accuracy: 0.4939 - val_loss: 0.9584 - val_accuracy: 0.5234\n",
      "Epoch 279/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1889 - accuracy: 0.4972 - val_loss: 0.9550 - val_accuracy: 0.5234\n",
      "Epoch 280/300\n",
      "28/28 [==============================] - 0s 18ms/step - loss: 1.1894 - accuracy: 0.4955 - val_loss: 0.9596 - val_accuracy: 0.5234\n",
      "Epoch 281/300\n",
      "28/28 [==============================] - 1s 31ms/step - loss: 1.1889 - accuracy: 0.4961 - val_loss: 0.9608 - val_accuracy: 0.5234\n",
      "Epoch 282/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1897 - accuracy: 0.4927 - val_loss: 0.9615 - val_accuracy: 0.5234\n",
      "Epoch 283/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1893 - accuracy: 0.4967 - val_loss: 0.9591 - val_accuracy: 0.5234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 284/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1898 - accuracy: 0.4950 - val_loss: 0.9586 - val_accuracy: 0.5234\n",
      "Epoch 285/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1899 - accuracy: 0.4922 - val_loss: 0.9591 - val_accuracy: 0.5234\n",
      "Epoch 286/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1891 - accuracy: 0.4967 - val_loss: 0.9637 - val_accuracy: 0.5234\n",
      "Epoch 287/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1890 - accuracy: 0.4955 - val_loss: 0.9619 - val_accuracy: 0.5234\n",
      "Epoch 288/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1891 - accuracy: 0.4961 - val_loss: 0.9628 - val_accuracy: 0.5234\n",
      "Epoch 289/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1894 - accuracy: 0.4939 - val_loss: 0.9569 - val_accuracy: 0.5234\n",
      "Epoch 290/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1896 - accuracy: 0.4939 - val_loss: 0.9587 - val_accuracy: 0.5234\n",
      "Epoch 291/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1889 - accuracy: 0.4972 - val_loss: 0.9591 - val_accuracy: 0.5234\n",
      "Epoch 292/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1892 - accuracy: 0.4961 - val_loss: 0.9625 - val_accuracy: 0.5234\n",
      "Epoch 293/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1890 - accuracy: 0.4933 - val_loss: 0.9586 - val_accuracy: 0.5234\n",
      "Epoch 294/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1897 - accuracy: 0.4950 - val_loss: 0.9684 - val_accuracy: 0.5234\n",
      "Epoch 295/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1896 - accuracy: 0.4950 - val_loss: 0.9612 - val_accuracy: 0.5234\n",
      "Epoch 296/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1894 - accuracy: 0.4950 - val_loss: 0.9590 - val_accuracy: 0.5234\n",
      "Epoch 297/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1895 - accuracy: 0.4939 - val_loss: 0.9576 - val_accuracy: 0.5234\n",
      "Epoch 298/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1894 - accuracy: 0.5000 - val_loss: 0.9640 - val_accuracy: 0.5234\n",
      "Epoch 299/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1903 - accuracy: 0.4911 - val_loss: 0.9642 - val_accuracy: 0.5234\n",
      "Epoch 300/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1893 - accuracy: 0.4972 - val_loss: 0.9556 - val_accuracy: 0.5234\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "56/56 [==============================] - 0s 1ms/step\n",
      "Recall: [0.93 0.01 0.61]\n",
      "Precision: [0.4  0.64 0.71]\n",
      "F1-score: [0.56 0.02 0.66]\n",
      "Accuracy: 0.5\n",
      "Brier score:0.19506517360902706\n",
      "Brier climat:0.20353918650793654\n",
      "Brier skill score:0.041633324001612126\n",
      "Recall: [0.93 0.01 0.61]\n",
      "Precision: [0.4  0.64 0.71]\n",
      "F1-score: [0.56 0.02 0.66]\n",
      "Accuracy: 0.5\n",
      "Brier score:0.22026805073958935\n",
      "Brier climat:0.22287698412698415\n",
      "Brier skill score:0.011705710204281794\n",
      "Recall: [0.93 0.01 0.61]\n",
      "Precision: [0.4  0.64 0.71]\n",
      "F1-score: [0.56 0.02 0.66]\n",
      "Accuracy: 0.5\n",
      "Brier score:0.15767881277506035\n",
      "Brier climat:0.2295758928571429\n",
      "Brier skill score:0.3131734747377052\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "Recall: [1. 0. 0.]\n",
      "Precision: [0.52 0.   0.  ]\n",
      "F1-score: [0.69 0.   0.  ]\n",
      "Accuracy: 0.52\n",
      "Brier score:0.24533295987828835\n",
      "Brier climat:0.3038368055555555\n",
      "Brier skill score:0.19255022633052898\n",
      "Recall: [1. 0. 0.]\n",
      "Precision: [0.52 0.   0.  ]\n",
      "F1-score: [0.69 0.   0.  ]\n",
      "Accuracy: 0.52\n",
      "Brier score:0.21701636511108002\n",
      "Brier climat:0.1995138888888889\n",
      "Brier skill score:-0.08772560306284438\n",
      "Recall: [1. 0. 0.]\n",
      "Precision: [0.52 0.   0.  ]\n",
      "F1-score: [0.69 0.   0.  ]\n",
      "Accuracy: 0.52\n",
      "Brier score:0.11107909896901519\n",
      "Brier climat:0.17265625\n",
      "Brier skill score:0.3566459426228985\n",
      "******** 18\n",
      "validation years [1999, 2000]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "{0: 2.0, 1: 1.0, 2: 1.0}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 60)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 183       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183\n",
      "Trainable params: 183\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - ETA: 0s - loss: 1.7877 - accuracy: 0.4068WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "28/28 [==============================] - 4s 107ms/step - loss: 1.7877 - accuracy: 0.4068 - val_loss: 1.5198 - val_accuracy: 0.3281\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.6176 - accuracy: 0.4665 - val_loss: 1.4057 - val_accuracy: 0.3438\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.5437 - accuracy: 0.4738 - val_loss: 1.3485 - val_accuracy: 0.3516\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.5003 - accuracy: 0.4883 - val_loss: 1.3078 - val_accuracy: 0.4531\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4720 - accuracy: 0.5028 - val_loss: 1.2948 - val_accuracy: 0.4453\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4532 - accuracy: 0.5073 - val_loss: 1.2906 - val_accuracy: 0.4375\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4381 - accuracy: 0.5100 - val_loss: 1.2839 - val_accuracy: 0.4453\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4248 - accuracy: 0.5095 - val_loss: 1.2852 - val_accuracy: 0.4453\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4128 - accuracy: 0.5078 - val_loss: 1.2846 - val_accuracy: 0.4375\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.4022 - accuracy: 0.5050 - val_loss: 1.2851 - val_accuracy: 0.4375\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3921 - accuracy: 0.5067 - val_loss: 1.2779 - val_accuracy: 0.4297\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3825 - accuracy: 0.5078 - val_loss: 1.2746 - val_accuracy: 0.4375\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3732 - accuracy: 0.5056 - val_loss: 1.2729 - val_accuracy: 0.4375\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.3646 - accuracy: 0.5028 - val_loss: 1.2756 - val_accuracy: 0.4219\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3565 - accuracy: 0.5073 - val_loss: 1.2686 - val_accuracy: 0.3906\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3480 - accuracy: 0.5067 - val_loss: 1.2718 - val_accuracy: 0.3828\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3402 - accuracy: 0.5033 - val_loss: 1.2619 - val_accuracy: 0.4062\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3341 - accuracy: 0.5017 - val_loss: 1.2695 - val_accuracy: 0.3828\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3268 - accuracy: 0.5045 - val_loss: 1.2633 - val_accuracy: 0.3828\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3208 - accuracy: 0.5056 - val_loss: 1.2632 - val_accuracy: 0.3828\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3147 - accuracy: 0.5017 - val_loss: 1.2637 - val_accuracy: 0.3828\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3089 - accuracy: 0.5011 - val_loss: 1.2544 - val_accuracy: 0.3828\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3049 - accuracy: 0.5045 - val_loss: 1.2599 - val_accuracy: 0.3906\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2992 - accuracy: 0.5045 - val_loss: 1.2503 - val_accuracy: 0.3828\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2934 - accuracy: 0.5011 - val_loss: 1.2514 - val_accuracy: 0.3828\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2902 - accuracy: 0.4978 - val_loss: 1.2578 - val_accuracy: 0.3750\n",
      "Epoch 27/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2852 - accuracy: 0.5000 - val_loss: 1.2512 - val_accuracy: 0.3828\n",
      "Epoch 28/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2808 - accuracy: 0.4989 - val_loss: 1.2592 - val_accuracy: 0.3750\n",
      "Epoch 29/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2770 - accuracy: 0.4983 - val_loss: 1.2493 - val_accuracy: 0.3828\n",
      "Epoch 30/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2727 - accuracy: 0.4961 - val_loss: 1.2537 - val_accuracy: 0.3750\n",
      "Epoch 31/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2697 - accuracy: 0.4983 - val_loss: 1.2605 - val_accuracy: 0.3750\n",
      "Epoch 32/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2663 - accuracy: 0.4972 - val_loss: 1.2556 - val_accuracy: 0.3750\n",
      "Epoch 33/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2631 - accuracy: 0.4944 - val_loss: 1.2565 - val_accuracy: 0.3750\n",
      "Epoch 34/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2596 - accuracy: 0.4989 - val_loss: 1.2478 - val_accuracy: 0.3750\n",
      "Epoch 35/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2582 - accuracy: 0.4972 - val_loss: 1.2521 - val_accuracy: 0.3750\n",
      "Epoch 36/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2542 - accuracy: 0.4972 - val_loss: 1.2386 - val_accuracy: 0.3750\n",
      "Epoch 37/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2512 - accuracy: 0.4994 - val_loss: 1.2425 - val_accuracy: 0.3750\n",
      "Epoch 38/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2484 - accuracy: 0.4967 - val_loss: 1.2412 - val_accuracy: 0.3750\n",
      "Epoch 39/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2471 - accuracy: 0.4955 - val_loss: 1.2388 - val_accuracy: 0.3750\n",
      "Epoch 40/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2436 - accuracy: 0.4967 - val_loss: 1.2444 - val_accuracy: 0.3750\n",
      "Epoch 41/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2410 - accuracy: 0.4967 - val_loss: 1.2392 - val_accuracy: 0.3750\n",
      "Epoch 42/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2389 - accuracy: 0.4989 - val_loss: 1.2374 - val_accuracy: 0.3750\n",
      "Epoch 43/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2367 - accuracy: 0.4955 - val_loss: 1.2397 - val_accuracy: 0.3750\n",
      "Epoch 44/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2349 - accuracy: 0.4950 - val_loss: 1.2319 - val_accuracy: 0.3750\n",
      "Epoch 45/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2322 - accuracy: 0.4955 - val_loss: 1.2300 - val_accuracy: 0.3750\n",
      "Epoch 46/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2307 - accuracy: 0.4972 - val_loss: 1.2299 - val_accuracy: 0.3750\n",
      "Epoch 47/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2286 - accuracy: 0.4967 - val_loss: 1.2239 - val_accuracy: 0.3750\n",
      "Epoch 48/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2266 - accuracy: 0.4972 - val_loss: 1.2189 - val_accuracy: 0.3750\n",
      "Epoch 49/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2252 - accuracy: 0.4961 - val_loss: 1.2226 - val_accuracy: 0.3750\n",
      "Epoch 50/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2230 - accuracy: 0.4978 - val_loss: 1.2214 - val_accuracy: 0.3750\n",
      "Epoch 51/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2222 - accuracy: 0.4955 - val_loss: 1.2215 - val_accuracy: 0.3750\n",
      "Epoch 52/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2221 - accuracy: 0.4967 - val_loss: 1.2124 - val_accuracy: 0.3750\n",
      "Epoch 53/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2194 - accuracy: 0.4961 - val_loss: 1.2209 - val_accuracy: 0.3750\n",
      "Epoch 54/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2177 - accuracy: 0.4955 - val_loss: 1.2134 - val_accuracy: 0.3750\n",
      "Epoch 55/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2168 - accuracy: 0.4983 - val_loss: 1.2140 - val_accuracy: 0.3750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2149 - accuracy: 0.4950 - val_loss: 1.2176 - val_accuracy: 0.3750\n",
      "Epoch 57/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2144 - accuracy: 0.4955 - val_loss: 1.2112 - val_accuracy: 0.3750\n",
      "Epoch 58/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2126 - accuracy: 0.4961 - val_loss: 1.2048 - val_accuracy: 0.3750\n",
      "Epoch 59/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2132 - accuracy: 0.4950 - val_loss: 1.2150 - val_accuracy: 0.3750\n",
      "Epoch 60/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2099 - accuracy: 0.4961 - val_loss: 1.2061 - val_accuracy: 0.3750\n",
      "Epoch 61/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2100 - accuracy: 0.4972 - val_loss: 1.2104 - val_accuracy: 0.3750\n",
      "Epoch 62/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2087 - accuracy: 0.4950 - val_loss: 1.2003 - val_accuracy: 0.3750\n",
      "Epoch 63/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2074 - accuracy: 0.4967 - val_loss: 1.2006 - val_accuracy: 0.3750\n",
      "Epoch 64/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2076 - accuracy: 0.4950 - val_loss: 1.1992 - val_accuracy: 0.3750\n",
      "Epoch 65/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2053 - accuracy: 0.4972 - val_loss: 1.1991 - val_accuracy: 0.3750\n",
      "Epoch 66/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2048 - accuracy: 0.4950 - val_loss: 1.2016 - val_accuracy: 0.3750\n",
      "Epoch 67/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2036 - accuracy: 0.4955 - val_loss: 1.2013 - val_accuracy: 0.3750\n",
      "Epoch 68/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2033 - accuracy: 0.4961 - val_loss: 1.1961 - val_accuracy: 0.3750\n",
      "Epoch 69/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2029 - accuracy: 0.4961 - val_loss: 1.1963 - val_accuracy: 0.3750\n",
      "Epoch 70/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2017 - accuracy: 0.4972 - val_loss: 1.1944 - val_accuracy: 0.3750\n",
      "Epoch 71/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2014 - accuracy: 0.4961 - val_loss: 1.1968 - val_accuracy: 0.3750\n",
      "Epoch 72/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2006 - accuracy: 0.4961 - val_loss: 1.2037 - val_accuracy: 0.3750\n",
      "Epoch 73/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2004 - accuracy: 0.4978 - val_loss: 1.1986 - val_accuracy: 0.3750\n",
      "Epoch 74/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2004 - accuracy: 0.4950 - val_loss: 1.1998 - val_accuracy: 0.3750\n",
      "Epoch 75/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1992 - accuracy: 0.4978 - val_loss: 1.1954 - val_accuracy: 0.3750\n",
      "Epoch 76/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1987 - accuracy: 0.4972 - val_loss: 1.1988 - val_accuracy: 0.3750\n",
      "Epoch 77/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1993 - accuracy: 0.4961 - val_loss: 1.2018 - val_accuracy: 0.3750\n",
      "Epoch 78/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1992 - accuracy: 0.4983 - val_loss: 1.1978 - val_accuracy: 0.3750\n",
      "Epoch 79/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1970 - accuracy: 0.4955 - val_loss: 1.1985 - val_accuracy: 0.3750\n",
      "Epoch 80/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1967 - accuracy: 0.4955 - val_loss: 1.1996 - val_accuracy: 0.3750\n",
      "Epoch 81/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1969 - accuracy: 0.4967 - val_loss: 1.2053 - val_accuracy: 0.3750\n",
      "Epoch 82/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1957 - accuracy: 0.4955 - val_loss: 1.1921 - val_accuracy: 0.3750\n",
      "Epoch 83/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1957 - accuracy: 0.4967 - val_loss: 1.1976 - val_accuracy: 0.3750\n",
      "Epoch 84/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1952 - accuracy: 0.4978 - val_loss: 1.1926 - val_accuracy: 0.3750\n",
      "Epoch 85/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1951 - accuracy: 0.4972 - val_loss: 1.2025 - val_accuracy: 0.3750\n",
      "Epoch 86/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1941 - accuracy: 0.4955 - val_loss: 1.1923 - val_accuracy: 0.3750\n",
      "Epoch 87/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1940 - accuracy: 0.4955 - val_loss: 1.1941 - val_accuracy: 0.3750\n",
      "Epoch 88/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1934 - accuracy: 0.4961 - val_loss: 1.1937 - val_accuracy: 0.3750\n",
      "Epoch 89/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1939 - accuracy: 0.4967 - val_loss: 1.1934 - val_accuracy: 0.3750\n",
      "Epoch 90/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1926 - accuracy: 0.4967 - val_loss: 1.1945 - val_accuracy: 0.3750\n",
      "Epoch 91/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1932 - accuracy: 0.4950 - val_loss: 1.1902 - val_accuracy: 0.3750\n",
      "Epoch 92/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1926 - accuracy: 0.4972 - val_loss: 1.1903 - val_accuracy: 0.3750\n",
      "Epoch 93/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1918 - accuracy: 0.4961 - val_loss: 1.1915 - val_accuracy: 0.3750\n",
      "Epoch 94/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1920 - accuracy: 0.4967 - val_loss: 1.1898 - val_accuracy: 0.3750\n",
      "Epoch 95/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1919 - accuracy: 0.4961 - val_loss: 1.1893 - val_accuracy: 0.3750\n",
      "Epoch 96/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1916 - accuracy: 0.4972 - val_loss: 1.1944 - val_accuracy: 0.3750\n",
      "Epoch 97/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1907 - accuracy: 0.4961 - val_loss: 1.1851 - val_accuracy: 0.3750\n",
      "Epoch 98/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1914 - accuracy: 0.4967 - val_loss: 1.1911 - val_accuracy: 0.3750\n",
      "Epoch 99/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1909 - accuracy: 0.4967 - val_loss: 1.1874 - val_accuracy: 0.3750\n",
      "Epoch 100/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1909 - accuracy: 0.4961 - val_loss: 1.1964 - val_accuracy: 0.3750\n",
      "Epoch 101/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1907 - accuracy: 0.4972 - val_loss: 1.1898 - val_accuracy: 0.3750\n",
      "Epoch 102/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1902 - accuracy: 0.4972 - val_loss: 1.1921 - val_accuracy: 0.3750\n",
      "Epoch 103/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1907 - accuracy: 0.4961 - val_loss: 1.1842 - val_accuracy: 0.3750\n",
      "Epoch 104/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1900 - accuracy: 0.4972 - val_loss: 1.1901 - val_accuracy: 0.3750\n",
      "Epoch 105/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1899 - accuracy: 0.4972 - val_loss: 1.1856 - val_accuracy: 0.3750\n",
      "Epoch 106/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1898 - accuracy: 0.4978 - val_loss: 1.1882 - val_accuracy: 0.3750\n",
      "Epoch 107/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1907 - accuracy: 0.4978 - val_loss: 1.1841 - val_accuracy: 0.3750\n",
      "Epoch 108/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1898 - accuracy: 0.4978 - val_loss: 1.1911 - val_accuracy: 0.3750\n",
      "Epoch 109/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1899 - accuracy: 0.4972 - val_loss: 1.1826 - val_accuracy: 0.3750\n",
      "Epoch 110/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1898 - accuracy: 0.4972 - val_loss: 1.1878 - val_accuracy: 0.3750\n",
      "Epoch 111/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1899 - accuracy: 0.4983 - val_loss: 1.1871 - val_accuracy: 0.3750\n",
      "Epoch 112/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1897 - accuracy: 0.4989 - val_loss: 1.1855 - val_accuracy: 0.3750\n",
      "Epoch 113/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1894 - accuracy: 0.4967 - val_loss: 1.1879 - val_accuracy: 0.3750\n",
      "Epoch 114/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1899 - accuracy: 0.4989 - val_loss: 1.1844 - val_accuracy: 0.3750\n",
      "Epoch 115/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1893 - accuracy: 0.4967 - val_loss: 1.1885 - val_accuracy: 0.3750\n",
      "Epoch 116/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1893 - accuracy: 0.4972 - val_loss: 1.1850 - val_accuracy: 0.3750\n",
      "Epoch 117/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1892 - accuracy: 0.4967 - val_loss: 1.1873 - val_accuracy: 0.3750\n",
      "Epoch 118/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1892 - accuracy: 0.4994 - val_loss: 1.1877 - val_accuracy: 0.3750\n",
      "Epoch 119/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1893 - accuracy: 0.4989 - val_loss: 1.1859 - val_accuracy: 0.3750\n",
      "Epoch 120/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1892 - accuracy: 0.4967 - val_loss: 1.1822 - val_accuracy: 0.3750\n",
      "Epoch 121/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1888 - accuracy: 0.4994 - val_loss: 1.1851 - val_accuracy: 0.3750\n",
      "Epoch 122/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1885 - accuracy: 0.4983 - val_loss: 1.1813 - val_accuracy: 0.3750\n",
      "Epoch 123/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1883 - accuracy: 0.4967 - val_loss: 1.1832 - val_accuracy: 0.3750\n",
      "Epoch 124/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1887 - accuracy: 0.4983 - val_loss: 1.1858 - val_accuracy: 0.3750\n",
      "Epoch 125/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1886 - accuracy: 0.4972 - val_loss: 1.1819 - val_accuracy: 0.3750\n",
      "Epoch 126/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1876 - accuracy: 0.4978 - val_loss: 1.1812 - val_accuracy: 0.3750\n",
      "Epoch 127/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1879 - accuracy: 0.4983 - val_loss: 1.1789 - val_accuracy: 0.3750\n",
      "Epoch 128/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1882 - accuracy: 0.4967 - val_loss: 1.1841 - val_accuracy: 0.3750\n",
      "Epoch 129/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1874 - accuracy: 0.4983 - val_loss: 1.1840 - val_accuracy: 0.3750\n",
      "Epoch 130/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1877 - accuracy: 0.4989 - val_loss: 1.1802 - val_accuracy: 0.3750\n",
      "Epoch 131/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1878 - accuracy: 0.4967 - val_loss: 1.1804 - val_accuracy: 0.3750\n",
      "Epoch 132/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1870 - accuracy: 0.4972 - val_loss: 1.1826 - val_accuracy: 0.3750\n",
      "Epoch 133/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1875 - accuracy: 0.4978 - val_loss: 1.1811 - val_accuracy: 0.3750\n",
      "Epoch 134/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1887 - accuracy: 0.4978 - val_loss: 1.1843 - val_accuracy: 0.3750\n",
      "Epoch 135/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1880 - accuracy: 0.4989 - val_loss: 1.1869 - val_accuracy: 0.3750\n",
      "Epoch 136/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1871 - accuracy: 0.4983 - val_loss: 1.1803 - val_accuracy: 0.3750\n",
      "Epoch 137/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1873 - accuracy: 0.4989 - val_loss: 1.1859 - val_accuracy: 0.3750\n",
      "Epoch 138/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1884 - accuracy: 0.4967 - val_loss: 1.1804 - val_accuracy: 0.3750\n",
      "Epoch 139/300\n",
      "28/28 [==============================] - 1s 29ms/step - loss: 1.1881 - accuracy: 0.4983 - val_loss: 1.1829 - val_accuracy: 0.3750\n",
      "Epoch 140/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1869 - accuracy: 0.4978 - val_loss: 1.1820 - val_accuracy: 0.3750\n",
      "Epoch 141/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1865 - accuracy: 0.4983 - val_loss: 1.1838 - val_accuracy: 0.3750\n",
      "Epoch 142/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1869 - accuracy: 0.4989 - val_loss: 1.1788 - val_accuracy: 0.3750\n",
      "Epoch 143/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1862 - accuracy: 0.4989 - val_loss: 1.1824 - val_accuracy: 0.3750\n",
      "Epoch 144/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1868 - accuracy: 0.4983 - val_loss: 1.1741 - val_accuracy: 0.3750\n",
      "Epoch 145/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1864 - accuracy: 0.4989 - val_loss: 1.1840 - val_accuracy: 0.3750\n",
      "Epoch 146/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1868 - accuracy: 0.4983 - val_loss: 1.1759 - val_accuracy: 0.3750\n",
      "Epoch 147/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1874 - accuracy: 0.4978 - val_loss: 1.1739 - val_accuracy: 0.3750\n",
      "Epoch 148/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1861 - accuracy: 0.4994 - val_loss: 1.1870 - val_accuracy: 0.3750\n",
      "Epoch 149/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1862 - accuracy: 0.4989 - val_loss: 1.1849 - val_accuracy: 0.3750\n",
      "Epoch 150/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1859 - accuracy: 0.4972 - val_loss: 1.1768 - val_accuracy: 0.3750\n",
      "Epoch 151/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1865 - accuracy: 0.4989 - val_loss: 1.1846 - val_accuracy: 0.3750\n",
      "Epoch 152/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1858 - accuracy: 0.4983 - val_loss: 1.1796 - val_accuracy: 0.3750\n",
      "Epoch 153/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1862 - accuracy: 0.4989 - val_loss: 1.1794 - val_accuracy: 0.3750\n",
      "Epoch 154/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1855 - accuracy: 0.4978 - val_loss: 1.1806 - val_accuracy: 0.3750\n",
      "Epoch 155/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1865 - accuracy: 0.4978 - val_loss: 1.1810 - val_accuracy: 0.3750\n",
      "Epoch 156/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1858 - accuracy: 0.4989 - val_loss: 1.1845 - val_accuracy: 0.3750\n",
      "Epoch 157/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1860 - accuracy: 0.4994 - val_loss: 1.1936 - val_accuracy: 0.3750\n",
      "Epoch 158/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1849 - accuracy: 0.4994 - val_loss: 1.1813 - val_accuracy: 0.3750\n",
      "Epoch 159/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1851 - accuracy: 0.4983 - val_loss: 1.1856 - val_accuracy: 0.3750\n",
      "Epoch 160/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1861 - accuracy: 0.4978 - val_loss: 1.1888 - val_accuracy: 0.3750\n",
      "Epoch 161/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1860 - accuracy: 0.4994 - val_loss: 1.1796 - val_accuracy: 0.3750\n",
      "Epoch 162/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1860 - accuracy: 0.4994 - val_loss: 1.1890 - val_accuracy: 0.3750\n",
      "Epoch 163/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1850 - accuracy: 0.4989 - val_loss: 1.1736 - val_accuracy: 0.3750\n",
      "Epoch 164/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1856 - accuracy: 0.4978 - val_loss: 1.1786 - val_accuracy: 0.3750\n",
      "Epoch 165/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1848 - accuracy: 0.4983 - val_loss: 1.1792 - val_accuracy: 0.3750\n",
      "Epoch 166/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1851 - accuracy: 0.4983 - val_loss: 1.1840 - val_accuracy: 0.3750\n",
      "Epoch 167/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1855 - accuracy: 0.4989 - val_loss: 1.1824 - val_accuracy: 0.3750\n",
      "Epoch 168/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1853 - accuracy: 0.4983 - val_loss: 1.1826 - val_accuracy: 0.3750\n",
      "Epoch 169/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1845 - accuracy: 0.4983 - val_loss: 1.1830 - val_accuracy: 0.3750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1850 - accuracy: 0.4989 - val_loss: 1.1829 - val_accuracy: 0.3750\n",
      "Epoch 171/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1848 - accuracy: 0.4978 - val_loss: 1.1796 - val_accuracy: 0.3750\n",
      "Epoch 172/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1848 - accuracy: 0.4989 - val_loss: 1.1821 - val_accuracy: 0.3750\n",
      "Epoch 173/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1846 - accuracy: 0.4972 - val_loss: 1.1826 - val_accuracy: 0.3750\n",
      "Epoch 174/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1842 - accuracy: 0.4994 - val_loss: 1.1830 - val_accuracy: 0.3750\n",
      "Epoch 175/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1853 - accuracy: 0.5011 - val_loss: 1.1894 - val_accuracy: 0.3750\n",
      "Epoch 176/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1847 - accuracy: 0.4983 - val_loss: 1.1896 - val_accuracy: 0.3750\n",
      "Epoch 177/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1848 - accuracy: 0.5000 - val_loss: 1.1893 - val_accuracy: 0.3750\n",
      "Epoch 178/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1845 - accuracy: 0.5006 - val_loss: 1.1837 - val_accuracy: 0.3750\n",
      "Epoch 179/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1844 - accuracy: 0.4989 - val_loss: 1.1881 - val_accuracy: 0.3750\n",
      "Epoch 180/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1843 - accuracy: 0.4994 - val_loss: 1.1810 - val_accuracy: 0.3750\n",
      "Epoch 181/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1847 - accuracy: 0.5000 - val_loss: 1.1765 - val_accuracy: 0.3750\n",
      "Epoch 182/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1842 - accuracy: 0.4994 - val_loss: 1.1858 - val_accuracy: 0.3750\n",
      "Epoch 183/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1852 - accuracy: 0.4989 - val_loss: 1.1909 - val_accuracy: 0.3750\n",
      "Epoch 184/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1846 - accuracy: 0.4972 - val_loss: 1.1795 - val_accuracy: 0.3750\n",
      "Epoch 185/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1844 - accuracy: 0.4983 - val_loss: 1.1900 - val_accuracy: 0.3750\n",
      "Epoch 186/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1844 - accuracy: 0.5000 - val_loss: 1.1839 - val_accuracy: 0.3750\n",
      "Epoch 187/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1842 - accuracy: 0.5011 - val_loss: 1.1855 - val_accuracy: 0.3750\n",
      "Epoch 188/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1848 - accuracy: 0.4978 - val_loss: 1.1934 - val_accuracy: 0.3750\n",
      "Epoch 189/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1854 - accuracy: 0.5000 - val_loss: 1.1804 - val_accuracy: 0.3750\n",
      "Epoch 190/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1848 - accuracy: 0.4983 - val_loss: 1.1875 - val_accuracy: 0.3750\n",
      "Epoch 191/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1845 - accuracy: 0.4978 - val_loss: 1.1832 - val_accuracy: 0.3750\n",
      "Epoch 192/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1846 - accuracy: 0.4989 - val_loss: 1.1888 - val_accuracy: 0.3750\n",
      "Epoch 193/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1842 - accuracy: 0.5022 - val_loss: 1.1802 - val_accuracy: 0.3750\n",
      "Epoch 194/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1843 - accuracy: 0.4978 - val_loss: 1.1808 - val_accuracy: 0.3750\n",
      "Epoch 195/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1841 - accuracy: 0.4983 - val_loss: 1.1903 - val_accuracy: 0.3750\n",
      "Epoch 196/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1843 - accuracy: 0.4994 - val_loss: 1.1881 - val_accuracy: 0.3750\n",
      "Epoch 197/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1847 - accuracy: 0.4994 - val_loss: 1.1950 - val_accuracy: 0.3750\n",
      "Epoch 198/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1837 - accuracy: 0.4994 - val_loss: 1.1803 - val_accuracy: 0.3750\n",
      "Epoch 199/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1835 - accuracy: 0.4989 - val_loss: 1.1812 - val_accuracy: 0.3750\n",
      "Epoch 200/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1838 - accuracy: 0.5000 - val_loss: 1.1941 - val_accuracy: 0.3750\n",
      "Epoch 201/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1839 - accuracy: 0.4989 - val_loss: 1.1862 - val_accuracy: 0.3750\n",
      "Epoch 202/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1836 - accuracy: 0.4978 - val_loss: 1.1883 - val_accuracy: 0.3750\n",
      "Epoch 203/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1837 - accuracy: 0.4994 - val_loss: 1.1822 - val_accuracy: 0.3750\n",
      "Epoch 204/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1835 - accuracy: 0.4989 - val_loss: 1.1759 - val_accuracy: 0.3750\n",
      "Epoch 205/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1837 - accuracy: 0.4994 - val_loss: 1.1829 - val_accuracy: 0.3750\n",
      "Epoch 206/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1833 - accuracy: 0.4989 - val_loss: 1.1849 - val_accuracy: 0.3750\n",
      "Epoch 207/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1844 - accuracy: 0.5000 - val_loss: 1.1874 - val_accuracy: 0.3750\n",
      "Epoch 208/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1838 - accuracy: 0.4978 - val_loss: 1.1840 - val_accuracy: 0.3750\n",
      "Epoch 209/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1831 - accuracy: 0.4994 - val_loss: 1.1800 - val_accuracy: 0.3750\n",
      "Epoch 210/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1845 - accuracy: 0.5022 - val_loss: 1.1944 - val_accuracy: 0.3750\n",
      "Epoch 211/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1832 - accuracy: 0.4989 - val_loss: 1.1807 - val_accuracy: 0.3750\n",
      "Epoch 212/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1833 - accuracy: 0.5006 - val_loss: 1.1797 - val_accuracy: 0.3750\n",
      "Epoch 213/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1836 - accuracy: 0.4994 - val_loss: 1.1865 - val_accuracy: 0.3750\n",
      "Epoch 214/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1840 - accuracy: 0.4994 - val_loss: 1.1788 - val_accuracy: 0.3750\n",
      "Epoch 215/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1832 - accuracy: 0.4994 - val_loss: 1.1811 - val_accuracy: 0.3750\n",
      "Epoch 216/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1833 - accuracy: 0.5000 - val_loss: 1.1868 - val_accuracy: 0.3750\n",
      "Epoch 217/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1838 - accuracy: 0.5000 - val_loss: 1.1865 - val_accuracy: 0.3750\n",
      "Epoch 218/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1839 - accuracy: 0.5000 - val_loss: 1.1850 - val_accuracy: 0.3750\n",
      "Epoch 219/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1829 - accuracy: 0.5000 - val_loss: 1.1835 - val_accuracy: 0.3750\n",
      "Epoch 220/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1834 - accuracy: 0.4983 - val_loss: 1.1776 - val_accuracy: 0.3750\n",
      "Epoch 221/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1835 - accuracy: 0.5000 - val_loss: 1.1876 - val_accuracy: 0.3750\n",
      "Epoch 222/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1830 - accuracy: 0.5000 - val_loss: 1.1834 - val_accuracy: 0.3750\n",
      "Epoch 223/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1837 - accuracy: 0.5017 - val_loss: 1.1878 - val_accuracy: 0.3750\n",
      "Epoch 224/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1828 - accuracy: 0.5017 - val_loss: 1.1841 - val_accuracy: 0.3750\n",
      "Epoch 225/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1830 - accuracy: 0.4978 - val_loss: 1.1823 - val_accuracy: 0.3750\n",
      "Epoch 226/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1829 - accuracy: 0.5011 - val_loss: 1.1814 - val_accuracy: 0.3750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1828 - accuracy: 0.4989 - val_loss: 1.1830 - val_accuracy: 0.3750\n",
      "Epoch 228/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1826 - accuracy: 0.4989 - val_loss: 1.1820 - val_accuracy: 0.3750\n",
      "Epoch 229/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1827 - accuracy: 0.4994 - val_loss: 1.1773 - val_accuracy: 0.3750\n",
      "Epoch 230/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1831 - accuracy: 0.5000 - val_loss: 1.1853 - val_accuracy: 0.3750\n",
      "Epoch 231/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1828 - accuracy: 0.5011 - val_loss: 1.1794 - val_accuracy: 0.3750\n",
      "Epoch 232/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1831 - accuracy: 0.5006 - val_loss: 1.1774 - val_accuracy: 0.3750\n",
      "Epoch 233/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1829 - accuracy: 0.4983 - val_loss: 1.1815 - val_accuracy: 0.3750\n",
      "Epoch 234/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1828 - accuracy: 0.5006 - val_loss: 1.1774 - val_accuracy: 0.3750\n",
      "Epoch 235/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1833 - accuracy: 0.4989 - val_loss: 1.1844 - val_accuracy: 0.3750\n",
      "Epoch 236/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1825 - accuracy: 0.5000 - val_loss: 1.1813 - val_accuracy: 0.3750\n",
      "Epoch 237/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1830 - accuracy: 0.5000 - val_loss: 1.1871 - val_accuracy: 0.3750\n",
      "Epoch 238/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1834 - accuracy: 0.5033 - val_loss: 1.1900 - val_accuracy: 0.3750\n",
      "Epoch 239/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1828 - accuracy: 0.4994 - val_loss: 1.1795 - val_accuracy: 0.3750\n",
      "Epoch 240/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1826 - accuracy: 0.4989 - val_loss: 1.1827 - val_accuracy: 0.3750\n",
      "Epoch 241/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1823 - accuracy: 0.5006 - val_loss: 1.1841 - val_accuracy: 0.3750\n",
      "Epoch 242/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1831 - accuracy: 0.5000 - val_loss: 1.1785 - val_accuracy: 0.3750\n",
      "Epoch 243/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1828 - accuracy: 0.5000 - val_loss: 1.1803 - val_accuracy: 0.3750\n",
      "Epoch 244/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1828 - accuracy: 0.5022 - val_loss: 1.1853 - val_accuracy: 0.3750\n",
      "Epoch 245/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1829 - accuracy: 0.4989 - val_loss: 1.1827 - val_accuracy: 0.3750\n",
      "Epoch 246/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1828 - accuracy: 0.5000 - val_loss: 1.1838 - val_accuracy: 0.3750\n",
      "Epoch 247/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1835 - accuracy: 0.5000 - val_loss: 1.1848 - val_accuracy: 0.3750\n",
      "Epoch 248/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1821 - accuracy: 0.4994 - val_loss: 1.1866 - val_accuracy: 0.3750\n",
      "Epoch 249/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1826 - accuracy: 0.5006 - val_loss: 1.1858 - val_accuracy: 0.3750\n",
      "Epoch 250/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1828 - accuracy: 0.5006 - val_loss: 1.1798 - val_accuracy: 0.3750\n",
      "Epoch 251/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1827 - accuracy: 0.5006 - val_loss: 1.1780 - val_accuracy: 0.3750\n",
      "Epoch 252/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1829 - accuracy: 0.5017 - val_loss: 1.1775 - val_accuracy: 0.3750\n",
      "Epoch 253/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1822 - accuracy: 0.5006 - val_loss: 1.1826 - val_accuracy: 0.3750\n",
      "Epoch 254/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1826 - accuracy: 0.5011 - val_loss: 1.1843 - val_accuracy: 0.3750\n",
      "Epoch 255/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1823 - accuracy: 0.5006 - val_loss: 1.1852 - val_accuracy: 0.3750\n",
      "Epoch 256/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1823 - accuracy: 0.4989 - val_loss: 1.1860 - val_accuracy: 0.3750\n",
      "Epoch 257/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1823 - accuracy: 0.5011 - val_loss: 1.1880 - val_accuracy: 0.3750\n",
      "Epoch 258/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1824 - accuracy: 0.5017 - val_loss: 1.1781 - val_accuracy: 0.3750\n",
      "Epoch 259/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1825 - accuracy: 0.5000 - val_loss: 1.1755 - val_accuracy: 0.3750\n",
      "Epoch 260/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1823 - accuracy: 0.4989 - val_loss: 1.1843 - val_accuracy: 0.3750\n",
      "Epoch 261/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1818 - accuracy: 0.4994 - val_loss: 1.1831 - val_accuracy: 0.3750\n",
      "Epoch 262/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1819 - accuracy: 0.5000 - val_loss: 1.1826 - val_accuracy: 0.3750\n",
      "Epoch 263/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1818 - accuracy: 0.5006 - val_loss: 1.1782 - val_accuracy: 0.3750\n",
      "Epoch 264/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1819 - accuracy: 0.5011 - val_loss: 1.1833 - val_accuracy: 0.3750\n",
      "Epoch 265/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1823 - accuracy: 0.5028 - val_loss: 1.1839 - val_accuracy: 0.3750\n",
      "Epoch 266/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1829 - accuracy: 0.5000 - val_loss: 1.1825 - val_accuracy: 0.3750\n",
      "Epoch 267/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1827 - accuracy: 0.5000 - val_loss: 1.1879 - val_accuracy: 0.3750\n",
      "Epoch 268/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1818 - accuracy: 0.4994 - val_loss: 1.1851 - val_accuracy: 0.3750\n",
      "Epoch 269/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1825 - accuracy: 0.5000 - val_loss: 1.1882 - val_accuracy: 0.3750\n",
      "Epoch 270/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1824 - accuracy: 0.4994 - val_loss: 1.1774 - val_accuracy: 0.3750\n",
      "Epoch 271/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1817 - accuracy: 0.5022 - val_loss: 1.1819 - val_accuracy: 0.3750\n",
      "Epoch 272/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1819 - accuracy: 0.5000 - val_loss: 1.1841 - val_accuracy: 0.3750\n",
      "Epoch 273/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1817 - accuracy: 0.5000 - val_loss: 1.1801 - val_accuracy: 0.3750\n",
      "Epoch 274/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1824 - accuracy: 0.5000 - val_loss: 1.1782 - val_accuracy: 0.3750\n",
      "Epoch 275/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1816 - accuracy: 0.5017 - val_loss: 1.1816 - val_accuracy: 0.3750\n",
      "Epoch 276/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1822 - accuracy: 0.5022 - val_loss: 1.1862 - val_accuracy: 0.3750\n",
      "Epoch 277/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1820 - accuracy: 0.5006 - val_loss: 1.1825 - val_accuracy: 0.3750\n",
      "Epoch 278/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1822 - accuracy: 0.5006 - val_loss: 1.1822 - val_accuracy: 0.3750\n",
      "Epoch 279/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1815 - accuracy: 0.5022 - val_loss: 1.1839 - val_accuracy: 0.3750\n",
      "Epoch 280/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1834 - accuracy: 0.5028 - val_loss: 1.1819 - val_accuracy: 0.3750\n",
      "Epoch 281/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1821 - accuracy: 0.4994 - val_loss: 1.1824 - val_accuracy: 0.3750\n",
      "Epoch 282/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1816 - accuracy: 0.5011 - val_loss: 1.1850 - val_accuracy: 0.3750\n",
      "Epoch 283/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1820 - accuracy: 0.4994 - val_loss: 1.1827 - val_accuracy: 0.3750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 284/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1819 - accuracy: 0.4994 - val_loss: 1.1855 - val_accuracy: 0.3750\n",
      "Epoch 285/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1818 - accuracy: 0.5011 - val_loss: 1.1787 - val_accuracy: 0.3750\n",
      "Epoch 286/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1823 - accuracy: 0.5000 - val_loss: 1.1744 - val_accuracy: 0.3750\n",
      "Epoch 287/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1819 - accuracy: 0.5028 - val_loss: 1.1800 - val_accuracy: 0.3750\n",
      "Epoch 288/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1823 - accuracy: 0.5011 - val_loss: 1.1830 - val_accuracy: 0.3750\n",
      "Epoch 289/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1822 - accuracy: 0.5011 - val_loss: 1.1870 - val_accuracy: 0.3750\n",
      "Epoch 290/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1818 - accuracy: 0.5006 - val_loss: 1.1803 - val_accuracy: 0.3750\n",
      "Epoch 291/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1819 - accuracy: 0.5017 - val_loss: 1.1828 - val_accuracy: 0.3750\n",
      "Epoch 292/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1814 - accuracy: 0.5006 - val_loss: 1.1858 - val_accuracy: 0.3750\n",
      "Epoch 293/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1817 - accuracy: 0.4994 - val_loss: 1.1870 - val_accuracy: 0.3750\n",
      "Epoch 294/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1821 - accuracy: 0.5011 - val_loss: 1.1872 - val_accuracy: 0.3750\n",
      "Epoch 295/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1812 - accuracy: 0.5022 - val_loss: 1.1818 - val_accuracy: 0.3750\n",
      "Epoch 296/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1812 - accuracy: 0.5006 - val_loss: 1.1805 - val_accuracy: 0.3750\n",
      "Epoch 297/300\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1.1820 - accuracy: 0.4994 - val_loss: 1.1924 - val_accuracy: 0.3750\n",
      "Epoch 298/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1810 - accuracy: 0.5006 - val_loss: 1.1798 - val_accuracy: 0.3750\n",
      "Epoch 299/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1817 - accuracy: 0.5006 - val_loss: 1.1830 - val_accuracy: 0.3750\n",
      "Epoch 300/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1816 - accuracy: 0.5011 - val_loss: 1.1813 - val_accuracy: 0.3750\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "56/56 [==============================] - 0s 1ms/step\n",
      "Recall: [0.94 0.   0.61]\n",
      "Precision: [0.41 0.   0.73]\n",
      "F1-score: [0.57 0.   0.66]\n",
      "Accuracy: 0.5\n",
      "Brier score:0.19947763393188275\n",
      "Brier climat:0.2084499007936508\n",
      "Brier skill score:0.04304279746647566\n",
      "Recall: [0.94 0.   0.61]\n",
      "Precision: [0.41 0.   0.73]\n",
      "F1-score: [0.57 0.   0.66]\n",
      "Accuracy: 0.5\n",
      "Brier score:0.2210074454093441\n",
      "Brier climat:0.22231894841269842\n",
      "Brier skill score:0.00589919578478626\n",
      "Recall: [0.94 0.   0.61]\n",
      "Precision: [0.41 0.   0.73]\n",
      "F1-score: [0.57 0.   0.66]\n",
      "Accuracy: 0.5\n",
      "Brier score:0.15081563673373624\n",
      "Brier climat:0.22585565476190478\n",
      "Brier skill score:0.33224768318187614\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "Recall: [1.   0.   0.09]\n",
      "Precision: [0.36 0.   0.67]\n",
      "F1-score: [0.53 0.   0.16]\n",
      "Accuracy: 0.38\n",
      "Brier score:0.24302066692863328\n",
      "Brier climat:0.23508680555555556\n",
      "Brier skill score:-0.03374864597070215\n",
      "Recall: [1.   0.   0.09]\n",
      "Precision: [0.36 0.   0.67]\n",
      "F1-score: [0.53 0.   0.16]\n",
      "Accuracy: 0.38\n",
      "Brier score:0.2113403262373451\n",
      "Brier climat:0.20732638888888888\n",
      "Brier skill score:-0.019360474901279456\n",
      "Recall: [1.   0.   0.09]\n",
      "Precision: [0.36 0.   0.67]\n",
      "F1-score: [0.53 0.   0.16]\n",
      "Accuracy: 0.38\n",
      "Brier score:0.23291032301008788\n",
      "Brier climat:0.22473958333333333\n",
      "Brier skill score:-0.03635647782008977\n",
      "******** 19\n",
      "validation years [2000, 2001]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "{0: 2.0, 1: 1.0, 2: 1.0}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 60)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 183       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183\n",
      "Trainable params: 183\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/28 [==========================>...] - ETA: 0s - loss: 1.9369 - accuracy: 0.3882WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "28/28 [==============================] - 4s 109ms/step - loss: 1.9231 - accuracy: 0.3878 - val_loss: 1.4758 - val_accuracy: 0.3594\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.7295 - accuracy: 0.4408 - val_loss: 1.4119 - val_accuracy: 0.3828\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.6102 - accuracy: 0.4587 - val_loss: 1.3714 - val_accuracy: 0.4297\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.5350 - accuracy: 0.4621 - val_loss: 1.3619 - val_accuracy: 0.3516\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4876 - accuracy: 0.4838 - val_loss: 1.3537 - val_accuracy: 0.3750\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.4594 - accuracy: 0.4967 - val_loss: 1.3524 - val_accuracy: 0.3750\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4399 - accuracy: 0.5011 - val_loss: 1.3556 - val_accuracy: 0.3750\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4257 - accuracy: 0.4967 - val_loss: 1.3590 - val_accuracy: 0.3516\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4143 - accuracy: 0.4967 - val_loss: 1.3569 - val_accuracy: 0.3672\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4028 - accuracy: 0.4961 - val_loss: 1.3580 - val_accuracy: 0.3672\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3929 - accuracy: 0.5000 - val_loss: 1.3603 - val_accuracy: 0.3359\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3837 - accuracy: 0.5028 - val_loss: 1.3605 - val_accuracy: 0.3359\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.3751 - accuracy: 0.5017 - val_loss: 1.3582 - val_accuracy: 0.3516\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3664 - accuracy: 0.4983 - val_loss: 1.3498 - val_accuracy: 0.3438\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3592 - accuracy: 0.5000 - val_loss: 1.3506 - val_accuracy: 0.3359\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3515 - accuracy: 0.4983 - val_loss: 1.3477 - val_accuracy: 0.3281\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3443 - accuracy: 0.4983 - val_loss: 1.3432 - val_accuracy: 0.3359\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3380 - accuracy: 0.4994 - val_loss: 1.3387 - val_accuracy: 0.3359\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3317 - accuracy: 0.5017 - val_loss: 1.3353 - val_accuracy: 0.3359\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3250 - accuracy: 0.4994 - val_loss: 1.3304 - val_accuracy: 0.3359\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3192 - accuracy: 0.5000 - val_loss: 1.3340 - val_accuracy: 0.3359\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3139 - accuracy: 0.5000 - val_loss: 1.3319 - val_accuracy: 0.3359\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3089 - accuracy: 0.5028 - val_loss: 1.3283 - val_accuracy: 0.3359\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3034 - accuracy: 0.5045 - val_loss: 1.3266 - val_accuracy: 0.3359\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2981 - accuracy: 0.5006 - val_loss: 1.3215 - val_accuracy: 0.3281\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2931 - accuracy: 0.5011 - val_loss: 1.3222 - val_accuracy: 0.3281\n",
      "Epoch 27/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2880 - accuracy: 0.5006 - val_loss: 1.3164 - val_accuracy: 0.3359\n",
      "Epoch 28/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2841 - accuracy: 0.4994 - val_loss: 1.3114 - val_accuracy: 0.3281\n",
      "Epoch 29/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2793 - accuracy: 0.5006 - val_loss: 1.3165 - val_accuracy: 0.3359\n",
      "Epoch 30/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2751 - accuracy: 0.4994 - val_loss: 1.3123 - val_accuracy: 0.3281\n",
      "Epoch 31/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2718 - accuracy: 0.5017 - val_loss: 1.3048 - val_accuracy: 0.3281\n",
      "Epoch 32/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2674 - accuracy: 0.5045 - val_loss: 1.3081 - val_accuracy: 0.3281\n",
      "Epoch 33/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2646 - accuracy: 0.4994 - val_loss: 1.3051 - val_accuracy: 0.3281\n",
      "Epoch 34/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2609 - accuracy: 0.4972 - val_loss: 1.3013 - val_accuracy: 0.3281\n",
      "Epoch 35/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2582 - accuracy: 0.5039 - val_loss: 1.3009 - val_accuracy: 0.3281\n",
      "Epoch 36/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2550 - accuracy: 0.5000 - val_loss: 1.3020 - val_accuracy: 0.3203\n",
      "Epoch 37/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2525 - accuracy: 0.4989 - val_loss: 1.2984 - val_accuracy: 0.3281\n",
      "Epoch 38/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2496 - accuracy: 0.5017 - val_loss: 1.2974 - val_accuracy: 0.3203\n",
      "Epoch 39/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2475 - accuracy: 0.4989 - val_loss: 1.2968 - val_accuracy: 0.3281\n",
      "Epoch 40/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2450 - accuracy: 0.5011 - val_loss: 1.2866 - val_accuracy: 0.3203\n",
      "Epoch 41/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2432 - accuracy: 0.4972 - val_loss: 1.2870 - val_accuracy: 0.3203\n",
      "Epoch 42/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2410 - accuracy: 0.5000 - val_loss: 1.2887 - val_accuracy: 0.3203\n",
      "Epoch 43/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2392 - accuracy: 0.4983 - val_loss: 1.2888 - val_accuracy: 0.3203\n",
      "Epoch 44/300\n",
      "28/28 [==============================] - 1s 29ms/step - loss: 1.2362 - accuracy: 0.4967 - val_loss: 1.2878 - val_accuracy: 0.3203\n",
      "Epoch 45/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2347 - accuracy: 0.4983 - val_loss: 1.2809 - val_accuracy: 0.3203\n",
      "Epoch 46/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2331 - accuracy: 0.5006 - val_loss: 1.2824 - val_accuracy: 0.3203\n",
      "Epoch 47/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2306 - accuracy: 0.4978 - val_loss: 1.2787 - val_accuracy: 0.3203\n",
      "Epoch 48/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2290 - accuracy: 0.4978 - val_loss: 1.2785 - val_accuracy: 0.3203\n",
      "Epoch 49/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2278 - accuracy: 0.4978 - val_loss: 1.2771 - val_accuracy: 0.3203\n",
      "Epoch 50/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2257 - accuracy: 0.5011 - val_loss: 1.2752 - val_accuracy: 0.3203\n",
      "Epoch 51/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2248 - accuracy: 0.4967 - val_loss: 1.2768 - val_accuracy: 0.3203\n",
      "Epoch 52/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2245 - accuracy: 0.4967 - val_loss: 1.2749 - val_accuracy: 0.3203\n",
      "Epoch 53/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2217 - accuracy: 0.5011 - val_loss: 1.2682 - val_accuracy: 0.3203\n",
      "Epoch 54/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2201 - accuracy: 0.5011 - val_loss: 1.2711 - val_accuracy: 0.3203\n",
      "Epoch 55/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2181 - accuracy: 0.4983 - val_loss: 1.2723 - val_accuracy: 0.3203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2179 - accuracy: 0.4978 - val_loss: 1.2704 - val_accuracy: 0.3203\n",
      "Epoch 57/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2159 - accuracy: 0.4989 - val_loss: 1.2617 - val_accuracy: 0.3203\n",
      "Epoch 58/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2151 - accuracy: 0.4989 - val_loss: 1.2664 - val_accuracy: 0.3203\n",
      "Epoch 59/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2130 - accuracy: 0.4994 - val_loss: 1.2640 - val_accuracy: 0.3203\n",
      "Epoch 60/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2117 - accuracy: 0.5011 - val_loss: 1.2610 - val_accuracy: 0.3203\n",
      "Epoch 61/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2103 - accuracy: 0.5011 - val_loss: 1.2616 - val_accuracy: 0.3203\n",
      "Epoch 62/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2098 - accuracy: 0.4994 - val_loss: 1.2599 - val_accuracy: 0.3203\n",
      "Epoch 63/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2084 - accuracy: 0.5006 - val_loss: 1.2563 - val_accuracy: 0.3203\n",
      "Epoch 64/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2073 - accuracy: 0.5022 - val_loss: 1.2594 - val_accuracy: 0.3203\n",
      "Epoch 65/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2066 - accuracy: 0.4989 - val_loss: 1.2568 - val_accuracy: 0.3203\n",
      "Epoch 66/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2055 - accuracy: 0.5006 - val_loss: 1.2600 - val_accuracy: 0.3203\n",
      "Epoch 67/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2043 - accuracy: 0.5011 - val_loss: 1.2557 - val_accuracy: 0.3203\n",
      "Epoch 68/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2038 - accuracy: 0.5033 - val_loss: 1.2533 - val_accuracy: 0.3203\n",
      "Epoch 69/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2027 - accuracy: 0.4994 - val_loss: 1.2541 - val_accuracy: 0.3203\n",
      "Epoch 70/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2021 - accuracy: 0.5017 - val_loss: 1.2539 - val_accuracy: 0.3203\n",
      "Epoch 71/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2004 - accuracy: 0.5000 - val_loss: 1.2546 - val_accuracy: 0.3203\n",
      "Epoch 72/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2000 - accuracy: 0.5011 - val_loss: 1.2535 - val_accuracy: 0.3203\n",
      "Epoch 73/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1995 - accuracy: 0.5022 - val_loss: 1.2573 - val_accuracy: 0.3203\n",
      "Epoch 74/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1982 - accuracy: 0.5000 - val_loss: 1.2558 - val_accuracy: 0.3203\n",
      "Epoch 75/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1977 - accuracy: 0.4989 - val_loss: 1.2528 - val_accuracy: 0.3203\n",
      "Epoch 76/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1971 - accuracy: 0.5022 - val_loss: 1.2513 - val_accuracy: 0.3203\n",
      "Epoch 77/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1968 - accuracy: 0.5011 - val_loss: 1.2563 - val_accuracy: 0.3203\n",
      "Epoch 78/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1954 - accuracy: 0.5022 - val_loss: 1.2556 - val_accuracy: 0.3203\n",
      "Epoch 79/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1947 - accuracy: 0.5011 - val_loss: 1.2527 - val_accuracy: 0.3203\n",
      "Epoch 80/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1941 - accuracy: 0.5033 - val_loss: 1.2536 - val_accuracy: 0.3203\n",
      "Epoch 81/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1947 - accuracy: 0.4989 - val_loss: 1.2558 - val_accuracy: 0.3203\n",
      "Epoch 82/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1934 - accuracy: 0.5022 - val_loss: 1.2564 - val_accuracy: 0.3203\n",
      "Epoch 83/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1923 - accuracy: 0.5000 - val_loss: 1.2549 - val_accuracy: 0.3203\n",
      "Epoch 84/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1920 - accuracy: 0.5050 - val_loss: 1.2526 - val_accuracy: 0.3203\n",
      "Epoch 85/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1915 - accuracy: 0.5045 - val_loss: 1.2517 - val_accuracy: 0.3203\n",
      "Epoch 86/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1907 - accuracy: 0.5022 - val_loss: 1.2498 - val_accuracy: 0.3203\n",
      "Epoch 87/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1906 - accuracy: 0.5011 - val_loss: 1.2497 - val_accuracy: 0.3203\n",
      "Epoch 88/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1906 - accuracy: 0.5022 - val_loss: 1.2520 - val_accuracy: 0.3203\n",
      "Epoch 89/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1898 - accuracy: 0.5061 - val_loss: 1.2531 - val_accuracy: 0.3203\n",
      "Epoch 90/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1901 - accuracy: 0.5017 - val_loss: 1.2526 - val_accuracy: 0.3203\n",
      "Epoch 91/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1890 - accuracy: 0.5039 - val_loss: 1.2495 - val_accuracy: 0.3203\n",
      "Epoch 92/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1887 - accuracy: 0.5022 - val_loss: 1.2527 - val_accuracy: 0.3203\n",
      "Epoch 93/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1889 - accuracy: 0.5061 - val_loss: 1.2522 - val_accuracy: 0.3203\n",
      "Epoch 94/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1886 - accuracy: 0.5028 - val_loss: 1.2497 - val_accuracy: 0.3203\n",
      "Epoch 95/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1883 - accuracy: 0.5028 - val_loss: 1.2529 - val_accuracy: 0.3203\n",
      "Epoch 96/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1886 - accuracy: 0.5022 - val_loss: 1.2532 - val_accuracy: 0.3203\n",
      "Epoch 97/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1872 - accuracy: 0.5033 - val_loss: 1.2547 - val_accuracy: 0.3203\n",
      "Epoch 98/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1865 - accuracy: 0.5033 - val_loss: 1.2540 - val_accuracy: 0.3203\n",
      "Epoch 99/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1864 - accuracy: 0.5028 - val_loss: 1.2519 - val_accuracy: 0.3203\n",
      "Epoch 100/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1873 - accuracy: 0.5022 - val_loss: 1.2544 - val_accuracy: 0.3203\n",
      "Epoch 101/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1865 - accuracy: 0.5006 - val_loss: 1.2520 - val_accuracy: 0.3203\n",
      "Epoch 102/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1867 - accuracy: 0.5045 - val_loss: 1.2539 - val_accuracy: 0.3203\n",
      "Epoch 103/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1867 - accuracy: 0.5028 - val_loss: 1.2538 - val_accuracy: 0.3203\n",
      "Epoch 104/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1856 - accuracy: 0.5017 - val_loss: 1.2554 - val_accuracy: 0.3203\n",
      "Epoch 105/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1854 - accuracy: 0.5022 - val_loss: 1.2530 - val_accuracy: 0.3203\n",
      "Epoch 106/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1848 - accuracy: 0.5028 - val_loss: 1.2545 - val_accuracy: 0.3203\n",
      "Epoch 107/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1851 - accuracy: 0.5045 - val_loss: 1.2543 - val_accuracy: 0.3203\n",
      "Epoch 108/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1852 - accuracy: 0.5073 - val_loss: 1.2525 - val_accuracy: 0.3203\n",
      "Epoch 109/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1846 - accuracy: 0.5022 - val_loss: 1.2525 - val_accuracy: 0.3203\n",
      "Epoch 110/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1860 - accuracy: 0.5050 - val_loss: 1.2519 - val_accuracy: 0.3203\n",
      "Epoch 111/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1848 - accuracy: 0.5033 - val_loss: 1.2507 - val_accuracy: 0.3203\n",
      "Epoch 112/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1849 - accuracy: 0.5067 - val_loss: 1.2527 - val_accuracy: 0.3203\n",
      "Epoch 113/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1855 - accuracy: 0.5033 - val_loss: 1.2490 - val_accuracy: 0.3203\n",
      "Epoch 114/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1845 - accuracy: 0.5011 - val_loss: 1.2515 - val_accuracy: 0.3203\n",
      "Epoch 115/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1844 - accuracy: 0.5050 - val_loss: 1.2519 - val_accuracy: 0.3203\n",
      "Epoch 116/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1841 - accuracy: 0.5045 - val_loss: 1.2488 - val_accuracy: 0.3203\n",
      "Epoch 117/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1846 - accuracy: 0.5006 - val_loss: 1.2490 - val_accuracy: 0.3203\n",
      "Epoch 118/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1832 - accuracy: 0.5061 - val_loss: 1.2505 - val_accuracy: 0.3203\n",
      "Epoch 119/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1832 - accuracy: 0.5045 - val_loss: 1.2481 - val_accuracy: 0.3203\n",
      "Epoch 120/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1831 - accuracy: 0.4983 - val_loss: 1.2473 - val_accuracy: 0.3203\n",
      "Epoch 121/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1842 - accuracy: 0.5011 - val_loss: 1.2536 - val_accuracy: 0.3203\n",
      "Epoch 122/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1832 - accuracy: 0.5056 - val_loss: 1.2537 - val_accuracy: 0.3203\n",
      "Epoch 123/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1830 - accuracy: 0.5039 - val_loss: 1.2493 - val_accuracy: 0.3203\n",
      "Epoch 124/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1830 - accuracy: 0.5033 - val_loss: 1.2485 - val_accuracy: 0.3203\n",
      "Epoch 125/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1825 - accuracy: 0.5022 - val_loss: 1.2546 - val_accuracy: 0.3203\n",
      "Epoch 126/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1820 - accuracy: 0.5050 - val_loss: 1.2513 - val_accuracy: 0.3203\n",
      "Epoch 127/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1822 - accuracy: 0.5033 - val_loss: 1.2513 - val_accuracy: 0.3203\n",
      "Epoch 128/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1835 - accuracy: 0.5045 - val_loss: 1.2521 - val_accuracy: 0.3203\n",
      "Epoch 129/300\n",
      "28/28 [==============================] - 2s 58ms/step - loss: 1.1829 - accuracy: 0.5033 - val_loss: 1.2500 - val_accuracy: 0.3203\n",
      "Epoch 130/300\n",
      "28/28 [==============================] - 1s 29ms/step - loss: 1.1824 - accuracy: 0.5017 - val_loss: 1.2531 - val_accuracy: 0.3203\n",
      "Epoch 131/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1819 - accuracy: 0.5028 - val_loss: 1.2522 - val_accuracy: 0.3203\n",
      "Epoch 132/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1821 - accuracy: 0.5017 - val_loss: 1.2513 - val_accuracy: 0.3203\n",
      "Epoch 133/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1824 - accuracy: 0.5050 - val_loss: 1.2486 - val_accuracy: 0.3203\n",
      "Epoch 134/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1820 - accuracy: 0.5028 - val_loss: 1.2508 - val_accuracy: 0.3203\n",
      "Epoch 135/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1809 - accuracy: 0.5022 - val_loss: 1.2557 - val_accuracy: 0.3203\n",
      "Epoch 136/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1814 - accuracy: 0.5022 - val_loss: 1.2505 - val_accuracy: 0.3203\n",
      "Epoch 137/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1814 - accuracy: 0.5028 - val_loss: 1.2525 - val_accuracy: 0.3203\n",
      "Epoch 138/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1816 - accuracy: 0.5045 - val_loss: 1.2536 - val_accuracy: 0.3203\n",
      "Epoch 139/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1812 - accuracy: 0.5050 - val_loss: 1.2517 - val_accuracy: 0.3203\n",
      "Epoch 140/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1808 - accuracy: 0.5017 - val_loss: 1.2533 - val_accuracy: 0.3203\n",
      "Epoch 141/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1814 - accuracy: 0.5033 - val_loss: 1.2512 - val_accuracy: 0.3203\n",
      "Epoch 142/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1804 - accuracy: 0.5017 - val_loss: 1.2516 - val_accuracy: 0.3203\n",
      "Epoch 143/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1811 - accuracy: 0.5045 - val_loss: 1.2553 - val_accuracy: 0.3203\n",
      "Epoch 144/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1805 - accuracy: 0.5056 - val_loss: 1.2496 - val_accuracy: 0.3203\n",
      "Epoch 145/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1808 - accuracy: 0.5039 - val_loss: 1.2509 - val_accuracy: 0.3203\n",
      "Epoch 146/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1800 - accuracy: 0.5028 - val_loss: 1.2496 - val_accuracy: 0.3203\n",
      "Epoch 147/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1807 - accuracy: 0.5033 - val_loss: 1.2513 - val_accuracy: 0.3203\n",
      "Epoch 148/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1800 - accuracy: 0.5022 - val_loss: 1.2553 - val_accuracy: 0.3203\n",
      "Epoch 149/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1798 - accuracy: 0.5050 - val_loss: 1.2483 - val_accuracy: 0.3203\n",
      "Epoch 150/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1796 - accuracy: 0.4994 - val_loss: 1.2495 - val_accuracy: 0.3203\n",
      "Epoch 151/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1802 - accuracy: 0.5022 - val_loss: 1.2482 - val_accuracy: 0.3203\n",
      "Epoch 152/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1793 - accuracy: 0.5045 - val_loss: 1.2521 - val_accuracy: 0.3203\n",
      "Epoch 153/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1799 - accuracy: 0.5028 - val_loss: 1.2474 - val_accuracy: 0.3203\n",
      "Epoch 154/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1792 - accuracy: 0.5022 - val_loss: 1.2523 - val_accuracy: 0.3203\n",
      "Epoch 155/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1793 - accuracy: 0.5017 - val_loss: 1.2477 - val_accuracy: 0.3203\n",
      "Epoch 156/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1797 - accuracy: 0.5028 - val_loss: 1.2523 - val_accuracy: 0.3203\n",
      "Epoch 157/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1803 - accuracy: 0.5039 - val_loss: 1.2465 - val_accuracy: 0.3203\n",
      "Epoch 158/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1795 - accuracy: 0.5011 - val_loss: 1.2479 - val_accuracy: 0.3203\n",
      "Epoch 159/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1799 - accuracy: 0.5006 - val_loss: 1.2473 - val_accuracy: 0.3203\n",
      "Epoch 160/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1797 - accuracy: 0.5045 - val_loss: 1.2504 - val_accuracy: 0.3203\n",
      "Epoch 161/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1790 - accuracy: 0.5006 - val_loss: 1.2477 - val_accuracy: 0.3203\n",
      "Epoch 162/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1793 - accuracy: 0.5028 - val_loss: 1.2503 - val_accuracy: 0.3203\n",
      "Epoch 163/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1791 - accuracy: 0.5039 - val_loss: 1.2466 - val_accuracy: 0.3203\n",
      "Epoch 164/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1796 - accuracy: 0.5022 - val_loss: 1.2508 - val_accuracy: 0.3203\n",
      "Epoch 165/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1794 - accuracy: 0.5045 - val_loss: 1.2466 - val_accuracy: 0.3203\n",
      "Epoch 166/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1787 - accuracy: 0.5017 - val_loss: 1.2507 - val_accuracy: 0.3203\n",
      "Epoch 167/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1789 - accuracy: 0.5022 - val_loss: 1.2485 - val_accuracy: 0.3203\n",
      "Epoch 168/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1799 - accuracy: 0.5045 - val_loss: 1.2480 - val_accuracy: 0.3203\n",
      "Epoch 169/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1792 - accuracy: 0.5006 - val_loss: 1.2497 - val_accuracy: 0.3203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1790 - accuracy: 0.5056 - val_loss: 1.2446 - val_accuracy: 0.3203\n",
      "Epoch 171/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1795 - accuracy: 0.5022 - val_loss: 1.2485 - val_accuracy: 0.3203\n",
      "Epoch 172/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1792 - accuracy: 0.5056 - val_loss: 1.2481 - val_accuracy: 0.3203\n",
      "Epoch 173/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1790 - accuracy: 0.5022 - val_loss: 1.2454 - val_accuracy: 0.3203\n",
      "Epoch 174/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1790 - accuracy: 0.5011 - val_loss: 1.2426 - val_accuracy: 0.3203\n",
      "Epoch 175/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1785 - accuracy: 0.5006 - val_loss: 1.2448 - val_accuracy: 0.3203\n",
      "Epoch 176/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1788 - accuracy: 0.5033 - val_loss: 1.2442 - val_accuracy: 0.3203\n",
      "Epoch 177/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1789 - accuracy: 0.4994 - val_loss: 1.2441 - val_accuracy: 0.3203\n",
      "Epoch 178/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1782 - accuracy: 0.5017 - val_loss: 1.2456 - val_accuracy: 0.3203\n",
      "Epoch 179/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1786 - accuracy: 0.5022 - val_loss: 1.2462 - val_accuracy: 0.3203\n",
      "Epoch 180/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1784 - accuracy: 0.5039 - val_loss: 1.2466 - val_accuracy: 0.3203\n",
      "Epoch 181/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1788 - accuracy: 0.5028 - val_loss: 1.2441 - val_accuracy: 0.3203\n",
      "Epoch 182/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1789 - accuracy: 0.5011 - val_loss: 1.2476 - val_accuracy: 0.3203\n",
      "Epoch 183/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1780 - accuracy: 0.5011 - val_loss: 1.2462 - val_accuracy: 0.3203\n",
      "Epoch 184/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1785 - accuracy: 0.5022 - val_loss: 1.2484 - val_accuracy: 0.3203\n",
      "Epoch 185/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1783 - accuracy: 0.5028 - val_loss: 1.2438 - val_accuracy: 0.3203\n",
      "Epoch 186/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1782 - accuracy: 0.5011 - val_loss: 1.2477 - val_accuracy: 0.3203\n",
      "Epoch 187/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1784 - accuracy: 0.5045 - val_loss: 1.2489 - val_accuracy: 0.3203\n",
      "Epoch 188/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1786 - accuracy: 0.5000 - val_loss: 1.2459 - val_accuracy: 0.3203\n",
      "Epoch 189/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1777 - accuracy: 0.5039 - val_loss: 1.2460 - val_accuracy: 0.3203\n",
      "Epoch 190/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1779 - accuracy: 0.5011 - val_loss: 1.2466 - val_accuracy: 0.3203\n",
      "Epoch 191/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1779 - accuracy: 0.5011 - val_loss: 1.2450 - val_accuracy: 0.3203\n",
      "Epoch 192/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1786 - accuracy: 0.5045 - val_loss: 1.2450 - val_accuracy: 0.3203\n",
      "Epoch 193/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1783 - accuracy: 0.5022 - val_loss: 1.2449 - val_accuracy: 0.3203\n",
      "Epoch 194/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1784 - accuracy: 0.5028 - val_loss: 1.2464 - val_accuracy: 0.3203\n",
      "Epoch 195/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1782 - accuracy: 0.5028 - val_loss: 1.2467 - val_accuracy: 0.3203\n",
      "Epoch 196/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1786 - accuracy: 0.5011 - val_loss: 1.2462 - val_accuracy: 0.3203\n",
      "Epoch 197/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1781 - accuracy: 0.5011 - val_loss: 1.2440 - val_accuracy: 0.3203\n",
      "Epoch 198/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1776 - accuracy: 0.5045 - val_loss: 1.2439 - val_accuracy: 0.3203\n",
      "Epoch 199/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1775 - accuracy: 0.5022 - val_loss: 1.2480 - val_accuracy: 0.3203\n",
      "Epoch 200/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1780 - accuracy: 0.5033 - val_loss: 1.2474 - val_accuracy: 0.3203\n",
      "Epoch 201/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1782 - accuracy: 0.5056 - val_loss: 1.2449 - val_accuracy: 0.3203\n",
      "Epoch 202/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1788 - accuracy: 0.5011 - val_loss: 1.2474 - val_accuracy: 0.3203\n",
      "Epoch 203/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1774 - accuracy: 0.5028 - val_loss: 1.2429 - val_accuracy: 0.3203\n",
      "Epoch 204/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1785 - accuracy: 0.5050 - val_loss: 1.2464 - val_accuracy: 0.3203\n",
      "Epoch 205/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1786 - accuracy: 0.5045 - val_loss: 1.2461 - val_accuracy: 0.3203\n",
      "Epoch 206/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1779 - accuracy: 0.5017 - val_loss: 1.2475 - val_accuracy: 0.3203\n",
      "Epoch 207/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1773 - accuracy: 0.5017 - val_loss: 1.2454 - val_accuracy: 0.3203\n",
      "Epoch 208/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1778 - accuracy: 0.5045 - val_loss: 1.2483 - val_accuracy: 0.3203\n",
      "Epoch 209/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1777 - accuracy: 0.5017 - val_loss: 1.2452 - val_accuracy: 0.3203\n",
      "Epoch 210/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1779 - accuracy: 0.5039 - val_loss: 1.2452 - val_accuracy: 0.3203\n",
      "Epoch 211/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1783 - accuracy: 0.5050 - val_loss: 1.2480 - val_accuracy: 0.3203\n",
      "Epoch 212/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1775 - accuracy: 0.5022 - val_loss: 1.2425 - val_accuracy: 0.3203\n",
      "Epoch 213/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1786 - accuracy: 0.5022 - val_loss: 1.2476 - val_accuracy: 0.3203\n",
      "Epoch 214/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1787 - accuracy: 0.5017 - val_loss: 1.2466 - val_accuracy: 0.3203\n",
      "Epoch 215/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1775 - accuracy: 0.5017 - val_loss: 1.2518 - val_accuracy: 0.3203\n",
      "Epoch 216/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1773 - accuracy: 0.5011 - val_loss: 1.2431 - val_accuracy: 0.3203\n",
      "Epoch 217/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1776 - accuracy: 0.5028 - val_loss: 1.2439 - val_accuracy: 0.3203\n",
      "Epoch 218/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1774 - accuracy: 0.5050 - val_loss: 1.2442 - val_accuracy: 0.3203\n",
      "Epoch 219/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1771 - accuracy: 0.5045 - val_loss: 1.2463 - val_accuracy: 0.3203\n",
      "Epoch 220/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1785 - accuracy: 0.5022 - val_loss: 1.2435 - val_accuracy: 0.3203\n",
      "Epoch 221/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1777 - accuracy: 0.5073 - val_loss: 1.2442 - val_accuracy: 0.3203\n",
      "Epoch 222/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1773 - accuracy: 0.5017 - val_loss: 1.2459 - val_accuracy: 0.3203\n",
      "Epoch 223/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1770 - accuracy: 0.5033 - val_loss: 1.2451 - val_accuracy: 0.3203\n",
      "Epoch 224/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1773 - accuracy: 0.5039 - val_loss: 1.2417 - val_accuracy: 0.3203\n",
      "Epoch 225/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1780 - accuracy: 0.5028 - val_loss: 1.2461 - val_accuracy: 0.3203\n",
      "Epoch 226/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1783 - accuracy: 0.5045 - val_loss: 1.2445 - val_accuracy: 0.3203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1773 - accuracy: 0.5045 - val_loss: 1.2468 - val_accuracy: 0.3203\n",
      "Epoch 228/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1775 - accuracy: 0.5006 - val_loss: 1.2426 - val_accuracy: 0.3203\n",
      "Epoch 229/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1769 - accuracy: 0.5028 - val_loss: 1.2457 - val_accuracy: 0.3203\n",
      "Epoch 230/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1778 - accuracy: 0.5039 - val_loss: 1.2446 - val_accuracy: 0.3203\n",
      "Epoch 231/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1768 - accuracy: 0.5045 - val_loss: 1.2464 - val_accuracy: 0.3203\n",
      "Epoch 232/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1781 - accuracy: 0.5022 - val_loss: 1.2434 - val_accuracy: 0.3203\n",
      "Epoch 233/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1776 - accuracy: 0.5017 - val_loss: 1.2433 - val_accuracy: 0.3203\n",
      "Epoch 234/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1770 - accuracy: 0.5011 - val_loss: 1.2455 - val_accuracy: 0.3203\n",
      "Epoch 235/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1775 - accuracy: 0.5067 - val_loss: 1.2425 - val_accuracy: 0.3203\n",
      "Epoch 236/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1766 - accuracy: 0.5006 - val_loss: 1.2415 - val_accuracy: 0.3203\n",
      "Epoch 237/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1776 - accuracy: 0.5050 - val_loss: 1.2432 - val_accuracy: 0.3203\n",
      "Epoch 238/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1771 - accuracy: 0.5033 - val_loss: 1.2424 - val_accuracy: 0.3203\n",
      "Epoch 239/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1772 - accuracy: 0.5028 - val_loss: 1.2463 - val_accuracy: 0.3203\n",
      "Epoch 240/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1766 - accuracy: 0.5045 - val_loss: 1.2401 - val_accuracy: 0.3203\n",
      "Epoch 241/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1768 - accuracy: 0.5033 - val_loss: 1.2426 - val_accuracy: 0.3203\n",
      "Epoch 242/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1769 - accuracy: 0.5028 - val_loss: 1.2447 - val_accuracy: 0.3203\n",
      "Epoch 243/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1770 - accuracy: 0.5056 - val_loss: 1.2409 - val_accuracy: 0.3203\n",
      "Epoch 244/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1770 - accuracy: 0.5039 - val_loss: 1.2456 - val_accuracy: 0.3203\n",
      "Epoch 245/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1767 - accuracy: 0.5022 - val_loss: 1.2434 - val_accuracy: 0.3203\n",
      "Epoch 246/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1773 - accuracy: 0.5028 - val_loss: 1.2466 - val_accuracy: 0.3203\n",
      "Epoch 247/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1770 - accuracy: 0.5028 - val_loss: 1.2418 - val_accuracy: 0.3203\n",
      "Epoch 248/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1771 - accuracy: 0.5022 - val_loss: 1.2426 - val_accuracy: 0.3203\n",
      "Epoch 249/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1767 - accuracy: 0.5050 - val_loss: 1.2401 - val_accuracy: 0.3203\n",
      "Epoch 250/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1771 - accuracy: 0.5033 - val_loss: 1.2446 - val_accuracy: 0.3203\n",
      "Epoch 251/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1774 - accuracy: 0.5073 - val_loss: 1.2425 - val_accuracy: 0.3203\n",
      "Epoch 252/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1768 - accuracy: 0.5033 - val_loss: 1.2413 - val_accuracy: 0.3203\n",
      "Epoch 253/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1767 - accuracy: 0.5017 - val_loss: 1.2407 - val_accuracy: 0.3203\n",
      "Epoch 254/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1769 - accuracy: 0.5022 - val_loss: 1.2451 - val_accuracy: 0.3203\n",
      "Epoch 255/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1771 - accuracy: 0.5022 - val_loss: 1.2458 - val_accuracy: 0.3125\n",
      "Epoch 256/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1766 - accuracy: 0.5033 - val_loss: 1.2377 - val_accuracy: 0.3203\n",
      "Epoch 257/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1773 - accuracy: 0.5033 - val_loss: 1.2420 - val_accuracy: 0.3203\n",
      "Epoch 258/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1766 - accuracy: 0.5033 - val_loss: 1.2424 - val_accuracy: 0.3203\n",
      "Epoch 259/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1763 - accuracy: 0.5045 - val_loss: 1.2419 - val_accuracy: 0.3203\n",
      "Epoch 260/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1769 - accuracy: 0.5028 - val_loss: 1.2451 - val_accuracy: 0.3203\n",
      "Epoch 261/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1765 - accuracy: 0.5033 - val_loss: 1.2424 - val_accuracy: 0.3203\n",
      "Epoch 262/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1761 - accuracy: 0.5022 - val_loss: 1.2402 - val_accuracy: 0.3203\n",
      "Epoch 263/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1769 - accuracy: 0.5033 - val_loss: 1.2409 - val_accuracy: 0.3203\n",
      "Epoch 264/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1769 - accuracy: 0.5039 - val_loss: 1.2391 - val_accuracy: 0.3203\n",
      "Epoch 265/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1765 - accuracy: 0.5017 - val_loss: 1.2404 - val_accuracy: 0.3203\n",
      "Epoch 266/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1766 - accuracy: 0.5045 - val_loss: 1.2415 - val_accuracy: 0.3203\n",
      "Epoch 267/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1762 - accuracy: 0.5017 - val_loss: 1.2411 - val_accuracy: 0.3203\n",
      "Epoch 268/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1767 - accuracy: 0.5039 - val_loss: 1.2404 - val_accuracy: 0.3203\n",
      "Epoch 269/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1760 - accuracy: 0.5022 - val_loss: 1.2396 - val_accuracy: 0.3203\n",
      "Epoch 270/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1771 - accuracy: 0.5050 - val_loss: 1.2409 - val_accuracy: 0.3203\n",
      "Epoch 271/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1766 - accuracy: 0.5022 - val_loss: 1.2398 - val_accuracy: 0.3203\n",
      "Epoch 272/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1764 - accuracy: 0.5045 - val_loss: 1.2410 - val_accuracy: 0.3203\n",
      "Epoch 273/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1768 - accuracy: 0.5033 - val_loss: 1.2406 - val_accuracy: 0.3125\n",
      "Epoch 274/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1764 - accuracy: 0.5067 - val_loss: 1.2377 - val_accuracy: 0.3203\n",
      "Epoch 275/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1763 - accuracy: 0.5045 - val_loss: 1.2412 - val_accuracy: 0.3203\n",
      "Epoch 276/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1762 - accuracy: 0.5056 - val_loss: 1.2418 - val_accuracy: 0.3203\n",
      "Epoch 277/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1762 - accuracy: 0.5033 - val_loss: 1.2435 - val_accuracy: 0.3203\n",
      "Epoch 278/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1762 - accuracy: 0.5033 - val_loss: 1.2409 - val_accuracy: 0.3203\n",
      "Epoch 279/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1772 - accuracy: 0.5061 - val_loss: 1.2389 - val_accuracy: 0.3203\n",
      "Epoch 280/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1772 - accuracy: 0.5045 - val_loss: 1.2430 - val_accuracy: 0.3203\n",
      "Epoch 281/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1763 - accuracy: 0.5039 - val_loss: 1.2389 - val_accuracy: 0.3203\n",
      "Epoch 282/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1767 - accuracy: 0.5045 - val_loss: 1.2363 - val_accuracy: 0.3203\n",
      "Epoch 283/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1761 - accuracy: 0.5039 - val_loss: 1.2387 - val_accuracy: 0.3203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 284/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1774 - accuracy: 0.5033 - val_loss: 1.2416 - val_accuracy: 0.3203\n",
      "Epoch 285/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1768 - accuracy: 0.5039 - val_loss: 1.2388 - val_accuracy: 0.3203\n",
      "Epoch 286/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1773 - accuracy: 0.5050 - val_loss: 1.2393 - val_accuracy: 0.3203\n",
      "Epoch 287/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1766 - accuracy: 0.5011 - val_loss: 1.2409 - val_accuracy: 0.3203\n",
      "Epoch 288/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1765 - accuracy: 0.5039 - val_loss: 1.2382 - val_accuracy: 0.3203\n",
      "Epoch 289/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1775 - accuracy: 0.5056 - val_loss: 1.2436 - val_accuracy: 0.3203\n",
      "Epoch 290/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1769 - accuracy: 0.5045 - val_loss: 1.2384 - val_accuracy: 0.3203\n",
      "Epoch 291/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1759 - accuracy: 0.5056 - val_loss: 1.2407 - val_accuracy: 0.3203\n",
      "Epoch 292/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1760 - accuracy: 0.5039 - val_loss: 1.2363 - val_accuracy: 0.3203\n",
      "Epoch 293/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1764 - accuracy: 0.5028 - val_loss: 1.2377 - val_accuracy: 0.3203\n",
      "Epoch 294/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1767 - accuracy: 0.5056 - val_loss: 1.2411 - val_accuracy: 0.3203\n",
      "Epoch 295/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1788 - accuracy: 0.5039 - val_loss: 1.2399 - val_accuracy: 0.3203\n",
      "Epoch 296/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1764 - accuracy: 0.5045 - val_loss: 1.2426 - val_accuracy: 0.3203\n",
      "Epoch 297/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1.1759 - accuracy: 0.5039 - val_loss: 1.2382 - val_accuracy: 0.3203\n",
      "Epoch 298/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1773 - accuracy: 0.5050 - val_loss: 1.2399 - val_accuracy: 0.3203\n",
      "Epoch 299/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1764 - accuracy: 0.5039 - val_loss: 1.2357 - val_accuracy: 0.3203\n",
      "Epoch 300/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1773 - accuracy: 0.5039 - val_loss: 1.2402 - val_accuracy: 0.3203\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "56/56 [==============================] - 0s 1ms/step\n",
      "Recall: [0.94 0.   0.61]\n",
      "Precision: [0.41 0.   0.71]\n",
      "F1-score: [0.57 0.   0.66]\n",
      "Accuracy: 0.5\n",
      "Brier score:0.19930551791547438\n",
      "Brier climat:0.21127728174603178\n",
      "Brier skill score:0.05666375358306719\n",
      "Recall: [0.94 0.   0.61]\n",
      "Precision: [0.41 0.   0.71]\n",
      "F1-score: [0.57 0.   0.66]\n",
      "Accuracy: 0.5\n",
      "Brier score:0.2197038691593463\n",
      "Brier climat:0.22116567460317463\n",
      "Brier skill score:0.006609549363621459\n",
      "Recall: [0.94 0.   0.61]\n",
      "Precision: [0.41 0.   0.71]\n",
      "F1-score: [0.57 0.   0.66]\n",
      "Accuracy: 0.5\n",
      "Brier score:0.15071037092339631\n",
      "Brier climat:0.22600446428571427\n",
      "Brier skill score:0.33315312421055254\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "Recall: [1.   0.   0.12]\n",
      "Precision: [0.3  0.   0.62]\n",
      "F1-score: [0.46 0.   0.2 ]\n",
      "Accuracy: 0.32\n",
      "Brier score:0.2608033466946974\n",
      "Brier climat:0.19550347222222222\n",
      "Brier skill score:-0.3340087709452597\n",
      "Recall: [1.   0.   0.12]\n",
      "Precision: [0.3  0.   0.62]\n",
      "F1-score: [0.46 0.   0.2 ]\n",
      "Accuracy: 0.32\n",
      "Brier score:0.24148171985786665\n",
      "Brier climat:0.22347222222222224\n",
      "Brier skill score:-0.08058942385123657\n",
      "Recall: [1.   0.   0.12]\n",
      "Precision: [0.3  0.   0.62]\n",
      "F1-score: [0.46 0.   0.2 ]\n",
      "Accuracy: 0.32\n",
      "Brier score:0.23082000182708431\n",
      "Brier climat:0.22265625\n",
      "Brier skill score:-0.0366652713637472\n",
      "******** 20\n",
      "validation years [2001, 2002]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "{0: 2.0, 1: 1.0, 2: 1.0}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 60)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 183       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183\n",
      "Trainable params: 183\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.6483 - accuracy: 0.4481WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "28/28 [==============================] - 4s 108ms/step - loss: 1.6483 - accuracy: 0.4481 - val_loss: 1.2068 - val_accuracy: 0.4766\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.5549 - accuracy: 0.4777 - val_loss: 1.1932 - val_accuracy: 0.5156\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4943 - accuracy: 0.5028 - val_loss: 1.1909 - val_accuracy: 0.5078\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.4572 - accuracy: 0.5167 - val_loss: 1.1875 - val_accuracy: 0.5078\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.4319 - accuracy: 0.5162 - val_loss: 1.1916 - val_accuracy: 0.4844\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4147 - accuracy: 0.5106 - val_loss: 1.1981 - val_accuracy: 0.4844\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3994 - accuracy: 0.5050 - val_loss: 1.1980 - val_accuracy: 0.4766\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3860 - accuracy: 0.4978 - val_loss: 1.1941 - val_accuracy: 0.4922\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3756 - accuracy: 0.4961 - val_loss: 1.1916 - val_accuracy: 0.4922\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3663 - accuracy: 0.4905 - val_loss: 1.1922 - val_accuracy: 0.4844\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3562 - accuracy: 0.4955 - val_loss: 1.1917 - val_accuracy: 0.4766\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3490 - accuracy: 0.4922 - val_loss: 1.1858 - val_accuracy: 0.4844\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.3409 - accuracy: 0.4939 - val_loss: 1.1790 - val_accuracy: 0.4844\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3336 - accuracy: 0.4939 - val_loss: 1.1790 - val_accuracy: 0.4766\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3260 - accuracy: 0.4950 - val_loss: 1.1777 - val_accuracy: 0.4844\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3210 - accuracy: 0.4933 - val_loss: 1.1785 - val_accuracy: 0.4844\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3148 - accuracy: 0.4939 - val_loss: 1.1752 - val_accuracy: 0.4844\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3098 - accuracy: 0.4900 - val_loss: 1.1705 - val_accuracy: 0.4844\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3044 - accuracy: 0.4927 - val_loss: 1.1737 - val_accuracy: 0.4766\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2996 - accuracy: 0.4911 - val_loss: 1.1703 - val_accuracy: 0.4766\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2950 - accuracy: 0.4939 - val_loss: 1.1681 - val_accuracy: 0.4844\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2903 - accuracy: 0.4866 - val_loss: 1.1673 - val_accuracy: 0.4844\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2865 - accuracy: 0.4905 - val_loss: 1.1623 - val_accuracy: 0.4844\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2819 - accuracy: 0.4888 - val_loss: 1.1630 - val_accuracy: 0.4844\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2793 - accuracy: 0.4916 - val_loss: 1.1601 - val_accuracy: 0.4844\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2740 - accuracy: 0.4883 - val_loss: 1.1584 - val_accuracy: 0.4844\n",
      "Epoch 27/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2711 - accuracy: 0.4894 - val_loss: 1.1571 - val_accuracy: 0.4844\n",
      "Epoch 28/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2675 - accuracy: 0.4866 - val_loss: 1.1580 - val_accuracy: 0.4844\n",
      "Epoch 29/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2640 - accuracy: 0.4877 - val_loss: 1.1562 - val_accuracy: 0.4844\n",
      "Epoch 30/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2610 - accuracy: 0.4883 - val_loss: 1.1502 - val_accuracy: 0.4844\n",
      "Epoch 31/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2592 - accuracy: 0.4900 - val_loss: 1.1485 - val_accuracy: 0.4844\n",
      "Epoch 32/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2560 - accuracy: 0.4877 - val_loss: 1.1464 - val_accuracy: 0.4922\n",
      "Epoch 33/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2539 - accuracy: 0.4877 - val_loss: 1.1455 - val_accuracy: 0.4844\n",
      "Epoch 34/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2512 - accuracy: 0.4877 - val_loss: 1.1444 - val_accuracy: 0.4844\n",
      "Epoch 35/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2484 - accuracy: 0.4877 - val_loss: 1.1433 - val_accuracy: 0.4844\n",
      "Epoch 36/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2458 - accuracy: 0.4872 - val_loss: 1.1412 - val_accuracy: 0.4844\n",
      "Epoch 37/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2440 - accuracy: 0.4883 - val_loss: 1.1370 - val_accuracy: 0.4844\n",
      "Epoch 38/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2417 - accuracy: 0.4860 - val_loss: 1.1395 - val_accuracy: 0.4922\n",
      "Epoch 39/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2392 - accuracy: 0.4866 - val_loss: 1.1377 - val_accuracy: 0.4844\n",
      "Epoch 40/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2378 - accuracy: 0.4849 - val_loss: 1.1335 - val_accuracy: 0.4844\n",
      "Epoch 41/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2357 - accuracy: 0.4866 - val_loss: 1.1357 - val_accuracy: 0.4844\n",
      "Epoch 42/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2346 - accuracy: 0.4849 - val_loss: 1.1309 - val_accuracy: 0.4766\n",
      "Epoch 43/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2326 - accuracy: 0.4866 - val_loss: 1.1349 - val_accuracy: 0.4844\n",
      "Epoch 44/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2302 - accuracy: 0.4866 - val_loss: 1.1344 - val_accuracy: 0.4844\n",
      "Epoch 45/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2284 - accuracy: 0.4849 - val_loss: 1.1304 - val_accuracy: 0.4844\n",
      "Epoch 46/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2272 - accuracy: 0.4872 - val_loss: 1.1300 - val_accuracy: 0.4844\n",
      "Epoch 47/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2255 - accuracy: 0.4877 - val_loss: 1.1271 - val_accuracy: 0.4844\n",
      "Epoch 48/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2249 - accuracy: 0.4883 - val_loss: 1.1262 - val_accuracy: 0.4922\n",
      "Epoch 49/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2237 - accuracy: 0.4872 - val_loss: 1.1271 - val_accuracy: 0.4844\n",
      "Epoch 50/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2207 - accuracy: 0.4872 - val_loss: 1.1224 - val_accuracy: 0.4844\n",
      "Epoch 51/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2204 - accuracy: 0.4860 - val_loss: 1.1265 - val_accuracy: 0.4844\n",
      "Epoch 52/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2191 - accuracy: 0.4872 - val_loss: 1.1244 - val_accuracy: 0.4844\n",
      "Epoch 53/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2178 - accuracy: 0.4860 - val_loss: 1.1242 - val_accuracy: 0.4844\n",
      "Epoch 54/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2161 - accuracy: 0.4872 - val_loss: 1.1234 - val_accuracy: 0.4844\n",
      "Epoch 55/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2151 - accuracy: 0.4872 - val_loss: 1.1229 - val_accuracy: 0.4922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2146 - accuracy: 0.4849 - val_loss: 1.1241 - val_accuracy: 0.4844\n",
      "Epoch 57/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2130 - accuracy: 0.4872 - val_loss: 1.1211 - val_accuracy: 0.4844\n",
      "Epoch 58/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2128 - accuracy: 0.4883 - val_loss: 1.1198 - val_accuracy: 0.4922\n",
      "Epoch 59/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2113 - accuracy: 0.4866 - val_loss: 1.1196 - val_accuracy: 0.4844\n",
      "Epoch 60/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2097 - accuracy: 0.4849 - val_loss: 1.1200 - val_accuracy: 0.4844\n",
      "Epoch 61/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2094 - accuracy: 0.4877 - val_loss: 1.1147 - val_accuracy: 0.4844\n",
      "Epoch 62/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2077 - accuracy: 0.4860 - val_loss: 1.1200 - val_accuracy: 0.4844\n",
      "Epoch 63/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2062 - accuracy: 0.4883 - val_loss: 1.1171 - val_accuracy: 0.4844\n",
      "Epoch 64/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2055 - accuracy: 0.4855 - val_loss: 1.1151 - val_accuracy: 0.4922\n",
      "Epoch 65/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2044 - accuracy: 0.4855 - val_loss: 1.1168 - val_accuracy: 0.4844\n",
      "Epoch 66/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2041 - accuracy: 0.4883 - val_loss: 1.1141 - val_accuracy: 0.4922\n",
      "Epoch 67/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2028 - accuracy: 0.4860 - val_loss: 1.1122 - val_accuracy: 0.4844\n",
      "Epoch 68/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2016 - accuracy: 0.4860 - val_loss: 1.1164 - val_accuracy: 0.4844\n",
      "Epoch 69/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2024 - accuracy: 0.4866 - val_loss: 1.1127 - val_accuracy: 0.4844\n",
      "Epoch 70/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2012 - accuracy: 0.4872 - val_loss: 1.1123 - val_accuracy: 0.4844\n",
      "Epoch 71/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2004 - accuracy: 0.4855 - val_loss: 1.1134 - val_accuracy: 0.4922\n",
      "Epoch 72/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1997 - accuracy: 0.4866 - val_loss: 1.1140 - val_accuracy: 0.4844\n",
      "Epoch 73/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1988 - accuracy: 0.4866 - val_loss: 1.1140 - val_accuracy: 0.4844\n",
      "Epoch 74/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1993 - accuracy: 0.4872 - val_loss: 1.1152 - val_accuracy: 0.4844\n",
      "Epoch 75/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1982 - accuracy: 0.4860 - val_loss: 1.1132 - val_accuracy: 0.4844\n",
      "Epoch 76/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1979 - accuracy: 0.4855 - val_loss: 1.1136 - val_accuracy: 0.4844\n",
      "Epoch 77/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1977 - accuracy: 0.4894 - val_loss: 1.1102 - val_accuracy: 0.4844\n",
      "Epoch 78/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1979 - accuracy: 0.4877 - val_loss: 1.1137 - val_accuracy: 0.4609\n",
      "Epoch 79/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1972 - accuracy: 0.4872 - val_loss: 1.1121 - val_accuracy: 0.4844\n",
      "Epoch 80/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1967 - accuracy: 0.4872 - val_loss: 1.1137 - val_accuracy: 0.4844\n",
      "Epoch 81/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1966 - accuracy: 0.4860 - val_loss: 1.1131 - val_accuracy: 0.4766\n",
      "Epoch 82/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1961 - accuracy: 0.4849 - val_loss: 1.1173 - val_accuracy: 0.4609\n",
      "Epoch 83/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1954 - accuracy: 0.4860 - val_loss: 1.1142 - val_accuracy: 0.4766\n",
      "Epoch 84/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1964 - accuracy: 0.4866 - val_loss: 1.1102 - val_accuracy: 0.4844\n",
      "Epoch 85/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1950 - accuracy: 0.4860 - val_loss: 1.1139 - val_accuracy: 0.4531\n",
      "Epoch 86/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1947 - accuracy: 0.4860 - val_loss: 1.1110 - val_accuracy: 0.4844\n",
      "Epoch 87/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1948 - accuracy: 0.4849 - val_loss: 1.1124 - val_accuracy: 0.4766\n",
      "Epoch 88/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1938 - accuracy: 0.4860 - val_loss: 1.1108 - val_accuracy: 0.4609\n",
      "Epoch 89/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1937 - accuracy: 0.4855 - val_loss: 1.1149 - val_accuracy: 0.4531\n",
      "Epoch 90/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1940 - accuracy: 0.4860 - val_loss: 1.1143 - val_accuracy: 0.4766\n",
      "Epoch 91/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1932 - accuracy: 0.4855 - val_loss: 1.1116 - val_accuracy: 0.4531\n",
      "Epoch 92/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1933 - accuracy: 0.4883 - val_loss: 1.1076 - val_accuracy: 0.4844\n",
      "Epoch 93/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1927 - accuracy: 0.4866 - val_loss: 1.1109 - val_accuracy: 0.4609\n",
      "Epoch 94/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1923 - accuracy: 0.4855 - val_loss: 1.1103 - val_accuracy: 0.4766\n",
      "Epoch 95/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1921 - accuracy: 0.4844 - val_loss: 1.1082 - val_accuracy: 0.4766\n",
      "Epoch 96/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1919 - accuracy: 0.4877 - val_loss: 1.1100 - val_accuracy: 0.4844\n",
      "Epoch 97/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1918 - accuracy: 0.4855 - val_loss: 1.1078 - val_accuracy: 0.4531\n",
      "Epoch 98/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1919 - accuracy: 0.4866 - val_loss: 1.1112 - val_accuracy: 0.4688\n",
      "Epoch 99/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1917 - accuracy: 0.4872 - val_loss: 1.1088 - val_accuracy: 0.4844\n",
      "Epoch 100/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1918 - accuracy: 0.4860 - val_loss: 1.1074 - val_accuracy: 0.4766\n",
      "Epoch 101/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1914 - accuracy: 0.4866 - val_loss: 1.1096 - val_accuracy: 0.4688\n",
      "Epoch 102/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1911 - accuracy: 0.4860 - val_loss: 1.1060 - val_accuracy: 0.4609\n",
      "Epoch 103/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1910 - accuracy: 0.4866 - val_loss: 1.1069 - val_accuracy: 0.4609\n",
      "Epoch 104/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1908 - accuracy: 0.4872 - val_loss: 1.1071 - val_accuracy: 0.4609\n",
      "Epoch 105/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1907 - accuracy: 0.4872 - val_loss: 1.1107 - val_accuracy: 0.4531\n",
      "Epoch 106/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1903 - accuracy: 0.4872 - val_loss: 1.1074 - val_accuracy: 0.4688\n",
      "Epoch 107/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1901 - accuracy: 0.4866 - val_loss: 1.1047 - val_accuracy: 0.4609\n",
      "Epoch 108/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1900 - accuracy: 0.4855 - val_loss: 1.1078 - val_accuracy: 0.4766\n",
      "Epoch 109/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1902 - accuracy: 0.4883 - val_loss: 1.1076 - val_accuracy: 0.4609\n",
      "Epoch 110/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1906 - accuracy: 0.4872 - val_loss: 1.1069 - val_accuracy: 0.4766\n",
      "Epoch 111/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1903 - accuracy: 0.4883 - val_loss: 1.1064 - val_accuracy: 0.4609\n",
      "Epoch 112/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1894 - accuracy: 0.4877 - val_loss: 1.1080 - val_accuracy: 0.4609\n",
      "Epoch 113/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1897 - accuracy: 0.4860 - val_loss: 1.1037 - val_accuracy: 0.4609\n",
      "Epoch 114/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1899 - accuracy: 0.4872 - val_loss: 1.1057 - val_accuracy: 0.4766\n",
      "Epoch 115/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1901 - accuracy: 0.4888 - val_loss: 1.1066 - val_accuracy: 0.4609\n",
      "Epoch 116/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1901 - accuracy: 0.4860 - val_loss: 1.1052 - val_accuracy: 0.4609\n",
      "Epoch 117/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1899 - accuracy: 0.4866 - val_loss: 1.1062 - val_accuracy: 0.4609\n",
      "Epoch 118/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1897 - accuracy: 0.4877 - val_loss: 1.1059 - val_accuracy: 0.4609\n",
      "Epoch 119/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1891 - accuracy: 0.4883 - val_loss: 1.1065 - val_accuracy: 0.4609\n",
      "Epoch 120/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1889 - accuracy: 0.4872 - val_loss: 1.1076 - val_accuracy: 0.4609\n",
      "Epoch 121/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1885 - accuracy: 0.4877 - val_loss: 1.1063 - val_accuracy: 0.4609\n",
      "Epoch 122/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1897 - accuracy: 0.4849 - val_loss: 1.1049 - val_accuracy: 0.4609\n",
      "Epoch 123/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1895 - accuracy: 0.4872 - val_loss: 1.1064 - val_accuracy: 0.4609\n",
      "Epoch 124/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1891 - accuracy: 0.4877 - val_loss: 1.1061 - val_accuracy: 0.4609\n",
      "Epoch 125/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1898 - accuracy: 0.4877 - val_loss: 1.1030 - val_accuracy: 0.4609\n",
      "Epoch 126/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1892 - accuracy: 0.4860 - val_loss: 1.1062 - val_accuracy: 0.4609\n",
      "Epoch 127/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1891 - accuracy: 0.4872 - val_loss: 1.1041 - val_accuracy: 0.4609\n",
      "Epoch 128/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1880 - accuracy: 0.4883 - val_loss: 1.1057 - val_accuracy: 0.4609\n",
      "Epoch 129/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1884 - accuracy: 0.4872 - val_loss: 1.1052 - val_accuracy: 0.4609\n",
      "Epoch 130/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1888 - accuracy: 0.4883 - val_loss: 1.1039 - val_accuracy: 0.4609\n",
      "Epoch 131/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1888 - accuracy: 0.4866 - val_loss: 1.1064 - val_accuracy: 0.4453\n",
      "Epoch 132/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1882 - accuracy: 0.4872 - val_loss: 1.1082 - val_accuracy: 0.4609\n",
      "Epoch 133/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1881 - accuracy: 0.4872 - val_loss: 1.1037 - val_accuracy: 0.4688\n",
      "Epoch 134/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1879 - accuracy: 0.4877 - val_loss: 1.1035 - val_accuracy: 0.4609\n",
      "Epoch 135/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1884 - accuracy: 0.4883 - val_loss: 1.1057 - val_accuracy: 0.4609\n",
      "Epoch 136/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1878 - accuracy: 0.4860 - val_loss: 1.1053 - val_accuracy: 0.4609\n",
      "Epoch 137/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1886 - accuracy: 0.4872 - val_loss: 1.1038 - val_accuracy: 0.4609\n",
      "Epoch 138/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1876 - accuracy: 0.4872 - val_loss: 1.1060 - val_accuracy: 0.4609\n",
      "Epoch 139/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1877 - accuracy: 0.4860 - val_loss: 1.1040 - val_accuracy: 0.4609\n",
      "Epoch 140/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1881 - accuracy: 0.4866 - val_loss: 1.1026 - val_accuracy: 0.4688\n",
      "Epoch 141/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1877 - accuracy: 0.4877 - val_loss: 1.1053 - val_accuracy: 0.4609\n",
      "Epoch 142/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1875 - accuracy: 0.4872 - val_loss: 1.1041 - val_accuracy: 0.4688\n",
      "Epoch 143/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1879 - accuracy: 0.4872 - val_loss: 1.1046 - val_accuracy: 0.4609\n",
      "Epoch 144/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1870 - accuracy: 0.4866 - val_loss: 1.1032 - val_accuracy: 0.4688\n",
      "Epoch 145/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1875 - accuracy: 0.4860 - val_loss: 1.1051 - val_accuracy: 0.4609\n",
      "Epoch 146/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1874 - accuracy: 0.4872 - val_loss: 1.1053 - val_accuracy: 0.4688\n",
      "Epoch 147/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1874 - accuracy: 0.4883 - val_loss: 1.1037 - val_accuracy: 0.4609\n",
      "Epoch 148/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1875 - accuracy: 0.4866 - val_loss: 1.1078 - val_accuracy: 0.4531\n",
      "Epoch 149/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1873 - accuracy: 0.4866 - val_loss: 1.1025 - val_accuracy: 0.4609\n",
      "Epoch 150/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1867 - accuracy: 0.4872 - val_loss: 1.1045 - val_accuracy: 0.4688\n",
      "Epoch 151/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1871 - accuracy: 0.4866 - val_loss: 1.1047 - val_accuracy: 0.4531\n",
      "Epoch 152/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1873 - accuracy: 0.4866 - val_loss: 1.1028 - val_accuracy: 0.4531\n",
      "Epoch 153/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1879 - accuracy: 0.4883 - val_loss: 1.1055 - val_accuracy: 0.4531\n",
      "Epoch 154/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1880 - accuracy: 0.4866 - val_loss: 1.1019 - val_accuracy: 0.4531\n",
      "Epoch 155/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1873 - accuracy: 0.4866 - val_loss: 1.1034 - val_accuracy: 0.4688\n",
      "Epoch 156/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1869 - accuracy: 0.4860 - val_loss: 1.1056 - val_accuracy: 0.4609\n",
      "Epoch 157/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1872 - accuracy: 0.4866 - val_loss: 1.1049 - val_accuracy: 0.4531\n",
      "Epoch 158/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1880 - accuracy: 0.4872 - val_loss: 1.1033 - val_accuracy: 0.4609\n",
      "Epoch 159/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1873 - accuracy: 0.4866 - val_loss: 1.1067 - val_accuracy: 0.4453\n",
      "Epoch 160/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1865 - accuracy: 0.4866 - val_loss: 1.1040 - val_accuracy: 0.4609\n",
      "Epoch 161/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1873 - accuracy: 0.4866 - val_loss: 1.1015 - val_accuracy: 0.4531\n",
      "Epoch 162/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1875 - accuracy: 0.4855 - val_loss: 1.1072 - val_accuracy: 0.4453\n",
      "Epoch 163/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1880 - accuracy: 0.4877 - val_loss: 1.1031 - val_accuracy: 0.4609\n",
      "Epoch 164/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1875 - accuracy: 0.4866 - val_loss: 1.1022 - val_accuracy: 0.4531\n",
      "Epoch 165/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1868 - accuracy: 0.4888 - val_loss: 1.1025 - val_accuracy: 0.4453\n",
      "Epoch 166/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1867 - accuracy: 0.4872 - val_loss: 1.1052 - val_accuracy: 0.4531\n",
      "Epoch 167/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1872 - accuracy: 0.4866 - val_loss: 1.1042 - val_accuracy: 0.4453\n",
      "Epoch 168/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1875 - accuracy: 0.4872 - val_loss: 1.1080 - val_accuracy: 0.4609\n",
      "Epoch 169/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1871 - accuracy: 0.4872 - val_loss: 1.1024 - val_accuracy: 0.4688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1861 - accuracy: 0.4888 - val_loss: 1.1052 - val_accuracy: 0.4609\n",
      "Epoch 171/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1873 - accuracy: 0.4877 - val_loss: 1.1075 - val_accuracy: 0.4453\n",
      "Epoch 172/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1865 - accuracy: 0.4866 - val_loss: 1.1041 - val_accuracy: 0.4609\n",
      "Epoch 173/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1865 - accuracy: 0.4872 - val_loss: 1.1051 - val_accuracy: 0.4531\n",
      "Epoch 174/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1873 - accuracy: 0.4860 - val_loss: 1.1036 - val_accuracy: 0.4688\n",
      "Epoch 175/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1874 - accuracy: 0.4877 - val_loss: 1.1033 - val_accuracy: 0.4453\n",
      "Epoch 176/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1866 - accuracy: 0.4877 - val_loss: 1.1035 - val_accuracy: 0.4531\n",
      "Epoch 177/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1864 - accuracy: 0.4860 - val_loss: 1.1057 - val_accuracy: 0.4609\n",
      "Epoch 178/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1865 - accuracy: 0.4883 - val_loss: 1.1022 - val_accuracy: 0.4688\n",
      "Epoch 179/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1862 - accuracy: 0.4860 - val_loss: 1.1046 - val_accuracy: 0.4531\n",
      "Epoch 180/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1866 - accuracy: 0.4872 - val_loss: 1.1056 - val_accuracy: 0.4609\n",
      "Epoch 181/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1861 - accuracy: 0.4872 - val_loss: 1.1041 - val_accuracy: 0.4688\n",
      "Epoch 182/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1870 - accuracy: 0.4860 - val_loss: 1.1025 - val_accuracy: 0.4531\n",
      "Epoch 183/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1872 - accuracy: 0.4883 - val_loss: 1.1062 - val_accuracy: 0.4531\n",
      "Epoch 184/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1880 - accuracy: 0.4866 - val_loss: 1.1056 - val_accuracy: 0.4453\n",
      "Epoch 185/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1866 - accuracy: 0.4872 - val_loss: 1.1040 - val_accuracy: 0.4609\n",
      "Epoch 186/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1872 - accuracy: 0.4888 - val_loss: 1.1040 - val_accuracy: 0.4688\n",
      "Epoch 187/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1869 - accuracy: 0.4894 - val_loss: 1.1042 - val_accuracy: 0.4531\n",
      "Epoch 188/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1863 - accuracy: 0.4866 - val_loss: 1.1052 - val_accuracy: 0.4609\n",
      "Epoch 189/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1862 - accuracy: 0.4866 - val_loss: 1.1037 - val_accuracy: 0.4531\n",
      "Epoch 190/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1869 - accuracy: 0.4888 - val_loss: 1.1010 - val_accuracy: 0.4688\n",
      "Epoch 191/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1861 - accuracy: 0.4872 - val_loss: 1.1016 - val_accuracy: 0.4531\n",
      "Epoch 192/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1867 - accuracy: 0.4872 - val_loss: 1.1069 - val_accuracy: 0.4531\n",
      "Epoch 193/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1864 - accuracy: 0.4872 - val_loss: 1.1005 - val_accuracy: 0.4688\n",
      "Epoch 194/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1861 - accuracy: 0.4866 - val_loss: 1.1070 - val_accuracy: 0.4609\n",
      "Epoch 195/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1864 - accuracy: 0.4860 - val_loss: 1.1011 - val_accuracy: 0.4609\n",
      "Epoch 196/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1872 - accuracy: 0.4866 - val_loss: 1.1039 - val_accuracy: 0.4609\n",
      "Epoch 197/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1864 - accuracy: 0.4888 - val_loss: 1.1013 - val_accuracy: 0.4531\n",
      "Epoch 198/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1866 - accuracy: 0.4872 - val_loss: 1.1016 - val_accuracy: 0.4609\n",
      "Epoch 199/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1866 - accuracy: 0.4866 - val_loss: 1.1021 - val_accuracy: 0.4688\n",
      "Epoch 200/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1874 - accuracy: 0.4872 - val_loss: 1.1040 - val_accuracy: 0.4453\n",
      "Epoch 201/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1858 - accuracy: 0.4888 - val_loss: 1.1042 - val_accuracy: 0.4531\n",
      "Epoch 202/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1857 - accuracy: 0.4872 - val_loss: 1.1036 - val_accuracy: 0.4453\n",
      "Epoch 203/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1863 - accuracy: 0.4860 - val_loss: 1.1018 - val_accuracy: 0.4688\n",
      "Epoch 204/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1864 - accuracy: 0.4872 - val_loss: 1.1047 - val_accuracy: 0.4688\n",
      "Epoch 205/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1864 - accuracy: 0.4877 - val_loss: 1.1033 - val_accuracy: 0.4688\n",
      "Epoch 206/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1864 - accuracy: 0.4860 - val_loss: 1.1033 - val_accuracy: 0.4609\n",
      "Epoch 207/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1859 - accuracy: 0.4883 - val_loss: 1.1019 - val_accuracy: 0.4688\n",
      "Epoch 208/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1861 - accuracy: 0.4872 - val_loss: 1.1018 - val_accuracy: 0.4688\n",
      "Epoch 209/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1861 - accuracy: 0.4872 - val_loss: 1.1015 - val_accuracy: 0.4688\n",
      "Epoch 210/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1870 - accuracy: 0.4888 - val_loss: 1.1035 - val_accuracy: 0.4609\n",
      "Epoch 211/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1874 - accuracy: 0.4888 - val_loss: 1.1029 - val_accuracy: 0.4531\n",
      "Epoch 212/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1864 - accuracy: 0.4877 - val_loss: 1.1021 - val_accuracy: 0.4688\n",
      "Epoch 213/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1857 - accuracy: 0.4860 - val_loss: 1.1029 - val_accuracy: 0.4688\n",
      "Epoch 214/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1859 - accuracy: 0.4883 - val_loss: 1.1024 - val_accuracy: 0.4688\n",
      "Epoch 215/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1861 - accuracy: 0.4877 - val_loss: 1.1025 - val_accuracy: 0.4609\n",
      "Epoch 216/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1860 - accuracy: 0.4888 - val_loss: 1.1073 - val_accuracy: 0.4609\n",
      "Epoch 217/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1861 - accuracy: 0.4860 - val_loss: 1.1035 - val_accuracy: 0.4688\n",
      "Epoch 218/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1858 - accuracy: 0.4860 - val_loss: 1.1023 - val_accuracy: 0.4609\n",
      "Epoch 219/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1858 - accuracy: 0.4866 - val_loss: 1.1021 - val_accuracy: 0.4688\n",
      "Epoch 220/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1863 - accuracy: 0.4860 - val_loss: 1.1063 - val_accuracy: 0.4609\n",
      "Epoch 221/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1862 - accuracy: 0.4888 - val_loss: 1.1010 - val_accuracy: 0.4531\n",
      "Epoch 222/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1855 - accuracy: 0.4872 - val_loss: 1.1033 - val_accuracy: 0.4688\n",
      "Epoch 223/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1877 - accuracy: 0.4883 - val_loss: 1.1031 - val_accuracy: 0.4609\n",
      "Epoch 224/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1867 - accuracy: 0.4888 - val_loss: 1.1008 - val_accuracy: 0.4609\n",
      "Epoch 225/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1856 - accuracy: 0.4883 - val_loss: 1.1032 - val_accuracy: 0.4609\n",
      "Epoch 226/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1859 - accuracy: 0.4877 - val_loss: 1.1007 - val_accuracy: 0.4688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1866 - accuracy: 0.4872 - val_loss: 1.1028 - val_accuracy: 0.4531\n",
      "Epoch 228/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1852 - accuracy: 0.4860 - val_loss: 1.1041 - val_accuracy: 0.4531\n",
      "Epoch 229/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1866 - accuracy: 0.4888 - val_loss: 1.0996 - val_accuracy: 0.4531\n",
      "Epoch 230/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1866 - accuracy: 0.4883 - val_loss: 1.1033 - val_accuracy: 0.4609\n",
      "Epoch 231/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1858 - accuracy: 0.4872 - val_loss: 1.1011 - val_accuracy: 0.4609\n",
      "Epoch 232/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1856 - accuracy: 0.4888 - val_loss: 1.1028 - val_accuracy: 0.4688\n",
      "Epoch 233/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1857 - accuracy: 0.4894 - val_loss: 1.1031 - val_accuracy: 0.4531\n",
      "Epoch 234/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1873 - accuracy: 0.4872 - val_loss: 1.1016 - val_accuracy: 0.4531\n",
      "Epoch 235/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1857 - accuracy: 0.4877 - val_loss: 1.1040 - val_accuracy: 0.4453\n",
      "Epoch 236/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1868 - accuracy: 0.4877 - val_loss: 1.1032 - val_accuracy: 0.4688\n",
      "Epoch 237/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1857 - accuracy: 0.4888 - val_loss: 1.1025 - val_accuracy: 0.4688\n",
      "Epoch 238/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1861 - accuracy: 0.4872 - val_loss: 1.1033 - val_accuracy: 0.4531\n",
      "Epoch 239/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1861 - accuracy: 0.4877 - val_loss: 1.1021 - val_accuracy: 0.4531\n",
      "Epoch 240/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1858 - accuracy: 0.4866 - val_loss: 1.1033 - val_accuracy: 0.4688\n",
      "Epoch 241/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1861 - accuracy: 0.4877 - val_loss: 1.1028 - val_accuracy: 0.4688\n",
      "Epoch 242/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1858 - accuracy: 0.4877 - val_loss: 1.1047 - val_accuracy: 0.4609\n",
      "Epoch 243/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1863 - accuracy: 0.4877 - val_loss: 1.1021 - val_accuracy: 0.4531\n",
      "Epoch 244/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1860 - accuracy: 0.4872 - val_loss: 1.1031 - val_accuracy: 0.4688\n",
      "Epoch 245/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1861 - accuracy: 0.4872 - val_loss: 1.1025 - val_accuracy: 0.4453\n",
      "Epoch 246/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1859 - accuracy: 0.4872 - val_loss: 1.1028 - val_accuracy: 0.4531\n",
      "Epoch 247/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1857 - accuracy: 0.4860 - val_loss: 1.1022 - val_accuracy: 0.4688\n",
      "Epoch 248/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1873 - accuracy: 0.4866 - val_loss: 1.1009 - val_accuracy: 0.4609\n",
      "Epoch 249/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1863 - accuracy: 0.4894 - val_loss: 1.1028 - val_accuracy: 0.4688\n",
      "Epoch 250/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1866 - accuracy: 0.4883 - val_loss: 1.1036 - val_accuracy: 0.4609\n",
      "Epoch 251/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1853 - accuracy: 0.4883 - val_loss: 1.1026 - val_accuracy: 0.4688\n",
      "Epoch 252/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1862 - accuracy: 0.4877 - val_loss: 1.1003 - val_accuracy: 0.4688\n",
      "Epoch 253/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1855 - accuracy: 0.4866 - val_loss: 1.1028 - val_accuracy: 0.4688\n",
      "Epoch 254/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1853 - accuracy: 0.4888 - val_loss: 1.1007 - val_accuracy: 0.4609\n",
      "Epoch 255/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1856 - accuracy: 0.4883 - val_loss: 1.1020 - val_accuracy: 0.4609\n",
      "Epoch 256/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1854 - accuracy: 0.4872 - val_loss: 1.1022 - val_accuracy: 0.4453\n",
      "Epoch 257/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1854 - accuracy: 0.4877 - val_loss: 1.1035 - val_accuracy: 0.4688\n",
      "Epoch 258/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1855 - accuracy: 0.4872 - val_loss: 1.1035 - val_accuracy: 0.4688\n",
      "Epoch 259/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1857 - accuracy: 0.4883 - val_loss: 1.1011 - val_accuracy: 0.4688\n",
      "Epoch 260/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1856 - accuracy: 0.4866 - val_loss: 1.1035 - val_accuracy: 0.4688\n",
      "Epoch 261/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1857 - accuracy: 0.4877 - val_loss: 1.1035 - val_accuracy: 0.4453\n",
      "Epoch 262/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1859 - accuracy: 0.4872 - val_loss: 1.1025 - val_accuracy: 0.4609\n",
      "Epoch 263/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1856 - accuracy: 0.4877 - val_loss: 1.1016 - val_accuracy: 0.4688\n",
      "Epoch 264/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1863 - accuracy: 0.4883 - val_loss: 1.1037 - val_accuracy: 0.4688\n",
      "Epoch 265/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1859 - accuracy: 0.4888 - val_loss: 1.1020 - val_accuracy: 0.4688\n",
      "Epoch 266/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1862 - accuracy: 0.4894 - val_loss: 1.1035 - val_accuracy: 0.4688\n",
      "Epoch 267/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1856 - accuracy: 0.4900 - val_loss: 1.1025 - val_accuracy: 0.4688\n",
      "Epoch 268/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1856 - accuracy: 0.4866 - val_loss: 1.1043 - val_accuracy: 0.4688\n",
      "Epoch 269/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1857 - accuracy: 0.4877 - val_loss: 1.1012 - val_accuracy: 0.4688\n",
      "Epoch 270/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1852 - accuracy: 0.4883 - val_loss: 1.1012 - val_accuracy: 0.4531\n",
      "Epoch 271/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1856 - accuracy: 0.4900 - val_loss: 1.1009 - val_accuracy: 0.4688\n",
      "Epoch 272/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1853 - accuracy: 0.4888 - val_loss: 1.1033 - val_accuracy: 0.4531\n",
      "Epoch 273/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1853 - accuracy: 0.4883 - val_loss: 1.1032 - val_accuracy: 0.4688\n",
      "Epoch 274/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1857 - accuracy: 0.4888 - val_loss: 1.1056 - val_accuracy: 0.4609\n",
      "Epoch 275/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1856 - accuracy: 0.4872 - val_loss: 1.1052 - val_accuracy: 0.4531\n",
      "Epoch 276/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1855 - accuracy: 0.4872 - val_loss: 1.1021 - val_accuracy: 0.4688\n",
      "Epoch 277/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1854 - accuracy: 0.4877 - val_loss: 1.1030 - val_accuracy: 0.4531\n",
      "Epoch 278/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1850 - accuracy: 0.4872 - val_loss: 1.1004 - val_accuracy: 0.4688\n",
      "Epoch 279/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1856 - accuracy: 0.4888 - val_loss: 1.1021 - val_accuracy: 0.4531\n",
      "Epoch 280/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1856 - accuracy: 0.4866 - val_loss: 1.1025 - val_accuracy: 0.4688\n",
      "Epoch 281/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1866 - accuracy: 0.4866 - val_loss: 1.1021 - val_accuracy: 0.4688\n",
      "Epoch 282/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1851 - accuracy: 0.4877 - val_loss: 1.1035 - val_accuracy: 0.4531\n",
      "Epoch 283/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1883 - accuracy: 0.4872 - val_loss: 1.1003 - val_accuracy: 0.4531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 284/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1858 - accuracy: 0.4883 - val_loss: 1.0997 - val_accuracy: 0.4688\n",
      "Epoch 285/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1862 - accuracy: 0.4866 - val_loss: 1.1004 - val_accuracy: 0.4531\n",
      "Epoch 286/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1860 - accuracy: 0.4866 - val_loss: 1.1017 - val_accuracy: 0.4531\n",
      "Epoch 287/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1866 - accuracy: 0.4883 - val_loss: 1.0998 - val_accuracy: 0.4688\n",
      "Epoch 288/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1858 - accuracy: 0.4877 - val_loss: 1.1047 - val_accuracy: 0.4609\n",
      "Epoch 289/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1860 - accuracy: 0.4860 - val_loss: 1.1057 - val_accuracy: 0.4609\n",
      "Epoch 290/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1859 - accuracy: 0.4872 - val_loss: 1.1031 - val_accuracy: 0.4609\n",
      "Epoch 291/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1854 - accuracy: 0.4900 - val_loss: 1.1005 - val_accuracy: 0.4688\n",
      "Epoch 292/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1854 - accuracy: 0.4883 - val_loss: 1.1035 - val_accuracy: 0.4688\n",
      "Epoch 293/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1856 - accuracy: 0.4900 - val_loss: 1.0994 - val_accuracy: 0.4453\n",
      "Epoch 294/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1852 - accuracy: 0.4872 - val_loss: 1.1041 - val_accuracy: 0.4531\n",
      "Epoch 295/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1850 - accuracy: 0.4883 - val_loss: 1.0998 - val_accuracy: 0.4688\n",
      "Epoch 296/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1851 - accuracy: 0.4883 - val_loss: 1.1021 - val_accuracy: 0.4609\n",
      "Epoch 297/300\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1.1852 - accuracy: 0.4872 - val_loss: 1.1018 - val_accuracy: 0.4453\n",
      "Epoch 298/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1857 - accuracy: 0.4860 - val_loss: 1.1045 - val_accuracy: 0.4609\n",
      "Epoch 299/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1854 - accuracy: 0.4872 - val_loss: 1.1028 - val_accuracy: 0.4531\n",
      "Epoch 300/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1852 - accuracy: 0.4866 - val_loss: 1.1049 - val_accuracy: 0.4688\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "56/56 [==============================] - 0s 1ms/step\n",
      "Recall: [0.94 0.   0.56]\n",
      "Precision: [0.41 0.   0.7 ]\n",
      "F1-score: [0.57 0.   0.63]\n",
      "Accuracy: 0.49\n",
      "Brier score:0.20183563721316564\n",
      "Brier climat:0.2123933531746032\n",
      "Brier skill score:0.04970831621438898\n",
      "Recall: [0.94 0.   0.56]\n",
      "Precision: [0.41 0.   0.7 ]\n",
      "F1-score: [0.57 0.   0.63]\n",
      "Accuracy: 0.49\n",
      "Brier score:0.22341538235519728\n",
      "Brier climat:0.2229141865079365\n",
      "Brier skill score:-0.0022483802180213175\n",
      "Recall: [0.94 0.   0.56]\n",
      "Precision: [0.41 0.   0.7 ]\n",
      "F1-score: [0.57 0.   0.63]\n",
      "Accuracy: 0.49\n",
      "Brier score:0.15254785258399403\n",
      "Brier climat:0.2236235119047619\n",
      "Brier skill score:0.3178362539581169\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "Recall: [0.88 0.   0.53]\n",
      "Precision: [0.32 0.   0.8 ]\n",
      "F1-score: [0.47 0.   0.64]\n",
      "Accuracy: 0.47\n",
      "Brier score:0.21854234205076106\n",
      "Brier climat:0.17987847222222225\n",
      "Brier skill score:-0.21494439746393534\n",
      "Recall: [0.88 0.   0.53]\n",
      "Precision: [0.32 0.   0.8 ]\n",
      "F1-score: [0.47 0.   0.64]\n",
      "Accuracy: 0.47\n",
      "Brier score:0.19636918009725163\n",
      "Brier climat:0.19899305555555558\n",
      "Brier skill score:0.013185763950297247\n",
      "Recall: [0.88 0.   0.53]\n",
      "Precision: [0.32 0.   0.8 ]\n",
      "F1-score: [0.47 0.   0.64]\n",
      "Accuracy: 0.47\n",
      "Brier score:0.22954194764006094\n",
      "Brier climat:0.2559895833333333\n",
      "Brier skill score:0.10331528083638453\n",
      "******** 21\n",
      "validation years [2002, 2003]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "{0: 2.0, 1: 1.0, 2: 1.0}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 60)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 183       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183\n",
      "Trainable params: 183\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 1.7705 - accuracy: 0.3912WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "28/28 [==============================] - 4s 108ms/step - loss: 1.7676 - accuracy: 0.3929 - val_loss: 1.4598 - val_accuracy: 0.3203\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.6357 - accuracy: 0.4062 - val_loss: 1.4108 - val_accuracy: 0.2969\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.5496 - accuracy: 0.4342 - val_loss: 1.3686 - val_accuracy: 0.3047\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4930 - accuracy: 0.4526 - val_loss: 1.3371 - val_accuracy: 0.3438\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.4541 - accuracy: 0.4660 - val_loss: 1.3241 - val_accuracy: 0.3516\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4269 - accuracy: 0.4743 - val_loss: 1.3097 - val_accuracy: 0.3438\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4065 - accuracy: 0.4805 - val_loss: 1.2964 - val_accuracy: 0.3594\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3905 - accuracy: 0.4860 - val_loss: 1.2880 - val_accuracy: 0.3828\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3760 - accuracy: 0.4883 - val_loss: 1.2842 - val_accuracy: 0.3828\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3637 - accuracy: 0.4849 - val_loss: 1.2797 - val_accuracy: 0.3906\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3530 - accuracy: 0.4888 - val_loss: 1.2735 - val_accuracy: 0.3828\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3435 - accuracy: 0.4860 - val_loss: 1.2694 - val_accuracy: 0.3828\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3340 - accuracy: 0.4916 - val_loss: 1.2687 - val_accuracy: 0.3750\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3261 - accuracy: 0.4939 - val_loss: 1.2625 - val_accuracy: 0.3750\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3175 - accuracy: 0.4933 - val_loss: 1.2630 - val_accuracy: 0.3750\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3124 - accuracy: 0.4944 - val_loss: 1.2640 - val_accuracy: 0.3750\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3054 - accuracy: 0.4944 - val_loss: 1.2605 - val_accuracy: 0.3750\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2994 - accuracy: 0.4972 - val_loss: 1.2609 - val_accuracy: 0.3750\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2939 - accuracy: 0.4978 - val_loss: 1.2626 - val_accuracy: 0.3750\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2889 - accuracy: 0.4972 - val_loss: 1.2624 - val_accuracy: 0.3672\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2836 - accuracy: 0.4967 - val_loss: 1.2599 - val_accuracy: 0.3828\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2793 - accuracy: 0.4955 - val_loss: 1.2591 - val_accuracy: 0.3828\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2752 - accuracy: 0.4916 - val_loss: 1.2628 - val_accuracy: 0.3750\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2710 - accuracy: 0.5000 - val_loss: 1.2594 - val_accuracy: 0.3750\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2678 - accuracy: 0.4967 - val_loss: 1.2605 - val_accuracy: 0.3750\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2632 - accuracy: 0.4961 - val_loss: 1.2590 - val_accuracy: 0.3750\n",
      "Epoch 27/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2604 - accuracy: 0.4978 - val_loss: 1.2619 - val_accuracy: 0.3672\n",
      "Epoch 28/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2571 - accuracy: 0.4983 - val_loss: 1.2594 - val_accuracy: 0.3672\n",
      "Epoch 29/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2544 - accuracy: 0.4967 - val_loss: 1.2555 - val_accuracy: 0.3750\n",
      "Epoch 30/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2518 - accuracy: 0.4983 - val_loss: 1.2536 - val_accuracy: 0.3750\n",
      "Epoch 31/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2496 - accuracy: 0.4972 - val_loss: 1.2562 - val_accuracy: 0.3672\n",
      "Epoch 32/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2471 - accuracy: 0.4994 - val_loss: 1.2541 - val_accuracy: 0.3750\n",
      "Epoch 33/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2455 - accuracy: 0.4989 - val_loss: 1.2533 - val_accuracy: 0.3750\n",
      "Epoch 34/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2438 - accuracy: 0.4989 - val_loss: 1.2536 - val_accuracy: 0.3750\n",
      "Epoch 35/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2400 - accuracy: 0.4978 - val_loss: 1.2566 - val_accuracy: 0.3750\n",
      "Epoch 36/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2396 - accuracy: 0.5000 - val_loss: 1.2531 - val_accuracy: 0.3672\n",
      "Epoch 37/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2363 - accuracy: 0.4983 - val_loss: 1.2487 - val_accuracy: 0.3750\n",
      "Epoch 38/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2345 - accuracy: 0.4978 - val_loss: 1.2512 - val_accuracy: 0.3750\n",
      "Epoch 39/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2334 - accuracy: 0.4978 - val_loss: 1.2475 - val_accuracy: 0.3750\n",
      "Epoch 40/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2314 - accuracy: 0.4978 - val_loss: 1.2501 - val_accuracy: 0.3750\n",
      "Epoch 41/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2301 - accuracy: 0.4994 - val_loss: 1.2449 - val_accuracy: 0.3750\n",
      "Epoch 42/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2285 - accuracy: 0.4978 - val_loss: 1.2464 - val_accuracy: 0.3750\n",
      "Epoch 43/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2275 - accuracy: 0.4972 - val_loss: 1.2473 - val_accuracy: 0.3750\n",
      "Epoch 44/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2265 - accuracy: 0.4967 - val_loss: 1.2444 - val_accuracy: 0.3750\n",
      "Epoch 45/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2258 - accuracy: 0.4978 - val_loss: 1.2443 - val_accuracy: 0.3750\n",
      "Epoch 46/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2235 - accuracy: 0.4967 - val_loss: 1.2396 - val_accuracy: 0.3750\n",
      "Epoch 47/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2224 - accuracy: 0.4972 - val_loss: 1.2415 - val_accuracy: 0.3750\n",
      "Epoch 48/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2221 - accuracy: 0.4955 - val_loss: 1.2432 - val_accuracy: 0.3750\n",
      "Epoch 49/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2212 - accuracy: 0.4989 - val_loss: 1.2401 - val_accuracy: 0.3750\n",
      "Epoch 50/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2188 - accuracy: 0.4989 - val_loss: 1.2402 - val_accuracy: 0.3750\n",
      "Epoch 51/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2179 - accuracy: 0.4978 - val_loss: 1.2360 - val_accuracy: 0.3750\n",
      "Epoch 52/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2168 - accuracy: 0.4967 - val_loss: 1.2376 - val_accuracy: 0.3750\n",
      "Epoch 53/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2155 - accuracy: 0.4983 - val_loss: 1.2388 - val_accuracy: 0.3750\n",
      "Epoch 54/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2150 - accuracy: 0.4983 - val_loss: 1.2392 - val_accuracy: 0.3750\n",
      "Epoch 55/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2145 - accuracy: 0.4950 - val_loss: 1.2371 - val_accuracy: 0.3750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2132 - accuracy: 0.4989 - val_loss: 1.2357 - val_accuracy: 0.3828\n",
      "Epoch 57/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2125 - accuracy: 0.4972 - val_loss: 1.2397 - val_accuracy: 0.3750\n",
      "Epoch 58/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2109 - accuracy: 0.4994 - val_loss: 1.2353 - val_accuracy: 0.3750\n",
      "Epoch 59/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2111 - accuracy: 0.4972 - val_loss: 1.2408 - val_accuracy: 0.3750\n",
      "Epoch 60/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2101 - accuracy: 0.4972 - val_loss: 1.2353 - val_accuracy: 0.3750\n",
      "Epoch 61/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2093 - accuracy: 0.4961 - val_loss: 1.2372 - val_accuracy: 0.3828\n",
      "Epoch 62/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2081 - accuracy: 0.5000 - val_loss: 1.2358 - val_accuracy: 0.3750\n",
      "Epoch 63/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2073 - accuracy: 0.4967 - val_loss: 1.2325 - val_accuracy: 0.3750\n",
      "Epoch 64/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2065 - accuracy: 0.4978 - val_loss: 1.2367 - val_accuracy: 0.3672\n",
      "Epoch 65/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2065 - accuracy: 0.5000 - val_loss: 1.2359 - val_accuracy: 0.3750\n",
      "Epoch 66/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2053 - accuracy: 0.4972 - val_loss: 1.2341 - val_accuracy: 0.3750\n",
      "Epoch 67/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2050 - accuracy: 0.4967 - val_loss: 1.2323 - val_accuracy: 0.3750\n",
      "Epoch 68/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2035 - accuracy: 0.4978 - val_loss: 1.2322 - val_accuracy: 0.3750\n",
      "Epoch 69/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2031 - accuracy: 0.4967 - val_loss: 1.2322 - val_accuracy: 0.3672\n",
      "Epoch 70/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2029 - accuracy: 0.4978 - val_loss: 1.2315 - val_accuracy: 0.3672\n",
      "Epoch 71/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2024 - accuracy: 0.4961 - val_loss: 1.2340 - val_accuracy: 0.3672\n",
      "Epoch 72/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2022 - accuracy: 0.4972 - val_loss: 1.2317 - val_accuracy: 0.3672\n",
      "Epoch 73/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2013 - accuracy: 0.4972 - val_loss: 1.2332 - val_accuracy: 0.3672\n",
      "Epoch 74/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2009 - accuracy: 0.4961 - val_loss: 1.2321 - val_accuracy: 0.3672\n",
      "Epoch 75/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2009 - accuracy: 0.4972 - val_loss: 1.2312 - val_accuracy: 0.3672\n",
      "Epoch 76/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2005 - accuracy: 0.4978 - val_loss: 1.2287 - val_accuracy: 0.3750\n",
      "Epoch 77/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1994 - accuracy: 0.4972 - val_loss: 1.2305 - val_accuracy: 0.3672\n",
      "Epoch 78/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1991 - accuracy: 0.4989 - val_loss: 1.2299 - val_accuracy: 0.3750\n",
      "Epoch 79/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1987 - accuracy: 0.4972 - val_loss: 1.2249 - val_accuracy: 0.3750\n",
      "Epoch 80/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1981 - accuracy: 0.4961 - val_loss: 1.2314 - val_accuracy: 0.3672\n",
      "Epoch 81/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1973 - accuracy: 0.4961 - val_loss: 1.2318 - val_accuracy: 0.3672\n",
      "Epoch 82/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1975 - accuracy: 0.4955 - val_loss: 1.2279 - val_accuracy: 0.3594\n",
      "Epoch 83/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1973 - accuracy: 0.4967 - val_loss: 1.2290 - val_accuracy: 0.3594\n",
      "Epoch 84/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1968 - accuracy: 0.4944 - val_loss: 1.2259 - val_accuracy: 0.3672\n",
      "Epoch 85/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1964 - accuracy: 0.4972 - val_loss: 1.2306 - val_accuracy: 0.3516\n",
      "Epoch 86/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1957 - accuracy: 0.4950 - val_loss: 1.2306 - val_accuracy: 0.3359\n",
      "Epoch 87/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1948 - accuracy: 0.4955 - val_loss: 1.2284 - val_accuracy: 0.3438\n",
      "Epoch 88/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1954 - accuracy: 0.4950 - val_loss: 1.2250 - val_accuracy: 0.3672\n",
      "Epoch 89/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1953 - accuracy: 0.4967 - val_loss: 1.2332 - val_accuracy: 0.3438\n",
      "Epoch 90/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1947 - accuracy: 0.4950 - val_loss: 1.2260 - val_accuracy: 0.3438\n",
      "Epoch 91/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1941 - accuracy: 0.4950 - val_loss: 1.2273 - val_accuracy: 0.3438\n",
      "Epoch 92/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1941 - accuracy: 0.4939 - val_loss: 1.2243 - val_accuracy: 0.3672\n",
      "Epoch 93/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1937 - accuracy: 0.4961 - val_loss: 1.2268 - val_accuracy: 0.3359\n",
      "Epoch 94/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1941 - accuracy: 0.4933 - val_loss: 1.2266 - val_accuracy: 0.3281\n",
      "Epoch 95/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1931 - accuracy: 0.4955 - val_loss: 1.2222 - val_accuracy: 0.3359\n",
      "Epoch 96/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1929 - accuracy: 0.4933 - val_loss: 1.2243 - val_accuracy: 0.3438\n",
      "Epoch 97/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1930 - accuracy: 0.4927 - val_loss: 1.2221 - val_accuracy: 0.3438\n",
      "Epoch 98/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1925 - accuracy: 0.4933 - val_loss: 1.2233 - val_accuracy: 0.3438\n",
      "Epoch 99/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1923 - accuracy: 0.4950 - val_loss: 1.2237 - val_accuracy: 0.3359\n",
      "Epoch 100/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1932 - accuracy: 0.4950 - val_loss: 1.2285 - val_accuracy: 0.3359\n",
      "Epoch 101/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1923 - accuracy: 0.4933 - val_loss: 1.2237 - val_accuracy: 0.3359\n",
      "Epoch 102/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1919 - accuracy: 0.4950 - val_loss: 1.2201 - val_accuracy: 0.3516\n",
      "Epoch 103/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1914 - accuracy: 0.4950 - val_loss: 1.2242 - val_accuracy: 0.3281\n",
      "Epoch 104/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1920 - accuracy: 0.4922 - val_loss: 1.2217 - val_accuracy: 0.3281\n",
      "Epoch 105/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1916 - accuracy: 0.4939 - val_loss: 1.2226 - val_accuracy: 0.3359\n",
      "Epoch 106/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1919 - accuracy: 0.4922 - val_loss: 1.2202 - val_accuracy: 0.3438\n",
      "Epoch 107/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1918 - accuracy: 0.4933 - val_loss: 1.2193 - val_accuracy: 0.3359\n",
      "Epoch 108/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1914 - accuracy: 0.4927 - val_loss: 1.2170 - val_accuracy: 0.3359\n",
      "Epoch 109/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1912 - accuracy: 0.4922 - val_loss: 1.2200 - val_accuracy: 0.3359\n",
      "Epoch 110/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1918 - accuracy: 0.4927 - val_loss: 1.2171 - val_accuracy: 0.3438\n",
      "Epoch 111/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1907 - accuracy: 0.4944 - val_loss: 1.2167 - val_accuracy: 0.3359\n",
      "Epoch 112/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1905 - accuracy: 0.4933 - val_loss: 1.2166 - val_accuracy: 0.3359\n",
      "Epoch 113/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1911 - accuracy: 0.4939 - val_loss: 1.2173 - val_accuracy: 0.3359\n",
      "Epoch 114/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1903 - accuracy: 0.4927 - val_loss: 1.2172 - val_accuracy: 0.3359\n",
      "Epoch 115/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1910 - accuracy: 0.4927 - val_loss: 1.2157 - val_accuracy: 0.3359\n",
      "Epoch 116/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1905 - accuracy: 0.4933 - val_loss: 1.2149 - val_accuracy: 0.3359\n",
      "Epoch 117/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1903 - accuracy: 0.4944 - val_loss: 1.2130 - val_accuracy: 0.3438\n",
      "Epoch 118/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1910 - accuracy: 0.4933 - val_loss: 1.2117 - val_accuracy: 0.3359\n",
      "Epoch 119/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1901 - accuracy: 0.4927 - val_loss: 1.2132 - val_accuracy: 0.3516\n",
      "Epoch 120/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1900 - accuracy: 0.4916 - val_loss: 1.2120 - val_accuracy: 0.3438\n",
      "Epoch 121/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1906 - accuracy: 0.4933 - val_loss: 1.2166 - val_accuracy: 0.3359\n",
      "Epoch 122/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1910 - accuracy: 0.4933 - val_loss: 1.2169 - val_accuracy: 0.3516\n",
      "Epoch 123/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1897 - accuracy: 0.4939 - val_loss: 1.2172 - val_accuracy: 0.3359\n",
      "Epoch 124/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1901 - accuracy: 0.4922 - val_loss: 1.2105 - val_accuracy: 0.3516\n",
      "Epoch 125/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1894 - accuracy: 0.4933 - val_loss: 1.2174 - val_accuracy: 0.3438\n",
      "Epoch 126/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1896 - accuracy: 0.4927 - val_loss: 1.2131 - val_accuracy: 0.3438\n",
      "Epoch 127/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1892 - accuracy: 0.4933 - val_loss: 1.2170 - val_accuracy: 0.3281\n",
      "Epoch 128/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1888 - accuracy: 0.4922 - val_loss: 1.2153 - val_accuracy: 0.3359\n",
      "Epoch 129/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1895 - accuracy: 0.4961 - val_loss: 1.2105 - val_accuracy: 0.3438\n",
      "Epoch 130/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1891 - accuracy: 0.4950 - val_loss: 1.2130 - val_accuracy: 0.3359\n",
      "Epoch 131/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1890 - accuracy: 0.4916 - val_loss: 1.2121 - val_accuracy: 0.3516\n",
      "Epoch 132/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1896 - accuracy: 0.4944 - val_loss: 1.2122 - val_accuracy: 0.3359\n",
      "Epoch 133/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1908 - accuracy: 0.4950 - val_loss: 1.2156 - val_accuracy: 0.3359\n",
      "Epoch 134/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1891 - accuracy: 0.4939 - val_loss: 1.2141 - val_accuracy: 0.3359\n",
      "Epoch 135/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1887 - accuracy: 0.4933 - val_loss: 1.2110 - val_accuracy: 0.3516\n",
      "Epoch 136/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1884 - accuracy: 0.4927 - val_loss: 1.2068 - val_accuracy: 0.3438\n",
      "Epoch 137/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1891 - accuracy: 0.4927 - val_loss: 1.2128 - val_accuracy: 0.3438\n",
      "Epoch 138/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1889 - accuracy: 0.4927 - val_loss: 1.2127 - val_accuracy: 0.3438\n",
      "Epoch 139/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1889 - accuracy: 0.4955 - val_loss: 1.2075 - val_accuracy: 0.3438\n",
      "Epoch 140/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1887 - accuracy: 0.4950 - val_loss: 1.2152 - val_accuracy: 0.3359\n",
      "Epoch 141/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1886 - accuracy: 0.4939 - val_loss: 1.2107 - val_accuracy: 0.3438\n",
      "Epoch 142/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1888 - accuracy: 0.4933 - val_loss: 1.2115 - val_accuracy: 0.3438\n",
      "Epoch 143/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1889 - accuracy: 0.4950 - val_loss: 1.2113 - val_accuracy: 0.3359\n",
      "Epoch 144/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1887 - accuracy: 0.4955 - val_loss: 1.2054 - val_accuracy: 0.3516\n",
      "Epoch 145/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1882 - accuracy: 0.4955 - val_loss: 1.2096 - val_accuracy: 0.3438\n",
      "Epoch 146/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1883 - accuracy: 0.4955 - val_loss: 1.2058 - val_accuracy: 0.3516\n",
      "Epoch 147/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1875 - accuracy: 0.4950 - val_loss: 1.2103 - val_accuracy: 0.3438\n",
      "Epoch 148/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1880 - accuracy: 0.4950 - val_loss: 1.2066 - val_accuracy: 0.3516\n",
      "Epoch 149/300\n",
      "28/28 [==============================] - 1s 29ms/step - loss: 1.1881 - accuracy: 0.4944 - val_loss: 1.2070 - val_accuracy: 0.3359\n",
      "Epoch 150/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1883 - accuracy: 0.4939 - val_loss: 1.2084 - val_accuracy: 0.3438\n",
      "Epoch 151/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1882 - accuracy: 0.4955 - val_loss: 1.2132 - val_accuracy: 0.3438\n",
      "Epoch 152/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1879 - accuracy: 0.4944 - val_loss: 1.2083 - val_accuracy: 0.3516\n",
      "Epoch 153/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1877 - accuracy: 0.4939 - val_loss: 1.2100 - val_accuracy: 0.3438\n",
      "Epoch 154/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1881 - accuracy: 0.4939 - val_loss: 1.2078 - val_accuracy: 0.3438\n",
      "Epoch 155/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1876 - accuracy: 0.4961 - val_loss: 1.2112 - val_accuracy: 0.3516\n",
      "Epoch 156/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1878 - accuracy: 0.4933 - val_loss: 1.2057 - val_accuracy: 0.3438\n",
      "Epoch 157/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1873 - accuracy: 0.4939 - val_loss: 1.2076 - val_accuracy: 0.3516\n",
      "Epoch 158/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1883 - accuracy: 0.4967 - val_loss: 1.2084 - val_accuracy: 0.3438\n",
      "Epoch 159/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1887 - accuracy: 0.4950 - val_loss: 1.2068 - val_accuracy: 0.3438\n",
      "Epoch 160/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1879 - accuracy: 0.4955 - val_loss: 1.2102 - val_accuracy: 0.3438\n",
      "Epoch 161/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1874 - accuracy: 0.4950 - val_loss: 1.2102 - val_accuracy: 0.3516\n",
      "Epoch 162/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1872 - accuracy: 0.4950 - val_loss: 1.2113 - val_accuracy: 0.3516\n",
      "Epoch 163/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1873 - accuracy: 0.4961 - val_loss: 1.2044 - val_accuracy: 0.3516\n",
      "Epoch 164/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1867 - accuracy: 0.4933 - val_loss: 1.2058 - val_accuracy: 0.3438\n",
      "Epoch 165/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1874 - accuracy: 0.4950 - val_loss: 1.2050 - val_accuracy: 0.3516\n",
      "Epoch 166/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1872 - accuracy: 0.4944 - val_loss: 1.2052 - val_accuracy: 0.3438\n",
      "Epoch 167/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1870 - accuracy: 0.4955 - val_loss: 1.2011 - val_accuracy: 0.3438\n",
      "Epoch 168/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1872 - accuracy: 0.4961 - val_loss: 1.2038 - val_accuracy: 0.3516\n",
      "Epoch 169/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1882 - accuracy: 0.4950 - val_loss: 1.1981 - val_accuracy: 0.3438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1872 - accuracy: 0.4961 - val_loss: 1.2049 - val_accuracy: 0.3438\n",
      "Epoch 171/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1877 - accuracy: 0.4950 - val_loss: 1.2072 - val_accuracy: 0.3594\n",
      "Epoch 172/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1879 - accuracy: 0.4955 - val_loss: 1.2079 - val_accuracy: 0.3516\n",
      "Epoch 173/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1865 - accuracy: 0.4955 - val_loss: 1.2056 - val_accuracy: 0.3516\n",
      "Epoch 174/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1868 - accuracy: 0.4961 - val_loss: 1.2041 - val_accuracy: 0.3438\n",
      "Epoch 175/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1860 - accuracy: 0.4944 - val_loss: 1.2045 - val_accuracy: 0.3438\n",
      "Epoch 176/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1862 - accuracy: 0.4950 - val_loss: 1.2047 - val_accuracy: 0.3516\n",
      "Epoch 177/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1869 - accuracy: 0.4961 - val_loss: 1.2012 - val_accuracy: 0.3516\n",
      "Epoch 178/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1874 - accuracy: 0.4961 - val_loss: 1.2031 - val_accuracy: 0.3594\n",
      "Epoch 179/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1871 - accuracy: 0.4950 - val_loss: 1.2040 - val_accuracy: 0.3516\n",
      "Epoch 180/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1864 - accuracy: 0.4944 - val_loss: 1.1994 - val_accuracy: 0.3516\n",
      "Epoch 181/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1863 - accuracy: 0.4950 - val_loss: 1.2022 - val_accuracy: 0.3516\n",
      "Epoch 182/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1867 - accuracy: 0.4961 - val_loss: 1.2007 - val_accuracy: 0.3594\n",
      "Epoch 183/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1867 - accuracy: 0.4950 - val_loss: 1.2014 - val_accuracy: 0.3594\n",
      "Epoch 184/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1872 - accuracy: 0.4944 - val_loss: 1.2019 - val_accuracy: 0.3516\n",
      "Epoch 185/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1874 - accuracy: 0.4961 - val_loss: 1.2034 - val_accuracy: 0.3594\n",
      "Epoch 186/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1861 - accuracy: 0.4972 - val_loss: 1.2002 - val_accuracy: 0.3516\n",
      "Epoch 187/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1859 - accuracy: 0.4955 - val_loss: 1.2020 - val_accuracy: 0.3516\n",
      "Epoch 188/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1863 - accuracy: 0.4961 - val_loss: 1.2034 - val_accuracy: 0.3594\n",
      "Epoch 189/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1861 - accuracy: 0.4939 - val_loss: 1.2014 - val_accuracy: 0.3516\n",
      "Epoch 190/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1869 - accuracy: 0.4944 - val_loss: 1.1997 - val_accuracy: 0.3594\n",
      "Epoch 191/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1860 - accuracy: 0.4950 - val_loss: 1.1982 - val_accuracy: 0.3516\n",
      "Epoch 192/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1864 - accuracy: 0.4961 - val_loss: 1.1992 - val_accuracy: 0.3594\n",
      "Epoch 193/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1867 - accuracy: 0.4950 - val_loss: 1.2002 - val_accuracy: 0.3594\n",
      "Epoch 194/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1856 - accuracy: 0.4955 - val_loss: 1.1990 - val_accuracy: 0.3438\n",
      "Epoch 195/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1860 - accuracy: 0.4939 - val_loss: 1.2014 - val_accuracy: 0.3438\n",
      "Epoch 196/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1856 - accuracy: 0.4967 - val_loss: 1.2024 - val_accuracy: 0.3438\n",
      "Epoch 197/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1867 - accuracy: 0.4950 - val_loss: 1.2012 - val_accuracy: 0.3516\n",
      "Epoch 198/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1864 - accuracy: 0.4944 - val_loss: 1.1994 - val_accuracy: 0.3594\n",
      "Epoch 199/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1858 - accuracy: 0.4955 - val_loss: 1.2009 - val_accuracy: 0.3438\n",
      "Epoch 200/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1857 - accuracy: 0.4950 - val_loss: 1.1995 - val_accuracy: 0.3594\n",
      "Epoch 201/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1863 - accuracy: 0.4955 - val_loss: 1.2013 - val_accuracy: 0.3594\n",
      "Epoch 202/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1867 - accuracy: 0.4939 - val_loss: 1.2008 - val_accuracy: 0.3594\n",
      "Epoch 203/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1857 - accuracy: 0.4944 - val_loss: 1.1989 - val_accuracy: 0.3438\n",
      "Epoch 204/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1859 - accuracy: 0.4950 - val_loss: 1.1984 - val_accuracy: 0.3438\n",
      "Epoch 205/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1857 - accuracy: 0.4961 - val_loss: 1.1965 - val_accuracy: 0.3594\n",
      "Epoch 206/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1859 - accuracy: 0.4944 - val_loss: 1.2002 - val_accuracy: 0.3594\n",
      "Epoch 207/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1857 - accuracy: 0.4955 - val_loss: 1.1983 - val_accuracy: 0.3594\n",
      "Epoch 208/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1855 - accuracy: 0.4967 - val_loss: 1.2005 - val_accuracy: 0.3594\n",
      "Epoch 209/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1861 - accuracy: 0.4950 - val_loss: 1.1985 - val_accuracy: 0.3594\n",
      "Epoch 210/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1854 - accuracy: 0.4950 - val_loss: 1.1938 - val_accuracy: 0.3594\n",
      "Epoch 211/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1869 - accuracy: 0.4950 - val_loss: 1.1985 - val_accuracy: 0.3516\n",
      "Epoch 212/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1877 - accuracy: 0.4972 - val_loss: 1.1993 - val_accuracy: 0.3516\n",
      "Epoch 213/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1860 - accuracy: 0.4927 - val_loss: 1.1963 - val_accuracy: 0.3594\n",
      "Epoch 214/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1861 - accuracy: 0.4955 - val_loss: 1.1980 - val_accuracy: 0.3594\n",
      "Epoch 215/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1863 - accuracy: 0.4950 - val_loss: 1.1961 - val_accuracy: 0.3516\n",
      "Epoch 216/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1854 - accuracy: 0.4955 - val_loss: 1.1983 - val_accuracy: 0.3438\n",
      "Epoch 217/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1866 - accuracy: 0.4961 - val_loss: 1.2006 - val_accuracy: 0.3594\n",
      "Epoch 218/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1872 - accuracy: 0.4955 - val_loss: 1.1959 - val_accuracy: 0.3516\n",
      "Epoch 219/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1856 - accuracy: 0.4939 - val_loss: 1.1966 - val_accuracy: 0.3594\n",
      "Epoch 220/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1861 - accuracy: 0.4967 - val_loss: 1.1955 - val_accuracy: 0.3594\n",
      "Epoch 221/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1854 - accuracy: 0.4955 - val_loss: 1.1968 - val_accuracy: 0.3516\n",
      "Epoch 222/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1860 - accuracy: 0.4955 - val_loss: 1.2004 - val_accuracy: 0.3438\n",
      "Epoch 223/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1858 - accuracy: 0.4961 - val_loss: 1.1966 - val_accuracy: 0.3438\n",
      "Epoch 224/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1854 - accuracy: 0.4950 - val_loss: 1.1994 - val_accuracy: 0.3516\n",
      "Epoch 225/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1855 - accuracy: 0.4961 - val_loss: 1.1992 - val_accuracy: 0.3594\n",
      "Epoch 226/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1853 - accuracy: 0.4950 - val_loss: 1.1981 - val_accuracy: 0.3438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1850 - accuracy: 0.4967 - val_loss: 1.1941 - val_accuracy: 0.3594\n",
      "Epoch 228/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1852 - accuracy: 0.4950 - val_loss: 1.2014 - val_accuracy: 0.3438\n",
      "Epoch 229/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1857 - accuracy: 0.4950 - val_loss: 1.2000 - val_accuracy: 0.3594\n",
      "Epoch 230/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1851 - accuracy: 0.4950 - val_loss: 1.1965 - val_accuracy: 0.3594\n",
      "Epoch 231/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1857 - accuracy: 0.4939 - val_loss: 1.1957 - val_accuracy: 0.3594\n",
      "Epoch 232/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1857 - accuracy: 0.4944 - val_loss: 1.1933 - val_accuracy: 0.3594\n",
      "Epoch 233/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1850 - accuracy: 0.4950 - val_loss: 1.1965 - val_accuracy: 0.3594\n",
      "Epoch 234/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1855 - accuracy: 0.4967 - val_loss: 1.1956 - val_accuracy: 0.3594\n",
      "Epoch 235/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1859 - accuracy: 0.4944 - val_loss: 1.1934 - val_accuracy: 0.3594\n",
      "Epoch 236/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1850 - accuracy: 0.4944 - val_loss: 1.1974 - val_accuracy: 0.3438\n",
      "Epoch 237/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1852 - accuracy: 0.4939 - val_loss: 1.1969 - val_accuracy: 0.3594\n",
      "Epoch 238/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1851 - accuracy: 0.4961 - val_loss: 1.1961 - val_accuracy: 0.3594\n",
      "Epoch 239/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1848 - accuracy: 0.4950 - val_loss: 1.1950 - val_accuracy: 0.3516\n",
      "Epoch 240/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1859 - accuracy: 0.4955 - val_loss: 1.1971 - val_accuracy: 0.3438\n",
      "Epoch 241/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1856 - accuracy: 0.4950 - val_loss: 1.1992 - val_accuracy: 0.3516\n",
      "Epoch 242/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1849 - accuracy: 0.4955 - val_loss: 1.1955 - val_accuracy: 0.3516\n",
      "Epoch 243/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1850 - accuracy: 0.4978 - val_loss: 1.1974 - val_accuracy: 0.3594\n",
      "Epoch 244/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1852 - accuracy: 0.4955 - val_loss: 1.1943 - val_accuracy: 0.3594\n",
      "Epoch 245/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1844 - accuracy: 0.4961 - val_loss: 1.1945 - val_accuracy: 0.3516\n",
      "Epoch 246/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1852 - accuracy: 0.4961 - val_loss: 1.1977 - val_accuracy: 0.3438\n",
      "Epoch 247/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1847 - accuracy: 0.4950 - val_loss: 1.1965 - val_accuracy: 0.3516\n",
      "Epoch 248/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1845 - accuracy: 0.4955 - val_loss: 1.1963 - val_accuracy: 0.3438\n",
      "Epoch 249/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1858 - accuracy: 0.4955 - val_loss: 1.1914 - val_accuracy: 0.3594\n",
      "Epoch 250/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1848 - accuracy: 0.4950 - val_loss: 1.1973 - val_accuracy: 0.3594\n",
      "Epoch 251/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1854 - accuracy: 0.4967 - val_loss: 1.1919 - val_accuracy: 0.3594\n",
      "Epoch 252/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1844 - accuracy: 0.4967 - val_loss: 1.1982 - val_accuracy: 0.3438\n",
      "Epoch 253/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1843 - accuracy: 0.4950 - val_loss: 1.1925 - val_accuracy: 0.3594\n",
      "Epoch 254/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1845 - accuracy: 0.4944 - val_loss: 1.1965 - val_accuracy: 0.3594\n",
      "Epoch 255/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1850 - accuracy: 0.4950 - val_loss: 1.1929 - val_accuracy: 0.3438\n",
      "Epoch 256/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1849 - accuracy: 0.4961 - val_loss: 1.1977 - val_accuracy: 0.3359\n",
      "Epoch 257/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1845 - accuracy: 0.4955 - val_loss: 1.1950 - val_accuracy: 0.3516\n",
      "Epoch 258/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1845 - accuracy: 0.4955 - val_loss: 1.1940 - val_accuracy: 0.3516\n",
      "Epoch 259/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1849 - accuracy: 0.4944 - val_loss: 1.1954 - val_accuracy: 0.3438\n",
      "Epoch 260/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1850 - accuracy: 0.4933 - val_loss: 1.1943 - val_accuracy: 0.3516\n",
      "Epoch 261/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1845 - accuracy: 0.4950 - val_loss: 1.1911 - val_accuracy: 0.3594\n",
      "Epoch 262/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1845 - accuracy: 0.4950 - val_loss: 1.1964 - val_accuracy: 0.3516\n",
      "Epoch 263/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1849 - accuracy: 0.4967 - val_loss: 1.1950 - val_accuracy: 0.3594\n",
      "Epoch 264/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1846 - accuracy: 0.4955 - val_loss: 1.1932 - val_accuracy: 0.3594\n",
      "Epoch 265/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1846 - accuracy: 0.4967 - val_loss: 1.1950 - val_accuracy: 0.3594\n",
      "Epoch 266/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1853 - accuracy: 0.4983 - val_loss: 1.1935 - val_accuracy: 0.3594\n",
      "Epoch 267/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1854 - accuracy: 0.4950 - val_loss: 1.1919 - val_accuracy: 0.3594\n",
      "Epoch 268/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1844 - accuracy: 0.4955 - val_loss: 1.1900 - val_accuracy: 0.3594\n",
      "Epoch 269/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1847 - accuracy: 0.4944 - val_loss: 1.1950 - val_accuracy: 0.3594\n",
      "Epoch 270/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1842 - accuracy: 0.4972 - val_loss: 1.1921 - val_accuracy: 0.3516\n",
      "Epoch 271/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1842 - accuracy: 0.4944 - val_loss: 1.1939 - val_accuracy: 0.3516\n",
      "Epoch 272/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1858 - accuracy: 0.4967 - val_loss: 1.1937 - val_accuracy: 0.3594\n",
      "Epoch 273/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1847 - accuracy: 0.4944 - val_loss: 1.1935 - val_accuracy: 0.3594\n",
      "Epoch 274/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1841 - accuracy: 0.4955 - val_loss: 1.1972 - val_accuracy: 0.3438\n",
      "Epoch 275/300\n",
      "28/28 [==============================] - 1s 29ms/step - loss: 1.1848 - accuracy: 0.4922 - val_loss: 1.1860 - val_accuracy: 0.3594\n",
      "Epoch 276/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1847 - accuracy: 0.4955 - val_loss: 1.1939 - val_accuracy: 0.3594\n",
      "Epoch 277/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1850 - accuracy: 0.4950 - val_loss: 1.1936 - val_accuracy: 0.3516\n",
      "Epoch 278/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1843 - accuracy: 0.4944 - val_loss: 1.1924 - val_accuracy: 0.3516\n",
      "Epoch 279/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1846 - accuracy: 0.4961 - val_loss: 1.1927 - val_accuracy: 0.3594\n",
      "Epoch 280/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1838 - accuracy: 0.4961 - val_loss: 1.1922 - val_accuracy: 0.3516\n",
      "Epoch 281/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1844 - accuracy: 0.4967 - val_loss: 1.1932 - val_accuracy: 0.3594\n",
      "Epoch 282/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1834 - accuracy: 0.4967 - val_loss: 1.1929 - val_accuracy: 0.3516\n",
      "Epoch 283/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1841 - accuracy: 0.4933 - val_loss: 1.1940 - val_accuracy: 0.3594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 284/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1853 - accuracy: 0.4961 - val_loss: 1.1904 - val_accuracy: 0.3594\n",
      "Epoch 285/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1843 - accuracy: 0.4933 - val_loss: 1.1951 - val_accuracy: 0.3438\n",
      "Epoch 286/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1840 - accuracy: 0.4955 - val_loss: 1.1913 - val_accuracy: 0.3594\n",
      "Epoch 287/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1837 - accuracy: 0.4955 - val_loss: 1.1925 - val_accuracy: 0.3516\n",
      "Epoch 288/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1841 - accuracy: 0.4961 - val_loss: 1.1928 - val_accuracy: 0.3594\n",
      "Epoch 289/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1838 - accuracy: 0.4972 - val_loss: 1.1943 - val_accuracy: 0.3516\n",
      "Epoch 290/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1839 - accuracy: 0.4955 - val_loss: 1.1929 - val_accuracy: 0.3516\n",
      "Epoch 291/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1839 - accuracy: 0.4961 - val_loss: 1.1909 - val_accuracy: 0.3594\n",
      "Epoch 292/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1844 - accuracy: 0.4967 - val_loss: 1.1919 - val_accuracy: 0.3594\n",
      "Epoch 293/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1840 - accuracy: 0.4972 - val_loss: 1.1881 - val_accuracy: 0.3594\n",
      "Epoch 294/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1848 - accuracy: 0.4961 - val_loss: 1.1939 - val_accuracy: 0.3594\n",
      "Epoch 295/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1845 - accuracy: 0.4950 - val_loss: 1.1884 - val_accuracy: 0.3594\n",
      "Epoch 296/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1845 - accuracy: 0.4978 - val_loss: 1.1898 - val_accuracy: 0.3516\n",
      "Epoch 297/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1837 - accuracy: 0.4939 - val_loss: 1.1923 - val_accuracy: 0.3516\n",
      "Epoch 298/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1841 - accuracy: 0.4972 - val_loss: 1.1886 - val_accuracy: 0.3516\n",
      "Epoch 299/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1843 - accuracy: 0.4961 - val_loss: 1.1925 - val_accuracy: 0.3516\n",
      "Epoch 300/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1850 - accuracy: 0.4944 - val_loss: 1.1942 - val_accuracy: 0.3516\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "56/56 [==============================] - 0s 1ms/step\n",
      "Recall: [0.94 0.   0.56]\n",
      "Precision: [0.41 0.43 0.73]\n",
      "F1-score: [0.57 0.01 0.64]\n",
      "Accuracy: 0.5\n",
      "Brier score:0.20348628505620878\n",
      "Brier climat:0.21458829365079365\n",
      "Brier skill score:0.05173631984161975\n",
      "Recall: [0.94 0.   0.56]\n",
      "Precision: [0.41 0.43 0.73]\n",
      "F1-score: [0.57 0.01 0.64]\n",
      "Accuracy: 0.5\n",
      "Brier score:0.21880159643847968\n",
      "Brier climat:0.2201984126984127\n",
      "Brier skill score:0.006343443818762284\n",
      "Recall: [0.94 0.   0.56]\n",
      "Precision: [0.41 0.43 0.73]\n",
      "F1-score: [0.57 0.01 0.64]\n",
      "Accuracy: 0.5\n",
      "Brier score:0.1518141826011152\n",
      "Brier climat:0.2240699404761905\n",
      "Brier skill score:0.322469661577623\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "Recall: [0.79 0.   0.52]\n",
      "Precision: [0.18 0.   0.67]\n",
      "F1-score: [0.29 0.   0.58]\n",
      "Accuracy: 0.35\n",
      "Brier score:0.21328916152607238\n",
      "Brier climat:0.14914930555555558\n",
      "Brier skill score:-0.4300379122222986\n",
      "Recall: [0.79 0.   0.52]\n",
      "Precision: [0.18 0.   0.67]\n",
      "F1-score: [0.29 0.   0.58]\n",
      "Accuracy: 0.35\n",
      "Brier score:0.2722521400678452\n",
      "Brier climat:0.23701388888888889\n",
      "Brier skill score:-0.14867589129123093\n",
      "Recall: [0.79 0.   0.52]\n",
      "Precision: [0.18 0.   0.67]\n",
      "F1-score: [0.29 0.   0.58]\n",
      "Accuracy: 0.35\n",
      "Brier score:0.21680516911964048\n",
      "Brier climat:0.24973958333333335\n",
      "Brier skill score:0.1318750266742238\n",
      "******** 22\n",
      "validation years [2003, 2004]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2005, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "{0: 2.0, 1: 1.0, 2: 1.0}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 60)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 183       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183\n",
      "Trainable params: 183\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/28 [===========================>..] - ETA: 0s - loss: 2.3484 - accuracy: 0.2049WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "28/28 [==============================] - 4s 134ms/step - loss: 2.3448 - accuracy: 0.2031 - val_loss: 1.6631 - val_accuracy: 0.1016\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.9010 - accuracy: 0.2946 - val_loss: 1.5509 - val_accuracy: 0.0469\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.7038 - accuracy: 0.4018 - val_loss: 1.4902 - val_accuracy: 0.1172\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.6074 - accuracy: 0.4263 - val_loss: 1.4600 - val_accuracy: 0.1953\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.5522 - accuracy: 0.4302 - val_loss: 1.4456 - val_accuracy: 0.2422\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.5155 - accuracy: 0.4515 - val_loss: 1.4357 - val_accuracy: 0.2812\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4894 - accuracy: 0.4570 - val_loss: 1.4341 - val_accuracy: 0.2812\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.4692 - accuracy: 0.4660 - val_loss: 1.4215 - val_accuracy: 0.2891\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4517 - accuracy: 0.4693 - val_loss: 1.4178 - val_accuracy: 0.2969\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.4369 - accuracy: 0.4766 - val_loss: 1.4089 - val_accuracy: 0.2969\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.4245 - accuracy: 0.4816 - val_loss: 1.4100 - val_accuracy: 0.2969\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.4115 - accuracy: 0.4855 - val_loss: 1.3982 - val_accuracy: 0.2969\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3997 - accuracy: 0.4849 - val_loss: 1.3960 - val_accuracy: 0.2969\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3891 - accuracy: 0.4900 - val_loss: 1.3878 - val_accuracy: 0.2969\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3789 - accuracy: 0.4900 - val_loss: 1.3812 - val_accuracy: 0.2969\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3693 - accuracy: 0.4905 - val_loss: 1.3737 - val_accuracy: 0.3125\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3614 - accuracy: 0.4877 - val_loss: 1.3723 - val_accuracy: 0.3047\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3527 - accuracy: 0.4888 - val_loss: 1.3628 - val_accuracy: 0.3125\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3450 - accuracy: 0.4883 - val_loss: 1.3574 - val_accuracy: 0.3047\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3370 - accuracy: 0.4866 - val_loss: 1.3558 - val_accuracy: 0.3047\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3306 - accuracy: 0.4872 - val_loss: 1.3466 - val_accuracy: 0.3047\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3239 - accuracy: 0.4872 - val_loss: 1.3454 - val_accuracy: 0.3047\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3177 - accuracy: 0.4916 - val_loss: 1.3398 - val_accuracy: 0.3125\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3120 - accuracy: 0.4939 - val_loss: 1.3353 - val_accuracy: 0.3125\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3067 - accuracy: 0.4922 - val_loss: 1.3318 - val_accuracy: 0.3125\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3020 - accuracy: 0.4922 - val_loss: 1.3240 - val_accuracy: 0.3125\n",
      "Epoch 27/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2974 - accuracy: 0.4944 - val_loss: 1.3281 - val_accuracy: 0.3047\n",
      "Epoch 28/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2925 - accuracy: 0.4927 - val_loss: 1.3235 - val_accuracy: 0.3047\n",
      "Epoch 29/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2884 - accuracy: 0.4955 - val_loss: 1.3207 - val_accuracy: 0.3047\n",
      "Epoch 30/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2837 - accuracy: 0.4950 - val_loss: 1.3164 - val_accuracy: 0.3047\n",
      "Epoch 31/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2805 - accuracy: 0.4944 - val_loss: 1.3142 - val_accuracy: 0.3047\n",
      "Epoch 32/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2772 - accuracy: 0.4944 - val_loss: 1.3117 - val_accuracy: 0.3047\n",
      "Epoch 33/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2746 - accuracy: 0.4972 - val_loss: 1.3080 - val_accuracy: 0.3125\n",
      "Epoch 34/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2715 - accuracy: 0.4961 - val_loss: 1.3083 - val_accuracy: 0.3047\n",
      "Epoch 35/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2679 - accuracy: 0.4944 - val_loss: 1.3042 - val_accuracy: 0.3125\n",
      "Epoch 36/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2670 - accuracy: 0.4955 - val_loss: 1.2973 - val_accuracy: 0.3125\n",
      "Epoch 37/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2624 - accuracy: 0.4950 - val_loss: 1.2976 - val_accuracy: 0.3047\n",
      "Epoch 38/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2599 - accuracy: 0.4955 - val_loss: 1.2941 - val_accuracy: 0.3125\n",
      "Epoch 39/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2571 - accuracy: 0.4939 - val_loss: 1.2939 - val_accuracy: 0.3125\n",
      "Epoch 40/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2555 - accuracy: 0.4944 - val_loss: 1.2937 - val_accuracy: 0.3047\n",
      "Epoch 41/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2531 - accuracy: 0.4944 - val_loss: 1.2873 - val_accuracy: 0.3047\n",
      "Epoch 42/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2514 - accuracy: 0.4927 - val_loss: 1.2847 - val_accuracy: 0.3125\n",
      "Epoch 43/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2486 - accuracy: 0.4955 - val_loss: 1.2833 - val_accuracy: 0.3125\n",
      "Epoch 44/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2457 - accuracy: 0.4967 - val_loss: 1.2839 - val_accuracy: 0.3047\n",
      "Epoch 45/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2438 - accuracy: 0.4967 - val_loss: 1.2822 - val_accuracy: 0.3125\n",
      "Epoch 46/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2424 - accuracy: 0.4961 - val_loss: 1.2826 - val_accuracy: 0.3047\n",
      "Epoch 47/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2397 - accuracy: 0.4972 - val_loss: 1.2755 - val_accuracy: 0.3125\n",
      "Epoch 48/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2387 - accuracy: 0.4961 - val_loss: 1.2751 - val_accuracy: 0.3125\n",
      "Epoch 49/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2358 - accuracy: 0.4978 - val_loss: 1.2752 - val_accuracy: 0.3125\n",
      "Epoch 50/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2351 - accuracy: 0.4972 - val_loss: 1.2675 - val_accuracy: 0.3125\n",
      "Epoch 51/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2331 - accuracy: 0.5000 - val_loss: 1.2694 - val_accuracy: 0.3125\n",
      "Epoch 52/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2312 - accuracy: 0.4978 - val_loss: 1.2684 - val_accuracy: 0.3125\n",
      "Epoch 53/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2296 - accuracy: 0.4961 - val_loss: 1.2657 - val_accuracy: 0.3047\n",
      "Epoch 54/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2280 - accuracy: 0.4994 - val_loss: 1.2682 - val_accuracy: 0.3125\n",
      "Epoch 55/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2268 - accuracy: 0.4978 - val_loss: 1.2662 - val_accuracy: 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2254 - accuracy: 0.4978 - val_loss: 1.2612 - val_accuracy: 0.3125\n",
      "Epoch 57/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2246 - accuracy: 0.4961 - val_loss: 1.2617 - val_accuracy: 0.3125\n",
      "Epoch 58/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2233 - accuracy: 0.4983 - val_loss: 1.2593 - val_accuracy: 0.3047\n",
      "Epoch 59/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2221 - accuracy: 0.4989 - val_loss: 1.2539 - val_accuracy: 0.3125\n",
      "Epoch 60/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2211 - accuracy: 0.4972 - val_loss: 1.2583 - val_accuracy: 0.3125\n",
      "Epoch 61/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2202 - accuracy: 0.4967 - val_loss: 1.2551 - val_accuracy: 0.3125\n",
      "Epoch 62/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2185 - accuracy: 0.4961 - val_loss: 1.2574 - val_accuracy: 0.3203\n",
      "Epoch 63/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2174 - accuracy: 0.4978 - val_loss: 1.2527 - val_accuracy: 0.3125\n",
      "Epoch 64/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2163 - accuracy: 0.4978 - val_loss: 1.2495 - val_accuracy: 0.3125\n",
      "Epoch 65/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2156 - accuracy: 0.4978 - val_loss: 1.2564 - val_accuracy: 0.3047\n",
      "Epoch 66/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2151 - accuracy: 0.4989 - val_loss: 1.2447 - val_accuracy: 0.3125\n",
      "Epoch 67/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2139 - accuracy: 0.4967 - val_loss: 1.2516 - val_accuracy: 0.3125\n",
      "Epoch 68/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2125 - accuracy: 0.4972 - val_loss: 1.2450 - val_accuracy: 0.3125\n",
      "Epoch 69/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2117 - accuracy: 0.4955 - val_loss: 1.2459 - val_accuracy: 0.3125\n",
      "Epoch 70/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2109 - accuracy: 0.4972 - val_loss: 1.2475 - val_accuracy: 0.3047\n",
      "Epoch 71/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2107 - accuracy: 0.4967 - val_loss: 1.2503 - val_accuracy: 0.3047\n",
      "Epoch 72/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2096 - accuracy: 0.4955 - val_loss: 1.2402 - val_accuracy: 0.3125\n",
      "Epoch 73/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2095 - accuracy: 0.4961 - val_loss: 1.2452 - val_accuracy: 0.3125\n",
      "Epoch 74/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2091 - accuracy: 0.4955 - val_loss: 1.2442 - val_accuracy: 0.3125\n",
      "Epoch 75/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2072 - accuracy: 0.4972 - val_loss: 1.2428 - val_accuracy: 0.3047\n",
      "Epoch 76/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2067 - accuracy: 0.4967 - val_loss: 1.2437 - val_accuracy: 0.3047\n",
      "Epoch 77/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2069 - accuracy: 0.4972 - val_loss: 1.2418 - val_accuracy: 0.3125\n",
      "Epoch 78/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2080 - accuracy: 0.4983 - val_loss: 1.2427 - val_accuracy: 0.3047\n",
      "Epoch 79/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2085 - accuracy: 0.4955 - val_loss: 1.2481 - val_accuracy: 0.3125\n",
      "Epoch 80/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2063 - accuracy: 0.4978 - val_loss: 1.2417 - val_accuracy: 0.3047\n",
      "Epoch 81/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2057 - accuracy: 0.4994 - val_loss: 1.2447 - val_accuracy: 0.3047\n",
      "Epoch 82/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2047 - accuracy: 0.4961 - val_loss: 1.2415 - val_accuracy: 0.3047\n",
      "Epoch 83/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2047 - accuracy: 0.4944 - val_loss: 1.2383 - val_accuracy: 0.3047\n",
      "Epoch 84/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2042 - accuracy: 0.4961 - val_loss: 1.2387 - val_accuracy: 0.3047\n",
      "Epoch 85/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2038 - accuracy: 0.4961 - val_loss: 1.2415 - val_accuracy: 0.3047\n",
      "Epoch 86/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2027 - accuracy: 0.4961 - val_loss: 1.2413 - val_accuracy: 0.3047\n",
      "Epoch 87/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2032 - accuracy: 0.4944 - val_loss: 1.2467 - val_accuracy: 0.3047\n",
      "Epoch 88/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2019 - accuracy: 0.4939 - val_loss: 1.2428 - val_accuracy: 0.3047\n",
      "Epoch 89/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2019 - accuracy: 0.4944 - val_loss: 1.2390 - val_accuracy: 0.3047\n",
      "Epoch 90/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2022 - accuracy: 0.4927 - val_loss: 1.2364 - val_accuracy: 0.3047\n",
      "Epoch 91/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2011 - accuracy: 0.4944 - val_loss: 1.2391 - val_accuracy: 0.3047\n",
      "Epoch 92/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2010 - accuracy: 0.4933 - val_loss: 1.2416 - val_accuracy: 0.3047\n",
      "Epoch 93/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2013 - accuracy: 0.4927 - val_loss: 1.2423 - val_accuracy: 0.3047\n",
      "Epoch 94/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2017 - accuracy: 0.4944 - val_loss: 1.2444 - val_accuracy: 0.3047\n",
      "Epoch 95/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2013 - accuracy: 0.4955 - val_loss: 1.2400 - val_accuracy: 0.3047\n",
      "Epoch 96/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2002 - accuracy: 0.4933 - val_loss: 1.2419 - val_accuracy: 0.3047\n",
      "Epoch 97/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2004 - accuracy: 0.4950 - val_loss: 1.2450 - val_accuracy: 0.3047\n",
      "Epoch 98/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2002 - accuracy: 0.4944 - val_loss: 1.2411 - val_accuracy: 0.3047\n",
      "Epoch 99/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2001 - accuracy: 0.4955 - val_loss: 1.2412 - val_accuracy: 0.3047\n",
      "Epoch 100/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1992 - accuracy: 0.4939 - val_loss: 1.2363 - val_accuracy: 0.3047\n",
      "Epoch 101/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1987 - accuracy: 0.4922 - val_loss: 1.2428 - val_accuracy: 0.3047\n",
      "Epoch 102/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1986 - accuracy: 0.4927 - val_loss: 1.2393 - val_accuracy: 0.3047\n",
      "Epoch 103/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1987 - accuracy: 0.4950 - val_loss: 1.2406 - val_accuracy: 0.3047\n",
      "Epoch 104/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1990 - accuracy: 0.4933 - val_loss: 1.2419 - val_accuracy: 0.3047\n",
      "Epoch 105/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1992 - accuracy: 0.4939 - val_loss: 1.2409 - val_accuracy: 0.3047\n",
      "Epoch 106/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1980 - accuracy: 0.4950 - val_loss: 1.2404 - val_accuracy: 0.3047\n",
      "Epoch 107/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1981 - accuracy: 0.4939 - val_loss: 1.2376 - val_accuracy: 0.3047\n",
      "Epoch 108/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1975 - accuracy: 0.4939 - val_loss: 1.2376 - val_accuracy: 0.3047\n",
      "Epoch 109/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1975 - accuracy: 0.4950 - val_loss: 1.2389 - val_accuracy: 0.3047\n",
      "Epoch 110/300\n",
      "28/28 [==============================] - 1s 29ms/step - loss: 1.1976 - accuracy: 0.4944 - val_loss: 1.2414 - val_accuracy: 0.3047\n",
      "Epoch 111/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1978 - accuracy: 0.4933 - val_loss: 1.2371 - val_accuracy: 0.3047\n",
      "Epoch 112/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1972 - accuracy: 0.4955 - val_loss: 1.2422 - val_accuracy: 0.3047\n",
      "Epoch 113/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1976 - accuracy: 0.4933 - val_loss: 1.2394 - val_accuracy: 0.3047\n",
      "Epoch 114/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1999 - accuracy: 0.4944 - val_loss: 1.2346 - val_accuracy: 0.3047\n",
      "Epoch 115/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1978 - accuracy: 0.4939 - val_loss: 1.2398 - val_accuracy: 0.3047\n",
      "Epoch 116/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1972 - accuracy: 0.4933 - val_loss: 1.2396 - val_accuracy: 0.3047\n",
      "Epoch 117/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1972 - accuracy: 0.4944 - val_loss: 1.2404 - val_accuracy: 0.3047\n",
      "Epoch 118/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1965 - accuracy: 0.4927 - val_loss: 1.2415 - val_accuracy: 0.3047\n",
      "Epoch 119/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1962 - accuracy: 0.4933 - val_loss: 1.2376 - val_accuracy: 0.3047\n",
      "Epoch 120/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1967 - accuracy: 0.4922 - val_loss: 1.2369 - val_accuracy: 0.3047\n",
      "Epoch 121/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1965 - accuracy: 0.4939 - val_loss: 1.2374 - val_accuracy: 0.3047\n",
      "Epoch 122/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1966 - accuracy: 0.4955 - val_loss: 1.2379 - val_accuracy: 0.3047\n",
      "Epoch 123/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1956 - accuracy: 0.4944 - val_loss: 1.2380 - val_accuracy: 0.3047\n",
      "Epoch 124/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1963 - accuracy: 0.4922 - val_loss: 1.2393 - val_accuracy: 0.3047\n",
      "Epoch 125/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1961 - accuracy: 0.4927 - val_loss: 1.2415 - val_accuracy: 0.3047\n",
      "Epoch 126/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1958 - accuracy: 0.4933 - val_loss: 1.2346 - val_accuracy: 0.3047\n",
      "Epoch 127/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1954 - accuracy: 0.4944 - val_loss: 1.2407 - val_accuracy: 0.3047\n",
      "Epoch 128/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1956 - accuracy: 0.4933 - val_loss: 1.2376 - val_accuracy: 0.3047\n",
      "Epoch 129/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1958 - accuracy: 0.4950 - val_loss: 1.2351 - val_accuracy: 0.3047\n",
      "Epoch 130/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1960 - accuracy: 0.4944 - val_loss: 1.2401 - val_accuracy: 0.3047\n",
      "Epoch 131/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1961 - accuracy: 0.4955 - val_loss: 1.2341 - val_accuracy: 0.3047\n",
      "Epoch 132/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1971 - accuracy: 0.4911 - val_loss: 1.2402 - val_accuracy: 0.3047\n",
      "Epoch 133/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1958 - accuracy: 0.4955 - val_loss: 1.2331 - val_accuracy: 0.3047\n",
      "Epoch 134/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1951 - accuracy: 0.4944 - val_loss: 1.2363 - val_accuracy: 0.3047\n",
      "Epoch 135/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1950 - accuracy: 0.4950 - val_loss: 1.2336 - val_accuracy: 0.3047\n",
      "Epoch 136/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1956 - accuracy: 0.4927 - val_loss: 1.2336 - val_accuracy: 0.3047\n",
      "Epoch 137/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1951 - accuracy: 0.4933 - val_loss: 1.2328 - val_accuracy: 0.3047\n",
      "Epoch 138/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1957 - accuracy: 0.4944 - val_loss: 1.2318 - val_accuracy: 0.3125\n",
      "Epoch 139/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1964 - accuracy: 0.4950 - val_loss: 1.2390 - val_accuracy: 0.3047\n",
      "Epoch 140/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1948 - accuracy: 0.4950 - val_loss: 1.2322 - val_accuracy: 0.3047\n",
      "Epoch 141/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1946 - accuracy: 0.4944 - val_loss: 1.2309 - val_accuracy: 0.3047\n",
      "Epoch 142/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1943 - accuracy: 0.4944 - val_loss: 1.2343 - val_accuracy: 0.3047\n",
      "Epoch 143/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1953 - accuracy: 0.4939 - val_loss: 1.2360 - val_accuracy: 0.3047\n",
      "Epoch 144/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1949 - accuracy: 0.4967 - val_loss: 1.2299 - val_accuracy: 0.3047\n",
      "Epoch 145/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1948 - accuracy: 0.4961 - val_loss: 1.2329 - val_accuracy: 0.3047\n",
      "Epoch 146/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1950 - accuracy: 0.4927 - val_loss: 1.2335 - val_accuracy: 0.3047\n",
      "Epoch 147/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1942 - accuracy: 0.4939 - val_loss: 1.2342 - val_accuracy: 0.3047\n",
      "Epoch 148/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1952 - accuracy: 0.4933 - val_loss: 1.2277 - val_accuracy: 0.3047\n",
      "Epoch 149/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1949 - accuracy: 0.4950 - val_loss: 1.2327 - val_accuracy: 0.3047\n",
      "Epoch 150/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1951 - accuracy: 0.4933 - val_loss: 1.2327 - val_accuracy: 0.3047\n",
      "Epoch 151/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1942 - accuracy: 0.4944 - val_loss: 1.2273 - val_accuracy: 0.3125\n",
      "Epoch 152/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1936 - accuracy: 0.4944 - val_loss: 1.2336 - val_accuracy: 0.3047\n",
      "Epoch 153/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1953 - accuracy: 0.4950 - val_loss: 1.2282 - val_accuracy: 0.3125\n",
      "Epoch 154/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1954 - accuracy: 0.4944 - val_loss: 1.2318 - val_accuracy: 0.3047\n",
      "Epoch 155/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1946 - accuracy: 0.4978 - val_loss: 1.2301 - val_accuracy: 0.3047\n",
      "Epoch 156/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1937 - accuracy: 0.4955 - val_loss: 1.2294 - val_accuracy: 0.3125\n",
      "Epoch 157/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1933 - accuracy: 0.4944 - val_loss: 1.2320 - val_accuracy: 0.3047\n",
      "Epoch 158/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1941 - accuracy: 0.4950 - val_loss: 1.2308 - val_accuracy: 0.3047\n",
      "Epoch 159/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1947 - accuracy: 0.4961 - val_loss: 1.2338 - val_accuracy: 0.3047\n",
      "Epoch 160/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1942 - accuracy: 0.4961 - val_loss: 1.2348 - val_accuracy: 0.3047\n",
      "Epoch 161/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1940 - accuracy: 0.4939 - val_loss: 1.2326 - val_accuracy: 0.3047\n",
      "Epoch 162/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1941 - accuracy: 0.4927 - val_loss: 1.2286 - val_accuracy: 0.3125\n",
      "Epoch 163/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1932 - accuracy: 0.4933 - val_loss: 1.2327 - val_accuracy: 0.3047\n",
      "Epoch 164/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1939 - accuracy: 0.4961 - val_loss: 1.2316 - val_accuracy: 0.3047\n",
      "Epoch 165/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1939 - accuracy: 0.4944 - val_loss: 1.2253 - val_accuracy: 0.3125\n",
      "Epoch 166/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1947 - accuracy: 0.4933 - val_loss: 1.2273 - val_accuracy: 0.3125\n",
      "Epoch 167/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1949 - accuracy: 0.4950 - val_loss: 1.2284 - val_accuracy: 0.3047\n",
      "Epoch 168/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1935 - accuracy: 0.4944 - val_loss: 1.2294 - val_accuracy: 0.3047\n",
      "Epoch 169/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1934 - accuracy: 0.4939 - val_loss: 1.2275 - val_accuracy: 0.3047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1932 - accuracy: 0.4950 - val_loss: 1.2291 - val_accuracy: 0.3047\n",
      "Epoch 171/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1928 - accuracy: 0.4944 - val_loss: 1.2275 - val_accuracy: 0.3125\n",
      "Epoch 172/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1939 - accuracy: 0.4944 - val_loss: 1.2306 - val_accuracy: 0.3047\n",
      "Epoch 173/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1928 - accuracy: 0.4950 - val_loss: 1.2300 - val_accuracy: 0.3125\n",
      "Epoch 174/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1932 - accuracy: 0.4950 - val_loss: 1.2291 - val_accuracy: 0.3047\n",
      "Epoch 175/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1951 - accuracy: 0.4961 - val_loss: 1.2337 - val_accuracy: 0.3047\n",
      "Epoch 176/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1944 - accuracy: 0.4944 - val_loss: 1.2308 - val_accuracy: 0.3125\n",
      "Epoch 177/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1934 - accuracy: 0.4967 - val_loss: 1.2286 - val_accuracy: 0.3125\n",
      "Epoch 178/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1930 - accuracy: 0.4950 - val_loss: 1.2322 - val_accuracy: 0.3047\n",
      "Epoch 179/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1942 - accuracy: 0.4961 - val_loss: 1.2232 - val_accuracy: 0.3125\n",
      "Epoch 180/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1951 - accuracy: 0.4955 - val_loss: 1.2292 - val_accuracy: 0.3125\n",
      "Epoch 181/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1951 - accuracy: 0.4944 - val_loss: 1.2265 - val_accuracy: 0.3125\n",
      "Epoch 182/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1937 - accuracy: 0.4961 - val_loss: 1.2273 - val_accuracy: 0.3125\n",
      "Epoch 183/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1929 - accuracy: 0.4944 - val_loss: 1.2283 - val_accuracy: 0.3125\n",
      "Epoch 184/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1926 - accuracy: 0.4933 - val_loss: 1.2258 - val_accuracy: 0.3125\n",
      "Epoch 185/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1928 - accuracy: 0.4955 - val_loss: 1.2296 - val_accuracy: 0.3125\n",
      "Epoch 186/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1931 - accuracy: 0.4933 - val_loss: 1.2309 - val_accuracy: 0.3047\n",
      "Epoch 187/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1927 - accuracy: 0.4961 - val_loss: 1.2296 - val_accuracy: 0.3125\n",
      "Epoch 188/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1933 - accuracy: 0.4950 - val_loss: 1.2289 - val_accuracy: 0.3047\n",
      "Epoch 189/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1925 - accuracy: 0.4955 - val_loss: 1.2295 - val_accuracy: 0.3125\n",
      "Epoch 190/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1932 - accuracy: 0.4939 - val_loss: 1.2268 - val_accuracy: 0.3125\n",
      "Epoch 191/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1922 - accuracy: 0.4961 - val_loss: 1.2274 - val_accuracy: 0.3125\n",
      "Epoch 192/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1927 - accuracy: 0.4939 - val_loss: 1.2254 - val_accuracy: 0.3125\n",
      "Epoch 193/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1932 - accuracy: 0.4961 - val_loss: 1.2306 - val_accuracy: 0.3047\n",
      "Epoch 194/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1923 - accuracy: 0.4950 - val_loss: 1.2310 - val_accuracy: 0.3047\n",
      "Epoch 195/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1924 - accuracy: 0.4961 - val_loss: 1.2276 - val_accuracy: 0.3125\n",
      "Epoch 196/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1930 - accuracy: 0.4944 - val_loss: 1.2275 - val_accuracy: 0.3125\n",
      "Epoch 197/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1928 - accuracy: 0.4955 - val_loss: 1.2291 - val_accuracy: 0.3125\n",
      "Epoch 198/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1926 - accuracy: 0.4961 - val_loss: 1.2263 - val_accuracy: 0.3047\n",
      "Epoch 199/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1922 - accuracy: 0.4939 - val_loss: 1.2275 - val_accuracy: 0.3125\n",
      "Epoch 200/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1929 - accuracy: 0.4972 - val_loss: 1.2290 - val_accuracy: 0.3047\n",
      "Epoch 201/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1931 - accuracy: 0.4955 - val_loss: 1.2276 - val_accuracy: 0.3125\n",
      "Epoch 202/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1926 - accuracy: 0.4955 - val_loss: 1.2266 - val_accuracy: 0.3125\n",
      "Epoch 203/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1918 - accuracy: 0.4955 - val_loss: 1.2285 - val_accuracy: 0.3125\n",
      "Epoch 204/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1918 - accuracy: 0.4939 - val_loss: 1.2247 - val_accuracy: 0.3125\n",
      "Epoch 205/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1931 - accuracy: 0.4972 - val_loss: 1.2288 - val_accuracy: 0.3125\n",
      "Epoch 206/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1931 - accuracy: 0.4961 - val_loss: 1.2314 - val_accuracy: 0.3047\n",
      "Epoch 207/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1927 - accuracy: 0.4955 - val_loss: 1.2266 - val_accuracy: 0.3125\n",
      "Epoch 208/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1924 - accuracy: 0.4961 - val_loss: 1.2283 - val_accuracy: 0.3125\n",
      "Epoch 209/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1921 - accuracy: 0.4961 - val_loss: 1.2276 - val_accuracy: 0.3125\n",
      "Epoch 210/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1920 - accuracy: 0.4961 - val_loss: 1.2318 - val_accuracy: 0.3125\n",
      "Epoch 211/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1917 - accuracy: 0.4955 - val_loss: 1.2253 - val_accuracy: 0.3125\n",
      "Epoch 212/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1919 - accuracy: 0.4955 - val_loss: 1.2305 - val_accuracy: 0.3125\n",
      "Epoch 213/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1920 - accuracy: 0.4961 - val_loss: 1.2259 - val_accuracy: 0.3125\n",
      "Epoch 214/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1918 - accuracy: 0.4939 - val_loss: 1.2286 - val_accuracy: 0.3125\n",
      "Epoch 215/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1921 - accuracy: 0.4961 - val_loss: 1.2253 - val_accuracy: 0.3125\n",
      "Epoch 216/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1920 - accuracy: 0.4933 - val_loss: 1.2255 - val_accuracy: 0.3125\n",
      "Epoch 217/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1931 - accuracy: 0.4961 - val_loss: 1.2345 - val_accuracy: 0.3047\n",
      "Epoch 218/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1914 - accuracy: 0.4972 - val_loss: 1.2231 - val_accuracy: 0.3125\n",
      "Epoch 219/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1918 - accuracy: 0.4961 - val_loss: 1.2224 - val_accuracy: 0.3125\n",
      "Epoch 220/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1918 - accuracy: 0.4961 - val_loss: 1.2278 - val_accuracy: 0.3125\n",
      "Epoch 221/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1921 - accuracy: 0.4950 - val_loss: 1.2265 - val_accuracy: 0.3125\n",
      "Epoch 222/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1913 - accuracy: 0.4961 - val_loss: 1.2247 - val_accuracy: 0.3125\n",
      "Epoch 223/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1915 - accuracy: 0.4961 - val_loss: 1.2276 - val_accuracy: 0.3125\n",
      "Epoch 224/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1914 - accuracy: 0.4944 - val_loss: 1.2248 - val_accuracy: 0.3125\n",
      "Epoch 225/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1915 - accuracy: 0.4961 - val_loss: 1.2270 - val_accuracy: 0.3125\n",
      "Epoch 226/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1920 - accuracy: 0.4939 - val_loss: 1.2300 - val_accuracy: 0.3047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1918 - accuracy: 0.4967 - val_loss: 1.2235 - val_accuracy: 0.3125\n",
      "Epoch 228/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1911 - accuracy: 0.4967 - val_loss: 1.2271 - val_accuracy: 0.3125\n",
      "Epoch 229/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1917 - accuracy: 0.4961 - val_loss: 1.2294 - val_accuracy: 0.3125\n",
      "Epoch 230/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1920 - accuracy: 0.4978 - val_loss: 1.2202 - val_accuracy: 0.3125\n",
      "Epoch 231/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1918 - accuracy: 0.4944 - val_loss: 1.2272 - val_accuracy: 0.3125\n",
      "Epoch 232/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1915 - accuracy: 0.4944 - val_loss: 1.2241 - val_accuracy: 0.3125\n",
      "Epoch 233/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1914 - accuracy: 0.4955 - val_loss: 1.2273 - val_accuracy: 0.3125\n",
      "Epoch 234/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1909 - accuracy: 0.4944 - val_loss: 1.2263 - val_accuracy: 0.3125\n",
      "Epoch 235/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1917 - accuracy: 0.4967 - val_loss: 1.2250 - val_accuracy: 0.3125\n",
      "Epoch 236/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1912 - accuracy: 0.4944 - val_loss: 1.2310 - val_accuracy: 0.3047\n",
      "Epoch 237/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1914 - accuracy: 0.4967 - val_loss: 1.2247 - val_accuracy: 0.3125\n",
      "Epoch 238/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1908 - accuracy: 0.4950 - val_loss: 1.2256 - val_accuracy: 0.3125\n",
      "Epoch 239/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1907 - accuracy: 0.4955 - val_loss: 1.2220 - val_accuracy: 0.3125\n",
      "Epoch 240/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1911 - accuracy: 0.4950 - val_loss: 1.2228 - val_accuracy: 0.3125\n",
      "Epoch 241/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1916 - accuracy: 0.4955 - val_loss: 1.2259 - val_accuracy: 0.3125\n",
      "Epoch 242/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1915 - accuracy: 0.4955 - val_loss: 1.2263 - val_accuracy: 0.3125\n",
      "Epoch 243/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1912 - accuracy: 0.4955 - val_loss: 1.2229 - val_accuracy: 0.3125\n",
      "Epoch 244/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1906 - accuracy: 0.4967 - val_loss: 1.2273 - val_accuracy: 0.3125\n",
      "Epoch 245/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1917 - accuracy: 0.4944 - val_loss: 1.2293 - val_accuracy: 0.3125\n",
      "Epoch 246/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1933 - accuracy: 0.4961 - val_loss: 1.2286 - val_accuracy: 0.3125\n",
      "Epoch 247/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1916 - accuracy: 0.4967 - val_loss: 1.2226 - val_accuracy: 0.3125\n",
      "Epoch 248/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1907 - accuracy: 0.4967 - val_loss: 1.2261 - val_accuracy: 0.3125\n",
      "Epoch 249/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1909 - accuracy: 0.4967 - val_loss: 1.2246 - val_accuracy: 0.3125\n",
      "Epoch 250/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1910 - accuracy: 0.4961 - val_loss: 1.2247 - val_accuracy: 0.3125\n",
      "Epoch 251/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1906 - accuracy: 0.4950 - val_loss: 1.2250 - val_accuracy: 0.3125\n",
      "Epoch 252/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1910 - accuracy: 0.4950 - val_loss: 1.2260 - val_accuracy: 0.3125\n",
      "Epoch 253/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1911 - accuracy: 0.4944 - val_loss: 1.2275 - val_accuracy: 0.3125\n",
      "Epoch 254/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1911 - accuracy: 0.4955 - val_loss: 1.2251 - val_accuracy: 0.3125\n",
      "Epoch 255/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1914 - accuracy: 0.4961 - val_loss: 1.2244 - val_accuracy: 0.3125\n",
      "Epoch 256/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1913 - accuracy: 0.4967 - val_loss: 1.2242 - val_accuracy: 0.3125\n",
      "Epoch 257/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1912 - accuracy: 0.4955 - val_loss: 1.2286 - val_accuracy: 0.3125\n",
      "Epoch 258/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1923 - accuracy: 0.4939 - val_loss: 1.2247 - val_accuracy: 0.3125\n",
      "Epoch 259/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1913 - accuracy: 0.4961 - val_loss: 1.2229 - val_accuracy: 0.3125\n",
      "Epoch 260/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1910 - accuracy: 0.4950 - val_loss: 1.2283 - val_accuracy: 0.3125\n",
      "Epoch 261/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1907 - accuracy: 0.4961 - val_loss: 1.2245 - val_accuracy: 0.3125\n",
      "Epoch 262/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1905 - accuracy: 0.4972 - val_loss: 1.2198 - val_accuracy: 0.3125\n",
      "Epoch 263/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1906 - accuracy: 0.4961 - val_loss: 1.2262 - val_accuracy: 0.3125\n",
      "Epoch 264/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1910 - accuracy: 0.4955 - val_loss: 1.2251 - val_accuracy: 0.3125\n",
      "Epoch 265/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1904 - accuracy: 0.4961 - val_loss: 1.2257 - val_accuracy: 0.3125\n",
      "Epoch 266/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1902 - accuracy: 0.4944 - val_loss: 1.2243 - val_accuracy: 0.3125\n",
      "Epoch 267/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1902 - accuracy: 0.4950 - val_loss: 1.2256 - val_accuracy: 0.3125\n",
      "Epoch 268/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1906 - accuracy: 0.4950 - val_loss: 1.2217 - val_accuracy: 0.3125\n",
      "Epoch 269/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1912 - accuracy: 0.4950 - val_loss: 1.2207 - val_accuracy: 0.3125\n",
      "Epoch 270/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1909 - accuracy: 0.4989 - val_loss: 1.2240 - val_accuracy: 0.3125\n",
      "Epoch 271/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1904 - accuracy: 0.4961 - val_loss: 1.2246 - val_accuracy: 0.3125\n",
      "Epoch 272/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1899 - accuracy: 0.4961 - val_loss: 1.2264 - val_accuracy: 0.3125\n",
      "Epoch 273/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1906 - accuracy: 0.4972 - val_loss: 1.2224 - val_accuracy: 0.3125\n",
      "Epoch 274/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1897 - accuracy: 0.4961 - val_loss: 1.2236 - val_accuracy: 0.3125\n",
      "Epoch 275/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1900 - accuracy: 0.4955 - val_loss: 1.2191 - val_accuracy: 0.3125\n",
      "Epoch 276/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1902 - accuracy: 0.4955 - val_loss: 1.2206 - val_accuracy: 0.3125\n",
      "Epoch 277/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1904 - accuracy: 0.4967 - val_loss: 1.2237 - val_accuracy: 0.3125\n",
      "Epoch 278/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1899 - accuracy: 0.4950 - val_loss: 1.2259 - val_accuracy: 0.3125\n",
      "Epoch 279/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1912 - accuracy: 0.4955 - val_loss: 1.2277 - val_accuracy: 0.3125\n",
      "Epoch 280/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1913 - accuracy: 0.4961 - val_loss: 1.2241 - val_accuracy: 0.3125\n",
      "Epoch 281/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1901 - accuracy: 0.4967 - val_loss: 1.2228 - val_accuracy: 0.3125\n",
      "Epoch 282/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1903 - accuracy: 0.4972 - val_loss: 1.2245 - val_accuracy: 0.3125\n",
      "Epoch 283/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1900 - accuracy: 0.4978 - val_loss: 1.2218 - val_accuracy: 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 284/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1904 - accuracy: 0.4950 - val_loss: 1.2242 - val_accuracy: 0.3125\n",
      "Epoch 285/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1901 - accuracy: 0.4967 - val_loss: 1.2188 - val_accuracy: 0.3125\n",
      "Epoch 286/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1895 - accuracy: 0.4961 - val_loss: 1.2223 - val_accuracy: 0.3125\n",
      "Epoch 287/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1907 - accuracy: 0.4972 - val_loss: 1.2222 - val_accuracy: 0.3125\n",
      "Epoch 288/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1901 - accuracy: 0.4967 - val_loss: 1.2215 - val_accuracy: 0.3125\n",
      "Epoch 289/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1902 - accuracy: 0.4961 - val_loss: 1.2201 - val_accuracy: 0.3125\n",
      "Epoch 290/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1925 - accuracy: 0.4961 - val_loss: 1.2265 - val_accuracy: 0.3125\n",
      "Epoch 291/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1901 - accuracy: 0.4955 - val_loss: 1.2206 - val_accuracy: 0.3125\n",
      "Epoch 292/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1895 - accuracy: 0.4972 - val_loss: 1.2228 - val_accuracy: 0.3125\n",
      "Epoch 293/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1903 - accuracy: 0.4955 - val_loss: 1.2253 - val_accuracy: 0.3125\n",
      "Epoch 294/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1898 - accuracy: 0.4955 - val_loss: 1.2242 - val_accuracy: 0.3125\n",
      "Epoch 295/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1901 - accuracy: 0.4961 - val_loss: 1.2242 - val_accuracy: 0.3125\n",
      "Epoch 296/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1897 - accuracy: 0.4950 - val_loss: 1.2273 - val_accuracy: 0.3125\n",
      "Epoch 297/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1.1894 - accuracy: 0.4955 - val_loss: 1.2249 - val_accuracy: 0.3125\n",
      "Epoch 298/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1898 - accuracy: 0.4955 - val_loss: 1.2256 - val_accuracy: 0.3125\n",
      "Epoch 299/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1902 - accuracy: 0.4978 - val_loss: 1.2248 - val_accuracy: 0.3125\n",
      "Epoch 300/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1906 - accuracy: 0.4978 - val_loss: 1.2222 - val_accuracy: 0.3125\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "56/56 [==============================] - 0s 1ms/step\n",
      "Recall: [0.94 0.   0.56]\n",
      "Precision: [0.42 0.   0.72]\n",
      "F1-score: [0.58 0.   0.63]\n",
      "Accuracy: 0.5\n",
      "Brier score:0.20860418576605833\n",
      "Brier climat:0.21696924603174605\n",
      "Brier skill score:0.0385541288393646\n",
      "Recall: [0.94 0.   0.56]\n",
      "Precision: [0.42 0.   0.72]\n",
      "F1-score: [0.58 0.   0.63]\n",
      "Accuracy: 0.5\n",
      "Brier score:0.21919245337046553\n",
      "Brier climat:0.21945436507936508\n",
      "Brier skill score:0.0011934677572024155\n",
      "Recall: [0.94 0.   0.56]\n",
      "Precision: [0.42 0.   0.72]\n",
      "F1-score: [0.58 0.   0.63]\n",
      "Accuracy: 0.5\n",
      "Brier score:0.15103069414485515\n",
      "Brier climat:0.22280505952380952\n",
      "Brier skill score:0.3221397464328425\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "Recall: [1.   0.   0.52]\n",
      "Precision: [0.08 0.   0.69]\n",
      "F1-score: [0.14 0.   0.6 ]\n",
      "Accuracy: 0.31\n",
      "Brier score:0.18492235791500714\n",
      "Brier climat:0.11581597222222223\n",
      "Brier skill score:-0.5966913230256949\n",
      "Recall: [1.   0.   0.52]\n",
      "Precision: [0.08 0.   0.69]\n",
      "F1-score: [0.14 0.   0.6 ]\n",
      "Accuracy: 0.31\n",
      "Brier score:0.2884486930122093\n",
      "Brier climat:0.24743055555555557\n",
      "Brier skill score:-0.1657763624405877\n",
      "Recall: [1.   0.   0.52]\n",
      "Precision: [0.08 0.   0.69]\n",
      "F1-score: [0.14 0.   0.6 ]\n",
      "Accuracy: 0.31\n",
      "Brier score:0.24605003691777508\n",
      "Brier climat:0.2674479166666667\n",
      "Brier skill score:0.0800076516414252\n",
      "******** 23\n",
      "validation years [2004, 2005]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2006, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "{0: 2.0, 1: 1.0, 2: 1.0}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 60)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 183       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183\n",
      "Trainable params: 183\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "28/28 [==============================] - ETA: 0s - loss: 2.0215 - accuracy: 0.2891WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "28/28 [==============================] - 4s 115ms/step - loss: 2.0215 - accuracy: 0.2891 - val_loss: 1.4157 - val_accuracy: 0.4531\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.7399 - accuracy: 0.3945 - val_loss: 1.3257 - val_accuracy: 0.5234\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.6097 - accuracy: 0.4364 - val_loss: 1.2674 - val_accuracy: 0.4844\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.5384 - accuracy: 0.4420 - val_loss: 1.2295 - val_accuracy: 0.5000\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4956 - accuracy: 0.4464 - val_loss: 1.2067 - val_accuracy: 0.5469\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4699 - accuracy: 0.4621 - val_loss: 1.1889 - val_accuracy: 0.5703\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4495 - accuracy: 0.4732 - val_loss: 1.1740 - val_accuracy: 0.5625\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4345 - accuracy: 0.4799 - val_loss: 1.1675 - val_accuracy: 0.5625\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4213 - accuracy: 0.4888 - val_loss: 1.1489 - val_accuracy: 0.5781\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.4077 - accuracy: 0.4944 - val_loss: 1.1454 - val_accuracy: 0.5625\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3962 - accuracy: 0.4939 - val_loss: 1.1321 - val_accuracy: 0.5625\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3860 - accuracy: 0.4872 - val_loss: 1.1216 - val_accuracy: 0.5625\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3764 - accuracy: 0.4900 - val_loss: 1.1191 - val_accuracy: 0.5469\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3680 - accuracy: 0.4911 - val_loss: 1.1096 - val_accuracy: 0.5234\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3597 - accuracy: 0.4950 - val_loss: 1.1019 - val_accuracy: 0.5156\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3512 - accuracy: 0.4972 - val_loss: 1.0955 - val_accuracy: 0.5078\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3442 - accuracy: 0.4967 - val_loss: 1.0901 - val_accuracy: 0.5078\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3377 - accuracy: 0.4950 - val_loss: 1.0804 - val_accuracy: 0.5078\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3319 - accuracy: 0.4961 - val_loss: 1.0783 - val_accuracy: 0.5078\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3240 - accuracy: 0.4939 - val_loss: 1.0688 - val_accuracy: 0.5156\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3201 - accuracy: 0.4967 - val_loss: 1.0667 - val_accuracy: 0.5078\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3138 - accuracy: 0.4916 - val_loss: 1.0626 - val_accuracy: 0.5078\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3087 - accuracy: 0.4939 - val_loss: 1.0600 - val_accuracy: 0.5078\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3050 - accuracy: 0.4972 - val_loss: 1.0550 - val_accuracy: 0.5078\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3007 - accuracy: 0.4894 - val_loss: 1.0546 - val_accuracy: 0.5156\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2957 - accuracy: 0.4877 - val_loss: 1.0493 - val_accuracy: 0.5156\n",
      "Epoch 27/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2918 - accuracy: 0.4894 - val_loss: 1.0490 - val_accuracy: 0.5156\n",
      "Epoch 28/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2882 - accuracy: 0.4866 - val_loss: 1.0449 - val_accuracy: 0.5156\n",
      "Epoch 29/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2845 - accuracy: 0.4872 - val_loss: 1.0410 - val_accuracy: 0.5156\n",
      "Epoch 30/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2814 - accuracy: 0.4877 - val_loss: 1.0396 - val_accuracy: 0.5156\n",
      "Epoch 31/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2796 - accuracy: 0.4877 - val_loss: 1.0399 - val_accuracy: 0.5078\n",
      "Epoch 32/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2746 - accuracy: 0.4877 - val_loss: 1.0361 - val_accuracy: 0.5078\n",
      "Epoch 33/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2718 - accuracy: 0.4855 - val_loss: 1.0354 - val_accuracy: 0.5078\n",
      "Epoch 34/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2690 - accuracy: 0.4855 - val_loss: 1.0325 - val_accuracy: 0.5078\n",
      "Epoch 35/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2661 - accuracy: 0.4872 - val_loss: 1.0329 - val_accuracy: 0.5000\n",
      "Epoch 36/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2647 - accuracy: 0.4894 - val_loss: 1.0314 - val_accuracy: 0.5078\n",
      "Epoch 37/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2611 - accuracy: 0.4883 - val_loss: 1.0269 - val_accuracy: 0.5078\n",
      "Epoch 38/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2596 - accuracy: 0.4833 - val_loss: 1.0271 - val_accuracy: 0.5078\n",
      "Epoch 39/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2567 - accuracy: 0.4866 - val_loss: 1.0259 - val_accuracy: 0.5078\n",
      "Epoch 40/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2546 - accuracy: 0.4894 - val_loss: 1.0233 - val_accuracy: 0.5078\n",
      "Epoch 41/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2523 - accuracy: 0.4900 - val_loss: 1.0215 - val_accuracy: 0.5078\n",
      "Epoch 42/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2512 - accuracy: 0.4866 - val_loss: 1.0212 - val_accuracy: 0.5078\n",
      "Epoch 43/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2492 - accuracy: 0.4894 - val_loss: 1.0200 - val_accuracy: 0.5078\n",
      "Epoch 44/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2472 - accuracy: 0.4888 - val_loss: 1.0207 - val_accuracy: 0.5000\n",
      "Epoch 45/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2451 - accuracy: 0.4922 - val_loss: 1.0161 - val_accuracy: 0.5078\n",
      "Epoch 46/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2442 - accuracy: 0.4911 - val_loss: 1.0163 - val_accuracy: 0.5078\n",
      "Epoch 47/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2428 - accuracy: 0.4905 - val_loss: 1.0138 - val_accuracy: 0.5078\n",
      "Epoch 48/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2409 - accuracy: 0.4944 - val_loss: 1.0109 - val_accuracy: 0.5078\n",
      "Epoch 49/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2390 - accuracy: 0.4961 - val_loss: 1.0127 - val_accuracy: 0.5000\n",
      "Epoch 50/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2388 - accuracy: 0.4939 - val_loss: 1.0114 - val_accuracy: 0.5000\n",
      "Epoch 51/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2367 - accuracy: 0.4916 - val_loss: 1.0078 - val_accuracy: 0.5078\n",
      "Epoch 52/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2345 - accuracy: 0.4927 - val_loss: 1.0110 - val_accuracy: 0.5000\n",
      "Epoch 53/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2337 - accuracy: 0.4927 - val_loss: 1.0102 - val_accuracy: 0.5000\n",
      "Epoch 54/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2339 - accuracy: 0.4911 - val_loss: 1.0088 - val_accuracy: 0.5000\n",
      "Epoch 55/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2323 - accuracy: 0.4916 - val_loss: 1.0063 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2296 - accuracy: 0.4944 - val_loss: 1.0079 - val_accuracy: 0.5000\n",
      "Epoch 57/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2283 - accuracy: 0.4944 - val_loss: 1.0069 - val_accuracy: 0.5000\n",
      "Epoch 58/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2279 - accuracy: 0.4916 - val_loss: 1.0072 - val_accuracy: 0.5000\n",
      "Epoch 59/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2268 - accuracy: 0.4922 - val_loss: 1.0029 - val_accuracy: 0.5000\n",
      "Epoch 60/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2255 - accuracy: 0.4927 - val_loss: 1.0023 - val_accuracy: 0.5000\n",
      "Epoch 61/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2247 - accuracy: 0.4894 - val_loss: 1.0017 - val_accuracy: 0.5000\n",
      "Epoch 62/300\n",
      "28/28 [==============================] - 1s 29ms/step - loss: 1.2231 - accuracy: 0.4911 - val_loss: 1.0037 - val_accuracy: 0.5000\n",
      "Epoch 63/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2223 - accuracy: 0.4911 - val_loss: 1.0033 - val_accuracy: 0.5000\n",
      "Epoch 64/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2225 - accuracy: 0.4911 - val_loss: 0.9978 - val_accuracy: 0.5078\n",
      "Epoch 65/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2209 - accuracy: 0.4900 - val_loss: 0.9986 - val_accuracy: 0.5000\n",
      "Epoch 66/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2190 - accuracy: 0.4911 - val_loss: 1.0015 - val_accuracy: 0.5000\n",
      "Epoch 67/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2183 - accuracy: 0.4905 - val_loss: 0.9999 - val_accuracy: 0.5000\n",
      "Epoch 68/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2169 - accuracy: 0.4911 - val_loss: 0.9966 - val_accuracy: 0.5000\n",
      "Epoch 69/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2161 - accuracy: 0.4900 - val_loss: 0.9981 - val_accuracy: 0.5000\n",
      "Epoch 70/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2155 - accuracy: 0.4911 - val_loss: 0.9964 - val_accuracy: 0.5000\n",
      "Epoch 71/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2141 - accuracy: 0.4939 - val_loss: 0.9956 - val_accuracy: 0.5000\n",
      "Epoch 72/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2135 - accuracy: 0.4922 - val_loss: 0.9978 - val_accuracy: 0.5000\n",
      "Epoch 73/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2139 - accuracy: 0.4939 - val_loss: 0.9942 - val_accuracy: 0.4922\n",
      "Epoch 74/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2123 - accuracy: 0.4905 - val_loss: 0.9936 - val_accuracy: 0.5078\n",
      "Epoch 75/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2111 - accuracy: 0.4944 - val_loss: 0.9941 - val_accuracy: 0.5000\n",
      "Epoch 76/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2105 - accuracy: 0.4939 - val_loss: 0.9918 - val_accuracy: 0.5000\n",
      "Epoch 77/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2096 - accuracy: 0.4939 - val_loss: 0.9927 - val_accuracy: 0.5000\n",
      "Epoch 78/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2105 - accuracy: 0.4883 - val_loss: 0.9899 - val_accuracy: 0.4922\n",
      "Epoch 79/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2098 - accuracy: 0.4939 - val_loss: 0.9920 - val_accuracy: 0.4922\n",
      "Epoch 80/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2093 - accuracy: 0.4933 - val_loss: 0.9923 - val_accuracy: 0.5000\n",
      "Epoch 81/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2081 - accuracy: 0.4944 - val_loss: 0.9905 - val_accuracy: 0.4922\n",
      "Epoch 82/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2070 - accuracy: 0.4927 - val_loss: 0.9933 - val_accuracy: 0.4922\n",
      "Epoch 83/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2071 - accuracy: 0.4916 - val_loss: 0.9886 - val_accuracy: 0.5000\n",
      "Epoch 84/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2071 - accuracy: 0.4927 - val_loss: 0.9927 - val_accuracy: 0.4922\n",
      "Epoch 85/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2060 - accuracy: 0.4916 - val_loss: 0.9926 - val_accuracy: 0.5000\n",
      "Epoch 86/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2064 - accuracy: 0.4900 - val_loss: 0.9907 - val_accuracy: 0.5000\n",
      "Epoch 87/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2060 - accuracy: 0.4922 - val_loss: 0.9920 - val_accuracy: 0.4922\n",
      "Epoch 88/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2058 - accuracy: 0.4922 - val_loss: 0.9928 - val_accuracy: 0.5000\n",
      "Epoch 89/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2061 - accuracy: 0.4916 - val_loss: 0.9915 - val_accuracy: 0.4922\n",
      "Epoch 90/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2062 - accuracy: 0.4849 - val_loss: 0.9931 - val_accuracy: 0.5000\n",
      "Epoch 91/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2054 - accuracy: 0.4922 - val_loss: 0.9959 - val_accuracy: 0.4922\n",
      "Epoch 92/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2054 - accuracy: 0.4916 - val_loss: 0.9952 - val_accuracy: 0.4922\n",
      "Epoch 93/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2058 - accuracy: 0.4883 - val_loss: 0.9924 - val_accuracy: 0.5078\n",
      "Epoch 94/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2059 - accuracy: 0.4916 - val_loss: 0.9941 - val_accuracy: 0.4922\n",
      "Epoch 95/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2055 - accuracy: 0.4939 - val_loss: 0.9922 - val_accuracy: 0.4922\n",
      "Epoch 96/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2056 - accuracy: 0.4939 - val_loss: 0.9941 - val_accuracy: 0.4922\n",
      "Epoch 97/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2046 - accuracy: 0.4883 - val_loss: 0.9948 - val_accuracy: 0.4922\n",
      "Epoch 98/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2038 - accuracy: 0.4905 - val_loss: 0.9972 - val_accuracy: 0.4922\n",
      "Epoch 99/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2042 - accuracy: 0.4888 - val_loss: 0.9929 - val_accuracy: 0.5000\n",
      "Epoch 100/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2038 - accuracy: 0.4883 - val_loss: 0.9951 - val_accuracy: 0.4922\n",
      "Epoch 101/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2041 - accuracy: 0.4888 - val_loss: 0.9962 - val_accuracy: 0.4922\n",
      "Epoch 102/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2036 - accuracy: 0.4916 - val_loss: 0.9952 - val_accuracy: 0.5000\n",
      "Epoch 103/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2031 - accuracy: 0.4894 - val_loss: 0.9948 - val_accuracy: 0.5000\n",
      "Epoch 104/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2027 - accuracy: 0.4888 - val_loss: 0.9935 - val_accuracy: 0.5000\n",
      "Epoch 105/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2027 - accuracy: 0.4916 - val_loss: 0.9929 - val_accuracy: 0.5000\n",
      "Epoch 106/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2039 - accuracy: 0.4877 - val_loss: 0.9931 - val_accuracy: 0.5078\n",
      "Epoch 107/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2022 - accuracy: 0.4900 - val_loss: 0.9953 - val_accuracy: 0.5000\n",
      "Epoch 108/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2037 - accuracy: 0.4877 - val_loss: 0.9990 - val_accuracy: 0.4922\n",
      "Epoch 109/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2026 - accuracy: 0.4905 - val_loss: 0.9950 - val_accuracy: 0.4922\n",
      "Epoch 110/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2026 - accuracy: 0.4922 - val_loss: 0.9964 - val_accuracy: 0.5000\n",
      "Epoch 111/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2023 - accuracy: 0.4905 - val_loss: 0.9943 - val_accuracy: 0.5078\n",
      "Epoch 112/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2018 - accuracy: 0.4883 - val_loss: 0.9956 - val_accuracy: 0.4922\n",
      "Epoch 113/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2015 - accuracy: 0.4900 - val_loss: 0.9932 - val_accuracy: 0.5078\n",
      "Epoch 114/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2021 - accuracy: 0.4888 - val_loss: 0.9959 - val_accuracy: 0.4922\n",
      "Epoch 115/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2017 - accuracy: 0.4900 - val_loss: 0.9948 - val_accuracy: 0.5078\n",
      "Epoch 116/300\n",
      "28/28 [==============================] - 1s 29ms/step - loss: 1.2013 - accuracy: 0.4905 - val_loss: 0.9908 - val_accuracy: 0.5000\n",
      "Epoch 117/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2014 - accuracy: 0.4888 - val_loss: 0.9933 - val_accuracy: 0.5156\n",
      "Epoch 118/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2012 - accuracy: 0.4894 - val_loss: 0.9959 - val_accuracy: 0.5000\n",
      "Epoch 119/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2007 - accuracy: 0.4905 - val_loss: 0.9949 - val_accuracy: 0.5000\n",
      "Epoch 120/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2008 - accuracy: 0.4916 - val_loss: 0.9931 - val_accuracy: 0.5000\n",
      "Epoch 121/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2009 - accuracy: 0.4883 - val_loss: 0.9955 - val_accuracy: 0.4922\n",
      "Epoch 122/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2017 - accuracy: 0.4916 - val_loss: 0.9951 - val_accuracy: 0.5000\n",
      "Epoch 123/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2013 - accuracy: 0.4894 - val_loss: 0.9942 - val_accuracy: 0.5078\n",
      "Epoch 124/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2011 - accuracy: 0.4872 - val_loss: 0.9953 - val_accuracy: 0.5000\n",
      "Epoch 125/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1999 - accuracy: 0.4911 - val_loss: 0.9953 - val_accuracy: 0.5000\n",
      "Epoch 126/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2002 - accuracy: 0.4905 - val_loss: 0.9947 - val_accuracy: 0.5000\n",
      "Epoch 127/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1994 - accuracy: 0.4894 - val_loss: 0.9938 - val_accuracy: 0.5000\n",
      "Epoch 128/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1996 - accuracy: 0.4911 - val_loss: 0.9945 - val_accuracy: 0.5000\n",
      "Epoch 129/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1997 - accuracy: 0.4894 - val_loss: 0.9942 - val_accuracy: 0.5000\n",
      "Epoch 130/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1998 - accuracy: 0.4905 - val_loss: 0.9932 - val_accuracy: 0.5000\n",
      "Epoch 131/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1998 - accuracy: 0.4900 - val_loss: 0.9933 - val_accuracy: 0.5000\n",
      "Epoch 132/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1993 - accuracy: 0.4916 - val_loss: 0.9966 - val_accuracy: 0.5000\n",
      "Epoch 133/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1990 - accuracy: 0.4911 - val_loss: 0.9931 - val_accuracy: 0.5078\n",
      "Epoch 134/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1990 - accuracy: 0.4900 - val_loss: 0.9961 - val_accuracy: 0.5000\n",
      "Epoch 135/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1994 - accuracy: 0.4888 - val_loss: 0.9944 - val_accuracy: 0.5000\n",
      "Epoch 136/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1996 - accuracy: 0.4939 - val_loss: 0.9972 - val_accuracy: 0.5000\n",
      "Epoch 137/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1992 - accuracy: 0.4916 - val_loss: 0.9930 - val_accuracy: 0.5000\n",
      "Epoch 138/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1985 - accuracy: 0.4900 - val_loss: 0.9956 - val_accuracy: 0.5000\n",
      "Epoch 139/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1988 - accuracy: 0.4911 - val_loss: 0.9961 - val_accuracy: 0.5000\n",
      "Epoch 140/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1986 - accuracy: 0.4894 - val_loss: 0.9950 - val_accuracy: 0.5000\n",
      "Epoch 141/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1989 - accuracy: 0.4905 - val_loss: 0.9960 - val_accuracy: 0.5000\n",
      "Epoch 142/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1988 - accuracy: 0.4922 - val_loss: 0.9946 - val_accuracy: 0.5000\n",
      "Epoch 143/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1985 - accuracy: 0.4905 - val_loss: 0.9940 - val_accuracy: 0.5000\n",
      "Epoch 144/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1982 - accuracy: 0.4922 - val_loss: 0.9949 - val_accuracy: 0.5000\n",
      "Epoch 145/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1983 - accuracy: 0.4922 - val_loss: 0.9956 - val_accuracy: 0.5000\n",
      "Epoch 146/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1980 - accuracy: 0.4911 - val_loss: 0.9948 - val_accuracy: 0.5000\n",
      "Epoch 147/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1980 - accuracy: 0.4888 - val_loss: 0.9972 - val_accuracy: 0.5000\n",
      "Epoch 148/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1978 - accuracy: 0.4922 - val_loss: 0.9954 - val_accuracy: 0.5000\n",
      "Epoch 149/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1980 - accuracy: 0.4905 - val_loss: 0.9952 - val_accuracy: 0.5000\n",
      "Epoch 150/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1983 - accuracy: 0.4888 - val_loss: 0.9962 - val_accuracy: 0.5000\n",
      "Epoch 151/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1974 - accuracy: 0.4916 - val_loss: 0.9977 - val_accuracy: 0.5000\n",
      "Epoch 152/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1971 - accuracy: 0.4922 - val_loss: 0.9956 - val_accuracy: 0.5000\n",
      "Epoch 153/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1982 - accuracy: 0.4900 - val_loss: 0.9954 - val_accuracy: 0.5000\n",
      "Epoch 154/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1974 - accuracy: 0.4905 - val_loss: 0.9969 - val_accuracy: 0.5000\n",
      "Epoch 155/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1971 - accuracy: 0.4916 - val_loss: 0.9949 - val_accuracy: 0.5000\n",
      "Epoch 156/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1975 - accuracy: 0.4911 - val_loss: 0.9974 - val_accuracy: 0.5000\n",
      "Epoch 157/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1968 - accuracy: 0.4911 - val_loss: 0.9942 - val_accuracy: 0.5000\n",
      "Epoch 158/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1971 - accuracy: 0.4922 - val_loss: 0.9960 - val_accuracy: 0.5000\n",
      "Epoch 159/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1966 - accuracy: 0.4905 - val_loss: 0.9939 - val_accuracy: 0.5078\n",
      "Epoch 160/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1964 - accuracy: 0.4911 - val_loss: 0.9937 - val_accuracy: 0.5078\n",
      "Epoch 161/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1963 - accuracy: 0.4905 - val_loss: 0.9954 - val_accuracy: 0.5000\n",
      "Epoch 162/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1968 - accuracy: 0.4911 - val_loss: 0.9950 - val_accuracy: 0.5000\n",
      "Epoch 163/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1964 - accuracy: 0.4894 - val_loss: 0.9958 - val_accuracy: 0.5000\n",
      "Epoch 164/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1961 - accuracy: 0.4922 - val_loss: 0.9920 - val_accuracy: 0.5078\n",
      "Epoch 165/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1960 - accuracy: 0.4911 - val_loss: 0.9935 - val_accuracy: 0.5000\n",
      "Epoch 166/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1971 - accuracy: 0.4888 - val_loss: 0.9927 - val_accuracy: 0.5000\n",
      "Epoch 167/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1958 - accuracy: 0.4900 - val_loss: 0.9943 - val_accuracy: 0.5000\n",
      "Epoch 168/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1959 - accuracy: 0.4911 - val_loss: 0.9958 - val_accuracy: 0.5000\n",
      "Epoch 169/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1965 - accuracy: 0.4922 - val_loss: 0.9933 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1967 - accuracy: 0.4927 - val_loss: 0.9952 - val_accuracy: 0.5000\n",
      "Epoch 171/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1961 - accuracy: 0.4939 - val_loss: 0.9956 - val_accuracy: 0.5000\n",
      "Epoch 172/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1964 - accuracy: 0.4911 - val_loss: 0.9946 - val_accuracy: 0.5078\n",
      "Epoch 173/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1955 - accuracy: 0.4894 - val_loss: 0.9951 - val_accuracy: 0.5000\n",
      "Epoch 174/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1960 - accuracy: 0.4911 - val_loss: 0.9965 - val_accuracy: 0.5000\n",
      "Epoch 175/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1962 - accuracy: 0.4894 - val_loss: 0.9927 - val_accuracy: 0.5078\n",
      "Epoch 176/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1964 - accuracy: 0.4922 - val_loss: 0.9967 - val_accuracy: 0.5000\n",
      "Epoch 177/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1965 - accuracy: 0.4911 - val_loss: 0.9942 - val_accuracy: 0.5078\n",
      "Epoch 178/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1953 - accuracy: 0.4916 - val_loss: 0.9947 - val_accuracy: 0.5078\n",
      "Epoch 179/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1962 - accuracy: 0.4916 - val_loss: 0.9922 - val_accuracy: 0.5078\n",
      "Epoch 180/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1959 - accuracy: 0.4905 - val_loss: 0.9939 - val_accuracy: 0.5078\n",
      "Epoch 181/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1955 - accuracy: 0.4900 - val_loss: 0.9944 - val_accuracy: 0.5000\n",
      "Epoch 182/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1955 - accuracy: 0.4916 - val_loss: 0.9945 - val_accuracy: 0.5000\n",
      "Epoch 183/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1958 - accuracy: 0.4905 - val_loss: 0.9962 - val_accuracy: 0.5000\n",
      "Epoch 184/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1955 - accuracy: 0.4900 - val_loss: 0.9922 - val_accuracy: 0.5078\n",
      "Epoch 185/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1950 - accuracy: 0.4922 - val_loss: 0.9922 - val_accuracy: 0.5078\n",
      "Epoch 186/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1950 - accuracy: 0.4894 - val_loss: 0.9944 - val_accuracy: 0.5000\n",
      "Epoch 187/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1950 - accuracy: 0.4905 - val_loss: 0.9935 - val_accuracy: 0.5078\n",
      "Epoch 188/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1949 - accuracy: 0.4922 - val_loss: 0.9957 - val_accuracy: 0.5078\n",
      "Epoch 189/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1950 - accuracy: 0.4905 - val_loss: 0.9934 - val_accuracy: 0.5078\n",
      "Epoch 190/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1954 - accuracy: 0.4888 - val_loss: 0.9943 - val_accuracy: 0.5078\n",
      "Epoch 191/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1954 - accuracy: 0.4888 - val_loss: 0.9978 - val_accuracy: 0.5000\n",
      "Epoch 192/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1951 - accuracy: 0.4911 - val_loss: 0.9934 - val_accuracy: 0.5078\n",
      "Epoch 193/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1947 - accuracy: 0.4905 - val_loss: 0.9956 - val_accuracy: 0.5000\n",
      "Epoch 194/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1950 - accuracy: 0.4905 - val_loss: 0.9949 - val_accuracy: 0.5000\n",
      "Epoch 195/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1960 - accuracy: 0.4894 - val_loss: 0.9923 - val_accuracy: 0.5078\n",
      "Epoch 196/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1951 - accuracy: 0.4916 - val_loss: 0.9936 - val_accuracy: 0.5078\n",
      "Epoch 197/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1955 - accuracy: 0.4916 - val_loss: 0.9971 - val_accuracy: 0.5000\n",
      "Epoch 198/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1952 - accuracy: 0.4888 - val_loss: 0.9895 - val_accuracy: 0.5078\n",
      "Epoch 199/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1945 - accuracy: 0.4933 - val_loss: 0.9948 - val_accuracy: 0.5000\n",
      "Epoch 200/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1951 - accuracy: 0.4900 - val_loss: 0.9951 - val_accuracy: 0.5078\n",
      "Epoch 201/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1945 - accuracy: 0.4922 - val_loss: 0.9937 - val_accuracy: 0.5078\n",
      "Epoch 202/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1951 - accuracy: 0.4883 - val_loss: 0.9947 - val_accuracy: 0.5078\n",
      "Epoch 203/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1955 - accuracy: 0.4905 - val_loss: 0.9969 - val_accuracy: 0.5078\n",
      "Epoch 204/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1945 - accuracy: 0.4933 - val_loss: 0.9935 - val_accuracy: 0.5078\n",
      "Epoch 205/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1944 - accuracy: 0.4922 - val_loss: 0.9959 - val_accuracy: 0.5078\n",
      "Epoch 206/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1947 - accuracy: 0.4905 - val_loss: 0.9948 - val_accuracy: 0.5078\n",
      "Epoch 207/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1944 - accuracy: 0.4883 - val_loss: 0.9978 - val_accuracy: 0.5078\n",
      "Epoch 208/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1939 - accuracy: 0.4911 - val_loss: 0.9950 - val_accuracy: 0.5000\n",
      "Epoch 209/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1950 - accuracy: 0.4900 - val_loss: 0.9951 - val_accuracy: 0.5078\n",
      "Epoch 210/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1947 - accuracy: 0.4916 - val_loss: 0.9951 - val_accuracy: 0.5078\n",
      "Epoch 211/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1940 - accuracy: 0.4911 - val_loss: 0.9934 - val_accuracy: 0.5000\n",
      "Epoch 212/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1946 - accuracy: 0.4916 - val_loss: 0.9944 - val_accuracy: 0.5078\n",
      "Epoch 213/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1942 - accuracy: 0.4911 - val_loss: 0.9959 - val_accuracy: 0.5000\n",
      "Epoch 214/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1938 - accuracy: 0.4927 - val_loss: 0.9927 - val_accuracy: 0.5078\n",
      "Epoch 215/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1950 - accuracy: 0.4916 - val_loss: 0.9931 - val_accuracy: 0.5078\n",
      "Epoch 216/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1951 - accuracy: 0.4905 - val_loss: 0.9909 - val_accuracy: 0.5078\n",
      "Epoch 217/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1946 - accuracy: 0.4894 - val_loss: 0.9961 - val_accuracy: 0.5078\n",
      "Epoch 218/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1941 - accuracy: 0.4927 - val_loss: 0.9943 - val_accuracy: 0.5078\n",
      "Epoch 219/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1942 - accuracy: 0.4933 - val_loss: 0.9943 - val_accuracy: 0.5078\n",
      "Epoch 220/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1943 - accuracy: 0.4883 - val_loss: 0.9944 - val_accuracy: 0.5078\n",
      "Epoch 221/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1939 - accuracy: 0.4933 - val_loss: 0.9928 - val_accuracy: 0.5078\n",
      "Epoch 222/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1945 - accuracy: 0.4927 - val_loss: 0.9958 - val_accuracy: 0.5078\n",
      "Epoch 223/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1946 - accuracy: 0.4916 - val_loss: 0.9961 - val_accuracy: 0.5000\n",
      "Epoch 224/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1949 - accuracy: 0.4916 - val_loss: 0.9926 - val_accuracy: 0.5078\n",
      "Epoch 225/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1939 - accuracy: 0.4922 - val_loss: 0.9950 - val_accuracy: 0.5078\n",
      "Epoch 226/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1942 - accuracy: 0.4916 - val_loss: 0.9942 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1943 - accuracy: 0.4911 - val_loss: 0.9960 - val_accuracy: 0.5078\n",
      "Epoch 228/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1946 - accuracy: 0.4922 - val_loss: 0.9950 - val_accuracy: 0.5078\n",
      "Epoch 229/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1942 - accuracy: 0.4922 - val_loss: 0.9925 - val_accuracy: 0.5078\n",
      "Epoch 230/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1940 - accuracy: 0.4922 - val_loss: 0.9935 - val_accuracy: 0.5078\n",
      "Epoch 231/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1941 - accuracy: 0.4916 - val_loss: 0.9938 - val_accuracy: 0.5078\n",
      "Epoch 232/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1944 - accuracy: 0.4944 - val_loss: 0.9926 - val_accuracy: 0.5078\n",
      "Epoch 233/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1938 - accuracy: 0.4911 - val_loss: 0.9947 - val_accuracy: 0.5078\n",
      "Epoch 234/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1943 - accuracy: 0.4916 - val_loss: 0.9921 - val_accuracy: 0.5078\n",
      "Epoch 235/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1945 - accuracy: 0.4900 - val_loss: 0.9951 - val_accuracy: 0.5078\n",
      "Epoch 236/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1955 - accuracy: 0.4916 - val_loss: 0.9926 - val_accuracy: 0.5078\n",
      "Epoch 237/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1937 - accuracy: 0.4911 - val_loss: 0.9947 - val_accuracy: 0.5078\n",
      "Epoch 238/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1942 - accuracy: 0.4939 - val_loss: 0.9907 - val_accuracy: 0.5078\n",
      "Epoch 239/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1943 - accuracy: 0.4894 - val_loss: 0.9956 - val_accuracy: 0.5078\n",
      "Epoch 240/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1935 - accuracy: 0.4927 - val_loss: 0.9928 - val_accuracy: 0.5078\n",
      "Epoch 241/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1939 - accuracy: 0.4927 - val_loss: 0.9939 - val_accuracy: 0.5078\n",
      "Epoch 242/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1938 - accuracy: 0.4933 - val_loss: 0.9927 - val_accuracy: 0.5078\n",
      "Epoch 243/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1943 - accuracy: 0.4905 - val_loss: 0.9953 - val_accuracy: 0.5078\n",
      "Epoch 244/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1944 - accuracy: 0.4900 - val_loss: 0.9906 - val_accuracy: 0.5078\n",
      "Epoch 245/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1940 - accuracy: 0.4927 - val_loss: 0.9950 - val_accuracy: 0.5078\n",
      "Epoch 246/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1935 - accuracy: 0.4922 - val_loss: 0.9943 - val_accuracy: 0.5078\n",
      "Epoch 247/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1935 - accuracy: 0.4905 - val_loss: 0.9941 - val_accuracy: 0.5078\n",
      "Epoch 248/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1954 - accuracy: 0.4927 - val_loss: 0.9925 - val_accuracy: 0.5078\n",
      "Epoch 249/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1947 - accuracy: 0.4922 - val_loss: 0.9939 - val_accuracy: 0.5078\n",
      "Epoch 250/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1935 - accuracy: 0.4911 - val_loss: 0.9938 - val_accuracy: 0.5078\n",
      "Epoch 251/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1934 - accuracy: 0.4933 - val_loss: 0.9958 - val_accuracy: 0.5078\n",
      "Epoch 252/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1935 - accuracy: 0.4922 - val_loss: 0.9952 - val_accuracy: 0.5078\n",
      "Epoch 253/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1936 - accuracy: 0.4916 - val_loss: 0.9938 - val_accuracy: 0.5078\n",
      "Epoch 254/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1936 - accuracy: 0.4900 - val_loss: 0.9953 - val_accuracy: 0.5078\n",
      "Epoch 255/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1937 - accuracy: 0.4927 - val_loss: 0.9940 - val_accuracy: 0.5078\n",
      "Epoch 256/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1932 - accuracy: 0.4927 - val_loss: 0.9928 - val_accuracy: 0.5078\n",
      "Epoch 257/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1933 - accuracy: 0.4916 - val_loss: 0.9945 - val_accuracy: 0.5078\n",
      "Epoch 258/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1932 - accuracy: 0.4922 - val_loss: 0.9947 - val_accuracy: 0.5078\n",
      "Epoch 259/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1931 - accuracy: 0.4927 - val_loss: 0.9932 - val_accuracy: 0.5078\n",
      "Epoch 260/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1933 - accuracy: 0.4927 - val_loss: 0.9941 - val_accuracy: 0.5078\n",
      "Epoch 261/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1935 - accuracy: 0.4939 - val_loss: 0.9919 - val_accuracy: 0.5078\n",
      "Epoch 262/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1929 - accuracy: 0.4916 - val_loss: 0.9960 - val_accuracy: 0.5078\n",
      "Epoch 263/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1934 - accuracy: 0.4927 - val_loss: 0.9937 - val_accuracy: 0.5078\n",
      "Epoch 264/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1937 - accuracy: 0.4927 - val_loss: 0.9946 - val_accuracy: 0.5078\n",
      "Epoch 265/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1934 - accuracy: 0.4916 - val_loss: 0.9935 - val_accuracy: 0.5078\n",
      "Epoch 266/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1928 - accuracy: 0.4927 - val_loss: 0.9931 - val_accuracy: 0.5078\n",
      "Epoch 267/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1930 - accuracy: 0.4922 - val_loss: 0.9953 - val_accuracy: 0.5078\n",
      "Epoch 268/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1930 - accuracy: 0.4916 - val_loss: 0.9954 - val_accuracy: 0.5078\n",
      "Epoch 269/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1924 - accuracy: 0.4911 - val_loss: 0.9924 - val_accuracy: 0.5078\n",
      "Epoch 270/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1929 - accuracy: 0.4916 - val_loss: 0.9924 - val_accuracy: 0.5078\n",
      "Epoch 271/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1935 - accuracy: 0.4927 - val_loss: 0.9916 - val_accuracy: 0.5078\n",
      "Epoch 272/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1933 - accuracy: 0.4927 - val_loss: 0.9930 - val_accuracy: 0.5078\n",
      "Epoch 273/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1937 - accuracy: 0.4933 - val_loss: 0.9956 - val_accuracy: 0.5078\n",
      "Epoch 274/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1928 - accuracy: 0.4922 - val_loss: 0.9944 - val_accuracy: 0.5078\n",
      "Epoch 275/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1932 - accuracy: 0.4916 - val_loss: 0.9931 - val_accuracy: 0.5078\n",
      "Epoch 276/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1934 - accuracy: 0.4916 - val_loss: 0.9953 - val_accuracy: 0.5078\n",
      "Epoch 277/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1933 - accuracy: 0.4905 - val_loss: 0.9948 - val_accuracy: 0.5078\n",
      "Epoch 278/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1932 - accuracy: 0.4922 - val_loss: 0.9953 - val_accuracy: 0.5078\n",
      "Epoch 279/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1925 - accuracy: 0.4927 - val_loss: 0.9924 - val_accuracy: 0.5078\n",
      "Epoch 280/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1928 - accuracy: 0.4905 - val_loss: 0.9932 - val_accuracy: 0.5078\n",
      "Epoch 281/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1929 - accuracy: 0.4922 - val_loss: 0.9954 - val_accuracy: 0.5078\n",
      "Epoch 282/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1934 - accuracy: 0.4922 - val_loss: 0.9934 - val_accuracy: 0.5078\n",
      "Epoch 283/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1933 - accuracy: 0.4911 - val_loss: 0.9950 - val_accuracy: 0.5078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 284/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1931 - accuracy: 0.4927 - val_loss: 0.9901 - val_accuracy: 0.5078\n",
      "Epoch 285/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1935 - accuracy: 0.4927 - val_loss: 0.9929 - val_accuracy: 0.5078\n",
      "Epoch 286/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1932 - accuracy: 0.4916 - val_loss: 0.9962 - val_accuracy: 0.5078\n",
      "Epoch 287/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1934 - accuracy: 0.4911 - val_loss: 0.9944 - val_accuracy: 0.5078\n",
      "Epoch 288/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1928 - accuracy: 0.4911 - val_loss: 0.9948 - val_accuracy: 0.5078\n",
      "Epoch 289/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1929 - accuracy: 0.4905 - val_loss: 0.9922 - val_accuracy: 0.5078\n",
      "Epoch 290/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1930 - accuracy: 0.4911 - val_loss: 0.9953 - val_accuracy: 0.5078\n",
      "Epoch 291/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1924 - accuracy: 0.4927 - val_loss: 0.9936 - val_accuracy: 0.5078\n",
      "Epoch 292/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1927 - accuracy: 0.4933 - val_loss: 0.9914 - val_accuracy: 0.5078\n",
      "Epoch 293/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1931 - accuracy: 0.4905 - val_loss: 0.9929 - val_accuracy: 0.5078\n",
      "Epoch 294/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1928 - accuracy: 0.4916 - val_loss: 0.9927 - val_accuracy: 0.5078\n",
      "Epoch 295/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1925 - accuracy: 0.4916 - val_loss: 0.9951 - val_accuracy: 0.5078\n",
      "Epoch 296/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1927 - accuracy: 0.4916 - val_loss: 0.9928 - val_accuracy: 0.5078\n",
      "Epoch 297/300\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 1.1938 - accuracy: 0.4927 - val_loss: 0.9936 - val_accuracy: 0.5078\n",
      "Epoch 298/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1926 - accuracy: 0.4916 - val_loss: 0.9939 - val_accuracy: 0.5078\n",
      "Epoch 299/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1924 - accuracy: 0.4933 - val_loss: 0.9952 - val_accuracy: 0.5078\n",
      "Epoch 300/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1919 - accuracy: 0.4922 - val_loss: 0.9934 - val_accuracy: 0.5078\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "56/56 [==============================] - 0s 1ms/step\n",
      "Recall: [0.94 0.   0.57]\n",
      "Precision: [0.4  0.   0.71]\n",
      "F1-score: [0.57 0.   0.64]\n",
      "Accuracy: 0.49\n",
      "Brier score:0.2034816758036305\n",
      "Brier climat:0.2108308531746032\n",
      "Brier skill score:0.03485816833879796\n",
      "Recall: [0.94 0.   0.57]\n",
      "Precision: [0.4  0.   0.71]\n",
      "F1-score: [0.57 0.   0.64]\n",
      "Accuracy: 0.49\n",
      "Brier score:0.22154234548058235\n",
      "Brier climat:0.22034722222222222\n",
      "Brier skill score:-0.00542381812807613\n",
      "Recall: [0.94 0.   0.57]\n",
      "Precision: [0.4  0.   0.71]\n",
      "F1-score: [0.57 0.   0.64]\n",
      "Accuracy: 0.49\n",
      "Brier score:0.15410695207082975\n",
      "Brier climat:0.22563244047619047\n",
      "Brier skill score:0.31700002116011483\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "Recall: [0.83 0.   0.76]\n",
      "Precision: [0.41 0.   0.64]\n",
      "F1-score: [0.55 0.   0.69]\n",
      "Accuracy: 0.51\n",
      "Brier score:0.17661762229022276\n",
      "Brier climat:0.20175347222222223\n",
      "Brier skill score:0.12458695087197047\n",
      "Recall: [0.83 0.   0.76]\n",
      "Precision: [0.41 0.   0.64]\n",
      "F1-score: [0.55 0.   0.69]\n",
      "Accuracy: 0.51\n",
      "Brier score:0.23097027098414302\n",
      "Brier climat:0.23493055555555556\n",
      "Brier skill score:0.016857256230665252\n",
      "Recall: [0.83 0.   0.76]\n",
      "Precision: [0.41 0.   0.64]\n",
      "F1-score: [0.55 0.   0.69]\n",
      "Accuracy: 0.51\n",
      "Brier score:0.1612059795825088\n",
      "Brier climat:0.22786458333333337\n",
      "Brier skill score:0.29253604388933296\n",
      "******** 24\n",
      "validation years [2005, 2006]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2007, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "{0: 2.0, 1: 1.0, 2: 1.0}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 60)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 183       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183\n",
      "Trainable params: 183\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.8531 - accuracy: 0.3538WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "28/28 [==============================] - 4s 114ms/step - loss: 1.8531 - accuracy: 0.3538 - val_loss: 1.1577 - val_accuracy: 0.6719\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.6803 - accuracy: 0.4062 - val_loss: 1.0464 - val_accuracy: 0.6719\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.6022 - accuracy: 0.4408 - val_loss: 1.0011 - val_accuracy: 0.6953\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.5555 - accuracy: 0.4531 - val_loss: 0.9824 - val_accuracy: 0.6875\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.5229 - accuracy: 0.4593 - val_loss: 0.9677 - val_accuracy: 0.7344\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.4982 - accuracy: 0.4710 - val_loss: 0.9600 - val_accuracy: 0.7266\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.4788 - accuracy: 0.4816 - val_loss: 0.9566 - val_accuracy: 0.7266\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4630 - accuracy: 0.4827 - val_loss: 0.9406 - val_accuracy: 0.7266\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4482 - accuracy: 0.4849 - val_loss: 0.9357 - val_accuracy: 0.7109\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.4358 - accuracy: 0.4877 - val_loss: 0.9259 - val_accuracy: 0.6953\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4239 - accuracy: 0.4894 - val_loss: 0.9220 - val_accuracy: 0.6953\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4134 - accuracy: 0.4933 - val_loss: 0.9162 - val_accuracy: 0.6875\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.4033 - accuracy: 0.4900 - val_loss: 0.9021 - val_accuracy: 0.6875\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3954 - accuracy: 0.4900 - val_loss: 0.8962 - val_accuracy: 0.6875\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3868 - accuracy: 0.4872 - val_loss: 0.8916 - val_accuracy: 0.6875\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3792 - accuracy: 0.4883 - val_loss: 0.8797 - val_accuracy: 0.6875\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3716 - accuracy: 0.4888 - val_loss: 0.8747 - val_accuracy: 0.6875\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.3652 - accuracy: 0.4877 - val_loss: 0.8637 - val_accuracy: 0.6875\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3586 - accuracy: 0.4855 - val_loss: 0.8656 - val_accuracy: 0.6875\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3546 - accuracy: 0.4877 - val_loss: 0.8551 - val_accuracy: 0.6875\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3480 - accuracy: 0.4866 - val_loss: 0.8485 - val_accuracy: 0.6875\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3433 - accuracy: 0.4855 - val_loss: 0.8434 - val_accuracy: 0.6875\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.3377 - accuracy: 0.4827 - val_loss: 0.8356 - val_accuracy: 0.6875\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3328 - accuracy: 0.4833 - val_loss: 0.8342 - val_accuracy: 0.6875\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3288 - accuracy: 0.4872 - val_loss: 0.8329 - val_accuracy: 0.6875\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3243 - accuracy: 0.4833 - val_loss: 0.8262 - val_accuracy: 0.6875\n",
      "Epoch 27/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3202 - accuracy: 0.4838 - val_loss: 0.8254 - val_accuracy: 0.6875\n",
      "Epoch 28/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.3164 - accuracy: 0.4855 - val_loss: 0.8248 - val_accuracy: 0.6875\n",
      "Epoch 29/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3129 - accuracy: 0.4849 - val_loss: 0.8190 - val_accuracy: 0.6875\n",
      "Epoch 30/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.3092 - accuracy: 0.4833 - val_loss: 0.8169 - val_accuracy: 0.6875\n",
      "Epoch 31/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3058 - accuracy: 0.4860 - val_loss: 0.8143 - val_accuracy: 0.6875\n",
      "Epoch 32/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3031 - accuracy: 0.4855 - val_loss: 0.8108 - val_accuracy: 0.6875\n",
      "Epoch 33/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2995 - accuracy: 0.4844 - val_loss: 0.8075 - val_accuracy: 0.6875\n",
      "Epoch 34/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2964 - accuracy: 0.4860 - val_loss: 0.8082 - val_accuracy: 0.6875\n",
      "Epoch 35/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2930 - accuracy: 0.4877 - val_loss: 0.8026 - val_accuracy: 0.6875\n",
      "Epoch 36/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2902 - accuracy: 0.4866 - val_loss: 0.7989 - val_accuracy: 0.6875\n",
      "Epoch 37/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2885 - accuracy: 0.4877 - val_loss: 0.7947 - val_accuracy: 0.6875\n",
      "Epoch 38/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2837 - accuracy: 0.4849 - val_loss: 0.7988 - val_accuracy: 0.6875\n",
      "Epoch 39/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2828 - accuracy: 0.4838 - val_loss: 0.7895 - val_accuracy: 0.6875\n",
      "Epoch 40/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2800 - accuracy: 0.4844 - val_loss: 0.7927 - val_accuracy: 0.6875\n",
      "Epoch 41/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2768 - accuracy: 0.4855 - val_loss: 0.7882 - val_accuracy: 0.6875\n",
      "Epoch 42/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2744 - accuracy: 0.4855 - val_loss: 0.7851 - val_accuracy: 0.6875\n",
      "Epoch 43/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2720 - accuracy: 0.4855 - val_loss: 0.7819 - val_accuracy: 0.6875\n",
      "Epoch 44/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2709 - accuracy: 0.4844 - val_loss: 0.7827 - val_accuracy: 0.6875\n",
      "Epoch 45/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2685 - accuracy: 0.4855 - val_loss: 0.7784 - val_accuracy: 0.6875\n",
      "Epoch 46/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2664 - accuracy: 0.4838 - val_loss: 0.7779 - val_accuracy: 0.6875\n",
      "Epoch 47/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2648 - accuracy: 0.4849 - val_loss: 0.7762 - val_accuracy: 0.6875\n",
      "Epoch 48/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2633 - accuracy: 0.4872 - val_loss: 0.7775 - val_accuracy: 0.6953\n",
      "Epoch 49/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2617 - accuracy: 0.4872 - val_loss: 0.7782 - val_accuracy: 0.6953\n",
      "Epoch 50/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2600 - accuracy: 0.4877 - val_loss: 0.7755 - val_accuracy: 0.6875\n",
      "Epoch 51/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2596 - accuracy: 0.4855 - val_loss: 0.7759 - val_accuracy: 0.6875\n",
      "Epoch 52/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2575 - accuracy: 0.4894 - val_loss: 0.7715 - val_accuracy: 0.6953\n",
      "Epoch 53/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2560 - accuracy: 0.4855 - val_loss: 0.7690 - val_accuracy: 0.7031\n",
      "Epoch 54/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2548 - accuracy: 0.4888 - val_loss: 0.7718 - val_accuracy: 0.7031\n",
      "Epoch 55/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2539 - accuracy: 0.4894 - val_loss: 0.7674 - val_accuracy: 0.6953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2519 - accuracy: 0.4866 - val_loss: 0.7702 - val_accuracy: 0.7031\n",
      "Epoch 57/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2505 - accuracy: 0.4866 - val_loss: 0.7656 - val_accuracy: 0.6953\n",
      "Epoch 58/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2500 - accuracy: 0.4883 - val_loss: 0.7677 - val_accuracy: 0.7031\n",
      "Epoch 59/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2487 - accuracy: 0.4883 - val_loss: 0.7671 - val_accuracy: 0.6953\n",
      "Epoch 60/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2478 - accuracy: 0.4838 - val_loss: 0.7611 - val_accuracy: 0.7031\n",
      "Epoch 61/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2466 - accuracy: 0.4860 - val_loss: 0.7632 - val_accuracy: 0.7031\n",
      "Epoch 62/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2455 - accuracy: 0.4883 - val_loss: 0.7603 - val_accuracy: 0.6875\n",
      "Epoch 63/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2465 - accuracy: 0.4844 - val_loss: 0.7580 - val_accuracy: 0.6875\n",
      "Epoch 64/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2450 - accuracy: 0.4877 - val_loss: 0.7627 - val_accuracy: 0.7031\n",
      "Epoch 65/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2423 - accuracy: 0.4883 - val_loss: 0.7614 - val_accuracy: 0.7031\n",
      "Epoch 66/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2426 - accuracy: 0.4872 - val_loss: 0.7633 - val_accuracy: 0.7031\n",
      "Epoch 67/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2414 - accuracy: 0.4888 - val_loss: 0.7601 - val_accuracy: 0.6953\n",
      "Epoch 68/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2408 - accuracy: 0.4888 - val_loss: 0.7571 - val_accuracy: 0.7031\n",
      "Epoch 69/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2396 - accuracy: 0.4877 - val_loss: 0.7545 - val_accuracy: 0.6953\n",
      "Epoch 70/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2380 - accuracy: 0.4894 - val_loss: 0.7537 - val_accuracy: 0.7031\n",
      "Epoch 71/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2373 - accuracy: 0.4883 - val_loss: 0.7511 - val_accuracy: 0.7031\n",
      "Epoch 72/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2370 - accuracy: 0.4860 - val_loss: 0.7515 - val_accuracy: 0.6953\n",
      "Epoch 73/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2360 - accuracy: 0.4888 - val_loss: 0.7515 - val_accuracy: 0.7031\n",
      "Epoch 74/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2349 - accuracy: 0.4866 - val_loss: 0.7567 - val_accuracy: 0.7031\n",
      "Epoch 75/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2340 - accuracy: 0.4866 - val_loss: 0.7494 - val_accuracy: 0.6953\n",
      "Epoch 76/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2336 - accuracy: 0.4900 - val_loss: 0.7506 - val_accuracy: 0.7031\n",
      "Epoch 77/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2322 - accuracy: 0.4888 - val_loss: 0.7472 - val_accuracy: 0.7031\n",
      "Epoch 78/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2332 - accuracy: 0.4888 - val_loss: 0.7416 - val_accuracy: 0.6953\n",
      "Epoch 79/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2324 - accuracy: 0.4866 - val_loss: 0.7448 - val_accuracy: 0.7031\n",
      "Epoch 80/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2320 - accuracy: 0.4877 - val_loss: 0.7442 - val_accuracy: 0.7031\n",
      "Epoch 81/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2305 - accuracy: 0.4883 - val_loss: 0.7513 - val_accuracy: 0.7031\n",
      "Epoch 82/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2297 - accuracy: 0.4872 - val_loss: 0.7454 - val_accuracy: 0.7031\n",
      "Epoch 83/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2297 - accuracy: 0.4894 - val_loss: 0.7433 - val_accuracy: 0.7031\n",
      "Epoch 84/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2293 - accuracy: 0.4872 - val_loss: 0.7485 - val_accuracy: 0.7031\n",
      "Epoch 85/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2278 - accuracy: 0.4872 - val_loss: 0.7464 - val_accuracy: 0.7031\n",
      "Epoch 86/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2283 - accuracy: 0.4872 - val_loss: 0.7487 - val_accuracy: 0.7031\n",
      "Epoch 87/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2284 - accuracy: 0.4844 - val_loss: 0.7456 - val_accuracy: 0.7031\n",
      "Epoch 88/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2272 - accuracy: 0.4872 - val_loss: 0.7460 - val_accuracy: 0.7031\n",
      "Epoch 89/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2263 - accuracy: 0.4872 - val_loss: 0.7447 - val_accuracy: 0.7031\n",
      "Epoch 90/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2256 - accuracy: 0.4866 - val_loss: 0.7448 - val_accuracy: 0.7031\n",
      "Epoch 91/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2249 - accuracy: 0.4866 - val_loss: 0.7423 - val_accuracy: 0.7031\n",
      "Epoch 92/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2244 - accuracy: 0.4860 - val_loss: 0.7461 - val_accuracy: 0.7031\n",
      "Epoch 93/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2251 - accuracy: 0.4872 - val_loss: 0.7455 - val_accuracy: 0.7031\n",
      "Epoch 94/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2243 - accuracy: 0.4888 - val_loss: 0.7410 - val_accuracy: 0.7031\n",
      "Epoch 95/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2238 - accuracy: 0.4844 - val_loss: 0.7465 - val_accuracy: 0.7031\n",
      "Epoch 96/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2240 - accuracy: 0.4866 - val_loss: 0.7398 - val_accuracy: 0.7031\n",
      "Epoch 97/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2230 - accuracy: 0.4877 - val_loss: 0.7359 - val_accuracy: 0.7031\n",
      "Epoch 98/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2225 - accuracy: 0.4844 - val_loss: 0.7417 - val_accuracy: 0.7031\n",
      "Epoch 99/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2220 - accuracy: 0.4877 - val_loss: 0.7382 - val_accuracy: 0.7031\n",
      "Epoch 100/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2218 - accuracy: 0.4860 - val_loss: 0.7366 - val_accuracy: 0.7031\n",
      "Epoch 101/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2218 - accuracy: 0.4866 - val_loss: 0.7442 - val_accuracy: 0.7031\n",
      "Epoch 102/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2210 - accuracy: 0.4872 - val_loss: 0.7339 - val_accuracy: 0.7031\n",
      "Epoch 103/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2211 - accuracy: 0.4860 - val_loss: 0.7359 - val_accuracy: 0.7031\n",
      "Epoch 104/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2207 - accuracy: 0.4855 - val_loss: 0.7326 - val_accuracy: 0.6953\n",
      "Epoch 105/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2209 - accuracy: 0.4866 - val_loss: 0.7374 - val_accuracy: 0.7031\n",
      "Epoch 106/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2196 - accuracy: 0.4888 - val_loss: 0.7313 - val_accuracy: 0.6953\n",
      "Epoch 107/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2199 - accuracy: 0.4866 - val_loss: 0.7300 - val_accuracy: 0.7031\n",
      "Epoch 108/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2199 - accuracy: 0.4844 - val_loss: 0.7369 - val_accuracy: 0.6953\n",
      "Epoch 109/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2200 - accuracy: 0.4849 - val_loss: 0.7323 - val_accuracy: 0.6953\n",
      "Epoch 110/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2195 - accuracy: 0.4838 - val_loss: 0.7341 - val_accuracy: 0.7031\n",
      "Epoch 111/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2193 - accuracy: 0.4855 - val_loss: 0.7306 - val_accuracy: 0.6953\n",
      "Epoch 112/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2195 - accuracy: 0.4855 - val_loss: 0.7298 - val_accuracy: 0.6953\n",
      "Epoch 113/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2185 - accuracy: 0.4838 - val_loss: 0.7286 - val_accuracy: 0.7031\n",
      "Epoch 114/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2194 - accuracy: 0.4833 - val_loss: 0.7324 - val_accuracy: 0.6953\n",
      "Epoch 115/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2190 - accuracy: 0.4844 - val_loss: 0.7288 - val_accuracy: 0.7031\n",
      "Epoch 116/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2185 - accuracy: 0.4849 - val_loss: 0.7227 - val_accuracy: 0.6953\n",
      "Epoch 117/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2185 - accuracy: 0.4844 - val_loss: 0.7260 - val_accuracy: 0.7031\n",
      "Epoch 118/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2181 - accuracy: 0.4860 - val_loss: 0.7294 - val_accuracy: 0.6953\n",
      "Epoch 119/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2184 - accuracy: 0.4838 - val_loss: 0.7309 - val_accuracy: 0.6953\n",
      "Epoch 120/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2176 - accuracy: 0.4844 - val_loss: 0.7253 - val_accuracy: 0.6953\n",
      "Epoch 121/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2178 - accuracy: 0.4816 - val_loss: 0.7338 - val_accuracy: 0.7031\n",
      "Epoch 122/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2175 - accuracy: 0.4860 - val_loss: 0.7255 - val_accuracy: 0.6953\n",
      "Epoch 123/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2170 - accuracy: 0.4833 - val_loss: 0.7274 - val_accuracy: 0.6953\n",
      "Epoch 124/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2181 - accuracy: 0.4844 - val_loss: 0.7272 - val_accuracy: 0.6953\n",
      "Epoch 125/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2170 - accuracy: 0.4833 - val_loss: 0.7250 - val_accuracy: 0.7031\n",
      "Epoch 126/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2175 - accuracy: 0.4821 - val_loss: 0.7258 - val_accuracy: 0.7031\n",
      "Epoch 127/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2165 - accuracy: 0.4866 - val_loss: 0.7277 - val_accuracy: 0.6953\n",
      "Epoch 128/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2171 - accuracy: 0.4833 - val_loss: 0.7252 - val_accuracy: 0.6953\n",
      "Epoch 129/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2169 - accuracy: 0.4883 - val_loss: 0.7210 - val_accuracy: 0.6953\n",
      "Epoch 130/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2169 - accuracy: 0.4838 - val_loss: 0.7268 - val_accuracy: 0.7031\n",
      "Epoch 131/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2167 - accuracy: 0.4838 - val_loss: 0.7243 - val_accuracy: 0.6875\n",
      "Epoch 132/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2164 - accuracy: 0.4844 - val_loss: 0.7272 - val_accuracy: 0.6953\n",
      "Epoch 133/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2163 - accuracy: 0.4838 - val_loss: 0.7270 - val_accuracy: 0.6953\n",
      "Epoch 134/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2158 - accuracy: 0.4838 - val_loss: 0.7296 - val_accuracy: 0.6953\n",
      "Epoch 135/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2157 - accuracy: 0.4849 - val_loss: 0.7260 - val_accuracy: 0.6953\n",
      "Epoch 136/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2160 - accuracy: 0.4833 - val_loss: 0.7244 - val_accuracy: 0.6953\n",
      "Epoch 137/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2160 - accuracy: 0.4872 - val_loss: 0.7157 - val_accuracy: 0.6953\n",
      "Epoch 138/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2159 - accuracy: 0.4844 - val_loss: 0.7249 - val_accuracy: 0.6953\n",
      "Epoch 139/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2165 - accuracy: 0.4849 - val_loss: 0.7314 - val_accuracy: 0.6953\n",
      "Epoch 140/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2154 - accuracy: 0.4844 - val_loss: 0.7207 - val_accuracy: 0.6953\n",
      "Epoch 141/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2152 - accuracy: 0.4838 - val_loss: 0.7291 - val_accuracy: 0.6953\n",
      "Epoch 142/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2155 - accuracy: 0.4849 - val_loss: 0.7269 - val_accuracy: 0.7031\n",
      "Epoch 143/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2152 - accuracy: 0.4872 - val_loss: 0.7204 - val_accuracy: 0.6953\n",
      "Epoch 144/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2166 - accuracy: 0.4866 - val_loss: 0.7210 - val_accuracy: 0.6953\n",
      "Epoch 145/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2149 - accuracy: 0.4855 - val_loss: 0.7274 - val_accuracy: 0.7031\n",
      "Epoch 146/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2152 - accuracy: 0.4855 - val_loss: 0.7272 - val_accuracy: 0.7031\n",
      "Epoch 147/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2152 - accuracy: 0.4860 - val_loss: 0.7245 - val_accuracy: 0.6953\n",
      "Epoch 148/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2154 - accuracy: 0.4838 - val_loss: 0.7203 - val_accuracy: 0.7031\n",
      "Epoch 149/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2148 - accuracy: 0.4860 - val_loss: 0.7231 - val_accuracy: 0.6953\n",
      "Epoch 150/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2156 - accuracy: 0.4849 - val_loss: 0.7206 - val_accuracy: 0.6953\n",
      "Epoch 151/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2150 - accuracy: 0.4860 - val_loss: 0.7196 - val_accuracy: 0.6953\n",
      "Epoch 152/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2152 - accuracy: 0.4866 - val_loss: 0.7276 - val_accuracy: 0.7031\n",
      "Epoch 153/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2156 - accuracy: 0.4866 - val_loss: 0.7259 - val_accuracy: 0.6953\n",
      "Epoch 154/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2151 - accuracy: 0.4877 - val_loss: 0.7237 - val_accuracy: 0.6953\n",
      "Epoch 155/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2148 - accuracy: 0.4838 - val_loss: 0.7289 - val_accuracy: 0.7031\n",
      "Epoch 156/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2148 - accuracy: 0.4883 - val_loss: 0.7269 - val_accuracy: 0.6953\n",
      "Epoch 157/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2149 - accuracy: 0.4877 - val_loss: 0.7241 - val_accuracy: 0.7031\n",
      "Epoch 158/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2147 - accuracy: 0.4855 - val_loss: 0.7242 - val_accuracy: 0.6953\n",
      "Epoch 159/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2149 - accuracy: 0.4855 - val_loss: 0.7219 - val_accuracy: 0.7031\n",
      "Epoch 160/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2147 - accuracy: 0.4855 - val_loss: 0.7228 - val_accuracy: 0.7031\n",
      "Epoch 161/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2146 - accuracy: 0.4855 - val_loss: 0.7191 - val_accuracy: 0.6953\n",
      "Epoch 162/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2146 - accuracy: 0.4860 - val_loss: 0.7203 - val_accuracy: 0.6953\n",
      "Epoch 163/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2141 - accuracy: 0.4877 - val_loss: 0.7229 - val_accuracy: 0.7031\n",
      "Epoch 164/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2145 - accuracy: 0.4844 - val_loss: 0.7194 - val_accuracy: 0.6953\n",
      "Epoch 165/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2157 - accuracy: 0.4877 - val_loss: 0.7196 - val_accuracy: 0.7031\n",
      "Epoch 166/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2137 - accuracy: 0.4866 - val_loss: 0.7280 - val_accuracy: 0.7031\n",
      "Epoch 167/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2149 - accuracy: 0.4872 - val_loss: 0.7221 - val_accuracy: 0.6953\n",
      "Epoch 168/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2145 - accuracy: 0.4866 - val_loss: 0.7269 - val_accuracy: 0.6953\n",
      "Epoch 169/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2145 - accuracy: 0.4866 - val_loss: 0.7232 - val_accuracy: 0.6953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2145 - accuracy: 0.4844 - val_loss: 0.7257 - val_accuracy: 0.7031\n",
      "Epoch 171/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2144 - accuracy: 0.4872 - val_loss: 0.7247 - val_accuracy: 0.6953\n",
      "Epoch 172/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2135 - accuracy: 0.4866 - val_loss: 0.7188 - val_accuracy: 0.7031\n",
      "Epoch 173/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2153 - accuracy: 0.4849 - val_loss: 0.7260 - val_accuracy: 0.6953\n",
      "Epoch 174/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2149 - accuracy: 0.4860 - val_loss: 0.7226 - val_accuracy: 0.7031\n",
      "Epoch 175/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2155 - accuracy: 0.4849 - val_loss: 0.7236 - val_accuracy: 0.6953\n",
      "Epoch 176/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2150 - accuracy: 0.4888 - val_loss: 0.7173 - val_accuracy: 0.7031\n",
      "Epoch 177/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2150 - accuracy: 0.4833 - val_loss: 0.7248 - val_accuracy: 0.6953\n",
      "Epoch 178/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2141 - accuracy: 0.4855 - val_loss: 0.7171 - val_accuracy: 0.7031\n",
      "Epoch 179/300\n",
      "28/28 [==============================] - 1s 48ms/step - loss: 1.2143 - accuracy: 0.4844 - val_loss: 0.7272 - val_accuracy: 0.7031\n",
      "Epoch 180/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2138 - accuracy: 0.4872 - val_loss: 0.7244 - val_accuracy: 0.6953\n",
      "Epoch 181/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2140 - accuracy: 0.4872 - val_loss: 0.7237 - val_accuracy: 0.6953\n",
      "Epoch 182/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2137 - accuracy: 0.4855 - val_loss: 0.7207 - val_accuracy: 0.7031\n",
      "Epoch 183/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2136 - accuracy: 0.4872 - val_loss: 0.7237 - val_accuracy: 0.7031\n",
      "Epoch 184/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2141 - accuracy: 0.4844 - val_loss: 0.7281 - val_accuracy: 0.7031\n",
      "Epoch 185/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2138 - accuracy: 0.4866 - val_loss: 0.7187 - val_accuracy: 0.6953\n",
      "Epoch 186/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2139 - accuracy: 0.4883 - val_loss: 0.7174 - val_accuracy: 0.7031\n",
      "Epoch 187/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2134 - accuracy: 0.4844 - val_loss: 0.7222 - val_accuracy: 0.7031\n",
      "Epoch 188/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2135 - accuracy: 0.4872 - val_loss: 0.7258 - val_accuracy: 0.7031\n",
      "Epoch 189/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2136 - accuracy: 0.4888 - val_loss: 0.7235 - val_accuracy: 0.7031\n",
      "Epoch 190/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2136 - accuracy: 0.4849 - val_loss: 0.7259 - val_accuracy: 0.7031\n",
      "Epoch 191/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2134 - accuracy: 0.4860 - val_loss: 0.7239 - val_accuracy: 0.6953\n",
      "Epoch 192/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2132 - accuracy: 0.4860 - val_loss: 0.7287 - val_accuracy: 0.6953\n",
      "Epoch 193/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2134 - accuracy: 0.4866 - val_loss: 0.7231 - val_accuracy: 0.6953\n",
      "Epoch 194/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2134 - accuracy: 0.4872 - val_loss: 0.7246 - val_accuracy: 0.7031\n",
      "Epoch 195/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2138 - accuracy: 0.4860 - val_loss: 0.7254 - val_accuracy: 0.7031\n",
      "Epoch 196/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2137 - accuracy: 0.4872 - val_loss: 0.7246 - val_accuracy: 0.6953\n",
      "Epoch 197/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2128 - accuracy: 0.4855 - val_loss: 0.7191 - val_accuracy: 0.7031\n",
      "Epoch 198/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2130 - accuracy: 0.4849 - val_loss: 0.7278 - val_accuracy: 0.7031\n",
      "Epoch 199/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2141 - accuracy: 0.4860 - val_loss: 0.7241 - val_accuracy: 0.7031\n",
      "Epoch 200/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2130 - accuracy: 0.4866 - val_loss: 0.7238 - val_accuracy: 0.7031\n",
      "Epoch 201/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2133 - accuracy: 0.4866 - val_loss: 0.7268 - val_accuracy: 0.7031\n",
      "Epoch 202/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2130 - accuracy: 0.4866 - val_loss: 0.7230 - val_accuracy: 0.7031\n",
      "Epoch 203/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2129 - accuracy: 0.4866 - val_loss: 0.7299 - val_accuracy: 0.7031\n",
      "Epoch 204/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2126 - accuracy: 0.4866 - val_loss: 0.7235 - val_accuracy: 0.7031\n",
      "Epoch 205/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2142 - accuracy: 0.4866 - val_loss: 0.7310 - val_accuracy: 0.7031\n",
      "Epoch 206/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2130 - accuracy: 0.4866 - val_loss: 0.7237 - val_accuracy: 0.7031\n",
      "Epoch 207/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2128 - accuracy: 0.4877 - val_loss: 0.7252 - val_accuracy: 0.7031\n",
      "Epoch 208/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2132 - accuracy: 0.4872 - val_loss: 0.7249 - val_accuracy: 0.7031\n",
      "Epoch 209/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2132 - accuracy: 0.4866 - val_loss: 0.7234 - val_accuracy: 0.7031\n",
      "Epoch 210/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2132 - accuracy: 0.4872 - val_loss: 0.7288 - val_accuracy: 0.6953\n",
      "Epoch 211/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2134 - accuracy: 0.4860 - val_loss: 0.7264 - val_accuracy: 0.7031\n",
      "Epoch 212/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2133 - accuracy: 0.4855 - val_loss: 0.7253 - val_accuracy: 0.6953\n",
      "Epoch 213/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2133 - accuracy: 0.4844 - val_loss: 0.7240 - val_accuracy: 0.7031\n",
      "Epoch 214/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2144 - accuracy: 0.4877 - val_loss: 0.7206 - val_accuracy: 0.7031\n",
      "Epoch 215/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2122 - accuracy: 0.4855 - val_loss: 0.7243 - val_accuracy: 0.7031\n",
      "Epoch 216/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2130 - accuracy: 0.4855 - val_loss: 0.7265 - val_accuracy: 0.7031\n",
      "Epoch 217/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2145 - accuracy: 0.4855 - val_loss: 0.7236 - val_accuracy: 0.7031\n",
      "Epoch 218/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2133 - accuracy: 0.4866 - val_loss: 0.7256 - val_accuracy: 0.7031\n",
      "Epoch 219/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2135 - accuracy: 0.4860 - val_loss: 0.7236 - val_accuracy: 0.7031\n",
      "Epoch 220/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2128 - accuracy: 0.4872 - val_loss: 0.7284 - val_accuracy: 0.7031\n",
      "Epoch 221/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2137 - accuracy: 0.4860 - val_loss: 0.7213 - val_accuracy: 0.7031\n",
      "Epoch 222/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2126 - accuracy: 0.4866 - val_loss: 0.7275 - val_accuracy: 0.7031\n",
      "Epoch 223/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2127 - accuracy: 0.4860 - val_loss: 0.7234 - val_accuracy: 0.7031\n",
      "Epoch 224/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2125 - accuracy: 0.4860 - val_loss: 0.7241 - val_accuracy: 0.7031\n",
      "Epoch 225/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2129 - accuracy: 0.4866 - val_loss: 0.7236 - val_accuracy: 0.7031\n",
      "Epoch 226/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2131 - accuracy: 0.4860 - val_loss: 0.7300 - val_accuracy: 0.7031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2135 - accuracy: 0.4860 - val_loss: 0.7259 - val_accuracy: 0.7031\n",
      "Epoch 228/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2130 - accuracy: 0.4872 - val_loss: 0.7181 - val_accuracy: 0.7031\n",
      "Epoch 229/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2124 - accuracy: 0.4866 - val_loss: 0.7217 - val_accuracy: 0.7031\n",
      "Epoch 230/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2124 - accuracy: 0.4877 - val_loss: 0.7284 - val_accuracy: 0.7031\n",
      "Epoch 231/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2139 - accuracy: 0.4860 - val_loss: 0.7218 - val_accuracy: 0.7031\n",
      "Epoch 232/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2126 - accuracy: 0.4866 - val_loss: 0.7249 - val_accuracy: 0.7031\n",
      "Epoch 233/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2128 - accuracy: 0.4860 - val_loss: 0.7227 - val_accuracy: 0.7031\n",
      "Epoch 234/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2129 - accuracy: 0.4866 - val_loss: 0.7218 - val_accuracy: 0.7031\n",
      "Epoch 235/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2124 - accuracy: 0.4855 - val_loss: 0.7220 - val_accuracy: 0.7031\n",
      "Epoch 236/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2127 - accuracy: 0.4844 - val_loss: 0.7254 - val_accuracy: 0.7031\n",
      "Epoch 237/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2123 - accuracy: 0.4877 - val_loss: 0.7266 - val_accuracy: 0.7031\n",
      "Epoch 238/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2127 - accuracy: 0.4866 - val_loss: 0.7276 - val_accuracy: 0.7031\n",
      "Epoch 239/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2124 - accuracy: 0.4877 - val_loss: 0.7263 - val_accuracy: 0.7031\n",
      "Epoch 240/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2124 - accuracy: 0.4844 - val_loss: 0.7252 - val_accuracy: 0.7031\n",
      "Epoch 241/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2123 - accuracy: 0.4849 - val_loss: 0.7263 - val_accuracy: 0.7031\n",
      "Epoch 242/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2120 - accuracy: 0.4866 - val_loss: 0.7192 - val_accuracy: 0.7031\n",
      "Epoch 243/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2121 - accuracy: 0.4860 - val_loss: 0.7212 - val_accuracy: 0.7031\n",
      "Epoch 244/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2124 - accuracy: 0.4860 - val_loss: 0.7188 - val_accuracy: 0.7031\n",
      "Epoch 245/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2135 - accuracy: 0.4860 - val_loss: 0.7244 - val_accuracy: 0.7031\n",
      "Epoch 246/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2131 - accuracy: 0.4872 - val_loss: 0.7202 - val_accuracy: 0.7031\n",
      "Epoch 247/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2129 - accuracy: 0.4860 - val_loss: 0.7236 - val_accuracy: 0.7031\n",
      "Epoch 248/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2123 - accuracy: 0.4877 - val_loss: 0.7231 - val_accuracy: 0.7031\n",
      "Epoch 249/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2123 - accuracy: 0.4866 - val_loss: 0.7202 - val_accuracy: 0.7031\n",
      "Epoch 250/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2122 - accuracy: 0.4860 - val_loss: 0.7166 - val_accuracy: 0.7031\n",
      "Epoch 251/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2134 - accuracy: 0.4872 - val_loss: 0.7182 - val_accuracy: 0.6953\n",
      "Epoch 252/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2119 - accuracy: 0.4855 - val_loss: 0.7274 - val_accuracy: 0.7031\n",
      "Epoch 253/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2138 - accuracy: 0.4877 - val_loss: 0.7282 - val_accuracy: 0.7031\n",
      "Epoch 254/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2132 - accuracy: 0.4883 - val_loss: 0.7253 - val_accuracy: 0.7031\n",
      "Epoch 255/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2127 - accuracy: 0.4872 - val_loss: 0.7152 - val_accuracy: 0.7031\n",
      "Epoch 256/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2120 - accuracy: 0.4872 - val_loss: 0.7272 - val_accuracy: 0.7031\n",
      "Epoch 257/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2118 - accuracy: 0.4860 - val_loss: 0.7241 - val_accuracy: 0.7031\n",
      "Epoch 258/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2123 - accuracy: 0.4872 - val_loss: 0.7184 - val_accuracy: 0.7031\n",
      "Epoch 259/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2118 - accuracy: 0.4860 - val_loss: 0.7293 - val_accuracy: 0.7031\n",
      "Epoch 260/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2118 - accuracy: 0.4860 - val_loss: 0.7268 - val_accuracy: 0.7031\n",
      "Epoch 261/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2122 - accuracy: 0.4872 - val_loss: 0.7276 - val_accuracy: 0.7031\n",
      "Epoch 262/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2128 - accuracy: 0.4872 - val_loss: 0.7179 - val_accuracy: 0.7031\n",
      "Epoch 263/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2127 - accuracy: 0.4849 - val_loss: 0.7260 - val_accuracy: 0.7031\n",
      "Epoch 264/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2120 - accuracy: 0.4888 - val_loss: 0.7270 - val_accuracy: 0.7031\n",
      "Epoch 265/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2124 - accuracy: 0.4866 - val_loss: 0.7245 - val_accuracy: 0.7031\n",
      "Epoch 266/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2118 - accuracy: 0.4872 - val_loss: 0.7222 - val_accuracy: 0.7031\n",
      "Epoch 267/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2126 - accuracy: 0.4877 - val_loss: 0.7242 - val_accuracy: 0.7031\n",
      "Epoch 268/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2123 - accuracy: 0.4883 - val_loss: 0.7186 - val_accuracy: 0.7031\n",
      "Epoch 269/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2130 - accuracy: 0.4872 - val_loss: 0.7239 - val_accuracy: 0.7031\n",
      "Epoch 270/300\n",
      "28/28 [==============================] - 1s 29ms/step - loss: 1.2117 - accuracy: 0.4872 - val_loss: 0.7242 - val_accuracy: 0.7031\n",
      "Epoch 271/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2123 - accuracy: 0.4860 - val_loss: 0.7227 - val_accuracy: 0.7031\n",
      "Epoch 272/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2123 - accuracy: 0.4872 - val_loss: 0.7296 - val_accuracy: 0.7031\n",
      "Epoch 273/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2119 - accuracy: 0.4888 - val_loss: 0.7239 - val_accuracy: 0.7031\n",
      "Epoch 274/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2122 - accuracy: 0.4866 - val_loss: 0.7245 - val_accuracy: 0.7031\n",
      "Epoch 275/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2128 - accuracy: 0.4866 - val_loss: 0.7185 - val_accuracy: 0.7031\n",
      "Epoch 276/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2113 - accuracy: 0.4860 - val_loss: 0.7223 - val_accuracy: 0.7031\n",
      "Epoch 277/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2124 - accuracy: 0.4883 - val_loss: 0.7266 - val_accuracy: 0.7031\n",
      "Epoch 278/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2124 - accuracy: 0.4866 - val_loss: 0.7241 - val_accuracy: 0.7031\n",
      "Epoch 279/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2116 - accuracy: 0.4883 - val_loss: 0.7237 - val_accuracy: 0.7031\n",
      "Epoch 280/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2115 - accuracy: 0.4860 - val_loss: 0.7198 - val_accuracy: 0.7031\n",
      "Epoch 281/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2121 - accuracy: 0.4872 - val_loss: 0.7229 - val_accuracy: 0.7031\n",
      "Epoch 282/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2115 - accuracy: 0.4872 - val_loss: 0.7176 - val_accuracy: 0.7031\n",
      "Epoch 283/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2117 - accuracy: 0.4883 - val_loss: 0.7225 - val_accuracy: 0.7031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 284/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2118 - accuracy: 0.4860 - val_loss: 0.7208 - val_accuracy: 0.7031\n",
      "Epoch 285/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2122 - accuracy: 0.4883 - val_loss: 0.7173 - val_accuracy: 0.7031\n",
      "Epoch 286/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2119 - accuracy: 0.4872 - val_loss: 0.7187 - val_accuracy: 0.7031\n",
      "Epoch 287/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2118 - accuracy: 0.4860 - val_loss: 0.7231 - val_accuracy: 0.7031\n",
      "Epoch 288/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2130 - accuracy: 0.4888 - val_loss: 0.7247 - val_accuracy: 0.7031\n",
      "Epoch 289/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2122 - accuracy: 0.4849 - val_loss: 0.7243 - val_accuracy: 0.7031\n",
      "Epoch 290/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2116 - accuracy: 0.4866 - val_loss: 0.7213 - val_accuracy: 0.7031\n",
      "Epoch 291/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2124 - accuracy: 0.4855 - val_loss: 0.7190 - val_accuracy: 0.7031\n",
      "Epoch 292/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2122 - accuracy: 0.4877 - val_loss: 0.7232 - val_accuracy: 0.7031\n",
      "Epoch 293/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2115 - accuracy: 0.4877 - val_loss: 0.7217 - val_accuracy: 0.7031\n",
      "Epoch 294/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2119 - accuracy: 0.4883 - val_loss: 0.7150 - val_accuracy: 0.7031\n",
      "Epoch 295/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2121 - accuracy: 0.4860 - val_loss: 0.7155 - val_accuracy: 0.7031\n",
      "Epoch 296/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2117 - accuracy: 0.4883 - val_loss: 0.7255 - val_accuracy: 0.7031\n",
      "Epoch 297/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1.2129 - accuracy: 0.4860 - val_loss: 0.7186 - val_accuracy: 0.7031\n",
      "Epoch 298/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.2117 - accuracy: 0.4883 - val_loss: 0.7202 - val_accuracy: 0.7031\n",
      "Epoch 299/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.2116 - accuracy: 0.4883 - val_loss: 0.7232 - val_accuracy: 0.7031\n",
      "Epoch 300/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.2119 - accuracy: 0.4860 - val_loss: 0.7235 - val_accuracy: 0.7031\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "56/56 [==============================] - 0s 1ms/step\n",
      "Recall: [0.94 0.   0.58]\n",
      "Precision: [0.41 0.   0.69]\n",
      "F1-score: [0.57 0.   0.63]\n",
      "Accuracy: 0.49\n",
      "Brier score:0.2023326802368308\n",
      "Brier climat:0.21045882936507937\n",
      "Brier skill score:0.03861158570901435\n",
      "Recall: [0.94 0.   0.58]\n",
      "Precision: [0.41 0.   0.69]\n",
      "F1-score: [0.57 0.   0.63]\n",
      "Accuracy: 0.49\n",
      "Brier score:0.2256885496140553\n",
      "Brier climat:0.22302579365079364\n",
      "Brier skill score:-0.011939228730784901\n",
      "Recall: [0.94 0.   0.58]\n",
      "Precision: [0.41 0.   0.69]\n",
      "F1-score: [0.57 0.   0.63]\n",
      "Accuracy: 0.49\n",
      "Brier score:0.15891982607081928\n",
      "Brier climat:0.22299107142857144\n",
      "Brier skill score:0.2873265057084381\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "Recall: [0.82 0.   1.  ]\n",
      "Precision: [0.63 0.   0.75]\n",
      "F1-score: [0.71 0.   0.86]\n",
      "Accuracy: 0.7\n",
      "Brier score:0.14046957810901162\n",
      "Brier climat:0.20696180555555554\n",
      "Brier skill score:0.32127777039853456\n",
      "Recall: [0.82 0.   1.  ]\n",
      "Precision: [0.63 0.   0.75]\n",
      "F1-score: [0.71 0.   0.86]\n",
      "Accuracy: 0.7\n",
      "Brier score:0.16114657591597825\n",
      "Brier climat:0.19743055555555558\n",
      "Brier skill score:0.18378097320081377\n",
      "Recall: [0.82 0.   1.  ]\n",
      "Precision: [0.63 0.   0.75]\n",
      "F1-score: [0.71 0.   0.86]\n",
      "Accuracy: 0.7\n",
      "Brier score:0.08149476688201998\n",
      "Brier climat:0.26484375000000004\n",
      "Brier skill score:0.6922911456962078\n",
      "******** 25\n",
      "validation years [2006, 2007]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2008, 2009, 2010, 1981, 1982, 1983}\n",
      "{0: 2.0, 1: 1.0, 2: 1.0}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 60)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 183       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183\n",
      "Trainable params: 183\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "28/28 [==============================] - ETA: 0s - loss: 2.4287 - accuracy: 0.2645WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "28/28 [==============================] - 4s 117ms/step - loss: 2.4287 - accuracy: 0.2645 - val_loss: 2.0992 - val_accuracy: 0.1484\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.9970 - accuracy: 0.3209 - val_loss: 1.6068 - val_accuracy: 0.3125\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.7543 - accuracy: 0.3750 - val_loss: 1.3411 - val_accuracy: 0.5156\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.6362 - accuracy: 0.4358 - val_loss: 1.2167 - val_accuracy: 0.6094\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.5736 - accuracy: 0.4782 - val_loss: 1.1581 - val_accuracy: 0.6328\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.5328 - accuracy: 0.4844 - val_loss: 1.1196 - val_accuracy: 0.5938\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.5039 - accuracy: 0.4799 - val_loss: 1.0884 - val_accuracy: 0.5938\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.4817 - accuracy: 0.4816 - val_loss: 1.0761 - val_accuracy: 0.5859\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4634 - accuracy: 0.4827 - val_loss: 1.0533 - val_accuracy: 0.5859\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.4487 - accuracy: 0.4849 - val_loss: 1.0458 - val_accuracy: 0.5859\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.4357 - accuracy: 0.4860 - val_loss: 1.0271 - val_accuracy: 0.5859\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4244 - accuracy: 0.4888 - val_loss: 1.0165 - val_accuracy: 0.5859\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4141 - accuracy: 0.4872 - val_loss: 1.0156 - val_accuracy: 0.5859\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4044 - accuracy: 0.4883 - val_loss: 1.0098 - val_accuracy: 0.5859\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3966 - accuracy: 0.4905 - val_loss: 1.0053 - val_accuracy: 0.5859\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3867 - accuracy: 0.4877 - val_loss: 0.9969 - val_accuracy: 0.5859\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3785 - accuracy: 0.4883 - val_loss: 0.9974 - val_accuracy: 0.5859\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3709 - accuracy: 0.4905 - val_loss: 0.9944 - val_accuracy: 0.5859\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3645 - accuracy: 0.4905 - val_loss: 0.9908 - val_accuracy: 0.5938\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.3582 - accuracy: 0.4911 - val_loss: 0.9864 - val_accuracy: 0.5938\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3525 - accuracy: 0.4888 - val_loss: 0.9839 - val_accuracy: 0.5938\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.3468 - accuracy: 0.4860 - val_loss: 0.9848 - val_accuracy: 0.6016\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3404 - accuracy: 0.4872 - val_loss: 0.9865 - val_accuracy: 0.5938\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.3355 - accuracy: 0.4877 - val_loss: 0.9798 - val_accuracy: 0.5938\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.3307 - accuracy: 0.4866 - val_loss: 0.9762 - val_accuracy: 0.6016\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3262 - accuracy: 0.4860 - val_loss: 0.9770 - val_accuracy: 0.6016\n",
      "Epoch 27/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3216 - accuracy: 0.4855 - val_loss: 0.9772 - val_accuracy: 0.6016\n",
      "Epoch 28/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.3169 - accuracy: 0.4866 - val_loss: 0.9720 - val_accuracy: 0.6094\n",
      "Epoch 29/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3129 - accuracy: 0.4877 - val_loss: 0.9736 - val_accuracy: 0.6016\n",
      "Epoch 30/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3099 - accuracy: 0.4872 - val_loss: 0.9777 - val_accuracy: 0.6094\n",
      "Epoch 31/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3060 - accuracy: 0.4872 - val_loss: 0.9762 - val_accuracy: 0.6094\n",
      "Epoch 32/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3020 - accuracy: 0.4883 - val_loss: 0.9756 - val_accuracy: 0.6172\n",
      "Epoch 33/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2982 - accuracy: 0.4860 - val_loss: 0.9700 - val_accuracy: 0.6172\n",
      "Epoch 34/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2951 - accuracy: 0.4877 - val_loss: 0.9721 - val_accuracy: 0.6172\n",
      "Epoch 35/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2924 - accuracy: 0.4877 - val_loss: 0.9722 - val_accuracy: 0.6094\n",
      "Epoch 36/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2887 - accuracy: 0.4860 - val_loss: 0.9714 - val_accuracy: 0.6172\n",
      "Epoch 37/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2856 - accuracy: 0.4855 - val_loss: 0.9693 - val_accuracy: 0.6094\n",
      "Epoch 38/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2831 - accuracy: 0.4872 - val_loss: 0.9701 - val_accuracy: 0.6094\n",
      "Epoch 39/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2801 - accuracy: 0.4855 - val_loss: 0.9662 - val_accuracy: 0.6250\n",
      "Epoch 40/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2786 - accuracy: 0.4866 - val_loss: 0.9661 - val_accuracy: 0.6094\n",
      "Epoch 41/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2761 - accuracy: 0.4888 - val_loss: 0.9638 - val_accuracy: 0.6250\n",
      "Epoch 42/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2733 - accuracy: 0.4827 - val_loss: 0.9620 - val_accuracy: 0.6172\n",
      "Epoch 43/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2709 - accuracy: 0.4833 - val_loss: 0.9654 - val_accuracy: 0.6172\n",
      "Epoch 44/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2692 - accuracy: 0.4866 - val_loss: 0.9586 - val_accuracy: 0.6250\n",
      "Epoch 45/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2657 - accuracy: 0.4821 - val_loss: 0.9587 - val_accuracy: 0.6172\n",
      "Epoch 46/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2631 - accuracy: 0.4844 - val_loss: 0.9586 - val_accuracy: 0.6172\n",
      "Epoch 47/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2611 - accuracy: 0.4838 - val_loss: 0.9577 - val_accuracy: 0.6172\n",
      "Epoch 48/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2589 - accuracy: 0.4844 - val_loss: 0.9567 - val_accuracy: 0.6172\n",
      "Epoch 49/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2569 - accuracy: 0.4872 - val_loss: 0.9546 - val_accuracy: 0.6172\n",
      "Epoch 50/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2556 - accuracy: 0.4833 - val_loss: 0.9566 - val_accuracy: 0.6172\n",
      "Epoch 51/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2533 - accuracy: 0.4860 - val_loss: 0.9514 - val_accuracy: 0.6172\n",
      "Epoch 52/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2521 - accuracy: 0.4838 - val_loss: 0.9559 - val_accuracy: 0.6172\n",
      "Epoch 53/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2498 - accuracy: 0.4821 - val_loss: 0.9546 - val_accuracy: 0.6172\n",
      "Epoch 54/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2487 - accuracy: 0.4827 - val_loss: 0.9527 - val_accuracy: 0.6094\n",
      "Epoch 55/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2463 - accuracy: 0.4849 - val_loss: 0.9521 - val_accuracy: 0.6172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2460 - accuracy: 0.4872 - val_loss: 0.9431 - val_accuracy: 0.6172\n",
      "Epoch 57/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2437 - accuracy: 0.4821 - val_loss: 0.9511 - val_accuracy: 0.6094\n",
      "Epoch 58/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2424 - accuracy: 0.4860 - val_loss: 0.9496 - val_accuracy: 0.6094\n",
      "Epoch 59/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2399 - accuracy: 0.4877 - val_loss: 0.9498 - val_accuracy: 0.6172\n",
      "Epoch 60/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2391 - accuracy: 0.4855 - val_loss: 0.9467 - val_accuracy: 0.6172\n",
      "Epoch 61/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2377 - accuracy: 0.4877 - val_loss: 0.9481 - val_accuracy: 0.6172\n",
      "Epoch 62/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2365 - accuracy: 0.4844 - val_loss: 0.9483 - val_accuracy: 0.6172\n",
      "Epoch 63/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2354 - accuracy: 0.4883 - val_loss: 0.9439 - val_accuracy: 0.6172\n",
      "Epoch 64/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2352 - accuracy: 0.4827 - val_loss: 0.9495 - val_accuracy: 0.6172\n",
      "Epoch 65/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2337 - accuracy: 0.4860 - val_loss: 0.9464 - val_accuracy: 0.6172\n",
      "Epoch 66/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2323 - accuracy: 0.4872 - val_loss: 0.9430 - val_accuracy: 0.6172\n",
      "Epoch 67/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2320 - accuracy: 0.4855 - val_loss: 0.9402 - val_accuracy: 0.6172\n",
      "Epoch 68/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2313 - accuracy: 0.4872 - val_loss: 0.9465 - val_accuracy: 0.6172\n",
      "Epoch 69/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2300 - accuracy: 0.4860 - val_loss: 0.9448 - val_accuracy: 0.6094\n",
      "Epoch 70/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2291 - accuracy: 0.4872 - val_loss: 0.9400 - val_accuracy: 0.6094\n",
      "Epoch 71/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2288 - accuracy: 0.4849 - val_loss: 0.9413 - val_accuracy: 0.6094\n",
      "Epoch 72/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2292 - accuracy: 0.4844 - val_loss: 0.9423 - val_accuracy: 0.6094\n",
      "Epoch 73/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2283 - accuracy: 0.4849 - val_loss: 0.9461 - val_accuracy: 0.6094\n",
      "Epoch 74/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2270 - accuracy: 0.4838 - val_loss: 0.9423 - val_accuracy: 0.6094\n",
      "Epoch 75/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2266 - accuracy: 0.4849 - val_loss: 0.9481 - val_accuracy: 0.6094\n",
      "Epoch 76/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2267 - accuracy: 0.4872 - val_loss: 0.9504 - val_accuracy: 0.6094\n",
      "Epoch 77/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2266 - accuracy: 0.4855 - val_loss: 0.9449 - val_accuracy: 0.6172\n",
      "Epoch 78/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2261 - accuracy: 0.4855 - val_loss: 0.9447 - val_accuracy: 0.6094\n",
      "Epoch 79/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2254 - accuracy: 0.4827 - val_loss: 0.9467 - val_accuracy: 0.6094\n",
      "Epoch 80/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2255 - accuracy: 0.4872 - val_loss: 0.9463 - val_accuracy: 0.6094\n",
      "Epoch 81/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2237 - accuracy: 0.4855 - val_loss: 0.9487 - val_accuracy: 0.6094\n",
      "Epoch 82/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2247 - accuracy: 0.4816 - val_loss: 0.9438 - val_accuracy: 0.6094\n",
      "Epoch 83/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2241 - accuracy: 0.4849 - val_loss: 0.9482 - val_accuracy: 0.6094\n",
      "Epoch 84/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2228 - accuracy: 0.4849 - val_loss: 0.9439 - val_accuracy: 0.6094\n",
      "Epoch 85/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2229 - accuracy: 0.4855 - val_loss: 0.9422 - val_accuracy: 0.6094\n",
      "Epoch 86/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2236 - accuracy: 0.4816 - val_loss: 0.9425 - val_accuracy: 0.6094\n",
      "Epoch 87/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2216 - accuracy: 0.4833 - val_loss: 0.9455 - val_accuracy: 0.6094\n",
      "Epoch 88/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2218 - accuracy: 0.4838 - val_loss: 0.9383 - val_accuracy: 0.6250\n",
      "Epoch 89/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2217 - accuracy: 0.4833 - val_loss: 0.9430 - val_accuracy: 0.6094\n",
      "Epoch 90/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2221 - accuracy: 0.4860 - val_loss: 0.9394 - val_accuracy: 0.6094\n",
      "Epoch 91/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2216 - accuracy: 0.4872 - val_loss: 0.9429 - val_accuracy: 0.6094\n",
      "Epoch 92/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2208 - accuracy: 0.4838 - val_loss: 0.9421 - val_accuracy: 0.6094\n",
      "Epoch 93/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2203 - accuracy: 0.4855 - val_loss: 0.9356 - val_accuracy: 0.6250\n",
      "Epoch 94/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2199 - accuracy: 0.4872 - val_loss: 0.9382 - val_accuracy: 0.6172\n",
      "Epoch 95/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2199 - accuracy: 0.4838 - val_loss: 0.9358 - val_accuracy: 0.6172\n",
      "Epoch 96/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2198 - accuracy: 0.4844 - val_loss: 0.9322 - val_accuracy: 0.6172\n",
      "Epoch 97/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2192 - accuracy: 0.4838 - val_loss: 0.9407 - val_accuracy: 0.6172\n",
      "Epoch 98/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2198 - accuracy: 0.4866 - val_loss: 0.9385 - val_accuracy: 0.6172\n",
      "Epoch 99/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2192 - accuracy: 0.4849 - val_loss: 0.9382 - val_accuracy: 0.6172\n",
      "Epoch 100/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2188 - accuracy: 0.4844 - val_loss: 0.9329 - val_accuracy: 0.6172\n",
      "Epoch 101/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2189 - accuracy: 0.4844 - val_loss: 0.9358 - val_accuracy: 0.6172\n",
      "Epoch 102/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2184 - accuracy: 0.4849 - val_loss: 0.9341 - val_accuracy: 0.6172\n",
      "Epoch 103/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2189 - accuracy: 0.4855 - val_loss: 0.9336 - val_accuracy: 0.6172\n",
      "Epoch 104/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2177 - accuracy: 0.4872 - val_loss: 0.9377 - val_accuracy: 0.6250\n",
      "Epoch 105/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2183 - accuracy: 0.4872 - val_loss: 0.9340 - val_accuracy: 0.6250\n",
      "Epoch 106/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2172 - accuracy: 0.4844 - val_loss: 0.9346 - val_accuracy: 0.6250\n",
      "Epoch 107/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2177 - accuracy: 0.4855 - val_loss: 0.9340 - val_accuracy: 0.6172\n",
      "Epoch 108/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2164 - accuracy: 0.4838 - val_loss: 0.9379 - val_accuracy: 0.6172\n",
      "Epoch 109/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2175 - accuracy: 0.4860 - val_loss: 0.9360 - val_accuracy: 0.6172\n",
      "Epoch 110/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2162 - accuracy: 0.4855 - val_loss: 0.9341 - val_accuracy: 0.6250\n",
      "Epoch 111/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2157 - accuracy: 0.4894 - val_loss: 0.9341 - val_accuracy: 0.6172\n",
      "Epoch 112/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2162 - accuracy: 0.4860 - val_loss: 0.9372 - val_accuracy: 0.6250\n",
      "Epoch 113/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2159 - accuracy: 0.4883 - val_loss: 0.9329 - val_accuracy: 0.6250\n",
      "Epoch 114/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2159 - accuracy: 0.4838 - val_loss: 0.9299 - val_accuracy: 0.6172\n",
      "Epoch 115/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2154 - accuracy: 0.4860 - val_loss: 0.9301 - val_accuracy: 0.6250\n",
      "Epoch 116/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2163 - accuracy: 0.4855 - val_loss: 0.9311 - val_accuracy: 0.6250\n",
      "Epoch 117/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2148 - accuracy: 0.4877 - val_loss: 0.9300 - val_accuracy: 0.6172\n",
      "Epoch 118/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2140 - accuracy: 0.4860 - val_loss: 0.9284 - val_accuracy: 0.6250\n",
      "Epoch 119/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2142 - accuracy: 0.4883 - val_loss: 0.9302 - val_accuracy: 0.6172\n",
      "Epoch 120/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2146 - accuracy: 0.4872 - val_loss: 0.9278 - val_accuracy: 0.6172\n",
      "Epoch 121/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2145 - accuracy: 0.4860 - val_loss: 0.9265 - val_accuracy: 0.6250\n",
      "Epoch 122/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2141 - accuracy: 0.4866 - val_loss: 0.9350 - val_accuracy: 0.6250\n",
      "Epoch 123/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2138 - accuracy: 0.4860 - val_loss: 0.9297 - val_accuracy: 0.6250\n",
      "Epoch 124/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2149 - accuracy: 0.4883 - val_loss: 0.9342 - val_accuracy: 0.6172\n",
      "Epoch 125/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2134 - accuracy: 0.4860 - val_loss: 0.9258 - val_accuracy: 0.6172\n",
      "Epoch 126/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2138 - accuracy: 0.4849 - val_loss: 0.9316 - val_accuracy: 0.6250\n",
      "Epoch 127/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2129 - accuracy: 0.4866 - val_loss: 0.9273 - val_accuracy: 0.6172\n",
      "Epoch 128/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2130 - accuracy: 0.4877 - val_loss: 0.9297 - val_accuracy: 0.6250\n",
      "Epoch 129/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2128 - accuracy: 0.4860 - val_loss: 0.9298 - val_accuracy: 0.6172\n",
      "Epoch 130/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2127 - accuracy: 0.4866 - val_loss: 0.9288 - val_accuracy: 0.6250\n",
      "Epoch 131/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2127 - accuracy: 0.4883 - val_loss: 0.9330 - val_accuracy: 0.6172\n",
      "Epoch 132/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2132 - accuracy: 0.4833 - val_loss: 0.9274 - val_accuracy: 0.6250\n",
      "Epoch 133/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2132 - accuracy: 0.4894 - val_loss: 0.9349 - val_accuracy: 0.6172\n",
      "Epoch 134/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2128 - accuracy: 0.4877 - val_loss: 0.9281 - val_accuracy: 0.6172\n",
      "Epoch 135/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2123 - accuracy: 0.4866 - val_loss: 0.9298 - val_accuracy: 0.6250\n",
      "Epoch 136/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2123 - accuracy: 0.4888 - val_loss: 0.9305 - val_accuracy: 0.6250\n",
      "Epoch 137/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2127 - accuracy: 0.4866 - val_loss: 0.9224 - val_accuracy: 0.6172\n",
      "Epoch 138/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2123 - accuracy: 0.4872 - val_loss: 0.9301 - val_accuracy: 0.6172\n",
      "Epoch 139/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2122 - accuracy: 0.4888 - val_loss: 0.9280 - val_accuracy: 0.6250\n",
      "Epoch 140/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2118 - accuracy: 0.4877 - val_loss: 0.9304 - val_accuracy: 0.6250\n",
      "Epoch 141/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2123 - accuracy: 0.4877 - val_loss: 0.9281 - val_accuracy: 0.6172\n",
      "Epoch 142/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2118 - accuracy: 0.4888 - val_loss: 0.9297 - val_accuracy: 0.6250\n",
      "Epoch 143/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2123 - accuracy: 0.4877 - val_loss: 0.9228 - val_accuracy: 0.6328\n",
      "Epoch 144/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2138 - accuracy: 0.4883 - val_loss: 0.9299 - val_accuracy: 0.6172\n",
      "Epoch 145/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2119 - accuracy: 0.4877 - val_loss: 0.9270 - val_accuracy: 0.6250\n",
      "Epoch 146/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2123 - accuracy: 0.4877 - val_loss: 0.9265 - val_accuracy: 0.6172\n",
      "Epoch 147/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2117 - accuracy: 0.4872 - val_loss: 0.9284 - val_accuracy: 0.6328\n",
      "Epoch 148/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2111 - accuracy: 0.4877 - val_loss: 0.9257 - val_accuracy: 0.6250\n",
      "Epoch 149/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2113 - accuracy: 0.4888 - val_loss: 0.9208 - val_accuracy: 0.6250\n",
      "Epoch 150/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2114 - accuracy: 0.4855 - val_loss: 0.9322 - val_accuracy: 0.6172\n",
      "Epoch 151/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2115 - accuracy: 0.4900 - val_loss: 0.9307 - val_accuracy: 0.6250\n",
      "Epoch 152/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2107 - accuracy: 0.4877 - val_loss: 0.9244 - val_accuracy: 0.6172\n",
      "Epoch 153/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2110 - accuracy: 0.4883 - val_loss: 0.9299 - val_accuracy: 0.6250\n",
      "Epoch 154/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2115 - accuracy: 0.4894 - val_loss: 0.9234 - val_accuracy: 0.6172\n",
      "Epoch 155/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2112 - accuracy: 0.4860 - val_loss: 0.9260 - val_accuracy: 0.6172\n",
      "Epoch 156/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2113 - accuracy: 0.4894 - val_loss: 0.9267 - val_accuracy: 0.6250\n",
      "Epoch 157/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2117 - accuracy: 0.4872 - val_loss: 0.9316 - val_accuracy: 0.6328\n",
      "Epoch 158/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2114 - accuracy: 0.4866 - val_loss: 0.9171 - val_accuracy: 0.6328\n",
      "Epoch 159/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2110 - accuracy: 0.4872 - val_loss: 0.9278 - val_accuracy: 0.6250\n",
      "Epoch 160/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2110 - accuracy: 0.4883 - val_loss: 0.9295 - val_accuracy: 0.6328\n",
      "Epoch 161/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2104 - accuracy: 0.4872 - val_loss: 0.9209 - val_accuracy: 0.6250\n",
      "Epoch 162/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2111 - accuracy: 0.4860 - val_loss: 0.9225 - val_accuracy: 0.6328\n",
      "Epoch 163/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2109 - accuracy: 0.4866 - val_loss: 0.9192 - val_accuracy: 0.6328\n",
      "Epoch 164/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2101 - accuracy: 0.4877 - val_loss: 0.9264 - val_accuracy: 0.6328\n",
      "Epoch 165/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2103 - accuracy: 0.4877 - val_loss: 0.9267 - val_accuracy: 0.6172\n",
      "Epoch 166/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2100 - accuracy: 0.4883 - val_loss: 0.9208 - val_accuracy: 0.6250\n",
      "Epoch 167/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2103 - accuracy: 0.4877 - val_loss: 0.9264 - val_accuracy: 0.6172\n",
      "Epoch 168/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2106 - accuracy: 0.4877 - val_loss: 0.9247 - val_accuracy: 0.6250\n",
      "Epoch 169/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2107 - accuracy: 0.4877 - val_loss: 0.9265 - val_accuracy: 0.6172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2114 - accuracy: 0.4872 - val_loss: 0.9250 - val_accuracy: 0.6172\n",
      "Epoch 171/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2114 - accuracy: 0.4888 - val_loss: 0.9256 - val_accuracy: 0.6328\n",
      "Epoch 172/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2097 - accuracy: 0.4883 - val_loss: 0.9249 - val_accuracy: 0.6172\n",
      "Epoch 173/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2119 - accuracy: 0.4855 - val_loss: 0.9274 - val_accuracy: 0.6328\n",
      "Epoch 174/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2116 - accuracy: 0.4883 - val_loss: 0.9267 - val_accuracy: 0.6172\n",
      "Epoch 175/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2102 - accuracy: 0.4883 - val_loss: 0.9223 - val_accuracy: 0.6250\n",
      "Epoch 176/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2099 - accuracy: 0.4872 - val_loss: 0.9239 - val_accuracy: 0.6328\n",
      "Epoch 177/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2104 - accuracy: 0.4872 - val_loss: 0.9122 - val_accuracy: 0.6250\n",
      "Epoch 178/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2102 - accuracy: 0.4866 - val_loss: 0.9269 - val_accuracy: 0.6172\n",
      "Epoch 179/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2099 - accuracy: 0.4883 - val_loss: 0.9242 - val_accuracy: 0.6328\n",
      "Epoch 180/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2099 - accuracy: 0.4894 - val_loss: 0.9231 - val_accuracy: 0.6250\n",
      "Epoch 181/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2094 - accuracy: 0.4877 - val_loss: 0.9242 - val_accuracy: 0.6328\n",
      "Epoch 182/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2100 - accuracy: 0.4883 - val_loss: 0.9280 - val_accuracy: 0.6328\n",
      "Epoch 183/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2095 - accuracy: 0.4872 - val_loss: 0.9235 - val_accuracy: 0.6328\n",
      "Epoch 184/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2110 - accuracy: 0.4905 - val_loss: 0.9190 - val_accuracy: 0.6328\n",
      "Epoch 185/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2100 - accuracy: 0.4883 - val_loss: 0.9238 - val_accuracy: 0.6250\n",
      "Epoch 186/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2095 - accuracy: 0.4883 - val_loss: 0.9248 - val_accuracy: 0.6250\n",
      "Epoch 187/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2095 - accuracy: 0.4883 - val_loss: 0.9267 - val_accuracy: 0.6328\n",
      "Epoch 188/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2100 - accuracy: 0.4872 - val_loss: 0.9261 - val_accuracy: 0.6328\n",
      "Epoch 189/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2093 - accuracy: 0.4883 - val_loss: 0.9175 - val_accuracy: 0.6250\n",
      "Epoch 190/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2105 - accuracy: 0.4877 - val_loss: 0.9181 - val_accuracy: 0.6328\n",
      "Epoch 191/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2099 - accuracy: 0.4877 - val_loss: 0.9264 - val_accuracy: 0.6250\n",
      "Epoch 192/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2098 - accuracy: 0.4877 - val_loss: 0.9283 - val_accuracy: 0.6328\n",
      "Epoch 193/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2090 - accuracy: 0.4877 - val_loss: 0.9240 - val_accuracy: 0.6328\n",
      "Epoch 194/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2092 - accuracy: 0.4883 - val_loss: 0.9276 - val_accuracy: 0.6250\n",
      "Epoch 195/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2098 - accuracy: 0.4872 - val_loss: 0.9224 - val_accuracy: 0.6250\n",
      "Epoch 196/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2096 - accuracy: 0.4883 - val_loss: 0.9210 - val_accuracy: 0.6172\n",
      "Epoch 197/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2090 - accuracy: 0.4883 - val_loss: 0.9242 - val_accuracy: 0.6328\n",
      "Epoch 198/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2093 - accuracy: 0.4872 - val_loss: 0.9315 - val_accuracy: 0.6250\n",
      "Epoch 199/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2096 - accuracy: 0.4900 - val_loss: 0.9252 - val_accuracy: 0.6328\n",
      "Epoch 200/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2098 - accuracy: 0.4877 - val_loss: 0.9245 - val_accuracy: 0.6328\n",
      "Epoch 201/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2097 - accuracy: 0.4888 - val_loss: 0.9199 - val_accuracy: 0.6250\n",
      "Epoch 202/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2094 - accuracy: 0.4894 - val_loss: 0.9220 - val_accuracy: 0.6328\n",
      "Epoch 203/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2095 - accuracy: 0.4883 - val_loss: 0.9284 - val_accuracy: 0.6250\n",
      "Epoch 204/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2098 - accuracy: 0.4888 - val_loss: 0.9240 - val_accuracy: 0.6172\n",
      "Epoch 205/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2087 - accuracy: 0.4877 - val_loss: 0.9220 - val_accuracy: 0.6250\n",
      "Epoch 206/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2092 - accuracy: 0.4894 - val_loss: 0.9227 - val_accuracy: 0.6328\n",
      "Epoch 207/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2087 - accuracy: 0.4888 - val_loss: 0.9202 - val_accuracy: 0.6250\n",
      "Epoch 208/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2088 - accuracy: 0.4888 - val_loss: 0.9245 - val_accuracy: 0.6172\n",
      "Epoch 209/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2090 - accuracy: 0.4877 - val_loss: 0.9227 - val_accuracy: 0.6250\n",
      "Epoch 210/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2103 - accuracy: 0.4866 - val_loss: 0.9300 - val_accuracy: 0.6328\n",
      "Epoch 211/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2098 - accuracy: 0.4888 - val_loss: 0.9220 - val_accuracy: 0.6328\n",
      "Epoch 212/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2089 - accuracy: 0.4888 - val_loss: 0.9180 - val_accuracy: 0.6250\n",
      "Epoch 213/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2098 - accuracy: 0.4883 - val_loss: 0.9239 - val_accuracy: 0.6250\n",
      "Epoch 214/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2095 - accuracy: 0.4877 - val_loss: 0.9269 - val_accuracy: 0.6250\n",
      "Epoch 215/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2093 - accuracy: 0.4883 - val_loss: 0.9290 - val_accuracy: 0.6172\n",
      "Epoch 216/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2088 - accuracy: 0.4883 - val_loss: 0.9214 - val_accuracy: 0.6250\n",
      "Epoch 217/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2087 - accuracy: 0.4883 - val_loss: 0.9192 - val_accuracy: 0.6250\n",
      "Epoch 218/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2087 - accuracy: 0.4872 - val_loss: 0.9217 - val_accuracy: 0.6250\n",
      "Epoch 219/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2094 - accuracy: 0.4894 - val_loss: 0.9148 - val_accuracy: 0.6328\n",
      "Epoch 220/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2091 - accuracy: 0.4888 - val_loss: 0.9219 - val_accuracy: 0.6250\n",
      "Epoch 221/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2107 - accuracy: 0.4888 - val_loss: 0.9284 - val_accuracy: 0.6172\n",
      "Epoch 222/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2105 - accuracy: 0.4877 - val_loss: 0.9232 - val_accuracy: 0.6250\n",
      "Epoch 223/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2091 - accuracy: 0.4900 - val_loss: 0.9301 - val_accuracy: 0.6250\n",
      "Epoch 224/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2088 - accuracy: 0.4883 - val_loss: 0.9201 - val_accuracy: 0.6250\n",
      "Epoch 225/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2097 - accuracy: 0.4883 - val_loss: 0.9234 - val_accuracy: 0.6250\n",
      "Epoch 226/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2098 - accuracy: 0.4877 - val_loss: 0.9241 - val_accuracy: 0.6328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2088 - accuracy: 0.4883 - val_loss: 0.9258 - val_accuracy: 0.6172\n",
      "Epoch 228/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2090 - accuracy: 0.4888 - val_loss: 0.9185 - val_accuracy: 0.6250\n",
      "Epoch 229/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2090 - accuracy: 0.4872 - val_loss: 0.9251 - val_accuracy: 0.6250\n",
      "Epoch 230/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2083 - accuracy: 0.4872 - val_loss: 0.9223 - val_accuracy: 0.6250\n",
      "Epoch 231/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2088 - accuracy: 0.4883 - val_loss: 0.9203 - val_accuracy: 0.6250\n",
      "Epoch 232/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2089 - accuracy: 0.4888 - val_loss: 0.9186 - val_accuracy: 0.6250\n",
      "Epoch 233/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2087 - accuracy: 0.4888 - val_loss: 0.9253 - val_accuracy: 0.6250\n",
      "Epoch 234/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2091 - accuracy: 0.4888 - val_loss: 0.9278 - val_accuracy: 0.6250\n",
      "Epoch 235/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2083 - accuracy: 0.4883 - val_loss: 0.9232 - val_accuracy: 0.6250\n",
      "Epoch 236/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2085 - accuracy: 0.4888 - val_loss: 0.9191 - val_accuracy: 0.6328\n",
      "Epoch 237/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2087 - accuracy: 0.4883 - val_loss: 0.9209 - val_accuracy: 0.6250\n",
      "Epoch 238/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2105 - accuracy: 0.4877 - val_loss: 0.9210 - val_accuracy: 0.6250\n",
      "Epoch 239/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2093 - accuracy: 0.4888 - val_loss: 0.9244 - val_accuracy: 0.6250\n",
      "Epoch 240/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2082 - accuracy: 0.4894 - val_loss: 0.9210 - val_accuracy: 0.6250\n",
      "Epoch 241/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2087 - accuracy: 0.4888 - val_loss: 0.9268 - val_accuracy: 0.6250\n",
      "Epoch 242/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2090 - accuracy: 0.4883 - val_loss: 0.9130 - val_accuracy: 0.6328\n",
      "Epoch 243/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2086 - accuracy: 0.4883 - val_loss: 0.9215 - val_accuracy: 0.6328\n",
      "Epoch 244/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2081 - accuracy: 0.4877 - val_loss: 0.9245 - val_accuracy: 0.6250\n",
      "Epoch 245/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2081 - accuracy: 0.4883 - val_loss: 0.9250 - val_accuracy: 0.6250\n",
      "Epoch 246/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2089 - accuracy: 0.4888 - val_loss: 0.9207 - val_accuracy: 0.6250\n",
      "Epoch 247/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2085 - accuracy: 0.4883 - val_loss: 0.9228 - val_accuracy: 0.6406\n",
      "Epoch 248/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2091 - accuracy: 0.4883 - val_loss: 0.9206 - val_accuracy: 0.6250\n",
      "Epoch 249/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2088 - accuracy: 0.4877 - val_loss: 0.9225 - val_accuracy: 0.6172\n",
      "Epoch 250/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2079 - accuracy: 0.4877 - val_loss: 0.9164 - val_accuracy: 0.6250\n",
      "Epoch 251/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2086 - accuracy: 0.4888 - val_loss: 0.9194 - val_accuracy: 0.6250\n",
      "Epoch 252/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2086 - accuracy: 0.4877 - val_loss: 0.9232 - val_accuracy: 0.6250\n",
      "Epoch 253/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2079 - accuracy: 0.4894 - val_loss: 0.9224 - val_accuracy: 0.6250\n",
      "Epoch 254/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2083 - accuracy: 0.4916 - val_loss: 0.9239 - val_accuracy: 0.6250\n",
      "Epoch 255/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2082 - accuracy: 0.4894 - val_loss: 0.9223 - val_accuracy: 0.6406\n",
      "Epoch 256/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2081 - accuracy: 0.4888 - val_loss: 0.9116 - val_accuracy: 0.6328\n",
      "Epoch 257/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2086 - accuracy: 0.4877 - val_loss: 0.9182 - val_accuracy: 0.6250\n",
      "Epoch 258/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2085 - accuracy: 0.4883 - val_loss: 0.9233 - val_accuracy: 0.6250\n",
      "Epoch 259/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2086 - accuracy: 0.4866 - val_loss: 0.9295 - val_accuracy: 0.6328\n",
      "Epoch 260/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2085 - accuracy: 0.4877 - val_loss: 0.9227 - val_accuracy: 0.6250\n",
      "Epoch 261/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2082 - accuracy: 0.4894 - val_loss: 0.9215 - val_accuracy: 0.6250\n",
      "Epoch 262/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2077 - accuracy: 0.4877 - val_loss: 0.9194 - val_accuracy: 0.6250\n",
      "Epoch 263/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2078 - accuracy: 0.4883 - val_loss: 0.9195 - val_accuracy: 0.6250\n",
      "Epoch 264/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2074 - accuracy: 0.4877 - val_loss: 0.9244 - val_accuracy: 0.6250\n",
      "Epoch 265/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2080 - accuracy: 0.4888 - val_loss: 0.9205 - val_accuracy: 0.6250\n",
      "Epoch 266/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2084 - accuracy: 0.4877 - val_loss: 0.9308 - val_accuracy: 0.6250\n",
      "Epoch 267/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2082 - accuracy: 0.4883 - val_loss: 0.9180 - val_accuracy: 0.6250\n",
      "Epoch 268/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2086 - accuracy: 0.4860 - val_loss: 0.9275 - val_accuracy: 0.6406\n",
      "Epoch 269/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2089 - accuracy: 0.4894 - val_loss: 0.9248 - val_accuracy: 0.6250\n",
      "Epoch 270/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2085 - accuracy: 0.4877 - val_loss: 0.9155 - val_accuracy: 0.6328\n",
      "Epoch 271/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2089 - accuracy: 0.4883 - val_loss: 0.9245 - val_accuracy: 0.6250\n",
      "Epoch 272/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2078 - accuracy: 0.4888 - val_loss: 0.9179 - val_accuracy: 0.6328\n",
      "Epoch 273/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2078 - accuracy: 0.4894 - val_loss: 0.9175 - val_accuracy: 0.6250\n",
      "Epoch 274/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2082 - accuracy: 0.4877 - val_loss: 0.9255 - val_accuracy: 0.6172\n",
      "Epoch 275/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2086 - accuracy: 0.4872 - val_loss: 0.9215 - val_accuracy: 0.6328\n",
      "Epoch 276/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2082 - accuracy: 0.4894 - val_loss: 0.9180 - val_accuracy: 0.6328\n",
      "Epoch 277/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2082 - accuracy: 0.4900 - val_loss: 0.9228 - val_accuracy: 0.6250\n",
      "Epoch 278/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2083 - accuracy: 0.4888 - val_loss: 0.9239 - val_accuracy: 0.6250\n",
      "Epoch 279/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2086 - accuracy: 0.4883 - val_loss: 0.9193 - val_accuracy: 0.6328\n",
      "Epoch 280/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2076 - accuracy: 0.4883 - val_loss: 0.9190 - val_accuracy: 0.6328\n",
      "Epoch 281/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2073 - accuracy: 0.4883 - val_loss: 0.9172 - val_accuracy: 0.6328\n",
      "Epoch 282/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2077 - accuracy: 0.4883 - val_loss: 0.9210 - val_accuracy: 0.6250\n",
      "Epoch 283/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2077 - accuracy: 0.4883 - val_loss: 0.9170 - val_accuracy: 0.6328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 284/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2083 - accuracy: 0.4883 - val_loss: 0.9186 - val_accuracy: 0.6406\n",
      "Epoch 285/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2072 - accuracy: 0.4883 - val_loss: 0.9187 - val_accuracy: 0.6250\n",
      "Epoch 286/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2071 - accuracy: 0.4883 - val_loss: 0.9116 - val_accuracy: 0.6328\n",
      "Epoch 287/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2085 - accuracy: 0.4860 - val_loss: 0.9165 - val_accuracy: 0.6328\n",
      "Epoch 288/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2074 - accuracy: 0.4877 - val_loss: 0.9154 - val_accuracy: 0.6328\n",
      "Epoch 289/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2078 - accuracy: 0.4888 - val_loss: 0.9193 - val_accuracy: 0.6250\n",
      "Epoch 290/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2080 - accuracy: 0.4877 - val_loss: 0.9194 - val_accuracy: 0.6172\n",
      "Epoch 291/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2070 - accuracy: 0.4883 - val_loss: 0.9228 - val_accuracy: 0.6250\n",
      "Epoch 292/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2069 - accuracy: 0.4866 - val_loss: 0.9223 - val_accuracy: 0.6250\n",
      "Epoch 293/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2079 - accuracy: 0.4866 - val_loss: 0.9261 - val_accuracy: 0.6250\n",
      "Epoch 294/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2077 - accuracy: 0.4888 - val_loss: 0.9191 - val_accuracy: 0.6328\n",
      "Epoch 295/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2080 - accuracy: 0.4866 - val_loss: 0.9271 - val_accuracy: 0.6250\n",
      "Epoch 296/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2073 - accuracy: 0.4900 - val_loss: 0.9207 - val_accuracy: 0.6250\n",
      "Epoch 297/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1.2070 - accuracy: 0.4888 - val_loss: 0.9181 - val_accuracy: 0.6328\n",
      "Epoch 298/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.2072 - accuracy: 0.4894 - val_loss: 0.9228 - val_accuracy: 0.6250\n",
      "Epoch 299/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.2083 - accuracy: 0.4883 - val_loss: 0.9197 - val_accuracy: 0.6250\n",
      "Epoch 300/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.2077 - accuracy: 0.4877 - val_loss: 0.9228 - val_accuracy: 0.6250\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "56/56 [==============================] - 0s 1ms/step\n",
      "Recall: [0.94 0.   0.56]\n",
      "Precision: [0.41 0.   0.71]\n",
      "F1-score: [0.57 0.   0.63]\n",
      "Accuracy: 0.49\n",
      "Brier score:0.20844519602884842\n",
      "Brier climat:0.21659722222222225\n",
      "Brier skill score:0.03763679935382591\n",
      "Recall: [0.94 0.   0.56]\n",
      "Precision: [0.41 0.   0.71]\n",
      "F1-score: [0.57 0.   0.63]\n",
      "Accuracy: 0.49\n",
      "Brier score:0.2259260481989039\n",
      "Brier climat:0.22336061507936508\n",
      "Brier skill score:-0.011485610919486744\n",
      "Recall: [0.94 0.   0.56]\n",
      "Precision: [0.41 0.   0.71]\n",
      "F1-score: [0.57 0.   0.63]\n",
      "Accuracy: 0.49\n",
      "Brier score:0.15234136544524496\n",
      "Brier climat:0.21785714285714283\n",
      "Brier skill score:0.3007281586119903\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "Recall: [0.83 0.   0.81]\n",
      "Precision: [0.25 0.   0.8 ]\n",
      "F1-score: [0.38 0.   0.8 ]\n",
      "Accuracy: 0.62\n",
      "Brier score:0.1221425933542152\n",
      "Brier climat:0.12102430555555557\n",
      "Brier skill score:-0.009240191823668864\n",
      "Recall: [0.83 0.   0.81]\n",
      "Precision: [0.25 0.   0.8 ]\n",
      "F1-score: [0.38 0.   0.8 ]\n",
      "Accuracy: 0.62\n",
      "Brier score:0.1622922759199295\n",
      "Brier climat:0.19274305555555557\n",
      "Brier skill score:0.15798639047127194\n",
      "Recall: [0.83 0.   0.81]\n",
      "Precision: [0.25 0.   0.8 ]\n",
      "F1-score: [0.38 0.   0.8 ]\n",
      "Accuracy: 0.62\n",
      "Brier score:0.2050756252318801\n",
      "Brier climat:0.33671875\n",
      "Brier skill score:0.3909587000074094\n",
      "******** 26\n",
      "validation years [2007, 2008]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2009, 2010, 1981, 1982, 1983}\n",
      "{0: 2.0, 1: 1.0, 2: 1.0}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 60)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 183       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183\n",
      "Trainable params: 183\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "28/28 [==============================] - ETA: 0s - loss: 2.2725 - accuracy: 0.2612WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "28/28 [==============================] - 4s 113ms/step - loss: 2.2725 - accuracy: 0.2612 - val_loss: 1.7243 - val_accuracy: 0.2812\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.8397 - accuracy: 0.3477 - val_loss: 1.6475 - val_accuracy: 0.2266\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.6423 - accuracy: 0.4414 - val_loss: 1.6281 - val_accuracy: 0.2422\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.5586 - accuracy: 0.4944 - val_loss: 1.6246 - val_accuracy: 0.3047\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.5116 - accuracy: 0.5067 - val_loss: 1.6173 - val_accuracy: 0.2734\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.4815 - accuracy: 0.5017 - val_loss: 1.6129 - val_accuracy: 0.2109\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4594 - accuracy: 0.5022 - val_loss: 1.6144 - val_accuracy: 0.2031\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4426 - accuracy: 0.5039 - val_loss: 1.6104 - val_accuracy: 0.1875\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.4282 - accuracy: 0.5089 - val_loss: 1.6044 - val_accuracy: 0.2031\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.4153 - accuracy: 0.5022 - val_loss: 1.6008 - val_accuracy: 0.1797\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.4043 - accuracy: 0.5033 - val_loss: 1.5931 - val_accuracy: 0.1875\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.3939 - accuracy: 0.5017 - val_loss: 1.5905 - val_accuracy: 0.1875\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.3853 - accuracy: 0.5045 - val_loss: 1.5911 - val_accuracy: 0.2188\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3759 - accuracy: 0.5033 - val_loss: 1.5794 - val_accuracy: 0.2188\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.3677 - accuracy: 0.5045 - val_loss: 1.5789 - val_accuracy: 0.2266\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3600 - accuracy: 0.5017 - val_loss: 1.5791 - val_accuracy: 0.2266\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3519 - accuracy: 0.5061 - val_loss: 1.5730 - val_accuracy: 0.2500\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.3442 - accuracy: 0.5050 - val_loss: 1.5711 - val_accuracy: 0.2500\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3369 - accuracy: 0.5045 - val_loss: 1.5605 - val_accuracy: 0.2656\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.3304 - accuracy: 0.5033 - val_loss: 1.5615 - val_accuracy: 0.2500\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3238 - accuracy: 0.5045 - val_loss: 1.5587 - val_accuracy: 0.2656\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3171 - accuracy: 0.5045 - val_loss: 1.5574 - val_accuracy: 0.2734\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3117 - accuracy: 0.5061 - val_loss: 1.5532 - val_accuracy: 0.2578\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3060 - accuracy: 0.5050 - val_loss: 1.5437 - val_accuracy: 0.2891\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3003 - accuracy: 0.5045 - val_loss: 1.5443 - val_accuracy: 0.2891\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2945 - accuracy: 0.5061 - val_loss: 1.5438 - val_accuracy: 0.2969\n",
      "Epoch 27/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2894 - accuracy: 0.5078 - val_loss: 1.5419 - val_accuracy: 0.2969\n",
      "Epoch 28/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2848 - accuracy: 0.5067 - val_loss: 1.5374 - val_accuracy: 0.3125\n",
      "Epoch 29/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2803 - accuracy: 0.5078 - val_loss: 1.5323 - val_accuracy: 0.3125\n",
      "Epoch 30/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2763 - accuracy: 0.5078 - val_loss: 1.5312 - val_accuracy: 0.3203\n",
      "Epoch 31/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2715 - accuracy: 0.5089 - val_loss: 1.5299 - val_accuracy: 0.3125\n",
      "Epoch 32/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2674 - accuracy: 0.5061 - val_loss: 1.5202 - val_accuracy: 0.3281\n",
      "Epoch 33/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2632 - accuracy: 0.5056 - val_loss: 1.5184 - val_accuracy: 0.3203\n",
      "Epoch 34/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2603 - accuracy: 0.5067 - val_loss: 1.5217 - val_accuracy: 0.3281\n",
      "Epoch 35/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2554 - accuracy: 0.5084 - val_loss: 1.5116 - val_accuracy: 0.3516\n",
      "Epoch 36/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2525 - accuracy: 0.5045 - val_loss: 1.5016 - val_accuracy: 0.3516\n",
      "Epoch 37/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2491 - accuracy: 0.5073 - val_loss: 1.5031 - val_accuracy: 0.3594\n",
      "Epoch 38/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2459 - accuracy: 0.5073 - val_loss: 1.4997 - val_accuracy: 0.3516\n",
      "Epoch 39/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2421 - accuracy: 0.5045 - val_loss: 1.4961 - val_accuracy: 0.3516\n",
      "Epoch 40/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2394 - accuracy: 0.5039 - val_loss: 1.4939 - val_accuracy: 0.3516\n",
      "Epoch 41/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2363 - accuracy: 0.5039 - val_loss: 1.4870 - val_accuracy: 0.3594\n",
      "Epoch 42/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2338 - accuracy: 0.5033 - val_loss: 1.4868 - val_accuracy: 0.3594\n",
      "Epoch 43/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2321 - accuracy: 0.5045 - val_loss: 1.4845 - val_accuracy: 0.3516\n",
      "Epoch 44/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2290 - accuracy: 0.5039 - val_loss: 1.4834 - val_accuracy: 0.3594\n",
      "Epoch 45/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2263 - accuracy: 0.5033 - val_loss: 1.4783 - val_accuracy: 0.3516\n",
      "Epoch 46/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2242 - accuracy: 0.5050 - val_loss: 1.4772 - val_accuracy: 0.3516\n",
      "Epoch 47/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2221 - accuracy: 0.5067 - val_loss: 1.4768 - val_accuracy: 0.3594\n",
      "Epoch 48/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2218 - accuracy: 0.5067 - val_loss: 1.4777 - val_accuracy: 0.3594\n",
      "Epoch 49/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2197 - accuracy: 0.5050 - val_loss: 1.4765 - val_accuracy: 0.3594\n",
      "Epoch 50/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2173 - accuracy: 0.5061 - val_loss: 1.4744 - val_accuracy: 0.3438\n",
      "Epoch 51/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2155 - accuracy: 0.5056 - val_loss: 1.4776 - val_accuracy: 0.3438\n",
      "Epoch 52/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2134 - accuracy: 0.5061 - val_loss: 1.4716 - val_accuracy: 0.3438\n",
      "Epoch 53/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2124 - accuracy: 0.5045 - val_loss: 1.4677 - val_accuracy: 0.3516\n",
      "Epoch 54/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2115 - accuracy: 0.5061 - val_loss: 1.4705 - val_accuracy: 0.3438\n",
      "Epoch 55/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2105 - accuracy: 0.5050 - val_loss: 1.4653 - val_accuracy: 0.3750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2085 - accuracy: 0.5073 - val_loss: 1.4709 - val_accuracy: 0.3594\n",
      "Epoch 57/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2079 - accuracy: 0.5073 - val_loss: 1.4667 - val_accuracy: 0.3750\n",
      "Epoch 58/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2067 - accuracy: 0.5095 - val_loss: 1.4693 - val_accuracy: 0.3672\n",
      "Epoch 59/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2049 - accuracy: 0.5061 - val_loss: 1.4730 - val_accuracy: 0.3672\n",
      "Epoch 60/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2038 - accuracy: 0.5067 - val_loss: 1.4681 - val_accuracy: 0.3516\n",
      "Epoch 61/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2032 - accuracy: 0.5089 - val_loss: 1.4684 - val_accuracy: 0.3672\n",
      "Epoch 62/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2017 - accuracy: 0.5078 - val_loss: 1.4669 - val_accuracy: 0.3750\n",
      "Epoch 63/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2011 - accuracy: 0.5089 - val_loss: 1.4629 - val_accuracy: 0.3516\n",
      "Epoch 64/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1993 - accuracy: 0.5095 - val_loss: 1.4629 - val_accuracy: 0.3750\n",
      "Epoch 65/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1989 - accuracy: 0.5089 - val_loss: 1.4619 - val_accuracy: 0.3672\n",
      "Epoch 66/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1978 - accuracy: 0.5095 - val_loss: 1.4611 - val_accuracy: 0.3438\n",
      "Epoch 67/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1976 - accuracy: 0.5089 - val_loss: 1.4617 - val_accuracy: 0.3672\n",
      "Epoch 68/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1961 - accuracy: 0.5112 - val_loss: 1.4621 - val_accuracy: 0.3672\n",
      "Epoch 69/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1949 - accuracy: 0.5128 - val_loss: 1.4594 - val_accuracy: 0.3438\n",
      "Epoch 70/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1938 - accuracy: 0.5106 - val_loss: 1.4569 - val_accuracy: 0.3672\n",
      "Epoch 71/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1934 - accuracy: 0.5128 - val_loss: 1.4582 - val_accuracy: 0.3672\n",
      "Epoch 72/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1918 - accuracy: 0.5123 - val_loss: 1.4550 - val_accuracy: 0.3750\n",
      "Epoch 73/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1909 - accuracy: 0.5123 - val_loss: 1.4554 - val_accuracy: 0.3672\n",
      "Epoch 74/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1901 - accuracy: 0.5123 - val_loss: 1.4531 - val_accuracy: 0.3672\n",
      "Epoch 75/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1897 - accuracy: 0.5117 - val_loss: 1.4533 - val_accuracy: 0.3672\n",
      "Epoch 76/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1887 - accuracy: 0.5123 - val_loss: 1.4530 - val_accuracy: 0.3750\n",
      "Epoch 77/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1884 - accuracy: 0.5140 - val_loss: 1.4494 - val_accuracy: 0.3672\n",
      "Epoch 78/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1869 - accuracy: 0.5145 - val_loss: 1.4487 - val_accuracy: 0.3594\n",
      "Epoch 79/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1867 - accuracy: 0.5145 - val_loss: 1.4458 - val_accuracy: 0.3516\n",
      "Epoch 80/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1863 - accuracy: 0.5123 - val_loss: 1.4418 - val_accuracy: 0.3672\n",
      "Epoch 81/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1859 - accuracy: 0.5134 - val_loss: 1.4461 - val_accuracy: 0.3594\n",
      "Epoch 82/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1845 - accuracy: 0.5145 - val_loss: 1.4374 - val_accuracy: 0.3672\n",
      "Epoch 83/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1845 - accuracy: 0.5134 - val_loss: 1.4370 - val_accuracy: 0.3594\n",
      "Epoch 84/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1838 - accuracy: 0.5145 - val_loss: 1.4387 - val_accuracy: 0.3594\n",
      "Epoch 85/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1831 - accuracy: 0.5140 - val_loss: 1.4395 - val_accuracy: 0.3594\n",
      "Epoch 86/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1826 - accuracy: 0.5117 - val_loss: 1.4371 - val_accuracy: 0.3594\n",
      "Epoch 87/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1819 - accuracy: 0.5134 - val_loss: 1.4376 - val_accuracy: 0.3516\n",
      "Epoch 88/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1818 - accuracy: 0.5128 - val_loss: 1.4345 - val_accuracy: 0.3750\n",
      "Epoch 89/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1819 - accuracy: 0.5151 - val_loss: 1.4330 - val_accuracy: 0.3672\n",
      "Epoch 90/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1812 - accuracy: 0.5117 - val_loss: 1.4330 - val_accuracy: 0.3516\n",
      "Epoch 91/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1802 - accuracy: 0.5128 - val_loss: 1.4285 - val_accuracy: 0.3594\n",
      "Epoch 92/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1794 - accuracy: 0.5112 - val_loss: 1.4303 - val_accuracy: 0.3594\n",
      "Epoch 93/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1793 - accuracy: 0.5156 - val_loss: 1.4355 - val_accuracy: 0.3594\n",
      "Epoch 94/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1790 - accuracy: 0.5140 - val_loss: 1.4287 - val_accuracy: 0.3594\n",
      "Epoch 95/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1794 - accuracy: 0.5134 - val_loss: 1.4329 - val_accuracy: 0.3516\n",
      "Epoch 96/300\n",
      "28/28 [==============================] - 1s 29ms/step - loss: 1.1794 - accuracy: 0.5134 - val_loss: 1.4340 - val_accuracy: 0.3672\n",
      "Epoch 97/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1790 - accuracy: 0.5140 - val_loss: 1.4277 - val_accuracy: 0.3672\n",
      "Epoch 98/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1782 - accuracy: 0.5106 - val_loss: 1.4294 - val_accuracy: 0.3516\n",
      "Epoch 99/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1771 - accuracy: 0.5156 - val_loss: 1.4259 - val_accuracy: 0.3672\n",
      "Epoch 100/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1774 - accuracy: 0.5151 - val_loss: 1.4288 - val_accuracy: 0.3672\n",
      "Epoch 101/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1766 - accuracy: 0.5145 - val_loss: 1.4253 - val_accuracy: 0.3594\n",
      "Epoch 102/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1761 - accuracy: 0.5140 - val_loss: 1.4249 - val_accuracy: 0.3750\n",
      "Epoch 103/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1765 - accuracy: 0.5112 - val_loss: 1.4255 - val_accuracy: 0.3516\n",
      "Epoch 104/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1761 - accuracy: 0.5128 - val_loss: 1.4228 - val_accuracy: 0.3672\n",
      "Epoch 105/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1750 - accuracy: 0.5140 - val_loss: 1.4216 - val_accuracy: 0.3594\n",
      "Epoch 106/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1753 - accuracy: 0.5123 - val_loss: 1.4220 - val_accuracy: 0.3594\n",
      "Epoch 107/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1752 - accuracy: 0.5106 - val_loss: 1.4265 - val_accuracy: 0.3516\n",
      "Epoch 108/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1743 - accuracy: 0.5140 - val_loss: 1.4232 - val_accuracy: 0.3750\n",
      "Epoch 109/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1751 - accuracy: 0.5112 - val_loss: 1.4202 - val_accuracy: 0.3594\n",
      "Epoch 110/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1743 - accuracy: 0.5134 - val_loss: 1.4239 - val_accuracy: 0.3516\n",
      "Epoch 111/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1731 - accuracy: 0.5123 - val_loss: 1.4242 - val_accuracy: 0.3828\n",
      "Epoch 112/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1733 - accuracy: 0.5128 - val_loss: 1.4225 - val_accuracy: 0.3828\n",
      "Epoch 113/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1737 - accuracy: 0.5106 - val_loss: 1.4290 - val_accuracy: 0.3516\n",
      "Epoch 114/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1735 - accuracy: 0.5128 - val_loss: 1.4207 - val_accuracy: 0.3828\n",
      "Epoch 115/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1736 - accuracy: 0.5128 - val_loss: 1.4222 - val_accuracy: 0.3516\n",
      "Epoch 116/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1733 - accuracy: 0.5117 - val_loss: 1.4284 - val_accuracy: 0.3828\n",
      "Epoch 117/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1719 - accuracy: 0.5134 - val_loss: 1.4225 - val_accuracy: 0.3594\n",
      "Epoch 118/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1716 - accuracy: 0.5112 - val_loss: 1.4228 - val_accuracy: 0.3594\n",
      "Epoch 119/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1716 - accuracy: 0.5123 - val_loss: 1.4220 - val_accuracy: 0.3750\n",
      "Epoch 120/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1711 - accuracy: 0.5128 - val_loss: 1.4220 - val_accuracy: 0.3828\n",
      "Epoch 121/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1707 - accuracy: 0.5112 - val_loss: 1.4225 - val_accuracy: 0.3828\n",
      "Epoch 122/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1714 - accuracy: 0.5134 - val_loss: 1.4241 - val_accuracy: 0.3594\n",
      "Epoch 123/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1706 - accuracy: 0.5112 - val_loss: 1.4188 - val_accuracy: 0.3594\n",
      "Epoch 124/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1702 - accuracy: 0.5117 - val_loss: 1.4162 - val_accuracy: 0.3828\n",
      "Epoch 125/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1700 - accuracy: 0.5123 - val_loss: 1.4236 - val_accuracy: 0.3594\n",
      "Epoch 126/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1695 - accuracy: 0.5117 - val_loss: 1.4221 - val_accuracy: 0.3672\n",
      "Epoch 127/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1699 - accuracy: 0.5128 - val_loss: 1.4170 - val_accuracy: 0.3594\n",
      "Epoch 128/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1701 - accuracy: 0.5128 - val_loss: 1.4225 - val_accuracy: 0.3594\n",
      "Epoch 129/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1697 - accuracy: 0.5123 - val_loss: 1.4207 - val_accuracy: 0.3594\n",
      "Epoch 130/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1693 - accuracy: 0.5123 - val_loss: 1.4220 - val_accuracy: 0.3516\n",
      "Epoch 131/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1693 - accuracy: 0.5123 - val_loss: 1.4204 - val_accuracy: 0.3594\n",
      "Epoch 132/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1692 - accuracy: 0.5134 - val_loss: 1.4250 - val_accuracy: 0.3984\n",
      "Epoch 133/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1688 - accuracy: 0.5095 - val_loss: 1.4239 - val_accuracy: 0.3516\n",
      "Epoch 134/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1687 - accuracy: 0.5134 - val_loss: 1.4227 - val_accuracy: 0.3828\n",
      "Epoch 135/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1693 - accuracy: 0.5106 - val_loss: 1.4241 - val_accuracy: 0.3594\n",
      "Epoch 136/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1686 - accuracy: 0.5128 - val_loss: 1.4197 - val_accuracy: 0.3672\n",
      "Epoch 137/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1682 - accuracy: 0.5123 - val_loss: 1.4178 - val_accuracy: 0.3750\n",
      "Epoch 138/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1680 - accuracy: 0.5095 - val_loss: 1.4219 - val_accuracy: 0.3906\n",
      "Epoch 139/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1685 - accuracy: 0.5106 - val_loss: 1.4221 - val_accuracy: 0.3594\n",
      "Epoch 140/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1682 - accuracy: 0.5140 - val_loss: 1.4216 - val_accuracy: 0.3750\n",
      "Epoch 141/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1683 - accuracy: 0.5106 - val_loss: 1.4198 - val_accuracy: 0.3906\n",
      "Epoch 142/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1676 - accuracy: 0.5106 - val_loss: 1.4193 - val_accuracy: 0.3594\n",
      "Epoch 143/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1669 - accuracy: 0.5123 - val_loss: 1.4176 - val_accuracy: 0.3906\n",
      "Epoch 144/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1673 - accuracy: 0.5084 - val_loss: 1.4211 - val_accuracy: 0.3906\n",
      "Epoch 145/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1676 - accuracy: 0.5128 - val_loss: 1.4151 - val_accuracy: 0.3594\n",
      "Epoch 146/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1670 - accuracy: 0.5084 - val_loss: 1.4231 - val_accuracy: 0.3906\n",
      "Epoch 147/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1673 - accuracy: 0.5123 - val_loss: 1.4186 - val_accuracy: 0.3906\n",
      "Epoch 148/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1666 - accuracy: 0.5100 - val_loss: 1.4211 - val_accuracy: 0.3906\n",
      "Epoch 149/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1668 - accuracy: 0.5100 - val_loss: 1.4238 - val_accuracy: 0.3594\n",
      "Epoch 150/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1660 - accuracy: 0.5106 - val_loss: 1.4221 - val_accuracy: 0.3906\n",
      "Epoch 151/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1660 - accuracy: 0.5123 - val_loss: 1.4211 - val_accuracy: 0.3672\n",
      "Epoch 152/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1666 - accuracy: 0.5084 - val_loss: 1.4186 - val_accuracy: 0.3594\n",
      "Epoch 153/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1666 - accuracy: 0.5123 - val_loss: 1.4200 - val_accuracy: 0.3828\n",
      "Epoch 154/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1660 - accuracy: 0.5106 - val_loss: 1.4207 - val_accuracy: 0.3906\n",
      "Epoch 155/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1656 - accuracy: 0.5106 - val_loss: 1.4171 - val_accuracy: 0.3672\n",
      "Epoch 156/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1666 - accuracy: 0.5106 - val_loss: 1.4201 - val_accuracy: 0.3672\n",
      "Epoch 157/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1654 - accuracy: 0.5100 - val_loss: 1.4183 - val_accuracy: 0.3828\n",
      "Epoch 158/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1662 - accuracy: 0.5089 - val_loss: 1.4176 - val_accuracy: 0.3906\n",
      "Epoch 159/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1652 - accuracy: 0.5117 - val_loss: 1.4166 - val_accuracy: 0.3906\n",
      "Epoch 160/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1654 - accuracy: 0.5123 - val_loss: 1.4145 - val_accuracy: 0.3672\n",
      "Epoch 161/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1661 - accuracy: 0.5095 - val_loss: 1.4202 - val_accuracy: 0.3672\n",
      "Epoch 162/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1648 - accuracy: 0.5123 - val_loss: 1.4195 - val_accuracy: 0.3984\n",
      "Epoch 163/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1661 - accuracy: 0.5084 - val_loss: 1.4199 - val_accuracy: 0.3906\n",
      "Epoch 164/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1657 - accuracy: 0.5117 - val_loss: 1.4149 - val_accuracy: 0.3750\n",
      "Epoch 165/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1654 - accuracy: 0.5100 - val_loss: 1.4170 - val_accuracy: 0.3906\n",
      "Epoch 166/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1660 - accuracy: 0.5100 - val_loss: 1.4213 - val_accuracy: 0.3906\n",
      "Epoch 167/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1653 - accuracy: 0.5078 - val_loss: 1.4139 - val_accuracy: 0.3828\n",
      "Epoch 168/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1646 - accuracy: 0.5100 - val_loss: 1.4130 - val_accuracy: 0.3906\n",
      "Epoch 169/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1639 - accuracy: 0.5095 - val_loss: 1.4162 - val_accuracy: 0.3906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1645 - accuracy: 0.5100 - val_loss: 1.4157 - val_accuracy: 0.3984\n",
      "Epoch 171/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1646 - accuracy: 0.5117 - val_loss: 1.4174 - val_accuracy: 0.3906\n",
      "Epoch 172/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1636 - accuracy: 0.5095 - val_loss: 1.4175 - val_accuracy: 0.3906\n",
      "Epoch 173/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1647 - accuracy: 0.5117 - val_loss: 1.4178 - val_accuracy: 0.3594\n",
      "Epoch 174/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1649 - accuracy: 0.5100 - val_loss: 1.4199 - val_accuracy: 0.3906\n",
      "Epoch 175/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1637 - accuracy: 0.5089 - val_loss: 1.4169 - val_accuracy: 0.3984\n",
      "Epoch 176/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1636 - accuracy: 0.5123 - val_loss: 1.4143 - val_accuracy: 0.3906\n",
      "Epoch 177/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1635 - accuracy: 0.5100 - val_loss: 1.4137 - val_accuracy: 0.3906\n",
      "Epoch 178/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1636 - accuracy: 0.5100 - val_loss: 1.4132 - val_accuracy: 0.3984\n",
      "Epoch 179/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1631 - accuracy: 0.5106 - val_loss: 1.4163 - val_accuracy: 0.3750\n",
      "Epoch 180/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1636 - accuracy: 0.5089 - val_loss: 1.4147 - val_accuracy: 0.3906\n",
      "Epoch 181/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1639 - accuracy: 0.5106 - val_loss: 1.4215 - val_accuracy: 0.3984\n",
      "Epoch 182/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1631 - accuracy: 0.5100 - val_loss: 1.4174 - val_accuracy: 0.3672\n",
      "Epoch 183/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1633 - accuracy: 0.5100 - val_loss: 1.4165 - val_accuracy: 0.3906\n",
      "Epoch 184/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1631 - accuracy: 0.5112 - val_loss: 1.4142 - val_accuracy: 0.3906\n",
      "Epoch 185/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1644 - accuracy: 0.5073 - val_loss: 1.4173 - val_accuracy: 0.3906\n",
      "Epoch 186/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1642 - accuracy: 0.5117 - val_loss: 1.4171 - val_accuracy: 0.3984\n",
      "Epoch 187/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1632 - accuracy: 0.5106 - val_loss: 1.4197 - val_accuracy: 0.3516\n",
      "Epoch 188/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1635 - accuracy: 0.5106 - val_loss: 1.4164 - val_accuracy: 0.3672\n",
      "Epoch 189/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1642 - accuracy: 0.5100 - val_loss: 1.4103 - val_accuracy: 0.3984\n",
      "Epoch 190/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1640 - accuracy: 0.5089 - val_loss: 1.4214 - val_accuracy: 0.3828\n",
      "Epoch 191/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1631 - accuracy: 0.5106 - val_loss: 1.4126 - val_accuracy: 0.3828\n",
      "Epoch 192/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1624 - accuracy: 0.5100 - val_loss: 1.4168 - val_accuracy: 0.3906\n",
      "Epoch 193/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1627 - accuracy: 0.5112 - val_loss: 1.4121 - val_accuracy: 0.3828\n",
      "Epoch 194/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1628 - accuracy: 0.5073 - val_loss: 1.4141 - val_accuracy: 0.3594\n",
      "Epoch 195/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1624 - accuracy: 0.5095 - val_loss: 1.4136 - val_accuracy: 0.3984\n",
      "Epoch 196/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1628 - accuracy: 0.5100 - val_loss: 1.4132 - val_accuracy: 0.3984\n",
      "Epoch 197/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1627 - accuracy: 0.5112 - val_loss: 1.4162 - val_accuracy: 0.3516\n",
      "Epoch 198/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1630 - accuracy: 0.5106 - val_loss: 1.4135 - val_accuracy: 0.3594\n",
      "Epoch 199/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1628 - accuracy: 0.5117 - val_loss: 1.4126 - val_accuracy: 0.3672\n",
      "Epoch 200/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1629 - accuracy: 0.5100 - val_loss: 1.4161 - val_accuracy: 0.3594\n",
      "Epoch 201/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1621 - accuracy: 0.5112 - val_loss: 1.4134 - val_accuracy: 0.3906\n",
      "Epoch 202/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1625 - accuracy: 0.5106 - val_loss: 1.4109 - val_accuracy: 0.3984\n",
      "Epoch 203/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1624 - accuracy: 0.5095 - val_loss: 1.4166 - val_accuracy: 0.3672\n",
      "Epoch 204/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1623 - accuracy: 0.5106 - val_loss: 1.4134 - val_accuracy: 0.3984\n",
      "Epoch 205/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1623 - accuracy: 0.5106 - val_loss: 1.4113 - val_accuracy: 0.4062\n",
      "Epoch 206/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1624 - accuracy: 0.5089 - val_loss: 1.4117 - val_accuracy: 0.3750\n",
      "Epoch 207/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1625 - accuracy: 0.5128 - val_loss: 1.4133 - val_accuracy: 0.3906\n",
      "Epoch 208/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1622 - accuracy: 0.5112 - val_loss: 1.4135 - val_accuracy: 0.3594\n",
      "Epoch 209/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1620 - accuracy: 0.5112 - val_loss: 1.4138 - val_accuracy: 0.3672\n",
      "Epoch 210/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1616 - accuracy: 0.5123 - val_loss: 1.4118 - val_accuracy: 0.3672\n",
      "Epoch 211/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1621 - accuracy: 0.5089 - val_loss: 1.4115 - val_accuracy: 0.3828\n",
      "Epoch 212/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1633 - accuracy: 0.5106 - val_loss: 1.4141 - val_accuracy: 0.3984\n",
      "Epoch 213/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1621 - accuracy: 0.5117 - val_loss: 1.4130 - val_accuracy: 0.3672\n",
      "Epoch 214/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1623 - accuracy: 0.5123 - val_loss: 1.4132 - val_accuracy: 0.3672\n",
      "Epoch 215/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1626 - accuracy: 0.5100 - val_loss: 1.4117 - val_accuracy: 0.3984\n",
      "Epoch 216/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1622 - accuracy: 0.5106 - val_loss: 1.4150 - val_accuracy: 0.3516\n",
      "Epoch 217/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1619 - accuracy: 0.5117 - val_loss: 1.4138 - val_accuracy: 0.3672\n",
      "Epoch 218/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1616 - accuracy: 0.5112 - val_loss: 1.4085 - val_accuracy: 0.3906\n",
      "Epoch 219/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1621 - accuracy: 0.5100 - val_loss: 1.4128 - val_accuracy: 0.3672\n",
      "Epoch 220/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1620 - accuracy: 0.5100 - val_loss: 1.4146 - val_accuracy: 0.3594\n",
      "Epoch 221/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1620 - accuracy: 0.5134 - val_loss: 1.4132 - val_accuracy: 0.3906\n",
      "Epoch 222/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1616 - accuracy: 0.5106 - val_loss: 1.4142 - val_accuracy: 0.3594\n",
      "Epoch 223/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1619 - accuracy: 0.5117 - val_loss: 1.4126 - val_accuracy: 0.3594\n",
      "Epoch 224/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1620 - accuracy: 0.5112 - val_loss: 1.4127 - val_accuracy: 0.3672\n",
      "Epoch 225/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1614 - accuracy: 0.5117 - val_loss: 1.4123 - val_accuracy: 0.3594\n",
      "Epoch 226/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1619 - accuracy: 0.5117 - val_loss: 1.4140 - val_accuracy: 0.3594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1620 - accuracy: 0.5128 - val_loss: 1.4145 - val_accuracy: 0.3750\n",
      "Epoch 228/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1613 - accuracy: 0.5089 - val_loss: 1.4067 - val_accuracy: 0.3594\n",
      "Epoch 229/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1616 - accuracy: 0.5134 - val_loss: 1.4125 - val_accuracy: 0.3984\n",
      "Epoch 230/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1620 - accuracy: 0.5123 - val_loss: 1.4109 - val_accuracy: 0.3438\n",
      "Epoch 231/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1609 - accuracy: 0.5123 - val_loss: 1.4105 - val_accuracy: 0.3828\n",
      "Epoch 232/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1619 - accuracy: 0.5123 - val_loss: 1.4079 - val_accuracy: 0.3828\n",
      "Epoch 233/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1620 - accuracy: 0.5123 - val_loss: 1.4154 - val_accuracy: 0.3828\n",
      "Epoch 234/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1616 - accuracy: 0.5140 - val_loss: 1.4122 - val_accuracy: 0.3438\n",
      "Epoch 235/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1610 - accuracy: 0.5123 - val_loss: 1.4153 - val_accuracy: 0.3906\n",
      "Epoch 236/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1621 - accuracy: 0.5117 - val_loss: 1.4110 - val_accuracy: 0.3984\n",
      "Epoch 237/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1621 - accuracy: 0.5128 - val_loss: 1.4099 - val_accuracy: 0.3828\n",
      "Epoch 238/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1622 - accuracy: 0.5123 - val_loss: 1.4154 - val_accuracy: 0.3672\n",
      "Epoch 239/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1615 - accuracy: 0.5112 - val_loss: 1.4086 - val_accuracy: 0.3828\n",
      "Epoch 240/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1614 - accuracy: 0.5112 - val_loss: 1.4141 - val_accuracy: 0.3984\n",
      "Epoch 241/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1611 - accuracy: 0.5128 - val_loss: 1.4094 - val_accuracy: 0.3438\n",
      "Epoch 242/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1608 - accuracy: 0.5134 - val_loss: 1.4079 - val_accuracy: 0.3594\n",
      "Epoch 243/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1625 - accuracy: 0.5140 - val_loss: 1.4100 - val_accuracy: 0.3828\n",
      "Epoch 244/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1621 - accuracy: 0.5134 - val_loss: 1.4175 - val_accuracy: 0.3594\n",
      "Epoch 245/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1615 - accuracy: 0.5106 - val_loss: 1.4163 - val_accuracy: 0.3438\n",
      "Epoch 246/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1613 - accuracy: 0.5162 - val_loss: 1.4062 - val_accuracy: 0.3984\n",
      "Epoch 247/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1612 - accuracy: 0.5100 - val_loss: 1.4118 - val_accuracy: 0.3438\n",
      "Epoch 248/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1608 - accuracy: 0.5123 - val_loss: 1.4138 - val_accuracy: 0.3594\n",
      "Epoch 249/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1605 - accuracy: 0.5140 - val_loss: 1.4136 - val_accuracy: 0.3594\n",
      "Epoch 250/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1608 - accuracy: 0.5134 - val_loss: 1.4089 - val_accuracy: 0.3672\n",
      "Epoch 251/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1612 - accuracy: 0.5128 - val_loss: 1.4118 - val_accuracy: 0.3906\n",
      "Epoch 252/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1617 - accuracy: 0.5128 - val_loss: 1.4149 - val_accuracy: 0.3750\n",
      "Epoch 253/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1618 - accuracy: 0.5123 - val_loss: 1.4127 - val_accuracy: 0.3594\n",
      "Epoch 254/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1607 - accuracy: 0.5123 - val_loss: 1.4111 - val_accuracy: 0.3438\n",
      "Epoch 255/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1627 - accuracy: 0.5117 - val_loss: 1.4211 - val_accuracy: 0.3516\n",
      "Epoch 256/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1609 - accuracy: 0.5128 - val_loss: 1.4101 - val_accuracy: 0.3516\n",
      "Epoch 257/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1612 - accuracy: 0.5134 - val_loss: 1.4142 - val_accuracy: 0.3672\n",
      "Epoch 258/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1617 - accuracy: 0.5140 - val_loss: 1.4083 - val_accuracy: 0.3594\n",
      "Epoch 259/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1612 - accuracy: 0.5100 - val_loss: 1.4133 - val_accuracy: 0.3516\n",
      "Epoch 260/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1613 - accuracy: 0.5156 - val_loss: 1.4142 - val_accuracy: 0.3438\n",
      "Epoch 261/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1609 - accuracy: 0.5134 - val_loss: 1.4104 - val_accuracy: 0.3750\n",
      "Epoch 262/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1610 - accuracy: 0.5123 - val_loss: 1.4065 - val_accuracy: 0.3750\n",
      "Epoch 263/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1610 - accuracy: 0.5128 - val_loss: 1.4147 - val_accuracy: 0.3750\n",
      "Epoch 264/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1609 - accuracy: 0.5140 - val_loss: 1.4084 - val_accuracy: 0.3438\n",
      "Epoch 265/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1606 - accuracy: 0.5117 - val_loss: 1.4130 - val_accuracy: 0.3438\n",
      "Epoch 266/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1611 - accuracy: 0.5134 - val_loss: 1.4136 - val_accuracy: 0.3516\n",
      "Epoch 267/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1614 - accuracy: 0.5112 - val_loss: 1.4123 - val_accuracy: 0.3438\n",
      "Epoch 268/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1605 - accuracy: 0.5128 - val_loss: 1.4107 - val_accuracy: 0.3594\n",
      "Epoch 269/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1606 - accuracy: 0.5128 - val_loss: 1.4104 - val_accuracy: 0.3438\n",
      "Epoch 270/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1611 - accuracy: 0.5112 - val_loss: 1.4120 - val_accuracy: 0.3594\n",
      "Epoch 271/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1607 - accuracy: 0.5128 - val_loss: 1.4156 - val_accuracy: 0.3438\n",
      "Epoch 272/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1608 - accuracy: 0.5134 - val_loss: 1.4116 - val_accuracy: 0.3594\n",
      "Epoch 273/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1607 - accuracy: 0.5140 - val_loss: 1.4109 - val_accuracy: 0.3516\n",
      "Epoch 274/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1619 - accuracy: 0.5117 - val_loss: 1.4113 - val_accuracy: 0.3438\n",
      "Epoch 275/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1607 - accuracy: 0.5151 - val_loss: 1.4163 - val_accuracy: 0.3438\n",
      "Epoch 276/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1605 - accuracy: 0.5134 - val_loss: 1.4123 - val_accuracy: 0.3438\n",
      "Epoch 277/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1610 - accuracy: 0.5134 - val_loss: 1.4113 - val_accuracy: 0.3516\n",
      "Epoch 278/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1618 - accuracy: 0.5156 - val_loss: 1.4146 - val_accuracy: 0.3672\n",
      "Epoch 279/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1621 - accuracy: 0.5128 - val_loss: 1.4126 - val_accuracy: 0.3516\n",
      "Epoch 280/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1610 - accuracy: 0.5140 - val_loss: 1.4157 - val_accuracy: 0.3438\n",
      "Epoch 281/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1622 - accuracy: 0.5134 - val_loss: 1.4122 - val_accuracy: 0.3516\n",
      "Epoch 282/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1607 - accuracy: 0.5140 - val_loss: 1.4125 - val_accuracy: 0.3516\n",
      "Epoch 283/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1610 - accuracy: 0.5123 - val_loss: 1.4118 - val_accuracy: 0.3906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 284/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1607 - accuracy: 0.5145 - val_loss: 1.4121 - val_accuracy: 0.3438\n",
      "Epoch 285/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1603 - accuracy: 0.5140 - val_loss: 1.4142 - val_accuracy: 0.3516\n",
      "Epoch 286/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1615 - accuracy: 0.5156 - val_loss: 1.4100 - val_accuracy: 0.3984\n",
      "Epoch 287/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1623 - accuracy: 0.5117 - val_loss: 1.4109 - val_accuracy: 0.3516\n",
      "Epoch 288/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1607 - accuracy: 0.5151 - val_loss: 1.4144 - val_accuracy: 0.3594\n",
      "Epoch 289/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1604 - accuracy: 0.5134 - val_loss: 1.4155 - val_accuracy: 0.3516\n",
      "Epoch 290/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1612 - accuracy: 0.5134 - val_loss: 1.4120 - val_accuracy: 0.3359\n",
      "Epoch 291/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1602 - accuracy: 0.5162 - val_loss: 1.4095 - val_accuracy: 0.3438\n",
      "Epoch 292/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1617 - accuracy: 0.5134 - val_loss: 1.4155 - val_accuracy: 0.3828\n",
      "Epoch 293/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1610 - accuracy: 0.5128 - val_loss: 1.4090 - val_accuracy: 0.3438\n",
      "Epoch 294/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1601 - accuracy: 0.5145 - val_loss: 1.4100 - val_accuracy: 0.3438\n",
      "Epoch 295/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1612 - accuracy: 0.5151 - val_loss: 1.4097 - val_accuracy: 0.3750\n",
      "Epoch 296/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1614 - accuracy: 0.5128 - val_loss: 1.4117 - val_accuracy: 0.3438\n",
      "Epoch 297/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1602 - accuracy: 0.5151 - val_loss: 1.4141 - val_accuracy: 0.3828\n",
      "Epoch 298/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1615 - accuracy: 0.5140 - val_loss: 1.4133 - val_accuracy: 0.3906\n",
      "Epoch 299/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1618 - accuracy: 0.5179 - val_loss: 1.4113 - val_accuracy: 0.3594\n",
      "Epoch 300/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1624 - accuracy: 0.5140 - val_loss: 1.4154 - val_accuracy: 0.3672\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "56/56 [==============================] - 0s 2ms/step\n",
      "Recall: [0.94 0.   0.65]\n",
      "Precision: [0.42 0.67 0.74]\n",
      "F1-score: [0.58 0.01 0.69]\n",
      "Accuracy: 0.52\n",
      "Brier score:0.19733663072927454\n",
      "Brier climat:0.21220734126984128\n",
      "Brier skill score:0.07007632465295932\n",
      "Recall: [0.94 0.   0.65]\n",
      "Precision: [0.42 0.67 0.74]\n",
      "F1-score: [0.58 0.01 0.69]\n",
      "Accuracy: 0.52\n",
      "Brier score:0.2190566133796404\n",
      "Brier climat:0.2221329365079365\n",
      "Brier skill score:0.013849018415088454\n",
      "Recall: [0.94 0.   0.65]\n",
      "Precision: [0.42 0.67 0.74]\n",
      "F1-score: [0.58 0.01 0.69]\n",
      "Accuracy: 0.52\n",
      "Brier score:0.14219865788796043\n",
      "Brier climat:0.22395833333333331\n",
      "Brier skill score:0.36506645780259517\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "Recall: [0.74 0.   0.4 ]\n",
      "Precision: [0.36 0.   0.4 ]\n",
      "F1-score: [0.49 0.   0.4 ]\n",
      "Accuracy: 0.37\n",
      "Brier score:0.2329576017300421\n",
      "Brier climat:0.18248263888888888\n",
      "Brier skill score:-0.2766014517791291\n",
      "Recall: [0.74 0.   0.4 ]\n",
      "Precision: [0.36 0.   0.4 ]\n",
      "F1-score: [0.49 0.   0.4 ]\n",
      "Accuracy: 0.37\n",
      "Brier score:0.23663121171039161\n",
      "Brier climat:0.20993055555555556\n",
      "Brier skill score:-0.12718804122713823\n",
      "Recall: [0.74 0.   0.4 ]\n",
      "Precision: [0.36 0.   0.4 ]\n",
      "F1-score: [0.49 0.   0.4 ]\n",
      "Accuracy: 0.37\n",
      "Brier score:0.31528468924224845\n",
      "Brier climat:0.25130208333333337\n",
      "Brier skill score:-0.2546043592644911\n",
      "******** 27\n",
      "validation years [2008, 2009]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2010, 1981, 1982, 1983}\n",
      "{0: 2.0, 1: 1.0, 2: 1.0}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 60)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 183       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183\n",
      "Trainable params: 183\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "Epoch 1/300\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 1.9335 - accuracy: 0.3825WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "28/28 [==============================] - 4s 138ms/step - loss: 1.9197 - accuracy: 0.3884 - val_loss: 1.5707 - val_accuracy: 0.3281\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.6737 - accuracy: 0.4704 - val_loss: 1.5384 - val_accuracy: 0.3594\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.5644 - accuracy: 0.4821 - val_loss: 1.5164 - val_accuracy: 0.3359\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.5014 - accuracy: 0.4905 - val_loss: 1.5061 - val_accuracy: 0.2891\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.4620 - accuracy: 0.4905 - val_loss: 1.5127 - val_accuracy: 0.2812\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4365 - accuracy: 0.5045 - val_loss: 1.5058 - val_accuracy: 0.2891\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4170 - accuracy: 0.4978 - val_loss: 1.4926 - val_accuracy: 0.2812\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4006 - accuracy: 0.4994 - val_loss: 1.4962 - val_accuracy: 0.3203\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3861 - accuracy: 0.5033 - val_loss: 1.4954 - val_accuracy: 0.3359\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3737 - accuracy: 0.5017 - val_loss: 1.4878 - val_accuracy: 0.3594\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.3626 - accuracy: 0.5050 - val_loss: 1.4794 - val_accuracy: 0.3594\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.3508 - accuracy: 0.5050 - val_loss: 1.4805 - val_accuracy: 0.3594\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.3408 - accuracy: 0.5033 - val_loss: 1.4703 - val_accuracy: 0.3672\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3305 - accuracy: 0.5006 - val_loss: 1.4659 - val_accuracy: 0.3594\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3215 - accuracy: 0.5033 - val_loss: 1.4663 - val_accuracy: 0.3672\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.3125 - accuracy: 0.5045 - val_loss: 1.4578 - val_accuracy: 0.3750\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.3040 - accuracy: 0.5028 - val_loss: 1.4580 - val_accuracy: 0.3750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2976 - accuracy: 0.5017 - val_loss: 1.4592 - val_accuracy: 0.3594\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2887 - accuracy: 0.5022 - val_loss: 1.4489 - val_accuracy: 0.3672\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2815 - accuracy: 0.5022 - val_loss: 1.4464 - val_accuracy: 0.3906\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2756 - accuracy: 0.5022 - val_loss: 1.4483 - val_accuracy: 0.3828\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2701 - accuracy: 0.5028 - val_loss: 1.4462 - val_accuracy: 0.3828\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2645 - accuracy: 0.5078 - val_loss: 1.4412 - val_accuracy: 0.3750\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2600 - accuracy: 0.5067 - val_loss: 1.4488 - val_accuracy: 0.4062\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2562 - accuracy: 0.5089 - val_loss: 1.4462 - val_accuracy: 0.3828\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 1s 29ms/step - loss: 1.2516 - accuracy: 0.5084 - val_loss: 1.4475 - val_accuracy: 0.3906\n",
      "Epoch 27/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2479 - accuracy: 0.5084 - val_loss: 1.4443 - val_accuracy: 0.3984\n",
      "Epoch 28/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2438 - accuracy: 0.5095 - val_loss: 1.4489 - val_accuracy: 0.4141\n",
      "Epoch 29/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2405 - accuracy: 0.5100 - val_loss: 1.4430 - val_accuracy: 0.3750\n",
      "Epoch 30/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2388 - accuracy: 0.5073 - val_loss: 1.4467 - val_accuracy: 0.4297\n",
      "Epoch 31/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2350 - accuracy: 0.5095 - val_loss: 1.4459 - val_accuracy: 0.3984\n",
      "Epoch 32/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2310 - accuracy: 0.5067 - val_loss: 1.4416 - val_accuracy: 0.4375\n",
      "Epoch 33/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2289 - accuracy: 0.5056 - val_loss: 1.4316 - val_accuracy: 0.4141\n",
      "Epoch 34/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2261 - accuracy: 0.5078 - val_loss: 1.4512 - val_accuracy: 0.4375\n",
      "Epoch 35/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2237 - accuracy: 0.5078 - val_loss: 1.4443 - val_accuracy: 0.4141\n",
      "Epoch 36/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2217 - accuracy: 0.5067 - val_loss: 1.4440 - val_accuracy: 0.4375\n",
      "Epoch 37/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2190 - accuracy: 0.5084 - val_loss: 1.4396 - val_accuracy: 0.4141\n",
      "Epoch 38/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2160 - accuracy: 0.5073 - val_loss: 1.4404 - val_accuracy: 0.4375\n",
      "Epoch 39/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2136 - accuracy: 0.5061 - val_loss: 1.4367 - val_accuracy: 0.4375\n",
      "Epoch 40/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2117 - accuracy: 0.5078 - val_loss: 1.4353 - val_accuracy: 0.4453\n",
      "Epoch 41/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2104 - accuracy: 0.5123 - val_loss: 1.4316 - val_accuracy: 0.4375\n",
      "Epoch 42/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2076 - accuracy: 0.5084 - val_loss: 1.4277 - val_accuracy: 0.4219\n",
      "Epoch 43/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2060 - accuracy: 0.5056 - val_loss: 1.4345 - val_accuracy: 0.4453\n",
      "Epoch 44/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2036 - accuracy: 0.5061 - val_loss: 1.4276 - val_accuracy: 0.4531\n",
      "Epoch 45/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2024 - accuracy: 0.5078 - val_loss: 1.4272 - val_accuracy: 0.4453\n",
      "Epoch 46/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2005 - accuracy: 0.5089 - val_loss: 1.4365 - val_accuracy: 0.4453\n",
      "Epoch 47/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1978 - accuracy: 0.5084 - val_loss: 1.4280 - val_accuracy: 0.4453\n",
      "Epoch 48/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1963 - accuracy: 0.5084 - val_loss: 1.4194 - val_accuracy: 0.4531\n",
      "Epoch 49/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1950 - accuracy: 0.5084 - val_loss: 1.4241 - val_accuracy: 0.4531\n",
      "Epoch 50/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1937 - accuracy: 0.5095 - val_loss: 1.4228 - val_accuracy: 0.4531\n",
      "Epoch 51/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1921 - accuracy: 0.5089 - val_loss: 1.4251 - val_accuracy: 0.4453\n",
      "Epoch 52/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1909 - accuracy: 0.5112 - val_loss: 1.4246 - val_accuracy: 0.4531\n",
      "Epoch 53/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1899 - accuracy: 0.5089 - val_loss: 1.4226 - val_accuracy: 0.4531\n",
      "Epoch 54/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1876 - accuracy: 0.5106 - val_loss: 1.4127 - val_accuracy: 0.4609\n",
      "Epoch 55/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1862 - accuracy: 0.5095 - val_loss: 1.4119 - val_accuracy: 0.4531\n",
      "Epoch 56/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1851 - accuracy: 0.5078 - val_loss: 1.4101 - val_accuracy: 0.4531\n",
      "Epoch 57/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1838 - accuracy: 0.5084 - val_loss: 1.4098 - val_accuracy: 0.4453\n",
      "Epoch 58/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1828 - accuracy: 0.5106 - val_loss: 1.4096 - val_accuracy: 0.4531\n",
      "Epoch 59/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1828 - accuracy: 0.5123 - val_loss: 1.4094 - val_accuracy: 0.4531\n",
      "Epoch 60/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1805 - accuracy: 0.5112 - val_loss: 1.3969 - val_accuracy: 0.4531\n",
      "Epoch 61/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1801 - accuracy: 0.5095 - val_loss: 1.4045 - val_accuracy: 0.4531\n",
      "Epoch 62/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1784 - accuracy: 0.5100 - val_loss: 1.3956 - val_accuracy: 0.4531\n",
      "Epoch 63/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1773 - accuracy: 0.5117 - val_loss: 1.3954 - val_accuracy: 0.4531\n",
      "Epoch 64/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1760 - accuracy: 0.5106 - val_loss: 1.3913 - val_accuracy: 0.4531\n",
      "Epoch 65/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1758 - accuracy: 0.5117 - val_loss: 1.3900 - val_accuracy: 0.4531\n",
      "Epoch 66/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1752 - accuracy: 0.5095 - val_loss: 1.3871 - val_accuracy: 0.4531\n",
      "Epoch 67/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1736 - accuracy: 0.5128 - val_loss: 1.3898 - val_accuracy: 0.4531\n",
      "Epoch 68/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1726 - accuracy: 0.5100 - val_loss: 1.3800 - val_accuracy: 0.4531\n",
      "Epoch 69/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1720 - accuracy: 0.5073 - val_loss: 1.3812 - val_accuracy: 0.4531\n",
      "Epoch 70/300\n",
      "28/28 [==============================] - 1s 30ms/step - loss: 1.1709 - accuracy: 0.5084 - val_loss: 1.3810 - val_accuracy: 0.4531\n",
      "Epoch 71/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1709 - accuracy: 0.5089 - val_loss: 1.3810 - val_accuracy: 0.4531\n",
      "Epoch 72/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1711 - accuracy: 0.5128 - val_loss: 1.3803 - val_accuracy: 0.4531\n",
      "Epoch 73/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1707 - accuracy: 0.5061 - val_loss: 1.3772 - val_accuracy: 0.4531\n",
      "Epoch 74/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1687 - accuracy: 0.5106 - val_loss: 1.3718 - val_accuracy: 0.4531\n",
      "Epoch 75/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1685 - accuracy: 0.5112 - val_loss: 1.3749 - val_accuracy: 0.4531\n",
      "Epoch 76/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1686 - accuracy: 0.5084 - val_loss: 1.3741 - val_accuracy: 0.4531\n",
      "Epoch 77/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1680 - accuracy: 0.5089 - val_loss: 1.3671 - val_accuracy: 0.4531\n",
      "Epoch 78/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1677 - accuracy: 0.5061 - val_loss: 1.3720 - val_accuracy: 0.4688\n",
      "Epoch 79/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1688 - accuracy: 0.5106 - val_loss: 1.3764 - val_accuracy: 0.4531\n",
      "Epoch 80/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1668 - accuracy: 0.5073 - val_loss: 1.3703 - val_accuracy: 0.4531\n",
      "Epoch 81/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1661 - accuracy: 0.5100 - val_loss: 1.3711 - val_accuracy: 0.4531\n",
      "Epoch 82/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1668 - accuracy: 0.5078 - val_loss: 1.3700 - val_accuracy: 0.4531\n",
      "Epoch 83/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1671 - accuracy: 0.5056 - val_loss: 1.3683 - val_accuracy: 0.4531\n",
      "Epoch 84/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1663 - accuracy: 0.5067 - val_loss: 1.3658 - val_accuracy: 0.4531\n",
      "Epoch 85/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1658 - accuracy: 0.5078 - val_loss: 1.3690 - val_accuracy: 0.4531\n",
      "Epoch 86/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1657 - accuracy: 0.5106 - val_loss: 1.3731 - val_accuracy: 0.4531\n",
      "Epoch 87/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1650 - accuracy: 0.5056 - val_loss: 1.3634 - val_accuracy: 0.4531\n",
      "Epoch 88/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1649 - accuracy: 0.5073 - val_loss: 1.3695 - val_accuracy: 0.4531\n",
      "Epoch 89/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1650 - accuracy: 0.5073 - val_loss: 1.3708 - val_accuracy: 0.4531\n",
      "Epoch 90/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1644 - accuracy: 0.5073 - val_loss: 1.3698 - val_accuracy: 0.4531\n",
      "Epoch 91/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1646 - accuracy: 0.5084 - val_loss: 1.3702 - val_accuracy: 0.4531\n",
      "Epoch 92/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1638 - accuracy: 0.5117 - val_loss: 1.3726 - val_accuracy: 0.4531\n",
      "Epoch 93/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1643 - accuracy: 0.5095 - val_loss: 1.3681 - val_accuracy: 0.4531\n",
      "Epoch 94/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1642 - accuracy: 0.5078 - val_loss: 1.3755 - val_accuracy: 0.4453\n",
      "Epoch 95/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1649 - accuracy: 0.5073 - val_loss: 1.3705 - val_accuracy: 0.4453\n",
      "Epoch 96/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1639 - accuracy: 0.5106 - val_loss: 1.3662 - val_accuracy: 0.4453\n",
      "Epoch 97/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1639 - accuracy: 0.5123 - val_loss: 1.3726 - val_accuracy: 0.4453\n",
      "Epoch 98/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1642 - accuracy: 0.5078 - val_loss: 1.3724 - val_accuracy: 0.4453\n",
      "Epoch 99/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1636 - accuracy: 0.5123 - val_loss: 1.3694 - val_accuracy: 0.4453\n",
      "Epoch 100/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1633 - accuracy: 0.5106 - val_loss: 1.3658 - val_accuracy: 0.4453\n",
      "Epoch 101/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1627 - accuracy: 0.5106 - val_loss: 1.3654 - val_accuracy: 0.4453\n",
      "Epoch 102/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1629 - accuracy: 0.5095 - val_loss: 1.3681 - val_accuracy: 0.4453\n",
      "Epoch 103/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1632 - accuracy: 0.5084 - val_loss: 1.3764 - val_accuracy: 0.4531\n",
      "Epoch 104/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1640 - accuracy: 0.5095 - val_loss: 1.3706 - val_accuracy: 0.4453\n",
      "Epoch 105/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1646 - accuracy: 0.5056 - val_loss: 1.3627 - val_accuracy: 0.4453\n",
      "Epoch 106/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1626 - accuracy: 0.5100 - val_loss: 1.3713 - val_accuracy: 0.4453\n",
      "Epoch 107/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1623 - accuracy: 0.5089 - val_loss: 1.3687 - val_accuracy: 0.4453\n",
      "Epoch 108/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1624 - accuracy: 0.5106 - val_loss: 1.3672 - val_accuracy: 0.4453\n",
      "Epoch 109/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1627 - accuracy: 0.5067 - val_loss: 1.3680 - val_accuracy: 0.4453\n",
      "Epoch 110/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1622 - accuracy: 0.5106 - val_loss: 1.3675 - val_accuracy: 0.4453\n",
      "Epoch 111/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1621 - accuracy: 0.5095 - val_loss: 1.3658 - val_accuracy: 0.4453\n",
      "Epoch 112/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1629 - accuracy: 0.5106 - val_loss: 1.3697 - val_accuracy: 0.4453\n",
      "Epoch 113/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1627 - accuracy: 0.5095 - val_loss: 1.3651 - val_accuracy: 0.4453\n",
      "Epoch 114/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1619 - accuracy: 0.5089 - val_loss: 1.3647 - val_accuracy: 0.4453\n",
      "Epoch 115/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1617 - accuracy: 0.5112 - val_loss: 1.3737 - val_accuracy: 0.4453\n",
      "Epoch 116/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1619 - accuracy: 0.5095 - val_loss: 1.3691 - val_accuracy: 0.4453\n",
      "Epoch 117/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1622 - accuracy: 0.5123 - val_loss: 1.3671 - val_accuracy: 0.4453\n",
      "Epoch 118/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1623 - accuracy: 0.5100 - val_loss: 1.3607 - val_accuracy: 0.4453\n",
      "Epoch 119/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1616 - accuracy: 0.5095 - val_loss: 1.3694 - val_accuracy: 0.4453\n",
      "Epoch 120/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1618 - accuracy: 0.5106 - val_loss: 1.3635 - val_accuracy: 0.4453\n",
      "Epoch 121/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1623 - accuracy: 0.5084 - val_loss: 1.3682 - val_accuracy: 0.4453\n",
      "Epoch 122/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1615 - accuracy: 0.5078 - val_loss: 1.3616 - val_accuracy: 0.4453\n",
      "Epoch 123/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1614 - accuracy: 0.5106 - val_loss: 1.3729 - val_accuracy: 0.4453\n",
      "Epoch 124/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1612 - accuracy: 0.5106 - val_loss: 1.3651 - val_accuracy: 0.4453\n",
      "Epoch 125/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1610 - accuracy: 0.5084 - val_loss: 1.3658 - val_accuracy: 0.4453\n",
      "Epoch 126/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1616 - accuracy: 0.5084 - val_loss: 1.3569 - val_accuracy: 0.4453\n",
      "Epoch 127/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1615 - accuracy: 0.5123 - val_loss: 1.3687 - val_accuracy: 0.4453\n",
      "Epoch 128/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1613 - accuracy: 0.5078 - val_loss: 1.3641 - val_accuracy: 0.4453\n",
      "Epoch 129/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1612 - accuracy: 0.5128 - val_loss: 1.3684 - val_accuracy: 0.4453\n",
      "Epoch 130/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1611 - accuracy: 0.5095 - val_loss: 1.3671 - val_accuracy: 0.4453\n",
      "Epoch 131/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1607 - accuracy: 0.5100 - val_loss: 1.3689 - val_accuracy: 0.4453\n",
      "Epoch 132/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1610 - accuracy: 0.5095 - val_loss: 1.3635 - val_accuracy: 0.4453\n",
      "Epoch 133/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1604 - accuracy: 0.5078 - val_loss: 1.3675 - val_accuracy: 0.4453\n",
      "Epoch 134/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1602 - accuracy: 0.5106 - val_loss: 1.3647 - val_accuracy: 0.4453\n",
      "Epoch 135/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1603 - accuracy: 0.5106 - val_loss: 1.3641 - val_accuracy: 0.4453\n",
      "Epoch 136/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1605 - accuracy: 0.5095 - val_loss: 1.3709 - val_accuracy: 0.4453\n",
      "Epoch 137/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1611 - accuracy: 0.5084 - val_loss: 1.3670 - val_accuracy: 0.4453\n",
      "Epoch 138/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1610 - accuracy: 0.5123 - val_loss: 1.3707 - val_accuracy: 0.4453\n",
      "Epoch 139/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1607 - accuracy: 0.5073 - val_loss: 1.3680 - val_accuracy: 0.4453\n",
      "Epoch 140/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1599 - accuracy: 0.5100 - val_loss: 1.3642 - val_accuracy: 0.4453\n",
      "Epoch 141/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1597 - accuracy: 0.5095 - val_loss: 1.3661 - val_accuracy: 0.4453\n",
      "Epoch 142/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1597 - accuracy: 0.5084 - val_loss: 1.3636 - val_accuracy: 0.4453\n",
      "Epoch 143/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1597 - accuracy: 0.5106 - val_loss: 1.3647 - val_accuracy: 0.4453\n",
      "Epoch 144/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1597 - accuracy: 0.5089 - val_loss: 1.3636 - val_accuracy: 0.4531\n",
      "Epoch 145/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1614 - accuracy: 0.5112 - val_loss: 1.3661 - val_accuracy: 0.4453\n",
      "Epoch 146/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1598 - accuracy: 0.5078 - val_loss: 1.3634 - val_accuracy: 0.4453\n",
      "Epoch 147/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1597 - accuracy: 0.5084 - val_loss: 1.3612 - val_accuracy: 0.4609\n",
      "Epoch 148/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1607 - accuracy: 0.5145 - val_loss: 1.3697 - val_accuracy: 0.4531\n",
      "Epoch 149/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1619 - accuracy: 0.5095 - val_loss: 1.3555 - val_accuracy: 0.4609\n",
      "Epoch 150/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1604 - accuracy: 0.5117 - val_loss: 1.3710 - val_accuracy: 0.4453\n",
      "Epoch 151/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1598 - accuracy: 0.5100 - val_loss: 1.3659 - val_accuracy: 0.4531\n",
      "Epoch 152/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1596 - accuracy: 0.5078 - val_loss: 1.3617 - val_accuracy: 0.4531\n",
      "Epoch 153/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1594 - accuracy: 0.5078 - val_loss: 1.3629 - val_accuracy: 0.4531\n",
      "Epoch 154/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1588 - accuracy: 0.5089 - val_loss: 1.3604 - val_accuracy: 0.4531\n",
      "Epoch 155/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1595 - accuracy: 0.5106 - val_loss: 1.3690 - val_accuracy: 0.4453\n",
      "Epoch 156/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1600 - accuracy: 0.5067 - val_loss: 1.3613 - val_accuracy: 0.4531\n",
      "Epoch 157/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1600 - accuracy: 0.5112 - val_loss: 1.3661 - val_accuracy: 0.4531\n",
      "Epoch 158/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1592 - accuracy: 0.5095 - val_loss: 1.3652 - val_accuracy: 0.4453\n",
      "Epoch 159/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1594 - accuracy: 0.5095 - val_loss: 1.3588 - val_accuracy: 0.4531\n",
      "Epoch 160/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1595 - accuracy: 0.5123 - val_loss: 1.3665 - val_accuracy: 0.4453\n",
      "Epoch 161/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1585 - accuracy: 0.5100 - val_loss: 1.3581 - val_accuracy: 0.4531\n",
      "Epoch 162/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1587 - accuracy: 0.5078 - val_loss: 1.3571 - val_accuracy: 0.4531\n",
      "Epoch 163/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1591 - accuracy: 0.5106 - val_loss: 1.3625 - val_accuracy: 0.4531\n",
      "Epoch 164/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1596 - accuracy: 0.5073 - val_loss: 1.3612 - val_accuracy: 0.4609\n",
      "Epoch 165/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1600 - accuracy: 0.5100 - val_loss: 1.3652 - val_accuracy: 0.4531\n",
      "Epoch 166/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1592 - accuracy: 0.5100 - val_loss: 1.3658 - val_accuracy: 0.4531\n",
      "Epoch 167/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1590 - accuracy: 0.5078 - val_loss: 1.3597 - val_accuracy: 0.4531\n",
      "Epoch 168/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1591 - accuracy: 0.5078 - val_loss: 1.3622 - val_accuracy: 0.4531\n",
      "Epoch 169/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1590 - accuracy: 0.5084 - val_loss: 1.3600 - val_accuracy: 0.4531\n",
      "Epoch 170/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1591 - accuracy: 0.5078 - val_loss: 1.3613 - val_accuracy: 0.4531\n",
      "Epoch 171/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1594 - accuracy: 0.5078 - val_loss: 1.3647 - val_accuracy: 0.4609\n",
      "Epoch 172/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1585 - accuracy: 0.5106 - val_loss: 1.3605 - val_accuracy: 0.4531\n",
      "Epoch 173/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1587 - accuracy: 0.5084 - val_loss: 1.3580 - val_accuracy: 0.4609\n",
      "Epoch 174/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1588 - accuracy: 0.5073 - val_loss: 1.3602 - val_accuracy: 0.4609\n",
      "Epoch 175/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1586 - accuracy: 0.5078 - val_loss: 1.3592 - val_accuracy: 0.4531\n",
      "Epoch 176/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1591 - accuracy: 0.5100 - val_loss: 1.3612 - val_accuracy: 0.4531\n",
      "Epoch 177/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1591 - accuracy: 0.5106 - val_loss: 1.3677 - val_accuracy: 0.4609\n",
      "Epoch 178/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1587 - accuracy: 0.5100 - val_loss: 1.3606 - val_accuracy: 0.4531\n",
      "Epoch 179/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1588 - accuracy: 0.5106 - val_loss: 1.3582 - val_accuracy: 0.4609\n",
      "Epoch 180/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1596 - accuracy: 0.5084 - val_loss: 1.3621 - val_accuracy: 0.4609\n",
      "Epoch 181/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1588 - accuracy: 0.5106 - val_loss: 1.3652 - val_accuracy: 0.4531\n",
      "Epoch 182/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1586 - accuracy: 0.5095 - val_loss: 1.3596 - val_accuracy: 0.4609\n",
      "Epoch 183/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1582 - accuracy: 0.5084 - val_loss: 1.3588 - val_accuracy: 0.4609\n",
      "Epoch 184/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1587 - accuracy: 0.5084 - val_loss: 1.3570 - val_accuracy: 0.4609\n",
      "Epoch 185/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1586 - accuracy: 0.5100 - val_loss: 1.3627 - val_accuracy: 0.4531\n",
      "Epoch 186/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1592 - accuracy: 0.5112 - val_loss: 1.3603 - val_accuracy: 0.4531\n",
      "Epoch 187/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1583 - accuracy: 0.5084 - val_loss: 1.3567 - val_accuracy: 0.4609\n",
      "Epoch 188/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1586 - accuracy: 0.5084 - val_loss: 1.3596 - val_accuracy: 0.4609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 189/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1585 - accuracy: 0.5089 - val_loss: 1.3680 - val_accuracy: 0.4609\n",
      "Epoch 190/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1589 - accuracy: 0.5112 - val_loss: 1.3591 - val_accuracy: 0.4609\n",
      "Epoch 191/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1593 - accuracy: 0.5078 - val_loss: 1.3511 - val_accuracy: 0.4609\n",
      "Epoch 192/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1587 - accuracy: 0.5084 - val_loss: 1.3634 - val_accuracy: 0.4609\n",
      "Epoch 193/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1587 - accuracy: 0.5095 - val_loss: 1.3564 - val_accuracy: 0.4609\n",
      "Epoch 194/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1596 - accuracy: 0.5095 - val_loss: 1.3598 - val_accuracy: 0.4609\n",
      "Epoch 195/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1594 - accuracy: 0.5117 - val_loss: 1.3568 - val_accuracy: 0.4609\n",
      "Epoch 196/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1583 - accuracy: 0.5050 - val_loss: 1.3573 - val_accuracy: 0.4609\n",
      "Epoch 197/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1585 - accuracy: 0.5100 - val_loss: 1.3559 - val_accuracy: 0.4609\n",
      "Epoch 198/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1583 - accuracy: 0.5078 - val_loss: 1.3559 - val_accuracy: 0.4609\n",
      "Epoch 199/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1585 - accuracy: 0.5123 - val_loss: 1.3607 - val_accuracy: 0.4609\n",
      "Epoch 200/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1585 - accuracy: 0.5067 - val_loss: 1.3539 - val_accuracy: 0.4609\n",
      "Epoch 201/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1580 - accuracy: 0.5100 - val_loss: 1.3587 - val_accuracy: 0.4609\n",
      "Epoch 202/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1582 - accuracy: 0.5084 - val_loss: 1.3564 - val_accuracy: 0.4609\n",
      "Epoch 203/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1575 - accuracy: 0.5095 - val_loss: 1.3586 - val_accuracy: 0.4609\n",
      "Epoch 204/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1582 - accuracy: 0.5100 - val_loss: 1.3589 - val_accuracy: 0.4609\n",
      "Epoch 205/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1585 - accuracy: 0.5106 - val_loss: 1.3563 - val_accuracy: 0.4688\n",
      "Epoch 206/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1575 - accuracy: 0.5106 - val_loss: 1.3531 - val_accuracy: 0.4609\n",
      "Epoch 207/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1578 - accuracy: 0.5078 - val_loss: 1.3553 - val_accuracy: 0.4609\n",
      "Epoch 208/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1580 - accuracy: 0.5106 - val_loss: 1.3589 - val_accuracy: 0.4609\n",
      "Epoch 209/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1587 - accuracy: 0.5089 - val_loss: 1.3572 - val_accuracy: 0.4609\n",
      "Epoch 210/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1581 - accuracy: 0.5089 - val_loss: 1.3613 - val_accuracy: 0.4609\n",
      "Epoch 211/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1582 - accuracy: 0.5067 - val_loss: 1.3547 - val_accuracy: 0.4609\n",
      "Epoch 212/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1577 - accuracy: 0.5100 - val_loss: 1.3551 - val_accuracy: 0.4609\n",
      "Epoch 213/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1585 - accuracy: 0.5106 - val_loss: 1.3589 - val_accuracy: 0.4609\n",
      "Epoch 214/300\n",
      "28/28 [==============================] - 1s 29ms/step - loss: 1.1578 - accuracy: 0.5084 - val_loss: 1.3506 - val_accuracy: 0.4609\n",
      "Epoch 215/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1576 - accuracy: 0.5100 - val_loss: 1.3580 - val_accuracy: 0.4609\n",
      "Epoch 216/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1584 - accuracy: 0.5073 - val_loss: 1.3538 - val_accuracy: 0.4688\n",
      "Epoch 217/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1592 - accuracy: 0.5100 - val_loss: 1.3569 - val_accuracy: 0.4688\n",
      "Epoch 218/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1585 - accuracy: 0.5106 - val_loss: 1.3557 - val_accuracy: 0.4688\n",
      "Epoch 219/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1581 - accuracy: 0.5061 - val_loss: 1.3517 - val_accuracy: 0.4688\n",
      "Epoch 220/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1583 - accuracy: 0.5095 - val_loss: 1.3580 - val_accuracy: 0.4609\n",
      "Epoch 221/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1587 - accuracy: 0.5095 - val_loss: 1.3558 - val_accuracy: 0.4609\n",
      "Epoch 222/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1581 - accuracy: 0.5061 - val_loss: 1.3418 - val_accuracy: 0.4609\n",
      "Epoch 223/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1583 - accuracy: 0.5100 - val_loss: 1.3587 - val_accuracy: 0.4609\n",
      "Epoch 224/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1576 - accuracy: 0.5078 - val_loss: 1.3526 - val_accuracy: 0.4609\n",
      "Epoch 225/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1576 - accuracy: 0.5100 - val_loss: 1.3573 - val_accuracy: 0.4609\n",
      "Epoch 226/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1572 - accuracy: 0.5073 - val_loss: 1.3535 - val_accuracy: 0.4609\n",
      "Epoch 227/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1574 - accuracy: 0.5084 - val_loss: 1.3505 - val_accuracy: 0.4609\n",
      "Epoch 228/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1566 - accuracy: 0.5089 - val_loss: 1.3532 - val_accuracy: 0.4609\n",
      "Epoch 229/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1585 - accuracy: 0.5078 - val_loss: 1.3530 - val_accuracy: 0.4609\n",
      "Epoch 230/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1583 - accuracy: 0.5095 - val_loss: 1.3553 - val_accuracy: 0.4609\n",
      "Epoch 231/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1572 - accuracy: 0.5089 - val_loss: 1.3536 - val_accuracy: 0.4609\n",
      "Epoch 232/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1570 - accuracy: 0.5117 - val_loss: 1.3526 - val_accuracy: 0.4688\n",
      "Epoch 233/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1580 - accuracy: 0.5084 - val_loss: 1.3544 - val_accuracy: 0.4609\n",
      "Epoch 234/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1582 - accuracy: 0.5078 - val_loss: 1.3590 - val_accuracy: 0.4609\n",
      "Epoch 235/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1574 - accuracy: 0.5100 - val_loss: 1.3525 - val_accuracy: 0.4609\n",
      "Epoch 236/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1573 - accuracy: 0.5089 - val_loss: 1.3512 - val_accuracy: 0.4609\n",
      "Epoch 237/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1574 - accuracy: 0.5100 - val_loss: 1.3553 - val_accuracy: 0.4688\n",
      "Epoch 238/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1586 - accuracy: 0.5106 - val_loss: 1.3574 - val_accuracy: 0.4531\n",
      "Epoch 239/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1578 - accuracy: 0.5084 - val_loss: 1.3520 - val_accuracy: 0.4609\n",
      "Epoch 240/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1566 - accuracy: 0.5073 - val_loss: 1.3553 - val_accuracy: 0.4609\n",
      "Epoch 241/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1572 - accuracy: 0.5112 - val_loss: 1.3572 - val_accuracy: 0.4688\n",
      "Epoch 242/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1578 - accuracy: 0.5073 - val_loss: 1.3571 - val_accuracy: 0.4688\n",
      "Epoch 243/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1579 - accuracy: 0.5073 - val_loss: 1.3490 - val_accuracy: 0.4688\n",
      "Epoch 244/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1567 - accuracy: 0.5106 - val_loss: 1.3545 - val_accuracy: 0.4609\n",
      "Epoch 245/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1572 - accuracy: 0.5117 - val_loss: 1.3588 - val_accuracy: 0.4531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 246/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1571 - accuracy: 0.5073 - val_loss: 1.3503 - val_accuracy: 0.4609\n",
      "Epoch 247/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1564 - accuracy: 0.5100 - val_loss: 1.3523 - val_accuracy: 0.4609\n",
      "Epoch 248/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1570 - accuracy: 0.5084 - val_loss: 1.3527 - val_accuracy: 0.4609\n",
      "Epoch 249/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1569 - accuracy: 0.5078 - val_loss: 1.3543 - val_accuracy: 0.4688\n",
      "Epoch 250/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1570 - accuracy: 0.5112 - val_loss: 1.3503 - val_accuracy: 0.4688\n",
      "Epoch 251/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1568 - accuracy: 0.5106 - val_loss: 1.3525 - val_accuracy: 0.4688\n",
      "Epoch 252/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1562 - accuracy: 0.5089 - val_loss: 1.3506 - val_accuracy: 0.4688\n",
      "Epoch 253/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1566 - accuracy: 0.5089 - val_loss: 1.3493 - val_accuracy: 0.4609\n",
      "Epoch 254/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1565 - accuracy: 0.5078 - val_loss: 1.3517 - val_accuracy: 0.4688\n",
      "Epoch 255/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1567 - accuracy: 0.5095 - val_loss: 1.3468 - val_accuracy: 0.4609\n",
      "Epoch 256/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1571 - accuracy: 0.5112 - val_loss: 1.3510 - val_accuracy: 0.4609\n",
      "Epoch 257/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1579 - accuracy: 0.5078 - val_loss: 1.3448 - val_accuracy: 0.4688\n",
      "Epoch 258/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1573 - accuracy: 0.5078 - val_loss: 1.3518 - val_accuracy: 0.4688\n",
      "Epoch 259/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1569 - accuracy: 0.5095 - val_loss: 1.3507 - val_accuracy: 0.4609\n",
      "Epoch 260/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1564 - accuracy: 0.5089 - val_loss: 1.3500 - val_accuracy: 0.4688\n",
      "Epoch 261/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1587 - accuracy: 0.5100 - val_loss: 1.3543 - val_accuracy: 0.4688\n",
      "Epoch 262/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1578 - accuracy: 0.5089 - val_loss: 1.3523 - val_accuracy: 0.4688\n",
      "Epoch 263/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1570 - accuracy: 0.5112 - val_loss: 1.3573 - val_accuracy: 0.4609\n",
      "Epoch 264/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1568 - accuracy: 0.5084 - val_loss: 1.3501 - val_accuracy: 0.4688\n",
      "Epoch 265/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1564 - accuracy: 0.5078 - val_loss: 1.3557 - val_accuracy: 0.4609\n",
      "Epoch 266/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1560 - accuracy: 0.5078 - val_loss: 1.3513 - val_accuracy: 0.4609\n",
      "Epoch 267/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1564 - accuracy: 0.5073 - val_loss: 1.3556 - val_accuracy: 0.4688\n",
      "Epoch 268/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1570 - accuracy: 0.5095 - val_loss: 1.3543 - val_accuracy: 0.4688\n",
      "Epoch 269/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1570 - accuracy: 0.5073 - val_loss: 1.3513 - val_accuracy: 0.4609\n",
      "Epoch 270/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1561 - accuracy: 0.5117 - val_loss: 1.3552 - val_accuracy: 0.4609\n",
      "Epoch 271/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1574 - accuracy: 0.5078 - val_loss: 1.3501 - val_accuracy: 0.4609\n",
      "Epoch 272/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1568 - accuracy: 0.5078 - val_loss: 1.3488 - val_accuracy: 0.4688\n",
      "Epoch 273/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1573 - accuracy: 0.5089 - val_loss: 1.3524 - val_accuracy: 0.4609\n",
      "Epoch 274/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1556 - accuracy: 0.5084 - val_loss: 1.3514 - val_accuracy: 0.4688\n",
      "Epoch 275/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1562 - accuracy: 0.5106 - val_loss: 1.3513 - val_accuracy: 0.4688\n",
      "Epoch 276/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1564 - accuracy: 0.5089 - val_loss: 1.3471 - val_accuracy: 0.4688\n",
      "Epoch 277/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1560 - accuracy: 0.5089 - val_loss: 1.3548 - val_accuracy: 0.4688\n",
      "Epoch 278/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1559 - accuracy: 0.5089 - val_loss: 1.3509 - val_accuracy: 0.4688\n",
      "Epoch 279/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1563 - accuracy: 0.5084 - val_loss: 1.3541 - val_accuracy: 0.4531\n",
      "Epoch 280/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1568 - accuracy: 0.5106 - val_loss: 1.3501 - val_accuracy: 0.4688\n",
      "Epoch 281/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1570 - accuracy: 0.5128 - val_loss: 1.3570 - val_accuracy: 0.4688\n",
      "Epoch 282/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1570 - accuracy: 0.5061 - val_loss: 1.3484 - val_accuracy: 0.4688\n",
      "Epoch 283/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1564 - accuracy: 0.5100 - val_loss: 1.3551 - val_accuracy: 0.4688\n",
      "Epoch 284/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1563 - accuracy: 0.5100 - val_loss: 1.3504 - val_accuracy: 0.4609\n",
      "Epoch 285/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1560 - accuracy: 0.5100 - val_loss: 1.3517 - val_accuracy: 0.4688\n",
      "Epoch 286/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1563 - accuracy: 0.5073 - val_loss: 1.3507 - val_accuracy: 0.4688\n",
      "Epoch 287/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1564 - accuracy: 0.5095 - val_loss: 1.3571 - val_accuracy: 0.4688\n",
      "Epoch 288/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1560 - accuracy: 0.5095 - val_loss: 1.3553 - val_accuracy: 0.4609\n",
      "Epoch 289/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1559 - accuracy: 0.5073 - val_loss: 1.3521 - val_accuracy: 0.4688\n",
      "Epoch 290/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1559 - accuracy: 0.5095 - val_loss: 1.3504 - val_accuracy: 0.4609\n",
      "Epoch 291/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1556 - accuracy: 0.5089 - val_loss: 1.3516 - val_accuracy: 0.4688\n",
      "Epoch 292/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1567 - accuracy: 0.5078 - val_loss: 1.3474 - val_accuracy: 0.4609\n",
      "Epoch 293/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1576 - accuracy: 0.5151 - val_loss: 1.3594 - val_accuracy: 0.4531\n",
      "Epoch 294/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1560 - accuracy: 0.5112 - val_loss: 1.3474 - val_accuracy: 0.4688\n",
      "Epoch 295/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1555 - accuracy: 0.5089 - val_loss: 1.3540 - val_accuracy: 0.4688\n",
      "Epoch 296/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1569 - accuracy: 0.5100 - val_loss: 1.3447 - val_accuracy: 0.4688\n",
      "Epoch 297/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1.1567 - accuracy: 0.5084 - val_loss: 1.3514 - val_accuracy: 0.4688\n",
      "Epoch 298/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1563 - accuracy: 0.5095 - val_loss: 1.3496 - val_accuracy: 0.4688\n",
      "Epoch 299/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1560 - accuracy: 0.5100 - val_loss: 1.3487 - val_accuracy: 0.4609\n",
      "Epoch 300/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1561 - accuracy: 0.5084 - val_loss: 1.3511 - val_accuracy: 0.4688\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "56/56 [==============================] - 0s 1ms/step\n",
      "Recall: [0.93 0.01 0.65]\n",
      "Precision: [0.42 0.27 0.71]\n",
      "F1-score: [0.58 0.01 0.68]\n",
      "Accuracy: 0.51\n",
      "Brier score:0.1926649786230719\n",
      "Brier climat:0.20997519841269843\n",
      "Brier skill score:0.08243935436414707\n",
      "Recall: [0.93 0.01 0.65]\n",
      "Precision: [0.42 0.27 0.71]\n",
      "F1-score: [0.58 0.01 0.68]\n",
      "Accuracy: 0.51\n",
      "Brier score:0.2242251969734843\n",
      "Brier climat:0.22462549603174603\n",
      "Brier skill score:0.0017820731187395689\n",
      "Recall: [0.93 0.01 0.65]\n",
      "Precision: [0.42 0.27 0.71]\n",
      "F1-score: [0.58 0.01 0.68]\n",
      "Accuracy: 0.51\n",
      "Brier score:0.14396397117273807\n",
      "Brier climat:0.22414434523809526\n",
      "Brier skill score:0.35771758587166824\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "Recall: [0.7  0.   0.48]\n",
      "Precision: [0.43 0.   0.53]\n",
      "F1-score: [0.53 0.   0.5 ]\n",
      "Accuracy: 0.47\n",
      "Brier score:0.2643563904655113\n",
      "Brier climat:0.2137326388888889\n",
      "Brier skill score:-0.23685550246230602\n",
      "Recall: [0.7  0.   0.48]\n",
      "Precision: [0.43 0.   0.53]\n",
      "F1-score: [0.53 0.   0.5 ]\n",
      "Accuracy: 0.47\n",
      "Brier score:0.167009089877859\n",
      "Brier climat:0.17503472222222224\n",
      "Brier skill score:0.04585165870217445\n",
      "Recall: [0.7  0.   0.48]\n",
      "Precision: [0.43 0.   0.53]\n",
      "F1-score: [0.53 0.   0.5 ]\n",
      "Accuracy: 0.47\n",
      "Brier score:0.31361570642118297\n",
      "Brier climat:0.24869791666666669\n",
      "Brier skill score:-0.26103069388203415\n",
      "******** 28\n",
      "validation years [2009, 2010]\n",
      "train years {1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 1981, 1982, 1983}\n",
      "{0: 2.0, 1: 1.0, 2: 1.0}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 60)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 183       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183\n",
      "Trainable params: 183\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "Epoch 1/300\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 1.6725 - accuracy: 0.4225WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "28/28 [==============================] - 4s 112ms/step - loss: 1.6664 - accuracy: 0.4235 - val_loss: 1.3473 - val_accuracy: 0.4375\n",
      "Epoch 2/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.5676 - accuracy: 0.4375 - val_loss: 1.3191 - val_accuracy: 0.4609\n",
      "Epoch 3/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.5105 - accuracy: 0.4604 - val_loss: 1.3015 - val_accuracy: 0.4844\n",
      "Epoch 4/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.4743 - accuracy: 0.4676 - val_loss: 1.2966 - val_accuracy: 0.5000\n",
      "Epoch 5/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.4493 - accuracy: 0.4732 - val_loss: 1.2877 - val_accuracy: 0.5234\n",
      "Epoch 6/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.4275 - accuracy: 0.4805 - val_loss: 1.2706 - val_accuracy: 0.5391\n",
      "Epoch 7/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.4109 - accuracy: 0.4927 - val_loss: 1.2638 - val_accuracy: 0.5547\n",
      "Epoch 8/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.3952 - accuracy: 0.4961 - val_loss: 1.2521 - val_accuracy: 0.5469\n",
      "Epoch 9/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.3809 - accuracy: 0.4994 - val_loss: 1.2355 - val_accuracy: 0.5547\n",
      "Epoch 10/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3699 - accuracy: 0.4955 - val_loss: 1.2347 - val_accuracy: 0.5391\n",
      "Epoch 11/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3580 - accuracy: 0.4978 - val_loss: 1.2320 - val_accuracy: 0.5312\n",
      "Epoch 12/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3462 - accuracy: 0.4955 - val_loss: 1.2190 - val_accuracy: 0.5312\n",
      "Epoch 13/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3355 - accuracy: 0.4967 - val_loss: 1.2175 - val_accuracy: 0.5000\n",
      "Epoch 14/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3268 - accuracy: 0.5006 - val_loss: 1.2070 - val_accuracy: 0.4922\n",
      "Epoch 15/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3180 - accuracy: 0.4961 - val_loss: 1.1971 - val_accuracy: 0.5000\n",
      "Epoch 16/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.3093 - accuracy: 0.5039 - val_loss: 1.1977 - val_accuracy: 0.4922\n",
      "Epoch 17/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.3021 - accuracy: 0.5033 - val_loss: 1.1966 - val_accuracy: 0.5000\n",
      "Epoch 18/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2951 - accuracy: 0.4955 - val_loss: 1.1817 - val_accuracy: 0.5156\n",
      "Epoch 19/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.2892 - accuracy: 0.4989 - val_loss: 1.1776 - val_accuracy: 0.4844\n",
      "Epoch 20/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2849 - accuracy: 0.4994 - val_loss: 1.1731 - val_accuracy: 0.5000\n",
      "Epoch 21/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2803 - accuracy: 0.4989 - val_loss: 1.1714 - val_accuracy: 0.4922\n",
      "Epoch 22/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2758 - accuracy: 0.4961 - val_loss: 1.1713 - val_accuracy: 0.4922\n",
      "Epoch 23/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2714 - accuracy: 0.4994 - val_loss: 1.1604 - val_accuracy: 0.4688\n",
      "Epoch 24/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.2680 - accuracy: 0.4972 - val_loss: 1.1617 - val_accuracy: 0.4609\n",
      "Epoch 25/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2639 - accuracy: 0.4933 - val_loss: 1.1531 - val_accuracy: 0.4766\n",
      "Epoch 26/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2599 - accuracy: 0.4989 - val_loss: 1.1502 - val_accuracy: 0.4609\n",
      "Epoch 27/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2563 - accuracy: 0.4955 - val_loss: 1.1458 - val_accuracy: 0.4688\n",
      "Epoch 28/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2531 - accuracy: 0.4967 - val_loss: 1.1466 - val_accuracy: 0.4766\n",
      "Epoch 29/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2503 - accuracy: 0.4950 - val_loss: 1.1395 - val_accuracy: 0.4688\n",
      "Epoch 30/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2474 - accuracy: 0.4922 - val_loss: 1.1395 - val_accuracy: 0.4766\n",
      "Epoch 31/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2444 - accuracy: 0.4944 - val_loss: 1.1389 - val_accuracy: 0.4766\n",
      "Epoch 32/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2430 - accuracy: 0.4933 - val_loss: 1.1358 - val_accuracy: 0.4688\n",
      "Epoch 33/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2394 - accuracy: 0.4916 - val_loss: 1.1294 - val_accuracy: 0.4766\n",
      "Epoch 34/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2377 - accuracy: 0.4944 - val_loss: 1.1302 - val_accuracy: 0.4688\n",
      "Epoch 35/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2355 - accuracy: 0.4955 - val_loss: 1.1264 - val_accuracy: 0.4766\n",
      "Epoch 36/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2321 - accuracy: 0.4955 - val_loss: 1.1227 - val_accuracy: 0.4766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2304 - accuracy: 0.4961 - val_loss: 1.1199 - val_accuracy: 0.4766\n",
      "Epoch 38/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2282 - accuracy: 0.4961 - val_loss: 1.1132 - val_accuracy: 0.4844\n",
      "Epoch 39/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2260 - accuracy: 0.4983 - val_loss: 1.1189 - val_accuracy: 0.4766\n",
      "Epoch 40/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2242 - accuracy: 0.4972 - val_loss: 1.1136 - val_accuracy: 0.4844\n",
      "Epoch 41/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2224 - accuracy: 0.4961 - val_loss: 1.1109 - val_accuracy: 0.4844\n",
      "Epoch 42/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2215 - accuracy: 0.4978 - val_loss: 1.1070 - val_accuracy: 0.4766\n",
      "Epoch 43/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2193 - accuracy: 0.4994 - val_loss: 1.1107 - val_accuracy: 0.4766\n",
      "Epoch 44/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2170 - accuracy: 0.4983 - val_loss: 1.1114 - val_accuracy: 0.4766\n",
      "Epoch 45/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2155 - accuracy: 0.4944 - val_loss: 1.1064 - val_accuracy: 0.4844\n",
      "Epoch 46/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2139 - accuracy: 0.5011 - val_loss: 1.1062 - val_accuracy: 0.4844\n",
      "Epoch 47/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.2130 - accuracy: 0.4955 - val_loss: 1.1069 - val_accuracy: 0.4766\n",
      "Epoch 48/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2119 - accuracy: 0.4972 - val_loss: 1.1036 - val_accuracy: 0.4766\n",
      "Epoch 49/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2106 - accuracy: 0.5000 - val_loss: 1.1037 - val_accuracy: 0.4766\n",
      "Epoch 50/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2092 - accuracy: 0.4972 - val_loss: 1.1007 - val_accuracy: 0.4766\n",
      "Epoch 51/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2075 - accuracy: 0.4989 - val_loss: 1.0996 - val_accuracy: 0.4766\n",
      "Epoch 52/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2070 - accuracy: 0.4978 - val_loss: 1.0968 - val_accuracy: 0.4766\n",
      "Epoch 53/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2054 - accuracy: 0.4989 - val_loss: 1.0966 - val_accuracy: 0.4844\n",
      "Epoch 54/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2052 - accuracy: 0.4994 - val_loss: 1.0986 - val_accuracy: 0.4609\n",
      "Epoch 55/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.2045 - accuracy: 0.5011 - val_loss: 1.0963 - val_accuracy: 0.4844\n",
      "Epoch 56/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.2037 - accuracy: 0.4955 - val_loss: 1.0913 - val_accuracy: 0.4844\n",
      "Epoch 57/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.2021 - accuracy: 0.4950 - val_loss: 1.0981 - val_accuracy: 0.4766\n",
      "Epoch 58/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.2013 - accuracy: 0.4983 - val_loss: 1.0934 - val_accuracy: 0.4844\n",
      "Epoch 59/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.2002 - accuracy: 0.4989 - val_loss: 1.0893 - val_accuracy: 0.4844\n",
      "Epoch 60/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1992 - accuracy: 0.4978 - val_loss: 1.0899 - val_accuracy: 0.4844\n",
      "Epoch 61/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1990 - accuracy: 0.5011 - val_loss: 1.0916 - val_accuracy: 0.4844\n",
      "Epoch 62/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1977 - accuracy: 0.4989 - val_loss: 1.0897 - val_accuracy: 0.4844\n",
      "Epoch 63/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1971 - accuracy: 0.4978 - val_loss: 1.0919 - val_accuracy: 0.4844\n",
      "Epoch 64/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1969 - accuracy: 0.5000 - val_loss: 1.0900 - val_accuracy: 0.4922\n",
      "Epoch 65/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1968 - accuracy: 0.5011 - val_loss: 1.0911 - val_accuracy: 0.4922\n",
      "Epoch 66/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1963 - accuracy: 0.4950 - val_loss: 1.0784 - val_accuracy: 0.4922\n",
      "Epoch 67/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1951 - accuracy: 0.5022 - val_loss: 1.0935 - val_accuracy: 0.5312\n",
      "Epoch 68/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1947 - accuracy: 0.5000 - val_loss: 1.0853 - val_accuracy: 0.4844\n",
      "Epoch 69/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1944 - accuracy: 0.4961 - val_loss: 1.0869 - val_accuracy: 0.5312\n",
      "Epoch 70/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1927 - accuracy: 0.5017 - val_loss: 1.0806 - val_accuracy: 0.5000\n",
      "Epoch 71/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1935 - accuracy: 0.5011 - val_loss: 1.0821 - val_accuracy: 0.4844\n",
      "Epoch 72/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1931 - accuracy: 0.4972 - val_loss: 1.0865 - val_accuracy: 0.4922\n",
      "Epoch 73/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1913 - accuracy: 0.4989 - val_loss: 1.0869 - val_accuracy: 0.5312\n",
      "Epoch 74/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1907 - accuracy: 0.5006 - val_loss: 1.0797 - val_accuracy: 0.5156\n",
      "Epoch 75/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1903 - accuracy: 0.5006 - val_loss: 1.0778 - val_accuracy: 0.5391\n",
      "Epoch 76/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1905 - accuracy: 0.4994 - val_loss: 1.0883 - val_accuracy: 0.5391\n",
      "Epoch 77/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1905 - accuracy: 0.4994 - val_loss: 1.0794 - val_accuracy: 0.5859\n",
      "Epoch 78/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1893 - accuracy: 0.5039 - val_loss: 1.0898 - val_accuracy: 0.5312\n",
      "Epoch 79/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1889 - accuracy: 0.5028 - val_loss: 1.0848 - val_accuracy: 0.5391\n",
      "Epoch 80/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1887 - accuracy: 0.4989 - val_loss: 1.0803 - val_accuracy: 0.5078\n",
      "Epoch 81/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1885 - accuracy: 0.5017 - val_loss: 1.0820 - val_accuracy: 0.5234\n",
      "Epoch 82/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1889 - accuracy: 0.5039 - val_loss: 1.0886 - val_accuracy: 0.5625\n",
      "Epoch 83/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1885 - accuracy: 0.4989 - val_loss: 1.0785 - val_accuracy: 0.5234\n",
      "Epoch 84/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1887 - accuracy: 0.4989 - val_loss: 1.0823 - val_accuracy: 0.5703\n",
      "Epoch 85/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1878 - accuracy: 0.5089 - val_loss: 1.0905 - val_accuracy: 0.5859\n",
      "Epoch 86/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1869 - accuracy: 0.5000 - val_loss: 1.0726 - val_accuracy: 0.5078\n",
      "Epoch 87/300\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1882 - accuracy: 0.5073 - val_loss: 1.0837 - val_accuracy: 0.5312\n",
      "Epoch 88/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1881 - accuracy: 0.4950 - val_loss: 1.0827 - val_accuracy: 0.5781\n",
      "Epoch 89/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1866 - accuracy: 0.5033 - val_loss: 1.0845 - val_accuracy: 0.5469\n",
      "Epoch 90/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1866 - accuracy: 0.5028 - val_loss: 1.0759 - val_accuracy: 0.5078\n",
      "Epoch 91/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1860 - accuracy: 0.5006 - val_loss: 1.0884 - val_accuracy: 0.5156\n",
      "Epoch 92/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1859 - accuracy: 0.5011 - val_loss: 1.0822 - val_accuracy: 0.5859\n",
      "Epoch 93/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1856 - accuracy: 0.5028 - val_loss: 1.0805 - val_accuracy: 0.5156\n",
      "Epoch 94/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1849 - accuracy: 0.5011 - val_loss: 1.0734 - val_accuracy: 0.5859\n",
      "Epoch 95/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1844 - accuracy: 0.4994 - val_loss: 1.0810 - val_accuracy: 0.5156\n",
      "Epoch 96/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1848 - accuracy: 0.5039 - val_loss: 1.0796 - val_accuracy: 0.5859\n",
      "Epoch 97/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1847 - accuracy: 0.5050 - val_loss: 1.0840 - val_accuracy: 0.5078\n",
      "Epoch 98/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1839 - accuracy: 0.5000 - val_loss: 1.0757 - val_accuracy: 0.5859\n",
      "Epoch 99/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1843 - accuracy: 0.4994 - val_loss: 1.0741 - val_accuracy: 0.5078\n",
      "Epoch 100/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1833 - accuracy: 0.5017 - val_loss: 1.0802 - val_accuracy: 0.5859\n",
      "Epoch 101/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1836 - accuracy: 0.5000 - val_loss: 1.0823 - val_accuracy: 0.5859\n",
      "Epoch 102/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1833 - accuracy: 0.5045 - val_loss: 1.0780 - val_accuracy: 0.5547\n",
      "Epoch 103/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1836 - accuracy: 0.5011 - val_loss: 1.0791 - val_accuracy: 0.5859\n",
      "Epoch 104/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1832 - accuracy: 0.4994 - val_loss: 1.0804 - val_accuracy: 0.5859\n",
      "Epoch 105/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1824 - accuracy: 0.5017 - val_loss: 1.0808 - val_accuracy: 0.5703\n",
      "Epoch 106/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1826 - accuracy: 0.4989 - val_loss: 1.0773 - val_accuracy: 0.5625\n",
      "Epoch 107/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1830 - accuracy: 0.5039 - val_loss: 1.0841 - val_accuracy: 0.5781\n",
      "Epoch 108/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1830 - accuracy: 0.4983 - val_loss: 1.0776 - val_accuracy: 0.5312\n",
      "Epoch 109/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1821 - accuracy: 0.5017 - val_loss: 1.0771 - val_accuracy: 0.5859\n",
      "Epoch 110/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1826 - accuracy: 0.5033 - val_loss: 1.0827 - val_accuracy: 0.5781\n",
      "Epoch 111/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1818 - accuracy: 0.5017 - val_loss: 1.0805 - val_accuracy: 0.5781\n",
      "Epoch 112/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1816 - accuracy: 0.5033 - val_loss: 1.0797 - val_accuracy: 0.5234\n",
      "Epoch 113/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1820 - accuracy: 0.5022 - val_loss: 1.0804 - val_accuracy: 0.5781\n",
      "Epoch 114/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1821 - accuracy: 0.5017 - val_loss: 1.0767 - val_accuracy: 0.5859\n",
      "Epoch 115/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1819 - accuracy: 0.5033 - val_loss: 1.0867 - val_accuracy: 0.5781\n",
      "Epoch 116/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1815 - accuracy: 0.5022 - val_loss: 1.0790 - val_accuracy: 0.5859\n",
      "Epoch 117/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1814 - accuracy: 0.5045 - val_loss: 1.0771 - val_accuracy: 0.5781\n",
      "Epoch 118/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1809 - accuracy: 0.5028 - val_loss: 1.0756 - val_accuracy: 0.5859\n",
      "Epoch 119/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1810 - accuracy: 0.5011 - val_loss: 1.0812 - val_accuracy: 0.5859\n",
      "Epoch 120/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1813 - accuracy: 0.5050 - val_loss: 1.0866 - val_accuracy: 0.4922\n",
      "Epoch 121/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1808 - accuracy: 0.5022 - val_loss: 1.0750 - val_accuracy: 0.5781\n",
      "Epoch 122/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1809 - accuracy: 0.5011 - val_loss: 1.0778 - val_accuracy: 0.5547\n",
      "Epoch 123/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1809 - accuracy: 0.5039 - val_loss: 1.0787 - val_accuracy: 0.5859\n",
      "Epoch 124/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1807 - accuracy: 0.5017 - val_loss: 1.0728 - val_accuracy: 0.5859\n",
      "Epoch 125/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1803 - accuracy: 0.5017 - val_loss: 1.0798 - val_accuracy: 0.5781\n",
      "Epoch 126/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1810 - accuracy: 0.5017 - val_loss: 1.0797 - val_accuracy: 0.5859\n",
      "Epoch 127/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1821 - accuracy: 0.5067 - val_loss: 1.0772 - val_accuracy: 0.5078\n",
      "Epoch 128/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1824 - accuracy: 0.4994 - val_loss: 1.0812 - val_accuracy: 0.5469\n",
      "Epoch 129/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1804 - accuracy: 0.5022 - val_loss: 1.0770 - val_accuracy: 0.5781\n",
      "Epoch 130/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1801 - accuracy: 0.5028 - val_loss: 1.0804 - val_accuracy: 0.5859\n",
      "Epoch 131/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1802 - accuracy: 0.5039 - val_loss: 1.0788 - val_accuracy: 0.5859\n",
      "Epoch 132/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1801 - accuracy: 0.5028 - val_loss: 1.0763 - val_accuracy: 0.5859\n",
      "Epoch 133/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1797 - accuracy: 0.5017 - val_loss: 1.0707 - val_accuracy: 0.5781\n",
      "Epoch 134/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1807 - accuracy: 0.5045 - val_loss: 1.0799 - val_accuracy: 0.5391\n",
      "Epoch 135/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1794 - accuracy: 0.5033 - val_loss: 1.0731 - val_accuracy: 0.5859\n",
      "Epoch 136/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1791 - accuracy: 0.5033 - val_loss: 1.0779 - val_accuracy: 0.5859\n",
      "Epoch 137/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1795 - accuracy: 0.5033 - val_loss: 1.0793 - val_accuracy: 0.5781\n",
      "Epoch 138/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1800 - accuracy: 0.5039 - val_loss: 1.0752 - val_accuracy: 0.5859\n",
      "Epoch 139/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1785 - accuracy: 0.5028 - val_loss: 1.0726 - val_accuracy: 0.5625\n",
      "Epoch 140/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1803 - accuracy: 0.5039 - val_loss: 1.0784 - val_accuracy: 0.5234\n",
      "Epoch 141/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1797 - accuracy: 0.5056 - val_loss: 1.0725 - val_accuracy: 0.5859\n",
      "Epoch 142/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1785 - accuracy: 0.5017 - val_loss: 1.0771 - val_accuracy: 0.5859\n",
      "Epoch 143/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1785 - accuracy: 0.5017 - val_loss: 1.0728 - val_accuracy: 0.5859\n",
      "Epoch 144/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1787 - accuracy: 0.5039 - val_loss: 1.0775 - val_accuracy: 0.5781\n",
      "Epoch 145/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1790 - accuracy: 0.5045 - val_loss: 1.0773 - val_accuracy: 0.5781\n",
      "Epoch 146/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1795 - accuracy: 0.5033 - val_loss: 1.0779 - val_accuracy: 0.5781\n",
      "Epoch 147/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1785 - accuracy: 0.5056 - val_loss: 1.0757 - val_accuracy: 0.5859\n",
      "Epoch 148/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1786 - accuracy: 0.5039 - val_loss: 1.0762 - val_accuracy: 0.5859\n",
      "Epoch 149/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1792 - accuracy: 0.5028 - val_loss: 1.0767 - val_accuracy: 0.5859\n",
      "Epoch 150/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1789 - accuracy: 0.5033 - val_loss: 1.0790 - val_accuracy: 0.5859\n",
      "Epoch 151/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1785 - accuracy: 0.5028 - val_loss: 1.0756 - val_accuracy: 0.5781\n",
      "Epoch 152/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1786 - accuracy: 0.5017 - val_loss: 1.0739 - val_accuracy: 0.5859\n",
      "Epoch 153/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1787 - accuracy: 0.5050 - val_loss: 1.0728 - val_accuracy: 0.5703\n",
      "Epoch 154/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1783 - accuracy: 0.5028 - val_loss: 1.0715 - val_accuracy: 0.5859\n",
      "Epoch 155/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1780 - accuracy: 0.5017 - val_loss: 1.0752 - val_accuracy: 0.5781\n",
      "Epoch 156/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1780 - accuracy: 0.5039 - val_loss: 1.0774 - val_accuracy: 0.5703\n",
      "Epoch 157/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1777 - accuracy: 0.5028 - val_loss: 1.0736 - val_accuracy: 0.5781\n",
      "Epoch 158/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1780 - accuracy: 0.5039 - val_loss: 1.0745 - val_accuracy: 0.5859\n",
      "Epoch 159/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1789 - accuracy: 0.5028 - val_loss: 1.0719 - val_accuracy: 0.5859\n",
      "Epoch 160/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1778 - accuracy: 0.5039 - val_loss: 1.0709 - val_accuracy: 0.5859\n",
      "Epoch 161/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1773 - accuracy: 0.5061 - val_loss: 1.0774 - val_accuracy: 0.5781\n",
      "Epoch 162/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1778 - accuracy: 0.5039 - val_loss: 1.0765 - val_accuracy: 0.5859\n",
      "Epoch 163/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1779 - accuracy: 0.5033 - val_loss: 1.0684 - val_accuracy: 0.5859\n",
      "Epoch 164/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1777 - accuracy: 0.5061 - val_loss: 1.0711 - val_accuracy: 0.5859\n",
      "Epoch 165/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1779 - accuracy: 0.5045 - val_loss: 1.0750 - val_accuracy: 0.5859\n",
      "Epoch 166/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1788 - accuracy: 0.5067 - val_loss: 1.0838 - val_accuracy: 0.5078\n",
      "Epoch 167/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1782 - accuracy: 0.5045 - val_loss: 1.0724 - val_accuracy: 0.5859\n",
      "Epoch 168/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1774 - accuracy: 0.5028 - val_loss: 1.0691 - val_accuracy: 0.5859\n",
      "Epoch 169/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1777 - accuracy: 0.5067 - val_loss: 1.0687 - val_accuracy: 0.5859\n",
      "Epoch 170/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1775 - accuracy: 0.5100 - val_loss: 1.0761 - val_accuracy: 0.5703\n",
      "Epoch 171/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1776 - accuracy: 0.5017 - val_loss: 1.0716 - val_accuracy: 0.5781\n",
      "Epoch 172/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1764 - accuracy: 0.5050 - val_loss: 1.0725 - val_accuracy: 0.5859\n",
      "Epoch 173/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1768 - accuracy: 0.5050 - val_loss: 1.0715 - val_accuracy: 0.5859\n",
      "Epoch 174/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1768 - accuracy: 0.5061 - val_loss: 1.0733 - val_accuracy: 0.5781\n",
      "Epoch 175/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1775 - accuracy: 0.5056 - val_loss: 1.0732 - val_accuracy: 0.5781\n",
      "Epoch 176/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1770 - accuracy: 0.5067 - val_loss: 1.0723 - val_accuracy: 0.5859\n",
      "Epoch 177/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1773 - accuracy: 0.5045 - val_loss: 1.0729 - val_accuracy: 0.5859\n",
      "Epoch 178/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1764 - accuracy: 0.5022 - val_loss: 1.0659 - val_accuracy: 0.5859\n",
      "Epoch 179/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1767 - accuracy: 0.5056 - val_loss: 1.0661 - val_accuracy: 0.5859\n",
      "Epoch 180/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1771 - accuracy: 0.5061 - val_loss: 1.0672 - val_accuracy: 0.5469\n",
      "Epoch 181/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1764 - accuracy: 0.5078 - val_loss: 1.0722 - val_accuracy: 0.5859\n",
      "Epoch 182/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1774 - accuracy: 0.5045 - val_loss: 1.0778 - val_accuracy: 0.5859\n",
      "Epoch 183/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1762 - accuracy: 0.5067 - val_loss: 1.0702 - val_accuracy: 0.5859\n",
      "Epoch 184/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1767 - accuracy: 0.5067 - val_loss: 1.0659 - val_accuracy: 0.5781\n",
      "Epoch 185/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1771 - accuracy: 0.5039 - val_loss: 1.0643 - val_accuracy: 0.5781\n",
      "Epoch 186/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1760 - accuracy: 0.5056 - val_loss: 1.0667 - val_accuracy: 0.5859\n",
      "Epoch 187/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1776 - accuracy: 0.5056 - val_loss: 1.0684 - val_accuracy: 0.5859\n",
      "Epoch 188/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1758 - accuracy: 0.5056 - val_loss: 1.0701 - val_accuracy: 0.5703\n",
      "Epoch 189/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1767 - accuracy: 0.5056 - val_loss: 1.0766 - val_accuracy: 0.5781\n",
      "Epoch 190/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1771 - accuracy: 0.5045 - val_loss: 1.0671 - val_accuracy: 0.5859\n",
      "Epoch 191/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1766 - accuracy: 0.5033 - val_loss: 1.0742 - val_accuracy: 0.5781\n",
      "Epoch 192/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1761 - accuracy: 0.5067 - val_loss: 1.0693 - val_accuracy: 0.5859\n",
      "Epoch 193/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1761 - accuracy: 0.5056 - val_loss: 1.0723 - val_accuracy: 0.5859\n",
      "Epoch 194/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1760 - accuracy: 0.5073 - val_loss: 1.0738 - val_accuracy: 0.5781\n",
      "Epoch 195/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1760 - accuracy: 0.5039 - val_loss: 1.0612 - val_accuracy: 0.5859\n",
      "Epoch 196/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1760 - accuracy: 0.5073 - val_loss: 1.0726 - val_accuracy: 0.5859\n",
      "Epoch 197/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1756 - accuracy: 0.5067 - val_loss: 1.0681 - val_accuracy: 0.5859\n",
      "Epoch 198/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1758 - accuracy: 0.5084 - val_loss: 1.0686 - val_accuracy: 0.5859\n",
      "Epoch 199/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1764 - accuracy: 0.5078 - val_loss: 1.0744 - val_accuracy: 0.5859\n",
      "Epoch 200/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1759 - accuracy: 0.5028 - val_loss: 1.0651 - val_accuracy: 0.5859\n",
      "Epoch 201/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1761 - accuracy: 0.5061 - val_loss: 1.0742 - val_accuracy: 0.5859\n",
      "Epoch 202/300\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 1.1756 - accuracy: 0.5050 - val_loss: 1.0694 - val_accuracy: 0.5781\n",
      "Epoch 203/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1753 - accuracy: 0.5056 - val_loss: 1.0723 - val_accuracy: 0.5781\n",
      "Epoch 204/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1759 - accuracy: 0.5067 - val_loss: 1.0722 - val_accuracy: 0.5781\n",
      "Epoch 205/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1755 - accuracy: 0.5089 - val_loss: 1.0740 - val_accuracy: 0.5859\n",
      "Epoch 206/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1763 - accuracy: 0.5061 - val_loss: 1.0748 - val_accuracy: 0.5781\n",
      "Epoch 207/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1756 - accuracy: 0.5045 - val_loss: 1.0707 - val_accuracy: 0.5703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 208/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1752 - accuracy: 0.5073 - val_loss: 1.0686 - val_accuracy: 0.5859\n",
      "Epoch 209/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1752 - accuracy: 0.5073 - val_loss: 1.0706 - val_accuracy: 0.5781\n",
      "Epoch 210/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1758 - accuracy: 0.5073 - val_loss: 1.0691 - val_accuracy: 0.5781\n",
      "Epoch 211/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1765 - accuracy: 0.5067 - val_loss: 1.0733 - val_accuracy: 0.5547\n",
      "Epoch 212/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1758 - accuracy: 0.5078 - val_loss: 1.0707 - val_accuracy: 0.5859\n",
      "Epoch 213/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1763 - accuracy: 0.5033 - val_loss: 1.0665 - val_accuracy: 0.5859\n",
      "Epoch 214/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1765 - accuracy: 0.5078 - val_loss: 1.0762 - val_accuracy: 0.5781\n",
      "Epoch 215/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1758 - accuracy: 0.5078 - val_loss: 1.0692 - val_accuracy: 0.5859\n",
      "Epoch 216/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1761 - accuracy: 0.5050 - val_loss: 1.0691 - val_accuracy: 0.5625\n",
      "Epoch 217/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1775 - accuracy: 0.5095 - val_loss: 1.0617 - val_accuracy: 0.5391\n",
      "Epoch 218/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1757 - accuracy: 0.5033 - val_loss: 1.0737 - val_accuracy: 0.5859\n",
      "Epoch 219/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1763 - accuracy: 0.5128 - val_loss: 1.0771 - val_accuracy: 0.5781\n",
      "Epoch 220/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1760 - accuracy: 0.5067 - val_loss: 1.0643 - val_accuracy: 0.5859\n",
      "Epoch 221/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1755 - accuracy: 0.5033 - val_loss: 1.0722 - val_accuracy: 0.5859\n",
      "Epoch 222/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1753 - accuracy: 0.5078 - val_loss: 1.0687 - val_accuracy: 0.5703\n",
      "Epoch 223/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1750 - accuracy: 0.5073 - val_loss: 1.0759 - val_accuracy: 0.5781\n",
      "Epoch 224/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1767 - accuracy: 0.5050 - val_loss: 1.0720 - val_accuracy: 0.5859\n",
      "Epoch 225/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1754 - accuracy: 0.5078 - val_loss: 1.0710 - val_accuracy: 0.5859\n",
      "Epoch 226/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1760 - accuracy: 0.5033 - val_loss: 1.0683 - val_accuracy: 0.5859\n",
      "Epoch 227/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1753 - accuracy: 0.5095 - val_loss: 1.0721 - val_accuracy: 0.5781\n",
      "Epoch 228/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1755 - accuracy: 0.5050 - val_loss: 1.0688 - val_accuracy: 0.5781\n",
      "Epoch 229/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1745 - accuracy: 0.5067 - val_loss: 1.0677 - val_accuracy: 0.5781\n",
      "Epoch 230/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1759 - accuracy: 0.5089 - val_loss: 1.0762 - val_accuracy: 0.5781\n",
      "Epoch 231/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1761 - accuracy: 0.5073 - val_loss: 1.0726 - val_accuracy: 0.5859\n",
      "Epoch 232/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1750 - accuracy: 0.5061 - val_loss: 1.0750 - val_accuracy: 0.5781\n",
      "Epoch 233/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1748 - accuracy: 0.5061 - val_loss: 1.0713 - val_accuracy: 0.5859\n",
      "Epoch 234/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1764 - accuracy: 0.5078 - val_loss: 1.0707 - val_accuracy: 0.5547\n",
      "Epoch 235/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1748 - accuracy: 0.5056 - val_loss: 1.0691 - val_accuracy: 0.5938\n",
      "Epoch 236/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1746 - accuracy: 0.5061 - val_loss: 1.0660 - val_accuracy: 0.5781\n",
      "Epoch 237/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1754 - accuracy: 0.5089 - val_loss: 1.0768 - val_accuracy: 0.5781\n",
      "Epoch 238/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1747 - accuracy: 0.5067 - val_loss: 1.0673 - val_accuracy: 0.5781\n",
      "Epoch 239/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1746 - accuracy: 0.5078 - val_loss: 1.0704 - val_accuracy: 0.5781\n",
      "Epoch 240/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1757 - accuracy: 0.5084 - val_loss: 1.0647 - val_accuracy: 0.5391\n",
      "Epoch 241/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1747 - accuracy: 0.5061 - val_loss: 1.0696 - val_accuracy: 0.5859\n",
      "Epoch 242/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1745 - accuracy: 0.5056 - val_loss: 1.0700 - val_accuracy: 0.5781\n",
      "Epoch 243/300\n",
      "28/28 [==============================] - 1s 29ms/step - loss: 1.1745 - accuracy: 0.5067 - val_loss: 1.0637 - val_accuracy: 0.5938\n",
      "Epoch 244/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1759 - accuracy: 0.5084 - val_loss: 1.0756 - val_accuracy: 0.5781\n",
      "Epoch 245/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1759 - accuracy: 0.5089 - val_loss: 1.0708 - val_accuracy: 0.5859\n",
      "Epoch 246/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1754 - accuracy: 0.5061 - val_loss: 1.0705 - val_accuracy: 0.5859\n",
      "Epoch 247/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1749 - accuracy: 0.5039 - val_loss: 1.0693 - val_accuracy: 0.5859\n",
      "Epoch 248/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1754 - accuracy: 0.5100 - val_loss: 1.0700 - val_accuracy: 0.5859\n",
      "Epoch 249/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1756 - accuracy: 0.5056 - val_loss: 1.0628 - val_accuracy: 0.5859\n",
      "Epoch 250/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1752 - accuracy: 0.5056 - val_loss: 1.0660 - val_accuracy: 0.5859\n",
      "Epoch 251/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1751 - accuracy: 0.5073 - val_loss: 1.0676 - val_accuracy: 0.5703\n",
      "Epoch 252/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1744 - accuracy: 0.5061 - val_loss: 1.0657 - val_accuracy: 0.5938\n",
      "Epoch 253/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1746 - accuracy: 0.5050 - val_loss: 1.0706 - val_accuracy: 0.5781\n",
      "Epoch 254/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1741 - accuracy: 0.5078 - val_loss: 1.0696 - val_accuracy: 0.5781\n",
      "Epoch 255/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1745 - accuracy: 0.5100 - val_loss: 1.0728 - val_accuracy: 0.5859\n",
      "Epoch 256/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1747 - accuracy: 0.5056 - val_loss: 1.0649 - val_accuracy: 0.5938\n",
      "Epoch 257/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1744 - accuracy: 0.5078 - val_loss: 1.0647 - val_accuracy: 0.5781\n",
      "Epoch 258/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1745 - accuracy: 0.5106 - val_loss: 1.0707 - val_accuracy: 0.5859\n",
      "Epoch 259/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1745 - accuracy: 0.5073 - val_loss: 1.0628 - val_accuracy: 0.5781\n",
      "Epoch 260/300\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 1.1744 - accuracy: 0.5039 - val_loss: 1.0679 - val_accuracy: 0.5938\n",
      "Epoch 261/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1741 - accuracy: 0.5056 - val_loss: 1.0705 - val_accuracy: 0.5781\n",
      "Epoch 262/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1740 - accuracy: 0.5067 - val_loss: 1.0630 - val_accuracy: 0.5938\n",
      "Epoch 263/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1741 - accuracy: 0.5078 - val_loss: 1.0702 - val_accuracy: 0.5781\n",
      "Epoch 264/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1742 - accuracy: 0.5073 - val_loss: 1.0660 - val_accuracy: 0.5781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 265/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1747 - accuracy: 0.5084 - val_loss: 1.0686 - val_accuracy: 0.5781\n",
      "Epoch 266/300\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.1742 - accuracy: 0.5050 - val_loss: 1.0656 - val_accuracy: 0.5781\n",
      "Epoch 267/300\n",
      "28/28 [==============================] - 1s 31ms/step - loss: 1.1750 - accuracy: 0.5073 - val_loss: 1.0617 - val_accuracy: 0.5938\n",
      "Epoch 268/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1745 - accuracy: 0.5073 - val_loss: 1.0687 - val_accuracy: 0.5781\n",
      "Epoch 269/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1745 - accuracy: 0.5061 - val_loss: 1.0661 - val_accuracy: 0.5938\n",
      "Epoch 270/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1743 - accuracy: 0.5056 - val_loss: 1.0655 - val_accuracy: 0.5781\n",
      "Epoch 271/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1747 - accuracy: 0.5100 - val_loss: 1.0648 - val_accuracy: 0.5781\n",
      "Epoch 272/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1741 - accuracy: 0.5067 - val_loss: 1.0659 - val_accuracy: 0.5781\n",
      "Epoch 273/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1740 - accuracy: 0.5073 - val_loss: 1.0700 - val_accuracy: 0.5859\n",
      "Epoch 274/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1741 - accuracy: 0.5056 - val_loss: 1.0669 - val_accuracy: 0.5781\n",
      "Epoch 275/300\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 1.1750 - accuracy: 0.5073 - val_loss: 1.0654 - val_accuracy: 0.5859\n",
      "Epoch 276/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1743 - accuracy: 0.5073 - val_loss: 1.0697 - val_accuracy: 0.5781\n",
      "Epoch 277/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1743 - accuracy: 0.5061 - val_loss: 1.0696 - val_accuracy: 0.5859\n",
      "Epoch 278/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1736 - accuracy: 0.5045 - val_loss: 1.0667 - val_accuracy: 0.5859\n",
      "Epoch 279/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1738 - accuracy: 0.5084 - val_loss: 1.0603 - val_accuracy: 0.5938\n",
      "Epoch 280/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1741 - accuracy: 0.5067 - val_loss: 1.0719 - val_accuracy: 0.5781\n",
      "Epoch 281/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1739 - accuracy: 0.5067 - val_loss: 1.0639 - val_accuracy: 0.5781\n",
      "Epoch 282/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1752 - accuracy: 0.5100 - val_loss: 1.0699 - val_accuracy: 0.5781\n",
      "Epoch 283/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1752 - accuracy: 0.5028 - val_loss: 1.0623 - val_accuracy: 0.5938\n",
      "Epoch 284/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1738 - accuracy: 0.5061 - val_loss: 1.0751 - val_accuracy: 0.5781\n",
      "Epoch 285/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1741 - accuracy: 0.5061 - val_loss: 1.0763 - val_accuracy: 0.5781\n",
      "Epoch 286/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1745 - accuracy: 0.5078 - val_loss: 1.0658 - val_accuracy: 0.5859\n",
      "Epoch 287/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1737 - accuracy: 0.5078 - val_loss: 1.0662 - val_accuracy: 0.5781\n",
      "Epoch 288/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1737 - accuracy: 0.5067 - val_loss: 1.0618 - val_accuracy: 0.5859\n",
      "Epoch 289/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1747 - accuracy: 0.5117 - val_loss: 1.0718 - val_accuracy: 0.5938\n",
      "Epoch 290/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1744 - accuracy: 0.5039 - val_loss: 1.0702 - val_accuracy: 0.5703\n",
      "Epoch 291/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1741 - accuracy: 0.5067 - val_loss: 1.0677 - val_accuracy: 0.5781\n",
      "Epoch 292/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1739 - accuracy: 0.5050 - val_loss: 1.0676 - val_accuracy: 0.5859\n",
      "Epoch 293/300\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 1.1746 - accuracy: 0.5056 - val_loss: 1.0721 - val_accuracy: 0.5781\n",
      "Epoch 294/300\n",
      "28/28 [==============================] - 1s 26ms/step - loss: 1.1740 - accuracy: 0.5050 - val_loss: 1.0666 - val_accuracy: 0.5859\n",
      "Epoch 295/300\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.1738 - accuracy: 0.5084 - val_loss: 1.0635 - val_accuracy: 0.5859\n",
      "Epoch 296/300\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 1.1746 - accuracy: 0.5033 - val_loss: 1.0657 - val_accuracy: 0.5938\n",
      "Epoch 297/300\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1.1746 - accuracy: 0.5128 - val_loss: 1.0719 - val_accuracy: 0.5781\n",
      "Epoch 298/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1741 - accuracy: 0.5045 - val_loss: 1.0766 - val_accuracy: 0.5625\n",
      "Epoch 299/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1750 - accuracy: 0.5067 - val_loss: 1.0646 - val_accuracy: 0.5859\n",
      "Epoch 300/300\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1740 - accuracy: 0.5084 - val_loss: 1.0672 - val_accuracy: 0.5781\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "56/56 [==============================] - 0s 1ms/step\n",
      "Recall: [0.93 0.01 0.65]\n",
      "Precision: [0.42 0.56 0.69]\n",
      "F1-score: [0.58 0.02 0.67]\n",
      "Accuracy: 0.51\n",
      "Brier score:0.18897540229625073\n",
      "Brier climat:0.20539930555555558\n",
      "Brier skill score:0.07996085096238348\n",
      "Recall: [0.93 0.01 0.65]\n",
      "Precision: [0.42 0.56 0.69]\n",
      "F1-score: [0.58 0.02 0.67]\n",
      "Accuracy: 0.51\n",
      "Brier score:0.22361617382476787\n",
      "Brier climat:0.2233234126984127\n",
      "Brier skill score:-0.0013109289474746433\n",
      "Recall: [0.93 0.01 0.65]\n",
      "Precision: [0.42 0.56 0.69]\n",
      "F1-score: [0.58 0.02 0.67]\n",
      "Accuracy: 0.51\n",
      "Brier score:0.15282359886463123\n",
      "Brier climat:0.22805059523809526\n",
      "Brier skill score:0.32986976550060565\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "Recall: [0.81 0.   0.63]\n",
      "Precision: [0.57 0.   0.61]\n",
      "F1-score: [0.67 0.   0.62]\n",
      "Accuracy: 0.58\n",
      "Brier score:0.2751601781215459\n",
      "Brier climat:0.2777951388888889\n",
      "Brier skill score:0.009485265933313891\n",
      "Recall: [0.81 0.   0.63]\n",
      "Precision: [0.57 0.   0.61]\n",
      "F1-score: [0.67 0.   0.62]\n",
      "Accuracy: 0.58\n",
      "Brier score:0.17861041294503105\n",
      "Brier climat:0.1932638888888889\n",
      "Brier skill score:0.07582107567069818\n",
      "Recall: [0.81 0.   0.63]\n",
      "Precision: [0.57 0.   0.61]\n",
      "F1-score: [0.67 0.   0.62]\n",
      "Accuracy: 0.58\n",
      "Brier score:0.15697023390577025\n",
      "Brier climat:0.19401041666666669\n",
      "Brier skill score:0.1909185259085131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/net/cfc/s2s/zhengwu/aienv/conda/envs/ai_2022/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "train_loss_lr, train_acc_lr = [],[]\n",
    "valid_loss_lr, valid_acc_lr = [],[]\n",
    "acc_tr_lr, f1_tr_lr, prec_tr_lr, rec_tr_lr = [],[],[],[]\n",
    "bss_tr1_lr, calib_y_tr1_lr, calib_x_tr1_lr = [],[],[]\n",
    "bss_tr2_lr, calib_y_tr2_lr, calib_x_tr2_lr = [],[],[]\n",
    "bss_tr3_lr, calib_y_tr3_lr, calib_x_tr3_lr = [],[],[]\n",
    "acc_va_lr, f1_va_lr, prec_va_lr, rec_va_lr = [],[],[],[]\n",
    "bss_va1_lr, calib_y_va1_lr, calib_x_va1_lr = [],[],[]\n",
    "bss_va2_lr, calib_y_va2_lr, calib_x_va2_lr = [],[],[]\n",
    "bss_va3_lr, calib_y_va3_lr, calib_x_va3_lr = [],[],[]\n",
    "acc_te_lr, f1_te_lr, prec_te_lr, rec_te_lr = [],[],[],[]\n",
    "bss_te1_lr, calib_y_te1_lr, calib_x_te1_lr = [],[],[]\n",
    "bss_te2_lr, calib_y_te2_lr, calib_x_te2_lr = [],[],[]\n",
    "bss_te3_lr, calib_y_te3_lr, calib_x_te3_lr = [],[],[]\n",
    "nbins = 10\n",
    "for ii in range(NYRS-1):\n",
    "    print(\"********\", ii)\n",
    "    val_year = [SYY+ii,SYY+ii+1]\n",
    "    X_train, Y_train, X_val, Y_val = cross_valid_n_out(X_all_new, y_all, all_years, val_year=val_year)\n",
    "    Y_train_2d = keras.utils.to_categorical(Y_train-1)\n",
    "    Y_val_2d = keras.utils.to_categorical(Y_val-1)\n",
    "    \n",
    "    # for imbalanced classes\n",
    "    class_weight = class_weight_creator(Y_train_2d)\n",
    "    class_weight[0] = 2.0\n",
    "    class_weight[1] = 1.0\n",
    "    class_weight[2] = 1.0\n",
    "    print(class_weight)\n",
    "    \n",
    "    mdim = np.shape(X_train)[1]\n",
    "    lr_model = build_logistic(mdim, out_neurons=n_cat, regval=0.01)\n",
    "    callbacks_path = '/net/cfc/s2s/zhengwu/code/tmp/precip_lr.h5'\n",
    "    batch_size = 64\n",
    "    epochs = 300\n",
    "    lr = 0.0008\n",
    "    history = train_model(lr_model, X_train, Y_train_2d, X_val, Y_val_2d, lr, callbacks_path, epochs, batch_size, class_weight)\n",
    "   \n",
    "    train_loss_lr.append(history.history['loss'])\n",
    "    train_acc_lr.append(history.history['accuracy'])\n",
    "    valid_loss_lr.append(history.history['val_loss'])\n",
    "    valid_acc_lr.append(history.history['val_accuracy'])\n",
    "    \n",
    "    # evaluation training\n",
    "    yprob = lr_model.predict(X_train)\n",
    "    ypred = np.argmax(yprob,axis=1)+1\n",
    "    y_prob_ref = clim_pr_y_cat1[0:len(Y_train)]\n",
    "    recall, precision, f1, acc, bs, bss, calib_y, calib_x = score(Y_train, Y_train_2d[:,0], ypred, \n",
    "                                                    yprob[:,0], nbins, y_prob_ref)\n",
    "    acc_tr_lr.append(acc)\n",
    "    f1_tr_lr.append(f1)\n",
    "    prec_tr_lr.append(precision)\n",
    "    rec_tr_lr.append(recall)\n",
    "    calib_y_tr1_lr.append(calib_y)\n",
    "    calib_x_tr1_lr.append(calib_x)\n",
    "    bss_tr1_lr.append(bss)\n",
    "    \n",
    "    y_prob_ref = clim_pr_y_cat3[0:len(Y_train)]\n",
    "    recall, precision, f1, acc, bs, bss, calib_y, calib_x = score(Y_train, Y_train_2d[:,1], ypred, \n",
    "                                                    yprob[:,1], nbins, y_prob_ref)\n",
    "    calib_y_tr2_lr.append(calib_y)\n",
    "    calib_x_tr2_lr.append(calib_x)\n",
    "    bss_tr2_lr.append(bss)\n",
    "    \n",
    "    y_prob_ref = clim_pr_y_cat4[0:len(Y_train)]\n",
    "    recall, precision, f1, acc, bs, bss, calib_y, calib_x = score(Y_train, Y_train_2d[:,2], ypred, \n",
    "                                                    yprob[:,2], nbins, y_prob_ref)\n",
    "    calib_y_tr3_lr.append(calib_y)\n",
    "    calib_x_tr3_lr.append(calib_x)\n",
    "    bss_tr3_lr.append(bss)\n",
    "    \n",
    "    # evaluation validation\n",
    "    yprob = lr_model.predict(X_val)\n",
    "    ypred = np.argmax(yprob,axis=1)+1\n",
    "    y_prob_ref = clim_pr_y_cat1[0:len(Y_val)]\n",
    "    recall, precision, f1, acc, bs, bss, calib_y, calib_x = score(Y_val, Y_val_2d[:,0], ypred, \n",
    "                                                    yprob[:,0], nbins, y_prob_ref)\n",
    "    acc_va_lr.append(acc)\n",
    "    f1_va_lr.append(f1)\n",
    "    prec_va_lr.append(precision)\n",
    "    rec_va_lr.append(recall)\n",
    "    calib_y_va1_lr.append(calib_y)\n",
    "    calib_x_va1_lr.append(calib_x)\n",
    "    bss_va1_lr.append(bss)\n",
    "    \n",
    "    y_prob_ref = clim_pr_y_cat3[0:len(Y_val)]\n",
    "    recall, precision, f1, acc, bs, bss, calib_y, calib_x = score(Y_val, Y_val_2d[:,1], ypred, \n",
    "                                                    yprob[:,1], nbins, y_prob_ref)\n",
    "    calib_y_va2_lr.append(calib_y)\n",
    "    calib_x_va2_lr.append(calib_x)\n",
    "    bss_va2_lr.append(bss)\n",
    "    \n",
    "    y_prob_ref = clim_pr_y_cat4[0:len(Y_val)]\n",
    "    recall, precision, f1, acc, bs, bss, calib_y, calib_x = score(Y_val, Y_val_2d[:,2], ypred, \n",
    "                                                    yprob[:,2], nbins, y_prob_ref)\n",
    "    calib_y_va3_lr.append(calib_y)\n",
    "    calib_x_va3_lr.append(calib_x)\n",
    "    bss_va3_lr.append(bss)\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f682c0b66a0>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We can also look at the optimisation history\n",
    "train_loss = np.array(train_loss_lr)\n",
    "valid_loss = np.array(valid_loss_lr)\n",
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=plt.figaspect(0.25))\n",
    "ax1.plot(np.nanmean(train_loss,axis=0), label='Training Loss')\n",
    "ax1.plot(np.nanmean(valid_loss,axis=0), label='Validation Loss')\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_title('loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(np.nanmean(train_acc_lr,axis=0), label='Training ACC')\n",
    "ax2.plot(np.nanmean(valid_acc_lr,axis=0), label='Validation ACC')\n",
    "ax2.set_title('Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('acc')\n",
    "ax2.legend()\n",
    "#plt.savefig('precip_accur_loss_week1_lr_t1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "def metrics_plot(bss_tr,bss_va,bss_te,calib_y_te,calib_x_te,rpss_tr,rpss_va,fig_name):\n",
    "    # BSS\n",
    "    train_bss = np.nanmean(bss_tr)\n",
    "    valid_bss = np.nanmean(bss_va)\n",
    "    #test_bss = np.nanmean(bss_te)\n",
    "    print(train_bss,valid_bss)\n",
    "    \n",
    "    train_bss_std = np.std(bss_tr)\n",
    "    valid_bss_std = np.std(bss_va)\n",
    "    #test_std = np.std(bss_te)\n",
    "    \n",
    "    # RPSS\n",
    "    train_rpss = np.nanmean(rpss_tr)\n",
    "    valid_rpss = np.nanmean(rpss_va)\n",
    "    print(train_bss,valid_bss)\n",
    "    \n",
    "    train_rpss_std = np.std(rpss_tr)\n",
    "    valid_rpss_std = np.std(rpss_va)\n",
    "    \n",
    "    # reliability diagram\n",
    "    calib_y = []\n",
    "    calib_x = []\n",
    "    for ii in range(len(calib_y_te)):\n",
    "        ytmp = list(calib_y_te[ii])\n",
    "        xtmp = list(calib_x_te[ii])\n",
    "        while len(ytmp) < 10:\n",
    "            ytmp.append(np.nan)\n",
    "            xtmp.append(np.nan)\n",
    "        calib_y.append(ytmp)\n",
    "        calib_x.append(xtmp)\n",
    "    calib_y = np.nanmean(calib_y,axis=0)\n",
    "    calib_x = np.nanmean(calib_x,axis=0)\n",
    "    fig1, ax1 = plt.subplots()\n",
    "    plt.plot(calib_y, calib_x, marker='o', color=\"darkorange\", label='NN')\n",
    "    plt.plot([0, 1], [0, 1], color=\"navy\", linestyle=\"--\", label='Best score')\n",
    "    plt.text(0.05, 0.95, 'BSS: '+str(round(valid_bss,2)))\n",
    "    plt.text(0.05, 0.90, 'std: '+str(round(valid_bss_std,2)))\n",
    "    plt.text(0.05, 0.85, 'RPSS: '+str(round(valid_rpss,2)))\n",
    "    plt.text(0.05, 0.80, 'std: '+str(round(valid_rpss_std,2)))\n",
    "    plt.savefig(fig_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1853365445781821 0.014986706576926632\n",
      "0.1853365445781821 0.014986706576926632\n",
      "0.1008931193271185 -0.05583013377467789\n",
      "0.1008931193271185 -0.05583013377467789\n",
      "0.2722106640139196 0.037049404827812715\n",
      "0.2722106640139196 0.037049404827812715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55417/4220087004.py:32: RuntimeWarning: Mean of empty slice\n",
      "  calib_y = np.nanmean(calib_y,axis=0)\n",
      "/tmp/ipykernel_55417/4220087004.py:33: RuntimeWarning: Mean of empty slice\n",
      "  calib_x = np.nanmean(calib_x,axis=0)\n",
      "/tmp/ipykernel_55417/4220087004.py:32: RuntimeWarning: Mean of empty slice\n",
      "  calib_y = np.nanmean(calib_y,axis=0)\n",
      "/tmp/ipykernel_55417/4220087004.py:33: RuntimeWarning: Mean of empty slice\n",
      "  calib_x = np.nanmean(calib_x,axis=0)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAzOElEQVR4nO3dd3xN9x/H8ddXFpHUChpiE2RIkNiUmqnWqFGtlrYURfurtlodVltKdSg1qqU6tLRmlMYuSo0oDRLUKCJqxkpkf39/XCIi4+Le3PV5Ph55xL3ne+/9nODt+J5zPl+ltUYIIYTtK2TpAoQQQpiGBLoQQtgJCXQhhLATEuhCCGEnJNCFEMJOOFvqg728vHTlypUt9fFCCGGTdu3adV5rXTqnbRYL9MqVKxMZGWmpjxdCCJuklDqe2zaZchFCCDshgS6EEHZCAl0IIeyEBLoQQtgJCXQhhLAT+Qa6UmqOUuqsUmpfLtuVUmqKUuqwUipKKVXP9GUKIYTIjzFH6HOBDnlsDwNq3PgaAMy4/7KEEELcrXwDXWu9CbiYx5DOwHfaYBtQXCnlbaoChRDCXqSmpnPo0AWzvb8p5tDLAyezPI698dwdlFIDlFKRSqnIc+fOmeCjhRDCNuzefZoGDb6mVatvSUhIMctnmCLQVQ7P5bhqhtZ6ltY6RGsdUrp0jneuCiGEXUlKSuOtt9YSGvoVp09fZerUMIoWdTXLZ5ni1v9YoEKWxz5AnAneVwghbF6XLvNZteoIzz0XzCeftKNEiSJm+yxTHKGHA31uXO3SCListT5tgvc1KycnJ4KDgwkKCqJevXps3boVgMTERHr37k1gYCABAQE0a9aMa9euATBu3Dj8/f2pU6cOwcHBbN++/a4+89ixYzRs2JAaNWrwxBNPkJKS83+7vv32W2rUqEGNGjX49ttvM5/XWvPOO+/g6+tL7dq1mTJlyj3uvRDCnK5eTSYpKQ2AESOasXr108yZ09msYQ4YQiKvL+An4DSQiuFovB8wCBh0Y7sCpgFHgL1ASH7vqbWmfv362pKKFi2a+euIiAjdokULrbXW48eP18OGDcvcduDAAZ2UlKS3bt2qGzVqpJOSkrTWWp87d06fOnXqrj6zR48e+qefftJaaz1w4EA9ffr0O8ZcuHBBV6lSRV+4cEFfvHhRV6lSRV+8eFFrrfWcOXP0M888o9PT07XWWp85c+auPl8IYX4REf/oihU/02+/vdYs7w9E6lxy1ZirXJ7UWntrrV201j5a69la65la65k3tmut9RCtdTWtdaDW2uZaKF65coUSJUoAcPr0acqXv3VOt2bNmri5uXH69Gm8vLxwc3MDwMvLi3Llyhn9GVpr1q9fT/fu3QHo27cvS5cuvWPcqlWraNu2LSVLlqREiRK0bduWiIgIAGbMmMGoUaMoVMjw21amTJl72l8hhOldvHidZ59dSocO83B3d6FjR98Cr8Fh7xS9fv06wcHB1KpVi/79+zNy5EgAnn/+eSZOnEjjxo159913+eeffwBo164dJ0+exNfXl8GDB7Nx48bM9xo1ahTh4eF5ft6FCxcoXrw4zs6G0xY+Pj6cOnXqjnGnTp2iQoVbpySyjjty5AgLFiwgJCSEsLCwzNqEEJa1bt1R/PymMW/eXt55pzm7dw+kSZMK+b/QxBw20IsUKcKePXs4cOAAERER9OnTB601wcHBHD16lOHDh3Px4kVCQ0OJiYnBw8ODXbt2MWvWLEqXLs0TTzzB3LlzAXjvvffo1KlTnp9n+J/S7ZS68wKhvMYlJydTuHBhIiMjeeGFF3j++efvYc+FEKZWpkxRqlQpwc6dL/DBBw9TuLBllppw2EDPqnHjxpw/f56b18Z7eHjw+OOPM336dJ5++mlWrlwJGE6ktmzZkrFjx/LFF1+waNGiPN+3ffv2BAcH079/f7y8vLh06RJpaYYTJbGxsTlO2fj4+HDy5K3L+rOO8/HxoVu3bgB07dqVqKio+995IcRd01ozd+4eXn75NwACA8uydevzBAc/aNG6JNCBAwcOkJ6eTqlSpdiyZQvx8fEApKSkEB0dTaVKlTh48OBtUxx79uyhUqVKeb7vqlWr2LNnD19//TVKKVq1asXChQsBw5UsnTt3vuM17du3Z/Xq1cTHxxMfH8/q1atp3749AF26dGH9+vUAbNy4EV/fgp+jE8LRHTsWT/v2P/Dcc8vYs+c/rl9PBXL+H3eBy+1sqbm/LH2VS6FChXRQUJAOCgrSderU0b/++qvWWutvv/1WBwYG6oCAAO3n56eHDx+uMzIydGRkpG7cuLGuXbu2DgwM1F27dtXnzp3TWms9cuRIvWzZsnw/88iRIzo0NFRXq1ZNd+/ePfOKmZ07d+p+/fpljps9e7auVq2arlatmp4zZ07m8/Hx8fqRRx7RAQEBulGjRnrPnj2m/JEIIfKQlpauP/98m3Z3H6c9PMbr6dN36PT0jAKvgzyuclE6hznbghASEqJlTVEhhK04c+YaNWt+QZMmFZg581EqVixmkTqUUru01iE5bbPYItFCCGHtUlPTmTdvL336BFG2rAd//TWQKlWKW8f0Sg5kDj2LyZMnk5iYmOO2uXPnMnTo0HzfI7e7PLPatGkT9erVw9nZOXNOHeD48ePUr1+f4OBg/P39mTlz5r3tiBDivu3aFUdIyFc899wy1qw5AkDVqiWsNsxBAv02eQW6MS5evMjYsWPZvn07O3bsYOzYsZknWLOqWLEic+fO5amnnrrteW9vb7Zu3cqePXvYvn07EyZMIC5O2uIIUZCuX09lxIi1NGz4NefOJbBkyRO0b1/d0mUZxSGnXBISEujZsyexsbGkp6czcuRIzpw5Q1xcHK1atcLLy4sNGzbwzTff8OGHH+Lt7Y2vr2/mXaK5yXqXJ5B5l+eTTz5527jKlSsDZN7xeZOr660ObMnJyWRkZJhgb4UQd6NLlwWsXn2E/v3rMmlSO4oXL2zpkozmkIEeERFBuXLlWLFiBQCXL1+mWLFifPrpp2zYsAEvLy9Onz7N6NGj2bVrF8WKFaNVq1bUrVsXgPDwcCIjI3nvvfdue9+87vI01smTJ+nYsSOHDx9m0qRJd9VeQAhxb65cScbV1YnChZ15++1mvPFGE1q3rmrpsu6aQ065BAYGsnbtWt588002b95MsWJ3nq3evn07LVu2pHTp0ri6uvLEE09kbuvUqdMdYQ7G3w2alwoVKhAVFcXhw4f59ttvOXPmzF29Xghxd1au/IeAgOm8956hncdDD1W2yTAHBw10X19fdu3aRWBgIG+99VaO4Qx3H8Z53eV5t8qVK4e/vz+bN2++p9cLIfJ2/nwizzyzhI4df8TT041OnWpauqT75pCBHhcXh7u7O08//TSvv/46f/31FwCenp5cvXoVgIYNG/L7779z4cIFUlNT+eWXX/J937zu8jRGbGws169fByA+Pp4tW7ZQs6bt/yETwtqsWXMEP79pzJ+/j1GjWvDXXwNo1MjH0mXdN4ecQ9+7dy/Dhw+nUKFCuLi4MGPGDAAGDBhAWFgY3t7ebNiwgTFjxtC4cWO8vb2pV68e6enpQO5z6CVLlmTkyJGEhoYChi6MN0+Qjho1ipCQEDp16sTOnTvp2rUr8fHxLF++nNGjR7N//35iYmJ47bXXUEqhteb1118nMDCwAH8yQjgGb29PfH1LMWNGRwIDy1q6HJORO0WFEHZPa83s2bvZvfs006Z1zHzOmq8pz01ed4o65JSLEMJxHD0aT5s23/PCC8uJjj5vXc20TMxhA/3mmqIBAQE89thjXLp0CYB///2XIkWKEBwcjJ+fH4MGDSIjI4OMjAxefvllAgICCAwMJDQ0lGPHjgEwZ84cAgMDqVOnDgEBASxbtizfz//www+pXr06NWvWZNWqVTmOuXjxIm3btqVGjRq0bdv2jpuUTpw4gYeHBx9//PH9/TCEsEPp6Rl89tmfBARMZ+fOU3z55aOsW9eHIkVcLF2a2ThsoN9c4GLfvn2ULFmSadOmZW6rVq0ae/bsISoqiujoaJYuXcqCBQuIi4sjKiqKvXv3smTJEooXL05sbCzjxo3jjz/+ICoqim3btlGnTp08Pzs6Opr58+ezf/9+IiIiGDx4cOb8fFYTJkygdevW/PPPP7Ru3ZoJEybctn3YsGGEhYWZ5gcihJ05fz6RsWM30rp1VaKjhzBgQH0KFbK/o/KsHDbQs2rcuHGONwA5OzvTpEkTDh8+zOnTp/H29s68u9PHx4cSJUpw9uxZPD098fDwAAyLY1SpUiXPz1u2bBm9evXCzc2NKlWqUL16dXbs2JHjuL59+wJ3rkG6dOlSqlatir+//73uthB2JyUlnTlzdpORoSlb1oM9ewYRHt4LH58HLF1agXD4QE9PT2fdunU5LiGXmJjIunXrCAwMpGfPnixfvpzg4GBee+01du/eDUBQUBBly5alSpUqPPfccyxfvjzz9TNnzsyxwZaxd5SeOXMGb29vwNDn5ezZs4ChdcHEiRMZPXr0/e28EHZk585T1K8/i379wlm79igAlStbb2dEc3DYQL+5SHSpUqUy56pvOnLkCMHBwTRt2pSOHTsSFhaGj48PBw8e5MMPP6RQoUK0bt2adevW4eTkREREBAsXLsTX15dhw4YxZswYAAYNGsSgQYPu+Oz7vaN09OjRDBs2LPN/BUI4ssTEVF5/fTWNGs0mPv464eG9aNeumqXLsozcVr4w95elVywqWrSo1lrrS5cu6WbNmunPP/9ca631sWPHtL+/f76vnzRpkh46dOgdz+/cuVMHBATk+drx48fr8ePHZz5u166d3rp16x3jfH19dVxcnNZa67i4OO3r66u11rpZs2a6UqVKulKlSrpYsWK6RIkSeurUqfnWLIQ9atPmOw1j9IAB4frSpeuWLsfsyGPFIoc9Qr+pWLFiTJkyhY8//pjU1NRcx/3111+ZrWwzMjKIioqiUqVKxMXFZd5pCsatNdqpUyfmz59PcnIyx44d459//qFBgwY5jrvZUz3rGqSbN2/m33//5d9//+WVV17h7bffNqpXuxD24vLlJJKSDAuujxzZgvXr+/Dll49RrJjtdEY0B4cPdIC6desSFBTE/Pnzcx1z9uxZHnvsMQICAqhTpw7Ozs4MHTqU1NRUXn/9dWrVqkVwcDALFizg888/B3KfQ/f396dnz574+fnRoUMHpk2bhpOTEwD9+/fn5g1XI0aMYM2aNdSoUYM1a9YwYsQIM+y9ELbl118P4e8/nbFjfwegRYtKtGqV94UIjkLuFBVC2IRz5xL43/8i+OmnfQQGlmH27E6Ehpa3dFkFTtYUFULYtNWrj9C792IuX05i7NiWjBjRDFdXJ0uXZXVkyiWLglpT9NNPP8XPz486derQunVrjh8/nrntjTfewN/fn9q1a/Pyyy/neEWMEI6mfHlPatf2YvfugYwa9ZCEeS4k0LMoqDVF69atS2RkJFFRUXTv3p033ngDgK1bt7JlyxaioqLYt28fO3fuZOPGjfdcjxC2KiNDM2vWLl588VcA/P3LsGnTc/j7l7FwZdbNIQM9ISGBjh07EhQUREBAAAsWLGDKlCmZa4q2atUKgG+++QZfX18eeughtmzZku/7Zl1TtESJEplrimbXqlUr3N3dAWjUqBGxsbGA4Vr0pKQkUlJSSE5OJjU1lbJl7ae1pxDGOHz4Iq1bf8fAgb9y8OCFzGZaIn8OOYduTWuKzp49O7MfS+PGjWnVqhXe3t5orRk6dCi1a9c25a4LYbXS0zOYPHkbI0duwMXFia++eox+/eo61J2e98uoI3SlVAel1EGl1GGl1B3Xzimliimlliul/lZK7VdKPWf6Uk3HWtYU/eGHH4iMjGT48OEAHD58mJiYGGJjYzl16hTr169n06ZN97KLQtic8+cT+eCDzbRtW43o6MH0719Pwvwu5RvoSiknYBoQBvgBTyql/LINGwJEa62DgJbAJ0opVxPXajLWsKbo2rVrGTduHOHh4bi5uQGwZMkSGjVqhIeHBx4eHoSFhbFt27a7qkEIW5KcnMZXX+3K0kxrIEuXPkH58o7RTMvUjDlCbwAc1lof1VqnAPOBztnGaMBTGRLQA7gIpJm0UhOy9Jqiu3fvZuDAgYSHh1OmzK2TPBUrVmTjxo2kpaWRmprKxo0bZcpF2K3t22OpX38WAwb8mtlMq1Ilx2qmZWrGzKGXB05meRwLNMw25gsgHIgDPIEntNYZ2d9IKTUAGACG8LIUS68pOnz4cK5du0aPHj0Aw88iPDyc7t27s379egIDA1FK0aFDBx577LGC+rEIUSASElIYOXIDkydvo3z5B1ix4inHbaZlYvneKaqU6gG011r3v/H4GaCB1vqlLGO6A02BV4FqwBogSGt9Jbf3lTtFhXBMbdt+z9q1R3nxxRAmTGjDAw+4Wbokm3K/a4rGAhWyPPbBcCSe1XPA4hvNwA4Dx4Ba91KsEML+XLqUlHn54ahRLdi48VmmT+8oYW5ixgT6TqCGUqrKjROdvTBMr2R1AmgNoJQqC9QEjpqyUCGEbQoPP3ijmZbhJrnmzSvRokXeHUnFvck30LXWacBQYBUQA/ystd6vlBqklLq5esP7QBOl1F5gHfCm1vq8uYoWQli/s2cT6NVrIZ07z8fLy53u3bNfHCdMzagbi7TWK4GV2Z6bmeXXcUA705YmhLBVERGH6d17MdeupfD++614882muLhI/xVzc8g7RYUQ5lWhwgMEBpZh+vSO+PmVtnQ5DsMhe7kIIUwrI0MzY8ZOBg40LJLu71+G339/VsK8gEmgCyHuy6FDF2jZci6DB6/k2LFLmUvDiYIngS6EuCdpaRlMnPgHderMYO/es3zzTWdWrXqawoVlJtdS5CcvhLgnFy4kMnHiFh55pAbTpj2Ct7enpUtyeBLoQgijJSenMXfuHl54oT5ly3rw99+DqFDhzm6lwjIk0IUQRvnzz5P06xdOTMx5qlUrSZs2VSXMrYxtzaHHzINZleGTQobvMfMsXZEQdu/atRReeSWCpk3nkJCQSkREb9q0qWrpskQObOcIPWYerB4AaTfW/Lx63PAYoHZvy9UlhJ3r0mU+69YdY+jQUMaPb42np/RfsVb5dls0l7vutjirsiHEs3MtBi0mgqcPePgYvhcuCdJTWYh7Fh9/ncKFnSlSxIU//jgBQLNmlmt5LW7Jq9ui7RyhXz2R8/Mpl2HtoNufcy4MHuVvBXzm9/K3HruXgUJyK7IQ2S1eHMOQISvp06cOEye2lSC3IbYT6J4Vcz5C96wIT26Bq7FwLfbG91O3Hp/aYnickW3l8ELOULTcrZDPKfiLlgMnl4LZPyEs7L//rjF06EoWLYohOPhBevUKsHRJ4i7ZTqA3H3f7HDqAszs0H38rkHOjM+D6eUPIZw/+a7Fw7m84uuL29wZAQdGyhqDPenTvmSX4PcqDi7tZdlmIgvLbb//Qu/diEhNTGT/+YV5/vYk007JBthPoN098bn7HMP3iWdEQ8sacEFWFDFMs7mWgbL2cx2gNyZduHd1nD/7LRyB2o2FMdoVL3gr7HIPfB9xk0VthvSpVKk7dut5Mm/YItWp5WboccY9s56SotUi5duPI/tTtoZ/1iD/x7J2vc/HI5Qg/S+gXKSUnc0WByMjQTJ++k7///o+vvupk6XLEXbCPk6LWwtUDStY0fOUmLRkS4u6cz7/5/fhaw/bs62g7ud15hO/hA55Zgt+9rJzMFffl4MHz9OsXzpYtJ2nfvhpJSWnSf8VOyO+iOTi7QbEqhq/cZKRBwpmcT+RejYXTf8I/pyA95fbXKSfwKJdz8Gc+Vw6cXM27j8LmpKam8/HHWxk7diPu7i7MnduZPn2CUPK/QrshgW4phZwNR96e5cG7Yc5jtL51Mjf7idyrsXBuLxz7DVIT7nyte9k7p3WyB7+xJ3Nj5t3buQthVeLjk5g0aSuPPVaTqVPDePBBD0uXJExMAt2aKQXupQ1fZevmPEZrSLmS+3z+5WNwajMkxd/52sIlbr9UM6fgP7oC1sgdurYqKSmNOXN2M2hQCGXKFCUq6kV8fOQEvb2SQLd1SoFbMcOXl3/u41IT8z6Re+YvSDyT0wcA2U6cpyUajtgl0K3aH3+coF+/cA4duoCvbynatKkqYW7nJNAdhYs7lKhh+MpNegpci7s9+De+nvPY3O7cFRZ39Woyb721jmnTdlK5cnFWr35ammk5CAl0cYuTKxSrbPi66a+pud+hK6xSly4L2LDhGP/7X0M++OBhPDzkBLmjkEAXecvpDl2A4BctU4/I0cWLhmZa7u4uvP9+K5RqRePGFSxdlihgttUPXRS82r2h3SzwrIShFUI5Q4fL3VPg8r+Wrk4ACxdGU7v2NMaM+R2AJk0qSJg7KDlCF/mr3fv2E6Dn98GCFrCwLfTaDEUftFxtDuz06asMGbKSJUsOUL++N717B1q6JGFhcoQu7p5XAHRdYTiBuqgDJF2ydEUOZ8WKQ/j5Tee33w4zcWIbtm3rT1CQ/MPq6CTQxb0p1xg6L4EL0bDkUcNlkaLAVK1agtDQcvz99yDeeKMpzs7yV1lIoIv7UbkdPDIP4rbC8u53tikQJpOensHnn2+jX79lANSuXZrVq5/B17eUhSsT1kQCXdyfmj2g7ZeGFgQRz97ZcEzct+joczRv/g2vvLKK//5LICkpzdIlCSslJ0XF/avzAiRdhM0jwK0EtP5C2gCbQEpKOh99tIX339+Ep6crP/zQlaeeCpRmWiJXRgW6UqoD8DngBHyttZ6Qw5iWwGTABTivtX7IZFUK69fgTbh+ASInGU6Wnt0tzbzu06VLSXz22Ta6dq3FlClhlClT1NIlCSuXb6ArpZyAaUBbIBbYqZQK11pHZxlTHJgOdNBan1BKlTFTvcKatZgI/+2AI0tvPSfNvO7K9eupzJ69m8GDQylTpih7975IuXKeli5L2Ahj5tAbAIe11ke11inAfKBztjFPAYu11icAtNY5LNkj7J5Shu6O2d1s5iXytGnTcYKCZvLSS7+xYYPh5yhhLu6GMYFeHjiZ5XHsjeey8gVKKKV+V0rtUkr1yemNlFIDlFKRSqnIc+fO3VvFwrpdPZnL89LMKzdXriQzePAKHnpoLmlpGaxd+wytW0szLXH3jJlDz+kMTPaFSJ2B+kBroAjwp1Jqm9b60G0v0noWMAsMa4refbnC6nlWlGZed6lLl/n8/vu/DBvWiPffb0XRotJMS9wbYwI9FsjaGMIHiMthzHmtdQKQoJTaBAQBhxCOJadmXs7uhudFpvPnE3F3d8Hd3YVx4x5GKUWjRj6WLkvYOGOmXHYCNZRSVZRSrkAvIDzbmGVAc6WUs1LKHWgIxJi2VGETbjbzcrlxRYZnJcNjOSEKgNaa+fP3Ubv2NEaP3gBA48YVJMyFSeR7hK61TlNKDQVWYbhscY7Wer9SatCN7TO11jFKqQggCsjAcGnjPnMWLqxY7d4Q9yccmA8D/rV0NVbj1KkrDB68kvDwg4SGlqNPnyBLlyTsjFHXoWutVwIrsz03M9vjScAk05UmbJqLB6Res3QVVuPXXw/Ru/diUlPT+fjjtrzySiOcnORGbWFacqeoMA9XD0hPhvRUcHKxdDUWV716SZo0qcDUqWFUr17S0uUIOyWHCMI8XDwM31MTLFuHhaSnZ/DZZ3/y7LNLAahVy4vffustYS7MSgJdmEdmoDvetMv+/Wdp2nQOr766mvPnE6WZligwEujCPG5e5ZLiOIGekpLOe+9tpG7dLzlyJJ4ff3yc5cufpHBhmdkUBUP+pAnzcHW8I/RLl5KYMmU7PXr4M3lye0qXlmZaomBJoAvzcJApl8TEVL76ahdDhzbIbKbl7S39V4RlSKAL87h5hG7HUy4bNhyjf//lHD0aT0BAGVq3riphLixK5tCFedjxEfrly0kMHLichx/+DqVgw4a+0kxLWAUJdGEex9cYvq94EmZVhph5Fi3HlLp0WcDXX+9m+PAmREW9SMuWlS1dkhCATLkIc4iZZ1iO7iY7WOTi3LkEihZ1xd3dhQ8/bI2TkyI0NHsXaSEsS47QheltfgfSrt/+nI0ucqG15scf997WTKtRIx8Jc2GVJNCF6eW2mIWNLXIRG3uFTp3m07v3YqpXL8mzzwZbuiQh8iRTLsL07GCRi/Dwgzz99GLS0zWffdael15qIM20hNWTP6HC9JqPMyxqkV2NbgVfyz3y9S1Fs2YV2bv3RemMKGyG/CkVpndzkQvPSoACDx94oArsmQKHFlq6uhylpWXw8cdb6dNnCWBoprVyZW+qVi1h4cqEMJ5MuQjzqN379itakq/A4kfg1ycg7DurutolKuoM/fqFExkZR+fONUlKSpP+K8ImyRG6KBhuD0C3CPBpASufgb1zLF0RyclpjB69gfr1Z3HixGV+/rk7S5Y8IWEubJYEuig4rh7QdQVUagur+8HfM/N/jRlduZLM9OmRPPlkANHRg+nRwx+llEVrEuJ+SKCLguXiDl2WQdXHYO2LsGtygX58QkIKn332J+npGZQuXZR9+17ku++6UqpUDidxhbAxEuii4DkXhk4LDVe9/D4MdkwskI9dt+4ogYEzePXV1WzcaLissmxZjwL5bCEKggS6sAwnV3h0PtR6ytAmYOtY0NosH3XpUhL9+4fTps33ODsXYuPGZ3n44Spm+SwhLEnO/gjLKeRsuOLFyRX+HGNYVLrZODDxPHbXrgvYvPk4b77ZlNGjH6JIEVm0WtgnCXRhWYWcoP1scHKDHR9CehI89Ml9h/qZM9fw8HClaFFXJkxojbNzIerXL2eiooWwTjLlIixPFYI2M6De/2DXZ7BuCOiMe3orrTXff/83fn7TGT36dwAaNvSRMBcOQY7QhXVQClp+ZjhS3/kRpKdA2y8NR/BGOnHiMoMG/cpvvx2mcWMf+vWra8aChbA+EujCeigFzSeAU2HY9p5hTr3DN4a59nwsW3aAp59egtaaKVM6MHhwqPRfEQ5HAl1YF6Wg6VhwdoM/3oH4Q5DwH1w9aejW2HzcbW0DtNYopahVy4uWLSszdWoYlSsXt1z9QliQ0ma6VCw/ISEhOjIy0iKfLWzEit5w4Mfbn3N2h3azSKvxJJ98spW9e8/yww+PW6Y+ISxAKbVLax2S0zY5QhfW69SWO59LS+TvHz/m+ZUJ/PXXabp2rSXNtIS4Qf4WCOuVbYWjpFRnPljbgokbmlKq9BUWLuxBt25+FipOCOsjgS6sk84AV09IuZL51NVkV77cVp/eDY/y6a9fUbJkEQsWKIT1MeoyAKVUB6XUQaXUYaXUiDzGhSql0pVS3U1XonA4qddheU9IucK1lCJ8/HsT0jMUpT0SiX5rDnNnh0mYC5GDfI/QlVJOwDSgLRAL7FRKhWuto3MYNxFYZY5ChYNIPAtLO8HpHax2m8iAT1M5cTqV+j6naVU3g9KPjLOqxTGEsCbGTLk0AA5rrY8CKKXmA52B6GzjXgIWAaEmrVA4jgsxsKQjF89e4rVdk5m7OJ6aNUux+edONG06xtLVCWH1jAn08sDJLI9jgYZZByilygNdgYfJI9CVUgOAAQAVK9rOCvCiAJzYAOGPg5MrXZePYcvOS7z9djNGjnxIrmARwkjG/E3JqUtS9ovXJwNvaq3T81rxRWs9C5gFhuvQjaxR2Lv93/LfwlfwLOtD0SeXM6m+C66uTgQHP2jpyoSwKcYEeixQIctjHyAu25gQYP6NMPcCHlFKpWmtl5qiSGGntEZvGcO3X4Tz6q9DeK5fAz4pVpkGDSxdmBC2yZhA3wnUUEpVAU4BvYCnsg7QWmeuFqCUmgv8KmEu8pSWzL/fD2LghDRWH+pCs6Y+DBjcxNJVCWHT8g10rXWaUmoohqtXnIA5Wuv9SqlBN7ZbdqVfYXuuX2TJuy/wzBe1UM6ufDE1jBcHh1KokCzQLMT9MOpsk9Z6JbAy23M5BrnW+tn7L0vYKx1/GLWkI/5uV2jTrDGfz+lPpUrFLV2WEHZBLh8QBSI1NZ1JI+ezb91Kfnz2PL6Dl7F0fDNLlyWEXZGG0cLs/vrrNA2CJvHOxMOk40pyty3gI2EuhKlJoAuzuX49lbdGrKVBg1n8F3ueJcP2sGDjJNwerGXp0oSwSzLlIswm4Uois2duom/9v/n4VU9KdPsJnAtbuiwh7JYcoQuTuno1mY8+2kJ6Yjxem3sQ/eonzP60DiV6fi9hLoSZyRG6MI2YeUTMmM7A7xtw8nIxGvw3jJYVduP1+CwIeM7S1QnhEOQIXdy3C39+R9++Swmb2o6irqlsGTKbluV3QshwCXMhCpAEurhvj/f9kx931WZkm43sfnUmjSvHGjbE/Jj3C4UQJiVTLuKenD59FU9PNzw8XPk4bDmuzmkElTtz+6BsS8gJIcxLjtDFXdFaM2fObmrXnsaoURsACPUvdGeYA3hKi2QhCpIEujDa0aPxtGv3A/36hRMU9CCDBoUYNhSreudgZ3doPq5gCxTCwcmUizDK4sUxPPPMEpycFDNmdGTAgPqGZlpRX0PsRqjWBc7uNkyzeFY0hLksFSdEgZJAF3nSWqOUIjCwDB06VGfy5PZUqFDMsPHMLlg/FCq1g04LoZCTZYsVwsHJlIvIUUpKOh98sImnnlqM1poaNUqxaFHPW2F+/QKEdwP3svDIPAlzIayABLq4Q2RkHKGhXzFypOGkZ0pK+u0DdAasfBoSThuOzN29LFClECI7CXSR6fr1VN54Yw0NG37N+fOJLFvWi59+6oabW7aZuT/fh38joNUUeDDXNcGFEAVM5tBFpoSEVObO3UO/fnX56KO2FC+eQ++VYxHw51jw6wN1BhR8kUKIXEmgO7grV5KZPn0nw4c3wcvLnZiYIZQq5Z7z4Mv/wsqnoHQgtJkBSpaME8KaSKA7sBUrDjFo0Ari4q7SqJEPLVtWzj3M05JgeXfD/Plji8All3FCCIuROXQHdO5cAr17L+bRR3+iWDE3tm59npYtK+f9ovUvGy5T7PAdlKheIHUKIe6OHKE7oG7dfmbbtljGjHmIt95qjqtrPpcc7vsG9n4FDd6C6p0KpkghxF2TQHcQp05doVixwnh4uPLZZ+1xc3MmIKBM/i88sxvWDYaKraHp++YvVAhxz2TKxc5prfnqq134+U3PbKZVv34548I8KR6Wd4PCpaDjj3LzkBBWTo7Q7diRIxd54YXlbNjwL61aVWbIkLu4ZlxnwG/PwNVYeGITuBvxD4AQwqIk0O3UwoXR9OmzBBcXJ2bNepT+/euhjLnMMGYebH4Hrh43PPbrC+UambdYIYRJSKDbmZvNtIKCytKxoy+ffdYeH58HjHtxzDxYPQDSEm89d+gXqNxWOicKYQNkDt1OpKSkM3bs7/TqtSizmdYvv/QwPszBcGSeNczB8HjzO6YtVghhFhLodmDHjlPUrz+LMWM24uxc6M5mWsbKbck4WUpOCJsggW7DEhNTef311TRuPJv4+OssX/4k8+Y9fmczLWPltWTcphFw9dS9va8QokBIoNuw69dT+eGHKAYMqEd09BAefdT3/t6w+TjD0nFZORWGBxtA5CT4ugr81hfORd3f5wghzMKoQFdKdVBKHVRKHVZKjchhe2+lVNSNr61KqSDTlyoALl9OYty4TaSlZVCqlKGZ1owZj/LAA273/+a1e0O7WeBZCVCG7+2/ht7boN9hCB4M/yyC74JgYXv4dzVoff+fK4QwCaXz+QuplHICDgFtgVhgJ/Ck1jo6y5gmQIzWOl4pFQaM0Vo3zOt9Q0JCdGRk5P3W71CWLz/IoEEr+O+/a6xb1yf//ivmkBQPf38Ju6cYFrjwCoSQ16FWL3ByLfh6hHAwSqldWuuQnLYZc4TeADistT6qtU4B5gOdsw7QWm/VWsffeLgN8LmfgsXtzp1L4MknF9Gp03xKlSrC9u39LRPmAIVLQMMR0P8YdJgLaIjoa5iO2TERki5Zpi4hhFGBXh44meVx7I3nctMP+C2nDUqpAUqpSKVU5Llz54yv0sF16/YzixZF8957LYmMHEBISDlLlwTObuDfF/pEQbcIKOkHm0fArAqwYZihd7oQokAZczlETrcX5jhPo5RqhSHQm+W0XWs9C5gFhikXI2t0SLGxVyhe3NBMa/LkDri5OeHvb4W33ysFldsbvs7+Dbs+gT1fGKZkfHsYpmMezPF/h0IIEzPmCD0WqJDlsQ8Ql32QUqoO8DXQWWt9wTTlOZ6MDM2XX0bi5zeNkSPXA1Cvnrd1hnl2ZYIg7DvDdEz91+DYbzAvFBa0hCPLDf1hhBBmY0yg7wRqKKWqKKVcgV5AeNYBSqmKwGLgGa31IdOX6Rj++ecCDz/8LYMGraBBg/K89FKe55Wtl6cPPPQRDDgJLT+Fy8dgaSf4xg+ivjKsfiSEMLl8A11rnQYMBVYBMcDPWuv9SqlBSqlBN4aNAkoB05VSe5RScvnKXfrll/3UqTOTPXv+Y/bsTqxZ8wxVq5awdFn3x+0BqD8M+h+Bjj+BS1FYMwBmVYQ/34PE85auUAi7ku9li+Yily0a3GymdfjwRd59dz2fftqecuU8LV2WeWgNsRsh8mM4ugKci4D/s4bQL1HD0tUJYRPyumxRAt1CkpPTGDduMzEx5/n55+7Gtba1JxdiYNenEP0dpKdC9c6GE6jlmhhOtAohcnS/16ELE9u2LZZ69Wbx/vubKFLE+d6badmyUrWh3VfwwnFo9A7EboL5zeCnJnBoIWSkG9r5zqoMnxQyfI+ZZ+mqhbBqcoRegBISUnj33fV8/vl2fHwe4MsvHyUsTKYaAEhNgP3fGo7aLx2BIqUh+RJkpN4a4+xuaE0gvdmFA5MjdCuRlJTG/Pn7GTw4lP37B0uYZ+VS1NAr5rmD0GkxpFy5PcxBerMLkQ9ZscjMLl1KYurU7bz1VvPMZlrFixe2dFnWq5AT1OgK6Sk5b5fe7ELkSo7QzWjp0gP4+U1j7NiNbN1q6J4gYW6k3Hqz59WzXQgHJ4FuBmfOXKNnz1/o2nUBZcoUZfv2/rRoUcnSZdmWnHqzO7sbnhdC5EimXMyge/df2LHjFB980Io33miKi4uTpUuyPTdPfG5+xzDN4lnREOZyQlSIXMlVLiZy4sRlSpQojKenG7t3n8bNzRk/v9KWLksIYWfkKhczysjQTJu2A3//6YwatQGAunW9JcyFEAVOplzuw8GD5+nffzl//HGCtm2r8r//NbJ0SUIIByaBfo9+/nk/ffosoUgRF775pjN9+wY53u37QgirIoF+l24206pf35vHH6/Np5+258EHPSxdlhBCyBy6sZKS0njnnXV07/4LWmuqVSvJjz92kzAXQlgNCXQjbN16krp1v2T8+D/w9HR1zGZaQgirJ4Geh2vXUnj55d9o1mwOiYmpRET0Zu7cLri5yUyVEML6SDLlISUlnYULoxkyJJTx41vj6elm6ZKEECJXEujZXLx4nSlTtvPuuy0oWbIIMTFDKFZM+q8IIayfTLlksWhRNH5+0/jgg02ZzbQkzIUQtkICHTh9+irduv1M9+6/UK6cJ5GRA6SZlhDC5siUC9Cz50J27jzFhAmtee21Jjg7y79zQgjb47CBfvz4JUqWLIKnpxtTp4ZRpIgzNWt6WbosIYS4Zw53KJqRoZk6dTv+/tMZOdLQTCs4+EEJcyGEzXOoI/QDB87Tv384W7acpEOH6gwbJs20hBD2w2ECff78ffTtuxQPD1e++64LTz9dR5ppCSHsit0HekaGplAhRWhoOXr08OOTT9pRtqz0XxFC2B+7nUO/fj2VESPW0q3bz5nNtH744XEJcyGE3bLLQN+8+TjBwV8yceIWSpUqQmpqhqVLEkIIs7OrQL96NZkhQ1bQosVcUlPTWbPmGb7+uhOurrJIsxDC/tnVHHpqagZLlx7klVca8sEHD1O0qKulSxJCiAJj84F+4UIin3++nVGjHqJkySIcODBEuiIKIRySUVMuSqkOSqmDSqnDSqkROWxXSqkpN7ZHKaXqmb7U22mt+eWX/fj5TefDD//gzz8NzbQkzIUQjirfQFdKOQHTgDDAD3hSKeWXbVgYUOPG1wBghonrvE1c3FUef/xnevZcSIUKDxAZ+QLNm0szLSGEYzNmyqUBcFhrfRRAKTUf6AxEZxnTGfhOa62BbUqp4kopb631aZNXDPTs+Qu7dp3mo4/aMGxYY2mmJYQQGBfo5YGTWR7HAg2NGFMeuC3QlVIDMBzBU7FixbutNdO0aY9QpIgLvr6l7vk9hBDC3hgT6DndH6/vYQxa61nALICQkJA7thsrKOjBe32pEELYLWPmKmKBClke+wBx9zBGCCGEGRkT6DuBGkqpKkopV6AXEJ5tTDjQ58bVLo2Ay+aaPxdCCJGzfKdctNZpSqmhwCrACZijtd6vlBp0Y/tMYCXwCHAYSASeM1/JQgghcmLUjUVa65UYQjvrczOz/FoDQ0xbmhBCiLsh1/sJIYSdkEAXQgg7IYEuhBB2QgJdCCHshDKcz7TAByt1Djh+jy/3As6bsBxbIPvsGGSfHcP97HMlrXXpnDZYLNDvh1IqUmsdYuk6CpLss2OQfXYM5tpnmXIRQgg7IYEuhBB2wlYDfZalC7AA2WfHIPvsGMyyzzY5hy6EEOJOtnqELoQQIhsJdCGEsBNWHejWuDi1uRmxz71v7GuUUmqrUirIEnWaUn77nGVcqFIqXSnVvSDrMwdj9lkp1VIptUcptV8ptbGgazQ1I/5sF1NKLVdK/X1jn226a6tSao5S6qxSal8u202fX1prq/zC0Kr3CFAVcAX+BvyyjXkE+A3DikmNgO2WrrsA9rkJUOLGr8McYZ+zjFuPoetnd0vXXQC/z8UxrNtb8cbjMpauuwD2+W1g4o1flwYuAq6Wrv0+9rkFUA/Yl8t2k+eXNR+hZy5OrbVOAW4uTp1V5uLUWuttQHGllHdBF2pC+e6z1nqr1jr+xsNtGFaHsmXG/D4DvAQsAs4WZHFmYsw+PwUs1lqfANBa2/p+G7PPGvBUSinAA0OgpxVsmaajtd6EYR9yY/L8suZAz23h6bsdY0vudn/6YfgX3pblu89KqfJAV2Am9sGY32dfoIRS6nel1C6lVJ8Cq848jNnnL4DaGJav3Av8T2udUTDlWYTJ88uoBS4sxGSLU9sQo/dHKdUKQ6A3M2tF5mfMPk8G3tRapxsO3myeMfvsDNQHWgNFgD+VUtu01ofMXZyZGLPP7YE9wMNANWCNUmqz1vqKmWuzFJPnlzUHuiMuTm3U/iil6gBfA2Fa6wsFVJu5GLPPIcD8G2HuBTyilErTWi8tkApNz9g/2+e11glAglJqExAE2GqgG7PPzwETtGGC+bBS6hhQC9hRMCUWOJPnlzVPuTji4tT57rNSqiKwGHjGho/Wssp3n7XWVbTWlbXWlYGFwGAbDnMw7s/2MqC5UspZKeUONARiCrhOUzJmn09g+B8JSqmyQE3gaIFWWbBMnl9We4SuHXBxaiP3eRRQCph+44g1Tdtwpzoj99muGLPPWusYpVQEEAVkAF9rrXO8/M0WGPn7/D4wVym1F8N0xJtaa5ttq6uU+gloCXgppWKB0YALmC+/5NZ/IYSwE9Y85SKEEOIuSKALIYSdkEAXQgg7IYEuhBB2QgJdCCHshAS6EELYCQl0IYSwE/8H4U/Ozz4LlTMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_name = 'precip_reliab_stat_index_1d_week4_lr_1.png'\n",
    "metrics_plot(bss_tr1_lr,bss_va1_lr,bss_te1_lr,calib_y_va1_lr,calib_x_va1_lr,rpss_tr_lr,rpss_va_lr,fig_name)\n",
    "fig_name = 'precip_reliab_stat_index_1d_week4_lr_2.png'\n",
    "metrics_plot(bss_tr2_lr,bss_va2_lr,bss_te2_lr,calib_y_va2_lr,calib_x_va2_lr,rpss_tr_lr,rpss_va_lr,fig_name)\n",
    "fig_name = 'precip_reliab_stat_index_1d_week4_lr_3.png'\n",
    "metrics_plot(bss_tr3_lr,bss_va3_lr,bss_te3_lr,calib_y_va3_lr,calib_x_va3_lr,rpss_tr_lr,rpss_va_lr,fig_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_plot(f1_tr,f1_va,f1_te,rec_tr,rec_va,rec_te,prec_tr,prec_va,prec_te,\n",
    "             acc_tr,acc_va,acc_te,fign1,fign2,fign3):\n",
    "    f1_tr = np.array(f1_tr)\n",
    "    f1_va = np.array(f1_va)\n",
    "    f1_te = np.array(f1_te)\n",
    "    rec_tr = np.array(rec_tr)\n",
    "    rec_va = np.array(rec_va)\n",
    "    rec_te = np.array(rec_te)\n",
    "    prec_tr = np.array(prec_tr)\n",
    "    prec_va = np.array(prec_va)\n",
    "    prec_te = np.array(prec_te)\n",
    "\n",
    "    fig1, ax1 = plt.subplots()\n",
    "    ax1.set_title('performance')\n",
    "    dat_plot = [acc_tr, acc_va, prec_tr[:,0], prec_va[:,0], rec_tr[:,0], rec_va[:,0], f1_tr[:,0], f1_va[:,0]]\n",
    "    labels = ['acc', 'acc', 'precision', 'precision', 'recall', 'recall', 'F1', 'F1']\n",
    "    bplot = ax1.boxplot(dat_plot, labels=labels, patch_artist=True)\n",
    "    colors = ['lightblue', 'lightgreen', 'lightblue', 'lightgreen', 'lightblue', 'lightgreen', 'lightblue', 'lightgreen']\n",
    "    for patch, color in zip(bplot['boxes'], colors):\n",
    "        patch.set_fc(color)\n",
    "    plt.savefig(fign1)\n",
    "\n",
    "    fig1, ax1 = plt.subplots()\n",
    "    ax1.set_title('performance')\n",
    "    dat_plot = [acc_tr, acc_va, prec_tr[:,1], prec_va[:,1], rec_tr[:,1], rec_va[:,1], f1_tr[:,1], f1_va[:,1]]\n",
    "    labels = ['acc', 'acc', 'precision', 'precision', 'recall', 'recall', 'F1', 'F1']\n",
    "    bplot = ax1.boxplot(dat_plot, labels=labels, patch_artist=True)\n",
    "    colors = ['lightblue', 'lightgreen', 'lightblue', 'lightgreen', 'lightblue', 'lightgreen', 'lightblue', 'lightgreen']\n",
    "    for patch, color in zip(bplot['boxes'], colors):\n",
    "        patch.set_fc(color)\n",
    "    plt.savefig(fign2)\n",
    "\n",
    "    fig1, ax1 = plt.subplots()\n",
    "    ax1.set_title('performance')\n",
    "    dat_plot = [acc_tr, acc_va, prec_tr[:,2], prec_va[:,2], rec_tr[:,2], rec_va[:,2], f1_tr[:,2], f1_va[:,2]]\n",
    "    labels = ['acc', 'acc', 'precision', 'precision', 'recall', 'recall', 'F1', 'F1']\n",
    "    bplot = ax1.boxplot(dat_plot, labels=labels, patch_artist=True)\n",
    "    colors = ['lightblue', 'lightgreen', 'lightblue', 'lightgreen', 'lightblue', 'lightgreen', 'lightblue', 'lightgreen']\n",
    "    for patch, color in zip(bplot['boxes'], colors):\n",
    "        patch.set_fc(color)\n",
    "    plt.savefig(fign3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fign1 = 'precip_metrics_stat_index_1d_week4_lr_1.png'\n",
    "fign2 = 'precip_metrics_stat_index_1d_week4_lr_2.png'\n",
    "fign3 = 'precip_metrics_stat_index_1d_week4_lr_3.png'\n",
    "acc_plot(f1_tr_lr,f1_va_lr,f1_te_lr,rec_tr_lr,rec_va_lr,rec_te_lr,\n",
    "         prec_tr_lr,prec_va_lr,prec_te_lr,acc_tr_lr,acc_va_lr,acc_te_lr,fign1,fign2,fign3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_va_lr = np.array(rec_va_lr)\n",
    "prec_va_lr = np.array(prec_va_lr)\n",
    "f1_va_lr = np.array(f1_va_lr)\n",
    "acc_va_lr = np.array(acc_va_lr)\n",
    "bss_va3_lr = np.array(bss_va3_lr)\n",
    "bss_va1_lr = np.array(bss_va1_lr)\n",
    "rpss_va_lr = np.array(rpss_va_lr)\n",
    "rec_tr_lr = np.array(rec_tr_lr)\n",
    "prec_tr_lr = np.array(prec_tr_lr)\n",
    "f1_tr_lr = np.array(f1_tr_lr)\n",
    "acc_tr_lr = np.array(acc_tr_lr)\n",
    "bss_tr3_lr = np.array(bss_tr3_lr)\n",
    "bss_tr1_lr = np.array(bss_tr1_lr)\n",
    "rpss_tr_lr = np.array(rpss_tr_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lead_num=28\n",
    "f_va={'recall_va':rec_va_lr[:,2], 'precision_va':prec_va_lr[:,2], 'f1_va':f1_va_lr[:,2], \n",
    " 'acc_va':acc_va_lr,'bss_va':bss_va3_lr, 'rpss_va':rpss_va_lr, 'calib_y_va_sel':calib_y_va3_lr, 'calib_x_va_sel':calib_x_va3_lr}\n",
    "np.save(root_results+'Flood_'+'scores_valid_Lead'+str(Lead_num)+'_CLIM0.3_lr.npy',f_va)\n",
    "\n",
    "d_va={'recall_va':rec_va_lr[:,0], 'precision_va':prec_va_lr[:,0], 'f1_va':f1_va_lr[:,0], \n",
    " 'acc_va':acc_va_lr,'bss_va':bss_va1_lr, 'rpss_va':rpss_va_lr, 'calib_y_va_sel':calib_y_va1_lr, 'calib_x_va_sel':calib_x_va1_lr}\n",
    "np.save(root_results+'Drought_'+'scores_valid_Lead'+str(Lead_num)+'_CLIM0.3_lr.npy',d_va)\n",
    "\n",
    "f_tr={'recall_tr':rec_tr_lr[:,2], 'precision_tr':prec_tr_lr[:,2], 'f1_tr':f1_tr_lr[:,2], \n",
    " 'acc_tr':acc_tr_lr,'bss_tr':bss_tr3_lr, 'rpss_tr':rpss_tr_lr, 'calib_y_tr_sel':calib_y_tr3_lr, 'calib_x_tr_sel':calib_x_tr3_lr}\n",
    "np.save(root_results+'Flood_'+'scores_train_Lead'+str(Lead_num)+'_CLIM0.3_lr.npy',f_tr)\n",
    "\n",
    "d_tr={'recall_tr':rec_tr_lr[:,0], 'precision_tr':prec_tr_lr[:,0], 'f1_tr':f1_tr_lr[:,0], \n",
    " 'acc_tr':acc_tr_lr,'bss_tr':bss_tr1_lr, 'rpss_tr':rpss_tr_lr, 'calib_y_tr_sel':calib_y_tr1_lr, 'calib_x_tr_sel':calib_x_tr1_lr}\n",
    "np.save(root_results+'Drought_'+'scores_train_Lead'+str(Lead_num)+'_CLIM0.3_lr.npy',d_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall_va': array([0.75      , 0.8627451 , 0.        , 0.        , 0.        ,\n",
       "        0.3030303 , 1.        , 0.3902439 , 0.54545455, 0.78571429,\n",
       "        0.        , 0.325     , 0.83116883, 0.73239437, 0.        ,\n",
       "        0.82051282, 1.        , 0.36363636, 0.25      , 0.09302326,\n",
       "        0.5       , 0.82758621, 0.83076923, 0.97826087, 1.        ,\n",
       "        0.8255814 , 0.48076923, 0.5       , 0.74285714]),\n",
       " 'precision_va': array([0.75      , 0.73333333, 0.        , 0.        , 0.        ,\n",
       "        0.17857143, 0.15625   , 0.94117647, 0.6122449 , 0.19642857,\n",
       "        0.        , 0.54166667, 0.74418605, 0.8125    , 0.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.36363636,\n",
       "        0.66666667, 0.57831325, 0.56842105, 0.61643836, 0.77631579,\n",
       "        0.76344086, 0.43859649, 0.54545455, 0.54166667]),\n",
       " 'f1_va': array([0.75      , 0.79279279, 0.        , 0.        , 0.        ,\n",
       "        0.2247191 , 0.27027027, 0.55172414, 0.57692308, 0.31428571,\n",
       "        0.        , 0.40625   , 0.78527607, 0.77037037, 0.        ,\n",
       "        0.90140845, 1.        , 0.53333333, 0.4       , 0.14814815,\n",
       "        0.57142857, 0.68085106, 0.675     , 0.75630252, 0.87407407,\n",
       "        0.79329609, 0.4587156 , 0.52173913, 0.62650602]),\n",
       " 'acc_va': array([0.609375 , 0.578125 , 0.4765625, 0.4609375, 0.2421875, 0.1484375,\n",
       "        0.40625  , 0.59375  , 0.453125 , 0.3671875, 0.421875 , 0.546875 ,\n",
       "        0.625    , 0.625    , 0.4453125, 0.75     , 0.7890625, 0.5703125,\n",
       "        0.4921875, 0.4140625, 0.484375 , 0.484375 , 0.53125  , 0.5859375,\n",
       "        0.6875   , 0.6171875, 0.390625 , 0.53125  , 0.453125 ]),\n",
       " 'bss_va': array([ 0.41494487,  0.50519849,  0.20947238, -0.11698822, -0.47230808,\n",
       "        -0.54775641, -0.7553811 ,  0.26615641,  0.05166998, -0.41962871,\n",
       "        -0.39918858,  0.02362978,  0.30305291,  0.42289707,  0.20061811,\n",
       "         0.7333305 ,  0.92132041,  0.46265859,  0.22102651,  0.10103107,\n",
       "         0.29289055,  0.14803789,  0.09440248,  0.34290066,  0.66750255,\n",
       "         0.53313785, -0.09251634, -0.12290444,  0.07694714]),\n",
       " 'rpss_va': array([ 3.64008642e-01,  3.00190837e-01,  8.16538561e-02,  1.31838377e-03,\n",
       "        -3.03722000e-01, -2.94851001e-01, -1.99670448e-01,  2.46856340e-01,\n",
       "        -8.64205153e-03, -2.44364137e-01, -1.92189451e-01, -5.06168622e-04,\n",
       "         3.32097820e-01,  4.41966187e-01,  2.69898710e-01,  6.41441071e-01,\n",
       "         7.00192614e-01,  3.69228771e-01,  2.45162163e-01,  1.27406387e-01,\n",
       "         2.61253706e-01,  1.76098716e-01,  2.23757142e-01,  2.04057398e-01,\n",
       "         3.79620872e-01,  4.54526998e-01, -5.62688186e-02, -4.66658357e-02,\n",
       "        -3.50362562e-02]),\n",
       " 'calib_y_va_sel': [array([0.05405405, 0.18181818, 0.5       , 0.71428571, 0.5       ,\n",
       "         0.8       , 0.55555556, 0.66666667, 0.63636364, 1.        ]),\n",
       "  array([0.        , 0.        , 0.8       , 0.57142857, 0.375     ,\n",
       "         0.57142857, 0.55555556, 0.57142857, 0.66666667, 1.        ]),\n",
       "  array([0.05084746, 0.1627907 , 0.61111111, 0.        , 0.        ,\n",
       "         0.        ]),\n",
       "  array([0.08333333, 0.17391304, 0.12903226, 0.5       , 0.42857143,\n",
       "         0.        , 0.        , 0.        , 0.        ]),\n",
       "  array([0.33333333, 0.2       , 0.19230769, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ]),\n",
       "  array([0.45      , 0.2173913 , 0.25      , 0.3125    , 0.        ,\n",
       "         0.        , 0.4       , 0.28571429, 0.        , 0.4       ]),\n",
       "  array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.30769231, 0.16666667, 0.36363636]),\n",
       "  array([0.        , 0.33333333, 0.12      , 0.85714286, 1.        ,\n",
       "         1.        , 1.        , 1.        , 0.        ]),\n",
       "  array([0.        , 0.55555556, 0.6       , 0.66666667, 0.5       ,\n",
       "         0.85714286, 0.625     , 0.57142857, 0.        ]),\n",
       "  array([0.        , 0.05263158, 0.08333333, 0.11764706, 0.1       ,\n",
       "         0.        , 0.16666667, 0.36363636, 0.2       ]),\n",
       "  array([0.03846154, 0.25      , 0.21052632, 0.13636364, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        ]),\n",
       "  array([0.04878049, 0.36111111, 0.53846154, 0.35714286, 0.5       ,\n",
       "         1.        , 0.6       , 0.625     , 0.        ]),\n",
       "  array([0.        , 0.46666667, 0.5       , 0.5       , 0.66666667,\n",
       "         0.83333333, 0.85714286, 0.75      , 0.54545455, 0.83333333]),\n",
       "  array([0.        , 0.35714286, 0.38888889, 0.625     , 1.        ,\n",
       "         1.        , 1.        , 1.        , 0.61111111, 0.87096774]),\n",
       "  array([0.05333333, 0.26666667, 0.54166667, 0.75      , 1.        ]),\n",
       "  array([0.08163265, 0.375     , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 1.        ]),\n",
       "  array([0., 0., 0., 0., 1., 1., 1., 1., 1.]),\n",
       "  array([0.03448276, 0.2       , 0.33333333, 1.        , 1.        ,\n",
       "         1.        ]),\n",
       "  array([0.07894737, 0.16666667, 0.42105263, 0.5625    , 0.93333333,\n",
       "         1.        ]),\n",
       "  array([0.        , 0.05882353, 0.42222222, 0.48148148, 0.76923077,\n",
       "         0.        , 0.        ]),\n",
       "  array([0.        , 0.        , 0.48275862, 0.70588235, 0.6       ,\n",
       "         0.5       , 0.77777778, 1.        ]),\n",
       "  array([0.        , 0.        , 0.25      , 0.33333333, 0.61111111,\n",
       "         0.68181818, 0.6       , 0.58823529, 0.42857143]),\n",
       "  array([0.        , 0.        , 0.33333333, 0.25      , 0.61904762,\n",
       "         0.77777778, 0.58333333, 0.64285714, 0.21428571]),\n",
       "  array([0.        , 0.        , 0.        , 0.        , 0.2       ,\n",
       "         0.65      , 0.78571429, 0.70833333, 0.375     ]),\n",
       "  array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.61538462, 0.71428571, 1.        , 1.        ]),\n",
       "  array([0.14285714, 0.33333333, 0.8       , 0.38095238, 0.52380952,\n",
       "         0.75      , 1.        , 1.        , 1.        ]),\n",
       "  array([0.30434783, 0.41666667, 0.33333333, 0.5       , 0.38461538,\n",
       "         0.5       , 0.375     , 0.5       ]),\n",
       "  array([0.26086957, 0.61111111, 0.5       , 0.47058824, 0.47058824,\n",
       "         0.75      , 0.33333333, 0.2       ]),\n",
       "  array([0.        , 0.        , 0.2       , 0.3       , 0.45454545,\n",
       "         0.82352941, 0.6       , 0.        , 0.        ])],\n",
       " 'calib_x_va_sel': [array([0.04574076, 0.14622892, 0.25514901, 0.3446054 , 0.46614389,\n",
       "         0.56348355, 0.65741543, 0.75225497, 0.85429832, 0.94250726]),\n",
       "  array([0.03466617, 0.15205894, 0.27103616, 0.35562282, 0.46429337,\n",
       "         0.53126327, 0.64663513, 0.75070325, 0.85836563, 0.94494554]),\n",
       "  array([0.04212717, 0.14292585, 0.24607276, 0.3488085 , 0.45732067,\n",
       "         0.50280367]),\n",
       "  array([0.05600734, 0.15108245, 0.24471451, 0.33944458, 0.42982415,\n",
       "         0.55350143, 0.63451314, 0.75411333, 0.8379454 ]),\n",
       "  array([0.04667887, 0.14822332, 0.24544048, 0.33181388, 0.45121347,\n",
       "         0.55576366, 0.62137071, 0.73018102, 0.86126444, 0.90055286]),\n",
       "  array([0.04776356, 0.15441408, 0.24843254, 0.335719  , 0.45443705,\n",
       "         0.54866861, 0.65955651, 0.76766921, 0.86745417, 0.93946714]),\n",
       "  array([0.03500402, 0.15599748, 0.25522211, 0.3196434 , 0.46633907,\n",
       "         0.54906598, 0.65284123, 0.75384178, 0.83049156, 0.94268295]),\n",
       "  array([0.03380622, 0.14675382, 0.25449279, 0.36664609, 0.45007324,\n",
       "         0.55068614, 0.64300791, 0.77389263, 0.80131506]),\n",
       "  array([0.0482212 , 0.14011833, 0.25578619, 0.35595673, 0.44832601,\n",
       "         0.54772691, 0.65071541, 0.76060531, 0.83033332]),\n",
       "  array([0.06163364, 0.14236776, 0.23303654, 0.34065401, 0.44030647,\n",
       "         0.55539456, 0.6428653 , 0.74112079, 0.84031705]),\n",
       "  array([0.0618665 , 0.14491947, 0.26464446, 0.35278108, 0.44585475,\n",
       "         0.54510017, 0.63948466, 0.75223234, 0.84766782]),\n",
       "  array([0.05256056, 0.1386532 , 0.2543622 , 0.32870104, 0.44796927,\n",
       "         0.5523282 , 0.64174003, 0.73256989, 0.82344666]),\n",
       "  array([0.03478937, 0.14105295, 0.23976309, 0.34628446, 0.44292627,\n",
       "         0.54143807, 0.66247806, 0.74757987, 0.86821443, 0.95170507]),\n",
       "  array([0.04471399, 0.15709015, 0.22836626, 0.33340139, 0.43077702,\n",
       "         0.56274517, 0.65595365, 0.75320926, 0.83758168, 0.95288818]),\n",
       "  array([0.03106529, 0.13632518, 0.2363326 , 0.34068881, 0.41496802]),\n",
       "  array([0.02570721, 0.12965583, 0.23903346, 0.33540542, 0.64246428,\n",
       "         0.73784211, 0.87036733, 0.93236267]),\n",
       "  array([0.04044908, 0.14407847, 0.24615574, 0.30324322, 0.58019843,\n",
       "         0.64711421, 0.75870185, 0.86769195, 0.92751664]),\n",
       "  array([0.04531006, 0.1370945 , 0.25861055, 0.35217699, 0.44417958,\n",
       "         0.53358942]),\n",
       "  array([0.05187883, 0.15194053, 0.24811215, 0.35617223, 0.44408031,\n",
       "         0.55225593]),\n",
       "  array([0.05195698, 0.14969773, 0.24359617, 0.34811406, 0.4338961 ,\n",
       "         0.56091766, 0.64607349]),\n",
       "  array([0.04048318, 0.1765191 , 0.24272358, 0.35670628, 0.44614722,\n",
       "         0.53526752, 0.65561948, 0.7668849 ]),\n",
       "  array([0.04823358, 0.16221641, 0.24871174, 0.34637158, 0.4456299 ,\n",
       "         0.54697707, 0.64929591, 0.74286344, 0.83397523]),\n",
       "  array([0.04615469, 0.15015684, 0.26782359, 0.36153985, 0.43719842,\n",
       "         0.55869164, 0.65077451, 0.74901233, 0.84021042]),\n",
       "  array([0.05529767, 0.14423755, 0.26126728, 0.33321111, 0.46107695,\n",
       "         0.55504073, 0.64504527, 0.74033745, 0.83722052]),\n",
       "  array([0.0561419 , 0.13359626, 0.25211689, 0.33084068, 0.46088877,\n",
       "         0.55593004, 0.65944754, 0.74735681, 0.86207251, 0.90860135]),\n",
       "  array([0.16012846, 0.24781038, 0.36497709, 0.4478325 , 0.53662155,\n",
       "         0.65848029, 0.74357123, 0.85743517, 0.91790247]),\n",
       "  array([0.07001832, 0.14989727, 0.23552397, 0.35301024, 0.44595098,\n",
       "         0.55579048, 0.6374002 , 0.73114384]),\n",
       "  array([0.06239633, 0.14905764, 0.24608787, 0.3622594 , 0.43404689,\n",
       "         0.55170713, 0.64302716, 0.74530671]),\n",
       "  array([0.04557806, 0.13349886, 0.25662851, 0.35221634, 0.44686717,\n",
       "         0.5225418 , 0.64309597, 0.74809462, 0.84360327])]}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "Lead_num=0\n",
    "datafn = root_results+'Flood_'+'scores_valid_Lead'+str(Lead_num)+'_lag_60d_lr.npy'\n",
    "tmp = np.load(datafn,allow_pickle=True)\n",
    "tmp=tmp.tolist()\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x7f40b1831670>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callback_paths = '/s2s_nobackup/mpyrina/New_runs_ind_shal/Lead_0d/Run48/weight_3cat_comb48_9'\n",
    "tf_model = tf.keras.models.load_model(callback_paths)\n",
    "tf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1920, 60, 6)\n"
     ]
    }
   ],
   "source": [
    "dummy = predictor_array.Index\n",
    "y_all_2d = keras.utils.to_categorical(Y_train-1)\n",
    "tmp = np.ndarray((1920,60,6))\n",
    "for ii in range(6):\n",
    "    tmp[:,:,ii] = dummy.values[:,ii,:]\n",
    "print(tmp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 6)\n",
      "(60, 6)\n",
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.35939628, 0.31695232, 0.32365143]], dtype=float32)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "expind = 10\n",
    "pca_input = tmp[expind,:,:]\n",
    "print(pca_input.shape)\n",
    "plt.imshow(pca_input)\n",
    "tf_model.input_size = pca_input.shape\n",
    "print(pca_input.shape)\n",
    "tf_model.predict(pca_input[np.newaxis,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_masks(N, s, p1, model):\n",
    "    cell_size = np.ceil(np.array(model.input_size) / s)\n",
    "    up_size = (s + 1) * cell_size\n",
    "\n",
    "    grid = np.random.rand(N, s, s) < p1\n",
    "    grid = grid.astype('float32')\n",
    "\n",
    "    masks = np.empty((N, *model.input_size))\n",
    "\n",
    "    for i in tqdm(range(N), desc='Generating masks'):\n",
    "        # Random shifts\n",
    "        x = np.random.randint(0, cell_size[0])\n",
    "        y = np.random.randint(0, cell_size[1])\n",
    "        # Linear upsampling and cropping\n",
    "        masks[i, :, :] = resize(grid[i], up_size, order=1, mode='reflect',\n",
    "                                anti_aliasing=False)[x:x + model.input_size[0], y:y + model.input_size[1]]\n",
    "    masks = masks.reshape(-1, *model.input_size, 1)\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "from skimage.transform import resize\n",
    "from tqdm import tqdm\n",
    "def explain(N, model, inp, masks):\n",
    "    preds = []\n",
    "    p1 = 0.5\n",
    "    # Make sure multiplication is being done for correct axes\n",
    "    masked = inp * masks\n",
    "    for i in tqdm(range(0, len(masks), batch_size), desc='Explaining'):\n",
    "        preds.append(model.predict(masked[i:min(i+batch_size, len(masks))]))\n",
    "    preds = np.concatenate(preds)\n",
    "    sal = preds.T.dot(masks.reshape(N, -1)).reshape(-1, *model.input_size)\n",
    "    sal = sal / N / p1\n",
    "    return sal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating masks: 100%|███████████████████| 1000/1000 [00:00<00:00, 4886.15it/s]\n",
      "Explaining:   0%|                                        | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Explaining:  20%|██████▍                         | 2/10 [00:00<00:00, 14.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Explaining:  40%|████████████▊                   | 4/10 [00:00<00:00, 14.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Explaining:  60%|███████████████████▏            | 6/10 [00:00<00:00, 14.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Explaining:  80%|█████████████████████████▌      | 8/10 [00:00<00:00, 14.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining: 100%|███████████████████████████████| 10/10 [00:00<00:00, 14.55it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f411dfe81c0>"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "masks = generate_masks(1000, 8, 0.5, tf_model)[:,:,:,0]\n",
    "sal = explain(1000, tf_model, pca_input, masks)\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "plt.subplot(1,3,1)\n",
    "plt.title('Explanation')\n",
    "plt.axis('off')\n",
    "plt.imshow(sal[1], cmap='coolwarm', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modeling with an LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import build_lstm\n",
    "from models import build_CNN\n",
    "from models import class_weight_creator\n",
    "from train import get_train_test_val, cross_valid_one_out\n",
    "from train import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 2\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 60, 4)]           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 60, 66)            18744     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 60, 35)            14280     \n",
      "_________________________________________________________________\n",
      "attention_with_context (Atte ((None, 60, 35), (None, 6 1295      \n",
      "_________________________________________________________________\n",
      "addition (Addition)          (None, 35)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 144       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 20        \n",
      "=================================================================\n",
      "Total params: 34,483\n",
      "Trainable params: 34,483\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "validation years [1981, 1982]\n",
      "train years {1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020}\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/cfc/s2s/zhengwu/code/models.py:271: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  weights = np.max(np.sum(Y, axis=0)) / np.sum(Y, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - ETA: 0s - loss: 5.7982 - accuracy: 0.3534WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "52/52 [==============================] - 8s 82ms/step - loss: 5.7816 - accuracy: 0.3538 - val_loss: 3.1268 - val_accuracy: 0.4186\n",
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_4cat_1/assets\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 4s 70ms/step - loss: 2.9412 - accuracy: 0.3857 - val_loss: 1.8099 - val_accuracy: 0.5174\n",
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_4cat_1/assets\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 4s 79ms/step - loss: 1.9168 - accuracy: 0.3936 - val_loss: 1.3589 - val_accuracy: 0.4651\n",
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_4cat_1/assets\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 4s 67ms/step - loss: 1.5401 - accuracy: 0.4091 - val_loss: 1.2021 - val_accuracy: 0.4128\n",
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_4cat_1/assets\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 4s 68ms/step - loss: 1.4128 - accuracy: 0.4190 - val_loss: 1.1498 - val_accuracy: 0.4651\n",
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_4cat_1/assets\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 4s 68ms/step - loss: 1.3390 - accuracy: 0.4154 - val_loss: 1.0605 - val_accuracy: 0.4186\n",
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_4cat_1/assets\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 4s 71ms/step - loss: 1.2215 - accuracy: 0.4259 - val_loss: 1.0502 - val_accuracy: 0.4419\n",
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_4cat_1/assets\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 4s 74ms/step - loss: 1.1889 - accuracy: 0.4469 - val_loss: 1.0319 - val_accuracy: 0.4942\n",
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_4cat_1/assets\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 4s 67ms/step - loss: 1.1914 - accuracy: 0.4444 - val_loss: 1.0374 - val_accuracy: 0.3721\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 3s 65ms/step - loss: 1.1910 - accuracy: 0.4271 - val_loss: 1.0148 - val_accuracy: 0.4535\n",
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_4cat_1/assets\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 4s 78ms/step - loss: 1.2025 - accuracy: 0.4176 - val_loss: 1.0129 - val_accuracy: 0.4535\n",
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_4cat_1/assets\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 4s 73ms/step - loss: 1.1705 - accuracy: 0.4447 - val_loss: 1.0203 - val_accuracy: 0.4535\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 3s 65ms/step - loss: 1.1916 - accuracy: 0.4129 - val_loss: 1.0125 - val_accuracy: 0.5465\n",
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_4cat_1/assets\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 3s 65ms/step - loss: 1.1863 - accuracy: 0.4123 - val_loss: 1.0024 - val_accuracy: 0.4535\n",
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_4cat_1/assets\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 5s 87ms/step - loss: 1.2108 - accuracy: 0.4187 - val_loss: 1.0299 - val_accuracy: 0.4419\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 4s 78ms/step - loss: 1.1791 - accuracy: 0.4468 - val_loss: 1.0191 - val_accuracy: 0.4535\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 4s 78ms/step - loss: 1.1879 - accuracy: 0.4440 - val_loss: 1.0098 - val_accuracy: 0.4535\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 4s 77ms/step - loss: 1.1657 - accuracy: 0.4577 - val_loss: 1.0135 - val_accuracy: 0.4709\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 4s 78ms/step - loss: 1.1829 - accuracy: 0.4435 - val_loss: 0.9860 - val_accuracy: 0.4884\n",
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_4cat_1/assets\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 4s 79ms/step - loss: 1.1764 - accuracy: 0.4550 - val_loss: 1.0440 - val_accuracy: 0.3837\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 4s 78ms/step - loss: 1.1685 - accuracy: 0.4384 - val_loss: 1.0116 - val_accuracy: 0.4767\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 4s 79ms/step - loss: 1.1592 - accuracy: 0.4656 - val_loss: 1.0114 - val_accuracy: 0.4535\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 4s 78ms/step - loss: 1.1651 - accuracy: 0.4634 - val_loss: 1.0283 - val_accuracy: 0.3721\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 4s 78ms/step - loss: 1.1454 - accuracy: 0.4621 - val_loss: 1.0139 - val_accuracy: 0.4767\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 4s 78ms/step - loss: 1.1566 - accuracy: 0.4643 - val_loss: 1.0527 - val_accuracy: 0.3895\n",
      "Epoch 26/100\n",
      "36/52 [===================>..........] - ETA: 1s - loss: 1.1440 - accuracy: 0.4804"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-4f8e99f08146>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mmodel_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mloss_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mloss_va\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-f6793a9cbc62>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_X, train_y, val_X, val_y, callbacks_path, epochs, batch_size, class_weight)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     )\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m#history = history.history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/net/cfc/s2s/s2s_ai_challenge/conda/envs/s2sai_2020/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/net/cfc/s2s/s2s_ai_challenge/conda/envs/s2sai_2020/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/net/cfc/s2s/s2s_ai_challenge/conda/envs/s2sai_2020/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/net/cfc/s2s/s2s_ai_challenge/conda/envs/s2sai_2020/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/net/cfc/s2s/s2s_ai_challenge/conda/envs/s2sai_2020/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/net/cfc/s2s/s2s_ai_challenge/conda/envs/s2sai_2020/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/net/cfc/s2s/s2s_ai_challenge/conda/envs/s2sai_2020/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train the lstm\n",
    "#del model_history\n",
    "from random import sample as random_s\n",
    "NY_train = 30 # its not used\n",
    "recall_tr1, precision_tr1, f1_tr1, acc_tr1, bss_tr1, bs_tr1, calib_y_tr1, calib_x_tr1 = [],[],[],[],[],[],[],[]\n",
    "recall_tr2, precision_tr2, f1_tr2, acc_tr2, bss_tr2, bs_tr2, calib_y_tr2, calib_x_tr2 = [],[],[],[],[],[],[],[]\n",
    "recall_tr3, precision_tr3, f1_tr3, acc_tr3, bss_tr3, bs_tr3, calib_y_tr3, calib_x_tr3 = [],[],[],[],[],[],[],[]\n",
    "recall_tr4, precision_tr4, f1_tr4, acc_tr4, bss_tr4, bs_tr4, calib_y_tr4, calib_x_tr4 = [],[],[],[],[],[],[],[]\n",
    "\n",
    "recall_va1, precision_va1, f1_va1, acc_va1, bss_va1, bs_va1, calib_y_va1, calib_x_va1 = [],[],[],[],[],[],[],[]\n",
    "recall_va2, precision_va2, f1_va2, acc_va2, bss_va2, bs_va2, calib_y_va2, calib_x_va2 = [],[],[],[],[],[],[],[]\n",
    "recall_va3, precision_va3, f1_va3, acc_va3, bss_va3, bs_va3, calib_y_va3, calib_x_va3 = [],[],[],[],[],[],[],[]\n",
    "recall_va4, precision_va4, f1_va4, acc_va4, bss_va4, bs_va4, calib_y_va4, calib_x_va4 = [],[],[],[],[],[],[],[]\n",
    "\n",
    "loss_tr, loss_va = [],[]\n",
    "metri_tr, metri_va = [],[]\n",
    "\n",
    "import random\n",
    "NS = 10\n",
    "#select_ind = random.sample(list(np.arange(EYY-SYY+1-5)), NS) #it selects NS indices our of 35\n",
    "for ii in range(NS):\n",
    "    model = build_lstm(ntimestep, nfeature, layers=2, neurons=[66,35], regval=[0.1,0.05], out_neurons=n_cat+1)\n",
    "    #train_X, train_y, val_X, val_y = cross_valid_one_out(X_all, y_all, all_years, val_year_ind=ii)\n",
    "    val_year = [SYY+ii,SYY+ii+1]\n",
    "    train_X, train_y, val_X, val_y = cross_valid_n_out(X_all_n, y_all_n, all_years, val_year=val_year)\n",
    "    \n",
    "    train_y_tmp = train_y.values\n",
    "    class_weight = class_weight_creator(train_y_tmp)\n",
    "    callbacks_path = root_results+'tmp/Run20/weight_4cat_'+str(ii+1)\n",
    "    history_path = root_results+'tmp/Run20/history_4cat_'+str(ii+1)\n",
    "    batch_size = 64\n",
    "    epochs = 100\n",
    "    model_history = train_model(model, train_X, train_y, val_X, val_y, callbacks_path, epochs, batch_size, class_weight)\n",
    "    loss_tr.append(model_history.history['loss'])\n",
    "    loss_va.append(model_history.history['val_loss'])\n",
    "    metri_tr.append(model_history.history['accuracy'])\n",
    "    metri_va.append(model_history.history['val_accuracy'])\n",
    "    \n",
    "    # train score\n",
    "    #ref_prob = clim_pr_y[0:int(train_y[:,1].shape[0])]\n",
    "    #print(ref_prob.shape)\n",
    "    y_prob = model.predict(train_X); #y_pred = np.argmax(y_prob,axis=1)\n",
    "    #print('y prediction shape', y_pred.shape)\n",
    "    nbins = 10\n",
    "    \n",
    "    recall, precision, f1, acc, bs, bss, calib_y, calib_x = score(np.argmax(train_y,axis=1), train_y[:,1], np.argmax(y_prob,axis=1), \n",
    "                                                    y_prob[:,1], nbins, clim_pr_y_cat1[0:int(train_y[:,1].shape[0])])\n",
    "    recall_tr1.append(recall); precision_tr1.append(precision); f1_tr1.append(f1)\n",
    "    acc_tr1.append(acc); bs_tr1.append(bs); bss_tr1.append(bss)\n",
    "    calib_y_tr1.append(calib_y); calib_x_tr1.append(calib_x)\n",
    "    \n",
    "    recall, precision, f1, acc, bs, bss, calib_y, calib_x = score(np.argmax(train_y,axis=1), train_y[:,2], np.argmax(y_prob,axis=1), y_prob[:,2], nbins, \n",
    "                                                                  clim_pr_y_cat3[0:int(train_y[:,2].shape[0])])\n",
    "    recall_tr3.append(recall); precision_tr3.append(precision); f1_tr3.append(f1)\n",
    "    acc_tr3.append(acc); bs_tr3.append(bs); bss_tr3.append(bss)\n",
    "    calib_y_tr3.append(calib_y); calib_x_tr3.append(calib_x)\n",
    "    \n",
    "    recall, precision, f1, acc, bs, bss, calib_y, calib_x = score(np.argmax(train_y,axis=1), train_y[:,3], np.argmax(y_prob,axis=1), y_prob[:,3], nbins, \n",
    "                                                                  clim_pr_y_cat4[0:int(train_y[:,3].shape[0])])\n",
    "    recall_tr4.append(recall); precision_tr4.append(precision); f1_tr4.append(f1)\n",
    "    acc_tr4.append(acc); bs_tr4.append(bs); bss_tr4.append(bss)\n",
    "    calib_y_tr4.append(calib_y); calib_x_tr4.append(calib_x)\n",
    "    \n",
    "    # validation score\n",
    "    y_prob = model.predict(val_X); #y_pred = np.argmax(y_prob,axis=1)\n",
    "    nbins = 10\n",
    "    \n",
    "    recall, precision, f1, acc, bs, bss, calib_y, calib_x = score(np.argmax(val_y,axis=1), val_y[:,1], np.argmax(y_prob,axis=1),\n",
    "                                                    y_prob[:,1], nbins, clim_pr_y_cat1[0:int(val_y[:,1].shape[0])])\n",
    "    recall_va1.append(recall); precision_va1.append(precision); f1_va1.append(f1)\n",
    "    acc_va1.append(acc); bs_va1.append(bs); bss_va1.append(bss)\n",
    "    calib_y_va1.append(calib_y); calib_x_va1.append(calib_x)\n",
    "    \n",
    "    recall, precision, f1, acc, bs, bss, calib_y, calib_x = score(np.argmax(val_y,axis=1), val_y[:,2], np.argmax(y_prob,axis=1),\n",
    "                                                    y_prob[:,2], nbins, clim_pr_y_cat3[0:int(val_y[:,2].shape[0])])\n",
    "    recall_va3.append(recall); precision_va3.append(precision); f1_va3.append(f1)\n",
    "    acc_va3.append(acc); bs_va3.append(bs); bss_va3.append(bss)\n",
    "    calib_y_va3.append(calib_y); calib_x_va3.append(calib_x)\n",
    "    \n",
    "    recall, precision, f1, acc, bs, bss, calib_y, calib_x = score(np.argmax(val_y,axis=1), val_y[:,3], np.argmax(y_prob,axis=1),\n",
    "                                                    y_prob[:,3], nbins, clim_pr_y_cat4[0:int(val_y[:,3].shape[0])])\n",
    "    recall_va4.append(recall); precision_va4.append(precision); f1_va4.append(f1)\n",
    "    acc_va4.append(acc); bs_va4.append(bs); bss_va4.append(bss)\n",
    "    calib_y_va4.append(calib_y); calib_x_va4.append(calib_x)\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    print('bss_va1 print',bss_va1)\n",
    "    print('bss_va3 print',bss_va3)\n",
    "    print('bss_va4 print',bss_va4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f1fd01e2ed0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We can also look at the optimisation history\n",
    "train_loss = np.array(loss_tr)\n",
    "valid_loss = np.array(loss_va)\n",
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=plt.figaspect(0.25))\n",
    "ax1.plot(np.nanmean(train_loss,axis=0), label='Training Loss')\n",
    "ax1.plot(np.nanmean(valid_loss,axis=0), label='Validation Loss')\n",
    "ax1.set_title('loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(np.nanmean(metri_tr,axis=0), label='Training ACC')\n",
    "ax2.plot(np.nanmean(metri_va,axis=0), label='Validation ACC')\n",
    "ax2.set_title('ACC')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('acc')\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1496490377657321 10 -0.1774735952122481 2\n",
      "-0.014039818243660651 1 -0.04453386507004695 0\n",
      "0.25750504777282646 10 -0.3404276573540068 4\n",
      "Brier Skill score std\n",
      "validation BSS-std:  0.18194772760421027\n",
      "validation BSS-std:  0.040803453723589386\n",
      "validation BSS-std:  0.4881124483244908\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(bss_tr1), np.sum(np.array(bss_tr1) >= 0, axis=0), np.mean(bss_va1), np.sum(np.array(bss_va1) >= 0, axis=0))\n",
    "print(np.mean(bss_tr3), np.sum(np.array(bss_tr3) >= 0, axis=0), np.mean(bss_va3), np.sum(np.array(bss_va3) >= 0, axis=0))\n",
    "print(np.mean(bss_tr4), np.sum(np.array(bss_tr4) >= 0, axis=0), np.mean(bss_va4), np.sum(np.array(bss_va4) >= 0, axis=0))\n",
    "print('Brier Skill score std')\n",
    "print('validation BSS-std: ',np.std(bss_va1,ddof=1))\n",
    "print('validation BSS-std: ',np.std(bss_va3,ddof=1))\n",
    "print('validation BSS-std: ',np.std(bss_va4,ddof=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall\n",
      "train:  0.5157282741738066 validation:  0.3168604651162791\n",
      "precision\n",
      "train:  0.5157282741738066 validation:  0.3168604651162791\n",
      "F1-score\n",
      "train:  0.5157282741738066 validation:  0.3168604651162791\n",
      "accuracy\n",
      "train:  0.5157282741738066 validation:  0.3168604651162791\n",
      "Brier Skill\n",
      "Brier Skill score\n",
      "train:  0.25750504777282646 validation:  -0.3404276573540068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/cfc/s2s/s2s_ai_challenge/conda/envs/s2sai_2020/lib/python3.7/site-packages/ipykernel_launcher.py:41: RuntimeWarning: Mean of empty slice\n",
      "/net/cfc/s2s/s2s_ai_challenge/conda/envs/s2sai_2020/lib/python3.7/site-packages/ipykernel_launcher.py:42: RuntimeWarning: Mean of empty slice\n",
      "/net/cfc/s2s/s2s_ai_challenge/conda/envs/s2sai_2020/lib/python3.7/site-packages/ipykernel_launcher.py:56: RuntimeWarning: Mean of empty slice\n",
      "/net/cfc/s2s/s2s_ai_challenge/conda/envs/s2sai_2020/lib/python3.7/site-packages/ipykernel_launcher.py:57: RuntimeWarning: Mean of empty slice\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# mean of all cross validation output\n",
    "\n",
    "recall_tr = recall_tr1\n",
    "recall_va = recall_va1\n",
    "precision_tr = precision_tr1\n",
    "precision_va = precision_va1\n",
    "f1_tr = f1_tr1\n",
    "f1_va = f1_va1\n",
    "acc_tr = acc_tr1\n",
    "acc_va = acc_va1\n",
    "bss_tr = bss_tr4\n",
    "bss_va = bss_va4\n",
    "calib_y_tr = calib_y_tr4\n",
    "calib_x_tr = calib_x_tr4\n",
    "calib_y_va = calib_y_va4\n",
    "calib_x_va = calib_x_va4\n",
    "\n",
    "print('recall')\n",
    "print('train: ',np.nanmean(recall_tr), 'validation: ',np.nanmean(recall_va))\n",
    "print('precision')\n",
    "print('train: ',np.nanmean(precision_tr), 'validation: ',np.nanmean(precision_va))\n",
    "print('F1-score')\n",
    "print('train: ',np.nanmean(f1_tr), 'validation: ',np.nanmean(f1_va))\n",
    "print('accuracy')\n",
    "print('train: ',np.nanmean(acc_tr), 'validation: ',np.nanmean(acc_va))\n",
    "print('Brier Skill')\n",
    "#print('train: ',np.nanmean(bs_tr), 'validation: ',np.nanmean(bs_va))\n",
    "print('Brier Skill score')\n",
    "print('train: ',np.nanmean(bss_tr), 'validation: ',np.nanmean(bss_va))\n",
    "\n",
    "calib_y = []\n",
    "calib_x = []\n",
    "for ii in range(len(calib_y_tr)):\n",
    "    ytmp = list(calib_y_tr[ii])\n",
    "    xtmp = list(calib_x_tr[ii])\n",
    "    while len(ytmp) < nbins:\n",
    "        ytmp.append(np.nan)\n",
    "        xtmp.append(np.nan)\n",
    "    calib_y.append(ytmp)\n",
    "    calib_x.append(xtmp)\n",
    "calib_y = np.nanmean(calib_y,axis=0)\n",
    "calib_x = np.nanmean(calib_x,axis=0)\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "visualization.plot_calibration_curve(calib_x, calib_y)\n",
    "# due the only one year data, the data point in calib_y and calib_x are not always equal to nbins=10\n",
    "calib_y = []\n",
    "calib_x = []\n",
    "for ii in range(len(calib_y_va)):\n",
    "    ytmp = list(calib_y_va[ii])\n",
    "    xtmp = list(calib_x_va[ii])\n",
    "    while len(ytmp) < nbins:\n",
    "        ytmp.append(np.nan)\n",
    "        xtmp.append(np.nan)\n",
    "    calib_y.append(ytmp)\n",
    "    calib_x.append(xtmp)\n",
    "calib_y = np.nanmean(calib_y,axis=0)\n",
    "calib_x = np.nanmean(calib_x,axis=0)\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "visualization.plot_calibration_curve(calib_x, calib_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall\n",
      "train:  0.5157282741738066 validation:  0.3168604651162791\n",
      "precision\n",
      "train:  0.5157282741738066 validation:  0.3168604651162791\n",
      "F1-score\n",
      "train:  0.5157282741738066 validation:  0.3168604651162791\n",
      "accuracy\n",
      "train:  0.5157282741738066 validation:  0.3168604651162791\n",
      "Brier Skill\n",
      "Brier Skill score\n",
      "train:  0.1496490377657321 validation:  -0.1774735952122481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/cfc/s2s/s2s_ai_challenge/conda/envs/s2sai_2020/lib/python3.7/site-packages/ipykernel_launcher.py:33: RuntimeWarning: Mean of empty slice\n",
      "/net/cfc/s2s/s2s_ai_challenge/conda/envs/s2sai_2020/lib/python3.7/site-packages/ipykernel_launcher.py:34: RuntimeWarning: Mean of empty slice\n",
      "/net/cfc/s2s/s2s_ai_challenge/conda/envs/s2sai_2020/lib/python3.7/site-packages/ipykernel_launcher.py:48: RuntimeWarning: Mean of empty slice\n",
      "/net/cfc/s2s/s2s_ai_challenge/conda/envs/s2sai_2020/lib/python3.7/site-packages/ipykernel_launcher.py:49: RuntimeWarning: Mean of empty slice\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc_tr = acc_tr1\n",
    "acc_va = acc_va1\n",
    "bss_tr = bss_tr1\n",
    "bss_va = bss_va1\n",
    "calib_y_tr = calib_y_tr1\n",
    "calib_x_tr = calib_x_tr1\n",
    "calib_y_va = calib_y_va1\n",
    "calib_x_va = calib_x_va1\n",
    "\n",
    "print('recall')\n",
    "print('train: ',np.nanmean(recall_tr), 'validation: ',np.nanmean(recall_va))\n",
    "print('precision')\n",
    "print('train: ',np.nanmean(precision_tr), 'validation: ',np.nanmean(precision_va))\n",
    "print('F1-score')\n",
    "print('train: ',np.nanmean(f1_tr), 'validation: ',np.nanmean(f1_va))\n",
    "print('accuracy')\n",
    "print('train: ',np.nanmean(acc_tr), 'validation: ',np.nanmean(acc_va))\n",
    "print('Brier Skill')\n",
    "#print('train: ',np.nanmean(bs_tr), 'validation: ',np.nanmean(bs_va))\n",
    "print('Brier Skill score')\n",
    "print('train: ',np.nanmean(bss_tr), 'validation: ',np.nanmean(bss_va))\n",
    "\n",
    "calib_y = []\n",
    "calib_x = []\n",
    "for ii in range(len(calib_y_tr)):\n",
    "    ytmp = list(calib_y_tr[ii])\n",
    "    xtmp = list(calib_x_tr[ii])\n",
    "    while len(ytmp) < nbins:\n",
    "        ytmp.append(np.nan)\n",
    "        xtmp.append(np.nan)\n",
    "    calib_y.append(ytmp)\n",
    "    calib_x.append(xtmp)\n",
    "calib_y = np.nanmean(calib_y,axis=0)\n",
    "calib_x = np.nanmean(calib_x,axis=0)\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "visualization.plot_calibration_curve(calib_x, calib_y)\n",
    "# due the only one year data, the data point in calib_y and calib_x are not always equal to nbins=10\n",
    "calib_y = []\n",
    "calib_x = []\n",
    "for ii in range(len(calib_y_va)):\n",
    "    ytmp = list(calib_y_va[ii])\n",
    "    xtmp = list(calib_x_va[ii])\n",
    "    while len(ytmp) < nbins:\n",
    "        ytmp.append(np.nan)\n",
    "        xtmp.append(np.nan)\n",
    "    calib_y.append(ytmp)\n",
    "    calib_x.append(xtmp)\n",
    "calib_y = np.nanmean(calib_y,axis=0)\n",
    "calib_x = np.nanmean(calib_x,axis=0)\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "visualization.plot_calibration_curve(calib_x, calib_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lat lon exist\n",
      "<xarray.Dataset>\n",
      "Dimensions:    (lat: 23, lon: 23, time: 14610)\n",
      "Coordinates:\n",
      "  * lat        (lat) float32 30.0 28.0 26.0 24.0 22.0 ... -8.0 -10.0 -12.0 -14.0\n",
      "  * lon        (lon) float32 16.0 18.0 20.0 22.0 24.0 ... 54.0 56.0 58.0 60.0\n",
      "  * time       (time) datetime64[ns] 1981-01-01 1981-01-02 ... 2020-12-31\n",
      "    dayofyear  (time) int64 1 2 3 4 5 6 7 8 ... 359 360 361 362 363 364 365 366\n",
      "Data variables:\n",
      "    t2m        (time, lat, lon) float32 nan nan nan nan nan ... nan nan nan nan\n",
      "starting\n",
      "start target date 4 10 1981\n",
      "end target 28 12 2020\n",
      "1981.10.04 1981.07.17 1981.09.14 0\n",
      "1981.10.05 1981.07.18 1981.09.15 1\n",
      "1981.10.06 1981.07.19 1981.09.16 1\n",
      "1981.10.07 1981.07.20 1981.09.17 1\n",
      "1981.10.08 1981.07.21 1981.09.18 1\n",
      "1981.10.09 1981.07.22 1981.09.19 1\n",
      "1981.10.10 1981.07.23 1981.09.20 1\n",
      "1981.10.11 1981.07.24 1981.09.21 1\n",
      "1981.10.12 1981.07.25 1981.09.22 1\n",
      "1981.10.13 1981.07.26 1981.09.23 1\n",
      "1981.10.14 1981.07.27 1981.09.24 1\n",
      "1981.10.15 1981.07.28 1981.09.25 1\n",
      "1981.10.16 1981.07.29 1981.09.26 1\n",
      "1981.10.17 1981.07.30 1981.09.27 1\n",
      "1981.10.18 1981.07.31 1981.09.28 1\n",
      "1981.10.19 1981.08.01 1981.09.29 1\n",
      "1981.10.20 1981.08.02 1981.09.30 1\n",
      "1981.10.21 1981.08.03 1981.10.01 1\n",
      "1981.10.22 1981.08.04 1981.10.02 1\n",
      "1981.10.23 1981.08.05 1981.10.03 1\n",
      "1981.10.24 1981.08.06 1981.10.04 1\n",
      "1981.10.25 1981.08.07 1981.10.05 1\n",
      "1981.10.26 1981.08.08 1981.10.06 1\n",
      "1981.10.27 1981.08.09 1981.10.07 1\n",
      "1981.10.28 1981.08.10 1981.10.08 1\n",
      "1981.10.29 1981.08.11 1981.10.09 1\n",
      "1981.10.30 1981.08.12 1981.10.10 1\n",
      "1981.10.31 1981.08.13 1981.10.11 1\n",
      "1981.11.01 1981.08.14 1981.10.12 1\n",
      "1981.11.02 1981.08.15 1981.10.13 1\n",
      "1981.11.03 1981.08.16 1981.10.14 1\n",
      "1981.11.04 1981.08.17 1981.10.15 1\n",
      "1981.11.05 1981.08.18 1981.10.16 1\n",
      "1981.11.06 1981.08.19 1981.10.17 1\n",
      "1981.11.07 1981.08.20 1981.10.18 1\n",
      "1981.11.08 1981.08.21 1981.10.19 1\n",
      "1981.11.09 1981.08.22 1981.10.20 1\n",
      "1981.11.10 1981.08.23 1981.10.21 1\n",
      "1981.11.11 1981.08.24 1981.10.22 1\n",
      "1981.11.12 1981.08.25 1981.10.23 1\n",
      "1981.11.13 1981.08.26 1981.10.24 1\n",
      "1981.11.14 1981.08.27 1981.10.25 1\n",
      "1981.11.15 1981.08.28 1981.10.26 1\n",
      "1981.11.16 1981.08.29 1981.10.27 1\n",
      "1981.11.17 1981.08.30 1981.10.28 1\n",
      "1981.11.18 1981.08.31 1981.10.29 1\n",
      "1981.11.19 1981.09.01 1981.10.30 1\n",
      "1981.11.20 1981.09.02 1981.10.31 1\n",
      "1981.11.21 1981.09.03 1981.11.01 1\n",
      "1981.11.22 1981.09.04 1981.11.02 1\n",
      "1981.11.23 1981.09.05 1981.11.03 1\n",
      "1981.11.24 1981.09.06 1981.11.04 1\n",
      "1981.11.25 1981.09.07 1981.11.05 1\n",
      "1981.11.26 1981.09.08 1981.11.06 1\n",
      "1981.11.27 1981.09.09 1981.11.07 1\n",
      "1981.11.28 1981.09.10 1981.11.08 1\n",
      "1981.11.29 1981.09.11 1981.11.09 1\n",
      "1981.11.30 1981.09.12 1981.11.10 1\n",
      "1981.12.01 1981.09.13 1981.11.11 1\n",
      "1981.12.02 1981.09.14 1981.11.12 1\n",
      "1981.12.03 1981.09.15 1981.11.13 1\n",
      "1981.12.04 1981.09.16 1981.11.14 1\n",
      "1981.12.05 1981.09.17 1981.11.15 1\n",
      "1981.12.06 1981.09.18 1981.11.16 1\n",
      "1981.12.07 1981.09.19 1981.11.17 1\n",
      "1981.12.08 1981.09.20 1981.11.18 1\n",
      "1981.12.09 1981.09.21 1981.11.19 1\n",
      "1981.12.10 1981.09.22 1981.11.20 1\n",
      "1981.12.11 1981.09.23 1981.11.21 1\n",
      "1981.12.12 1981.09.24 1981.11.22 1\n",
      "1981.12.13 1981.09.25 1981.11.23 1\n",
      "1981.12.14 1981.09.26 1981.11.24 1\n",
      "1981.12.15 1981.09.27 1981.11.25 1\n",
      "1981.12.16 1981.09.28 1981.11.26 1\n",
      "1981.12.17 1981.09.29 1981.11.27 1\n",
      "1981.12.18 1981.09.30 1981.11.28 1\n",
      "1981.12.19 1981.10.01 1981.11.29 1\n",
      "1981.12.20 1981.10.02 1981.11.30 1\n",
      "1981.12.21 1981.10.03 1981.12.01 1\n",
      "1981.12.22 1981.10.04 1981.12.02 1\n",
      "1981.12.23 1981.10.05 1981.12.03 1\n",
      "1981.12.24 1981.10.06 1981.12.04 1\n",
      "1981.12.25 1981.10.07 1981.12.05 1\n",
      "1981.12.26 1981.10.08 1981.12.06 1\n",
      "1981.12.27 1981.10.09 1981.12.07 1\n",
      "1981.12.28 1981.10.10 1981.12.08 1\n",
      "1982.10.04 1982.07.17 1982.09.14 0\n",
      "1982.10.05 1982.07.18 1982.09.15 1\n",
      "1982.10.06 1982.07.19 1982.09.16 1\n",
      "1982.10.07 1982.07.20 1982.09.17 1\n",
      "1982.10.08 1982.07.21 1982.09.18 1\n",
      "1982.10.09 1982.07.22 1982.09.19 1\n",
      "1982.10.10 1982.07.23 1982.09.20 1\n",
      "1982.10.11 1982.07.24 1982.09.21 1\n",
      "1982.10.12 1982.07.25 1982.09.22 1\n",
      "1982.10.13 1982.07.26 1982.09.23 1\n",
      "1982.10.14 1982.07.27 1982.09.24 1\n",
      "1982.10.15 1982.07.28 1982.09.25 1\n",
      "1982.10.16 1982.07.29 1982.09.26 1\n",
      "1982.10.17 1982.07.30 1982.09.27 1\n",
      "1982.10.18 1982.07.31 1982.09.28 1\n",
      "1982.10.19 1982.08.01 1982.09.29 1\n",
      "1982.10.20 1982.08.02 1982.09.30 1\n",
      "1982.10.21 1982.08.03 1982.10.01 1\n",
      "1982.10.22 1982.08.04 1982.10.02 1\n",
      "1982.10.23 1982.08.05 1982.10.03 1\n",
      "1982.10.24 1982.08.06 1982.10.04 1\n",
      "1982.10.25 1982.08.07 1982.10.05 1\n",
      "1982.10.26 1982.08.08 1982.10.06 1\n",
      "1982.10.27 1982.08.09 1982.10.07 1\n",
      "1982.10.28 1982.08.10 1982.10.08 1\n",
      "1982.10.29 1982.08.11 1982.10.09 1\n",
      "1982.10.30 1982.08.12 1982.10.10 1\n",
      "1982.10.31 1982.08.13 1982.10.11 1\n",
      "1982.11.01 1982.08.14 1982.10.12 1\n",
      "1982.11.02 1982.08.15 1982.10.13 1\n",
      "1982.11.03 1982.08.16 1982.10.14 1\n",
      "1982.11.04 1982.08.17 1982.10.15 1\n",
      "1982.11.05 1982.08.18 1982.10.16 1\n",
      "1982.11.06 1982.08.19 1982.10.17 1\n",
      "1982.11.07 1982.08.20 1982.10.18 1\n",
      "1982.11.08 1982.08.21 1982.10.19 1\n",
      "1982.11.09 1982.08.22 1982.10.20 1\n",
      "1982.11.10 1982.08.23 1982.10.21 1\n",
      "1982.11.11 1982.08.24 1982.10.22 1\n",
      "1982.11.12 1982.08.25 1982.10.23 1\n",
      "1982.11.13 1982.08.26 1982.10.24 1\n",
      "1982.11.14 1982.08.27 1982.10.25 1\n",
      "1982.11.15 1982.08.28 1982.10.26 1\n",
      "1982.11.16 1982.08.29 1982.10.27 1\n",
      "1982.11.17 1982.08.30 1982.10.28 1\n",
      "1982.11.18 1982.08.31 1982.10.29 1\n",
      "1982.11.19 1982.09.01 1982.10.30 1\n",
      "1982.11.20 1982.09.02 1982.10.31 1\n",
      "1982.11.21 1982.09.03 1982.11.01 1\n",
      "1982.11.22 1982.09.04 1982.11.02 1\n",
      "1982.11.23 1982.09.05 1982.11.03 1\n",
      "1982.11.24 1982.09.06 1982.11.04 1\n",
      "1982.11.25 1982.09.07 1982.11.05 1\n",
      "1982.11.26 1982.09.08 1982.11.06 1\n",
      "1982.11.27 1982.09.09 1982.11.07 1\n",
      "1982.11.28 1982.09.10 1982.11.08 1\n",
      "1982.11.29 1982.09.11 1982.11.09 1\n",
      "1982.11.30 1982.09.12 1982.11.10 1\n",
      "1982.12.01 1982.09.13 1982.11.11 1\n",
      "1982.12.02 1982.09.14 1982.11.12 1\n",
      "1982.12.03 1982.09.15 1982.11.13 1\n",
      "1982.12.04 1982.09.16 1982.11.14 1\n",
      "1982.12.05 1982.09.17 1982.11.15 1\n",
      "1982.12.06 1982.09.18 1982.11.16 1\n",
      "1982.12.07 1982.09.19 1982.11.17 1\n",
      "1982.12.08 1982.09.20 1982.11.18 1\n",
      "1982.12.09 1982.09.21 1982.11.19 1\n",
      "1982.12.10 1982.09.22 1982.11.20 1\n",
      "1982.12.11 1982.09.23 1982.11.21 1\n",
      "1982.12.12 1982.09.24 1982.11.22 1\n",
      "1982.12.13 1982.09.25 1982.11.23 1\n",
      "1982.12.14 1982.09.26 1982.11.24 1\n",
      "1982.12.15 1982.09.27 1982.11.25 1\n",
      "1982.12.16 1982.09.28 1982.11.26 1\n",
      "1982.12.17 1982.09.29 1982.11.27 1\n",
      "1982.12.18 1982.09.30 1982.11.28 1\n",
      "1982.12.19 1982.10.01 1982.11.29 1\n",
      "1982.12.20 1982.10.02 1982.11.30 1\n",
      "1982.12.21 1982.10.03 1982.12.01 1\n",
      "1982.12.22 1982.10.04 1982.12.02 1\n",
      "1982.12.23 1982.10.05 1982.12.03 1\n",
      "1982.12.24 1982.10.06 1982.12.04 1\n",
      "1982.12.25 1982.10.07 1982.12.05 1\n",
      "1982.12.26 1982.10.08 1982.12.06 1\n",
      "1982.12.27 1982.10.09 1982.12.07 1\n",
      "1982.12.28 1982.10.10 1982.12.08 1\n",
      "1983.10.04 1983.07.17 1983.09.14 0\n",
      "1983.10.05 1983.07.18 1983.09.15 1\n",
      "1983.10.06 1983.07.19 1983.09.16 1\n",
      "1983.10.07 1983.07.20 1983.09.17 1\n",
      "1983.10.08 1983.07.21 1983.09.18 1\n",
      "1983.10.09 1983.07.22 1983.09.19 1\n",
      "1983.10.10 1983.07.23 1983.09.20 1\n",
      "1983.10.11 1983.07.24 1983.09.21 1\n",
      "1983.10.12 1983.07.25 1983.09.22 1\n",
      "1983.10.13 1983.07.26 1983.09.23 1\n",
      "1983.10.14 1983.07.27 1983.09.24 1\n",
      "1983.10.15 1983.07.28 1983.09.25 1\n",
      "1983.10.16 1983.07.29 1983.09.26 1\n",
      "1983.10.17 1983.07.30 1983.09.27 1\n",
      "1983.10.18 1983.07.31 1983.09.28 1\n",
      "1983.10.19 1983.08.01 1983.09.29 1\n",
      "1983.10.20 1983.08.02 1983.09.30 1\n",
      "1983.10.21 1983.08.03 1983.10.01 1\n",
      "1983.10.22 1983.08.04 1983.10.02 1\n",
      "1983.10.23 1983.08.05 1983.10.03 1\n",
      "1983.10.24 1983.08.06 1983.10.04 1\n",
      "1983.10.25 1983.08.07 1983.10.05 1\n",
      "1983.10.26 1983.08.08 1983.10.06 1\n",
      "1983.10.27 1983.08.09 1983.10.07 1\n",
      "1983.10.28 1983.08.10 1983.10.08 1\n",
      "1983.10.29 1983.08.11 1983.10.09 1\n",
      "1983.10.30 1983.08.12 1983.10.10 1\n",
      "1983.10.31 1983.08.13 1983.10.11 1\n",
      "1983.11.01 1983.08.14 1983.10.12 1\n",
      "1983.11.02 1983.08.15 1983.10.13 1\n",
      "1983.11.03 1983.08.16 1983.10.14 1\n",
      "1983.11.04 1983.08.17 1983.10.15 1\n",
      "1983.11.05 1983.08.18 1983.10.16 1\n",
      "1983.11.06 1983.08.19 1983.10.17 1\n",
      "1983.11.07 1983.08.20 1983.10.18 1\n",
      "1983.11.08 1983.08.21 1983.10.19 1\n",
      "1983.11.09 1983.08.22 1983.10.20 1\n",
      "1983.11.10 1983.08.23 1983.10.21 1\n",
      "1983.11.11 1983.08.24 1983.10.22 1\n",
      "1983.11.12 1983.08.25 1983.10.23 1\n",
      "1983.11.13 1983.08.26 1983.10.24 1\n",
      "1983.11.14 1983.08.27 1983.10.25 1\n",
      "1983.11.15 1983.08.28 1983.10.26 1\n",
      "1983.11.16 1983.08.29 1983.10.27 1\n",
      "1983.11.17 1983.08.30 1983.10.28 1\n",
      "1983.11.18 1983.08.31 1983.10.29 1\n",
      "1983.11.19 1983.09.01 1983.10.30 1\n",
      "1983.11.20 1983.09.02 1983.10.31 1\n",
      "1983.11.21 1983.09.03 1983.11.01 1\n",
      "1983.11.22 1983.09.04 1983.11.02 1\n",
      "1983.11.23 1983.09.05 1983.11.03 1\n",
      "1983.11.24 1983.09.06 1983.11.04 1\n",
      "1983.11.25 1983.09.07 1983.11.05 1\n",
      "1983.11.26 1983.09.08 1983.11.06 1\n",
      "1983.11.27 1983.09.09 1983.11.07 1\n",
      "1983.11.28 1983.09.10 1983.11.08 1\n",
      "1983.11.29 1983.09.11 1983.11.09 1\n",
      "1983.11.30 1983.09.12 1983.11.10 1\n",
      "1983.12.01 1983.09.13 1983.11.11 1\n",
      "1983.12.02 1983.09.14 1983.11.12 1\n",
      "1983.12.03 1983.09.15 1983.11.13 1\n",
      "1983.12.04 1983.09.16 1983.11.14 1\n",
      "1983.12.05 1983.09.17 1983.11.15 1\n",
      "1983.12.06 1983.09.18 1983.11.16 1\n",
      "1983.12.07 1983.09.19 1983.11.17 1\n",
      "1983.12.08 1983.09.20 1983.11.18 1\n",
      "1983.12.09 1983.09.21 1983.11.19 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1983.12.10 1983.09.22 1983.11.20 1\n",
      "1983.12.11 1983.09.23 1983.11.21 1\n",
      "1983.12.12 1983.09.24 1983.11.22 1\n",
      "1983.12.13 1983.09.25 1983.11.23 1\n",
      "1983.12.14 1983.09.26 1983.11.24 1\n",
      "1983.12.15 1983.09.27 1983.11.25 1\n",
      "1983.12.16 1983.09.28 1983.11.26 1\n",
      "1983.12.17 1983.09.29 1983.11.27 1\n",
      "1983.12.18 1983.09.30 1983.11.28 1\n",
      "1983.12.19 1983.10.01 1983.11.29 1\n",
      "1983.12.20 1983.10.02 1983.11.30 1\n",
      "1983.12.21 1983.10.03 1983.12.01 1\n",
      "1983.12.22 1983.10.04 1983.12.02 1\n",
      "1983.12.23 1983.10.05 1983.12.03 1\n",
      "1983.12.24 1983.10.06 1983.12.04 1\n",
      "1983.12.25 1983.10.07 1983.12.05 1\n",
      "1983.12.26 1983.10.08 1983.12.06 1\n",
      "1983.12.27 1983.10.09 1983.12.07 1\n",
      "1983.12.28 1983.10.10 1983.12.08 1\n",
      "1984.10.04 1984.07.17 1984.09.14 0\n",
      "1984.10.05 1984.07.18 1984.09.15 1\n",
      "1984.10.06 1984.07.19 1984.09.16 1\n",
      "1984.10.07 1984.07.20 1984.09.17 1\n",
      "1984.10.08 1984.07.21 1984.09.18 1\n",
      "1984.10.09 1984.07.22 1984.09.19 1\n",
      "1984.10.10 1984.07.23 1984.09.20 1\n",
      "1984.10.11 1984.07.24 1984.09.21 1\n",
      "1984.10.12 1984.07.25 1984.09.22 1\n",
      "1984.10.13 1984.07.26 1984.09.23 1\n",
      "1984.10.14 1984.07.27 1984.09.24 1\n",
      "1984.10.15 1984.07.28 1984.09.25 1\n",
      "1984.10.16 1984.07.29 1984.09.26 1\n",
      "1984.10.17 1984.07.30 1984.09.27 1\n",
      "1984.10.18 1984.07.31 1984.09.28 1\n",
      "1984.10.19 1984.08.01 1984.09.29 1\n",
      "1984.10.20 1984.08.02 1984.09.30 1\n",
      "1984.10.21 1984.08.03 1984.10.01 1\n",
      "1984.10.22 1984.08.04 1984.10.02 1\n",
      "1984.10.23 1984.08.05 1984.10.03 1\n",
      "1984.10.24 1984.08.06 1984.10.04 1\n",
      "1984.10.25 1984.08.07 1984.10.05 1\n",
      "1984.10.26 1984.08.08 1984.10.06 1\n",
      "1984.10.27 1984.08.09 1984.10.07 1\n",
      "1984.10.28 1984.08.10 1984.10.08 1\n",
      "1984.10.29 1984.08.11 1984.10.09 1\n",
      "1984.10.30 1984.08.12 1984.10.10 1\n",
      "1984.10.31 1984.08.13 1984.10.11 1\n",
      "1984.11.01 1984.08.14 1984.10.12 1\n",
      "1984.11.02 1984.08.15 1984.10.13 1\n",
      "1984.11.03 1984.08.16 1984.10.14 1\n",
      "1984.11.04 1984.08.17 1984.10.15 1\n",
      "1984.11.05 1984.08.18 1984.10.16 1\n",
      "1984.11.06 1984.08.19 1984.10.17 1\n",
      "1984.11.07 1984.08.20 1984.10.18 1\n",
      "1984.11.08 1984.08.21 1984.10.19 1\n",
      "1984.11.09 1984.08.22 1984.10.20 1\n",
      "1984.11.10 1984.08.23 1984.10.21 1\n",
      "1984.11.11 1984.08.24 1984.10.22 1\n",
      "1984.11.12 1984.08.25 1984.10.23 1\n",
      "1984.11.13 1984.08.26 1984.10.24 1\n",
      "1984.11.14 1984.08.27 1984.10.25 1\n",
      "1984.11.15 1984.08.28 1984.10.26 1\n",
      "1984.11.16 1984.08.29 1984.10.27 1\n",
      "1984.11.17 1984.08.30 1984.10.28 1\n",
      "1984.11.18 1984.08.31 1984.10.29 1\n",
      "1984.11.19 1984.09.01 1984.10.30 1\n",
      "1984.11.20 1984.09.02 1984.10.31 1\n",
      "1984.11.21 1984.09.03 1984.11.01 1\n",
      "1984.11.22 1984.09.04 1984.11.02 1\n",
      "1984.11.23 1984.09.05 1984.11.03 1\n",
      "1984.11.24 1984.09.06 1984.11.04 1\n",
      "1984.11.25 1984.09.07 1984.11.05 1\n",
      "1984.11.26 1984.09.08 1984.11.06 1\n",
      "1984.11.27 1984.09.09 1984.11.07 1\n",
      "1984.11.28 1984.09.10 1984.11.08 1\n",
      "1984.11.29 1984.09.11 1984.11.09 1\n",
      "1984.11.30 1984.09.12 1984.11.10 1\n",
      "1984.12.01 1984.09.13 1984.11.11 1\n",
      "1984.12.02 1984.09.14 1984.11.12 1\n",
      "1984.12.03 1984.09.15 1984.11.13 1\n",
      "1984.12.04 1984.09.16 1984.11.14 1\n",
      "1984.12.05 1984.09.17 1984.11.15 1\n",
      "1984.12.06 1984.09.18 1984.11.16 1\n",
      "1984.12.07 1984.09.19 1984.11.17 1\n",
      "1984.12.08 1984.09.20 1984.11.18 1\n",
      "1984.12.09 1984.09.21 1984.11.19 1\n",
      "1984.12.10 1984.09.22 1984.11.20 1\n",
      "1984.12.11 1984.09.23 1984.11.21 1\n",
      "1984.12.12 1984.09.24 1984.11.22 1\n",
      "1984.12.13 1984.09.25 1984.11.23 1\n",
      "1984.12.14 1984.09.26 1984.11.24 1\n",
      "1984.12.15 1984.09.27 1984.11.25 1\n",
      "1984.12.16 1984.09.28 1984.11.26 1\n",
      "1984.12.17 1984.09.29 1984.11.27 1\n",
      "1984.12.18 1984.09.30 1984.11.28 1\n",
      "1984.12.19 1984.10.01 1984.11.29 1\n",
      "1984.12.20 1984.10.02 1984.11.30 1\n",
      "1984.12.21 1984.10.03 1984.12.01 1\n",
      "1984.12.22 1984.10.04 1984.12.02 1\n",
      "1984.12.23 1984.10.05 1984.12.03 1\n",
      "1984.12.24 1984.10.06 1984.12.04 1\n",
      "1984.12.25 1984.10.07 1984.12.05 1\n",
      "1984.12.26 1984.10.08 1984.12.06 1\n",
      "1984.12.27 1984.10.09 1984.12.07 1\n",
      "1984.12.28 1984.10.10 1984.12.08 1\n",
      "1985.10.04 1985.07.17 1985.09.14 0\n",
      "1985.10.05 1985.07.18 1985.09.15 1\n",
      "1985.10.06 1985.07.19 1985.09.16 1\n",
      "1985.10.07 1985.07.20 1985.09.17 1\n",
      "1985.10.08 1985.07.21 1985.09.18 1\n",
      "1985.10.09 1985.07.22 1985.09.19 1\n",
      "1985.10.10 1985.07.23 1985.09.20 1\n",
      "1985.10.11 1985.07.24 1985.09.21 1\n",
      "1985.10.12 1985.07.25 1985.09.22 1\n",
      "1985.10.13 1985.07.26 1985.09.23 1\n",
      "1985.10.14 1985.07.27 1985.09.24 1\n",
      "1985.10.15 1985.07.28 1985.09.25 1\n",
      "1985.10.16 1985.07.29 1985.09.26 1\n",
      "1985.10.17 1985.07.30 1985.09.27 1\n",
      "1985.10.18 1985.07.31 1985.09.28 1\n",
      "1985.10.19 1985.08.01 1985.09.29 1\n",
      "1985.10.20 1985.08.02 1985.09.30 1\n",
      "1985.10.21 1985.08.03 1985.10.01 1\n",
      "1985.10.22 1985.08.04 1985.10.02 1\n",
      "1985.10.23 1985.08.05 1985.10.03 1\n",
      "1985.10.24 1985.08.06 1985.10.04 1\n",
      "1985.10.25 1985.08.07 1985.10.05 1\n",
      "1985.10.26 1985.08.08 1985.10.06 1\n",
      "1985.10.27 1985.08.09 1985.10.07 1\n",
      "1985.10.28 1985.08.10 1985.10.08 1\n",
      "1985.10.29 1985.08.11 1985.10.09 1\n",
      "1985.10.30 1985.08.12 1985.10.10 1\n",
      "1985.10.31 1985.08.13 1985.10.11 1\n",
      "1985.11.01 1985.08.14 1985.10.12 1\n",
      "1985.11.02 1985.08.15 1985.10.13 1\n",
      "1985.11.03 1985.08.16 1985.10.14 1\n",
      "1985.11.04 1985.08.17 1985.10.15 1\n",
      "1985.11.05 1985.08.18 1985.10.16 1\n",
      "1985.11.06 1985.08.19 1985.10.17 1\n",
      "1985.11.07 1985.08.20 1985.10.18 1\n",
      "1985.11.08 1985.08.21 1985.10.19 1\n",
      "1985.11.09 1985.08.22 1985.10.20 1\n",
      "1985.11.10 1985.08.23 1985.10.21 1\n",
      "1985.11.11 1985.08.24 1985.10.22 1\n",
      "1985.11.12 1985.08.25 1985.10.23 1\n",
      "1985.11.13 1985.08.26 1985.10.24 1\n",
      "1985.11.14 1985.08.27 1985.10.25 1\n",
      "1985.11.15 1985.08.28 1985.10.26 1\n",
      "1985.11.16 1985.08.29 1985.10.27 1\n",
      "1985.11.17 1985.08.30 1985.10.28 1\n",
      "1985.11.18 1985.08.31 1985.10.29 1\n",
      "1985.11.19 1985.09.01 1985.10.30 1\n",
      "1985.11.20 1985.09.02 1985.10.31 1\n",
      "1985.11.21 1985.09.03 1985.11.01 1\n",
      "1985.11.22 1985.09.04 1985.11.02 1\n",
      "1985.11.23 1985.09.05 1985.11.03 1\n",
      "1985.11.24 1985.09.06 1985.11.04 1\n",
      "1985.11.25 1985.09.07 1985.11.05 1\n",
      "1985.11.26 1985.09.08 1985.11.06 1\n",
      "1985.11.27 1985.09.09 1985.11.07 1\n",
      "1985.11.28 1985.09.10 1985.11.08 1\n",
      "1985.11.29 1985.09.11 1985.11.09 1\n",
      "1985.11.30 1985.09.12 1985.11.10 1\n",
      "1985.12.01 1985.09.13 1985.11.11 1\n",
      "1985.12.02 1985.09.14 1985.11.12 1\n",
      "1985.12.03 1985.09.15 1985.11.13 1\n",
      "1985.12.04 1985.09.16 1985.11.14 1\n",
      "1985.12.05 1985.09.17 1985.11.15 1\n",
      "1985.12.06 1985.09.18 1985.11.16 1\n",
      "1985.12.07 1985.09.19 1985.11.17 1\n",
      "1985.12.08 1985.09.20 1985.11.18 1\n",
      "1985.12.09 1985.09.21 1985.11.19 1\n",
      "1985.12.10 1985.09.22 1985.11.20 1\n",
      "1985.12.11 1985.09.23 1985.11.21 1\n",
      "1985.12.12 1985.09.24 1985.11.22 1\n",
      "1985.12.13 1985.09.25 1985.11.23 1\n",
      "1985.12.14 1985.09.26 1985.11.24 1\n",
      "1985.12.15 1985.09.27 1985.11.25 1\n",
      "1985.12.16 1985.09.28 1985.11.26 1\n",
      "1985.12.17 1985.09.29 1985.11.27 1\n",
      "1985.12.18 1985.09.30 1985.11.28 1\n",
      "1985.12.19 1985.10.01 1985.11.29 1\n",
      "1985.12.20 1985.10.02 1985.11.30 1\n",
      "1985.12.21 1985.10.03 1985.12.01 1\n",
      "1985.12.22 1985.10.04 1985.12.02 1\n",
      "1985.12.23 1985.10.05 1985.12.03 1\n",
      "1985.12.24 1985.10.06 1985.12.04 1\n",
      "1985.12.25 1985.10.07 1985.12.05 1\n",
      "1985.12.26 1985.10.08 1985.12.06 1\n",
      "1985.12.27 1985.10.09 1985.12.07 1\n",
      "1985.12.28 1985.10.10 1985.12.08 1\n",
      "1986.10.04 1986.07.17 1986.09.14 0\n",
      "1986.10.05 1986.07.18 1986.09.15 1\n",
      "1986.10.06 1986.07.19 1986.09.16 1\n",
      "1986.10.07 1986.07.20 1986.09.17 1\n",
      "1986.10.08 1986.07.21 1986.09.18 1\n",
      "1986.10.09 1986.07.22 1986.09.19 1\n",
      "1986.10.10 1986.07.23 1986.09.20 1\n",
      "1986.10.11 1986.07.24 1986.09.21 1\n",
      "1986.10.12 1986.07.25 1986.09.22 1\n",
      "1986.10.13 1986.07.26 1986.09.23 1\n",
      "1986.10.14 1986.07.27 1986.09.24 1\n",
      "1986.10.15 1986.07.28 1986.09.25 1\n",
      "1986.10.16 1986.07.29 1986.09.26 1\n",
      "1986.10.17 1986.07.30 1986.09.27 1\n",
      "1986.10.18 1986.07.31 1986.09.28 1\n",
      "1986.10.19 1986.08.01 1986.09.29 1\n",
      "1986.10.20 1986.08.02 1986.09.30 1\n",
      "1986.10.21 1986.08.03 1986.10.01 1\n",
      "1986.10.22 1986.08.04 1986.10.02 1\n",
      "1986.10.23 1986.08.05 1986.10.03 1\n",
      "1986.10.24 1986.08.06 1986.10.04 1\n",
      "1986.10.25 1986.08.07 1986.10.05 1\n",
      "1986.10.26 1986.08.08 1986.10.06 1\n",
      "1986.10.27 1986.08.09 1986.10.07 1\n",
      "1986.10.28 1986.08.10 1986.10.08 1\n",
      "1986.10.29 1986.08.11 1986.10.09 1\n",
      "1986.10.30 1986.08.12 1986.10.10 1\n",
      "1986.10.31 1986.08.13 1986.10.11 1\n",
      "1986.11.01 1986.08.14 1986.10.12 1\n",
      "1986.11.02 1986.08.15 1986.10.13 1\n",
      "1986.11.03 1986.08.16 1986.10.14 1\n",
      "1986.11.04 1986.08.17 1986.10.15 1\n",
      "1986.11.05 1986.08.18 1986.10.16 1\n",
      "1986.11.06 1986.08.19 1986.10.17 1\n",
      "1986.11.07 1986.08.20 1986.10.18 1\n",
      "1986.11.08 1986.08.21 1986.10.19 1\n",
      "1986.11.09 1986.08.22 1986.10.20 1\n",
      "1986.11.10 1986.08.23 1986.10.21 1\n",
      "1986.11.11 1986.08.24 1986.10.22 1\n",
      "1986.11.12 1986.08.25 1986.10.23 1\n",
      "1986.11.13 1986.08.26 1986.10.24 1\n",
      "1986.11.14 1986.08.27 1986.10.25 1\n",
      "1986.11.15 1986.08.28 1986.10.26 1\n",
      "1986.11.16 1986.08.29 1986.10.27 1\n",
      "1986.11.17 1986.08.30 1986.10.28 1\n",
      "1986.11.18 1986.08.31 1986.10.29 1\n",
      "1986.11.19 1986.09.01 1986.10.30 1\n",
      "1986.11.20 1986.09.02 1986.10.31 1\n",
      "1986.11.21 1986.09.03 1986.11.01 1\n",
      "1986.11.22 1986.09.04 1986.11.02 1\n",
      "1986.11.23 1986.09.05 1986.11.03 1\n",
      "1986.11.24 1986.09.06 1986.11.04 1\n",
      "1986.11.25 1986.09.07 1986.11.05 1\n",
      "1986.11.26 1986.09.08 1986.11.06 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1986.11.27 1986.09.09 1986.11.07 1\n",
      "1986.11.28 1986.09.10 1986.11.08 1\n",
      "1986.11.29 1986.09.11 1986.11.09 1\n",
      "1986.11.30 1986.09.12 1986.11.10 1\n",
      "1986.12.01 1986.09.13 1986.11.11 1\n",
      "1986.12.02 1986.09.14 1986.11.12 1\n",
      "1986.12.03 1986.09.15 1986.11.13 1\n",
      "1986.12.04 1986.09.16 1986.11.14 1\n",
      "1986.12.05 1986.09.17 1986.11.15 1\n",
      "1986.12.06 1986.09.18 1986.11.16 1\n",
      "1986.12.07 1986.09.19 1986.11.17 1\n",
      "1986.12.08 1986.09.20 1986.11.18 1\n",
      "1986.12.09 1986.09.21 1986.11.19 1\n",
      "1986.12.10 1986.09.22 1986.11.20 1\n",
      "1986.12.11 1986.09.23 1986.11.21 1\n",
      "1986.12.12 1986.09.24 1986.11.22 1\n",
      "1986.12.13 1986.09.25 1986.11.23 1\n",
      "1986.12.14 1986.09.26 1986.11.24 1\n",
      "1986.12.15 1986.09.27 1986.11.25 1\n",
      "1986.12.16 1986.09.28 1986.11.26 1\n",
      "1986.12.17 1986.09.29 1986.11.27 1\n",
      "1986.12.18 1986.09.30 1986.11.28 1\n",
      "1986.12.19 1986.10.01 1986.11.29 1\n",
      "1986.12.20 1986.10.02 1986.11.30 1\n",
      "1986.12.21 1986.10.03 1986.12.01 1\n",
      "1986.12.22 1986.10.04 1986.12.02 1\n",
      "1986.12.23 1986.10.05 1986.12.03 1\n",
      "1986.12.24 1986.10.06 1986.12.04 1\n",
      "1986.12.25 1986.10.07 1986.12.05 1\n",
      "1986.12.26 1986.10.08 1986.12.06 1\n",
      "1986.12.27 1986.10.09 1986.12.07 1\n",
      "1986.12.28 1986.10.10 1986.12.08 1\n",
      "1987.10.04 1987.07.17 1987.09.14 0\n",
      "1987.10.05 1987.07.18 1987.09.15 1\n",
      "1987.10.06 1987.07.19 1987.09.16 1\n",
      "1987.10.07 1987.07.20 1987.09.17 1\n",
      "1987.10.08 1987.07.21 1987.09.18 1\n",
      "1987.10.09 1987.07.22 1987.09.19 1\n",
      "1987.10.10 1987.07.23 1987.09.20 1\n",
      "1987.10.11 1987.07.24 1987.09.21 1\n",
      "1987.10.12 1987.07.25 1987.09.22 1\n",
      "1987.10.13 1987.07.26 1987.09.23 1\n",
      "1987.10.14 1987.07.27 1987.09.24 1\n",
      "1987.10.15 1987.07.28 1987.09.25 1\n",
      "1987.10.16 1987.07.29 1987.09.26 1\n",
      "1987.10.17 1987.07.30 1987.09.27 1\n",
      "1987.10.18 1987.07.31 1987.09.28 1\n",
      "1987.10.19 1987.08.01 1987.09.29 1\n",
      "1987.10.20 1987.08.02 1987.09.30 1\n",
      "1987.10.21 1987.08.03 1987.10.01 1\n",
      "1987.10.22 1987.08.04 1987.10.02 1\n",
      "1987.10.23 1987.08.05 1987.10.03 1\n",
      "1987.10.24 1987.08.06 1987.10.04 1\n",
      "1987.10.25 1987.08.07 1987.10.05 1\n",
      "1987.10.26 1987.08.08 1987.10.06 1\n",
      "1987.10.27 1987.08.09 1987.10.07 1\n",
      "1987.10.28 1987.08.10 1987.10.08 1\n",
      "1987.10.29 1987.08.11 1987.10.09 1\n",
      "1987.10.30 1987.08.12 1987.10.10 1\n",
      "1987.10.31 1987.08.13 1987.10.11 1\n",
      "1987.11.01 1987.08.14 1987.10.12 1\n",
      "1987.11.02 1987.08.15 1987.10.13 1\n",
      "1987.11.03 1987.08.16 1987.10.14 1\n",
      "1987.11.04 1987.08.17 1987.10.15 1\n",
      "1987.11.05 1987.08.18 1987.10.16 1\n",
      "1987.11.06 1987.08.19 1987.10.17 1\n",
      "1987.11.07 1987.08.20 1987.10.18 1\n",
      "1987.11.08 1987.08.21 1987.10.19 1\n",
      "1987.11.09 1987.08.22 1987.10.20 1\n",
      "1987.11.10 1987.08.23 1987.10.21 1\n",
      "1987.11.11 1987.08.24 1987.10.22 1\n",
      "1987.11.12 1987.08.25 1987.10.23 1\n",
      "1987.11.13 1987.08.26 1987.10.24 1\n",
      "1987.11.14 1987.08.27 1987.10.25 1\n",
      "1987.11.15 1987.08.28 1987.10.26 1\n",
      "1987.11.16 1987.08.29 1987.10.27 1\n",
      "1987.11.17 1987.08.30 1987.10.28 1\n",
      "1987.11.18 1987.08.31 1987.10.29 1\n",
      "1987.11.19 1987.09.01 1987.10.30 1\n",
      "1987.11.20 1987.09.02 1987.10.31 1\n",
      "1987.11.21 1987.09.03 1987.11.01 1\n",
      "1987.11.22 1987.09.04 1987.11.02 1\n",
      "1987.11.23 1987.09.05 1987.11.03 1\n",
      "1987.11.24 1987.09.06 1987.11.04 1\n",
      "1987.11.25 1987.09.07 1987.11.05 1\n",
      "1987.11.26 1987.09.08 1987.11.06 1\n",
      "1987.11.27 1987.09.09 1987.11.07 1\n",
      "1987.11.28 1987.09.10 1987.11.08 1\n",
      "1987.11.29 1987.09.11 1987.11.09 1\n",
      "1987.11.30 1987.09.12 1987.11.10 1\n",
      "1987.12.01 1987.09.13 1987.11.11 1\n",
      "1987.12.02 1987.09.14 1987.11.12 1\n",
      "1987.12.03 1987.09.15 1987.11.13 1\n",
      "1987.12.04 1987.09.16 1987.11.14 1\n",
      "1987.12.05 1987.09.17 1987.11.15 1\n",
      "1987.12.06 1987.09.18 1987.11.16 1\n",
      "1987.12.07 1987.09.19 1987.11.17 1\n",
      "1987.12.08 1987.09.20 1987.11.18 1\n",
      "1987.12.09 1987.09.21 1987.11.19 1\n",
      "1987.12.10 1987.09.22 1987.11.20 1\n",
      "1987.12.11 1987.09.23 1987.11.21 1\n",
      "1987.12.12 1987.09.24 1987.11.22 1\n",
      "1987.12.13 1987.09.25 1987.11.23 1\n",
      "1987.12.14 1987.09.26 1987.11.24 1\n",
      "1987.12.15 1987.09.27 1987.11.25 1\n",
      "1987.12.16 1987.09.28 1987.11.26 1\n",
      "1987.12.17 1987.09.29 1987.11.27 1\n",
      "1987.12.18 1987.09.30 1987.11.28 1\n",
      "1987.12.19 1987.10.01 1987.11.29 1\n",
      "1987.12.20 1987.10.02 1987.11.30 1\n",
      "1987.12.21 1987.10.03 1987.12.01 1\n",
      "1987.12.22 1987.10.04 1987.12.02 1\n",
      "1987.12.23 1987.10.05 1987.12.03 1\n",
      "1987.12.24 1987.10.06 1987.12.04 1\n",
      "1987.12.25 1987.10.07 1987.12.05 1\n",
      "1987.12.26 1987.10.08 1987.12.06 1\n",
      "1987.12.27 1987.10.09 1987.12.07 1\n",
      "1987.12.28 1987.10.10 1987.12.08 1\n",
      "1988.10.04 1988.07.17 1988.09.14 0\n",
      "1988.10.05 1988.07.18 1988.09.15 1\n",
      "1988.10.06 1988.07.19 1988.09.16 1\n",
      "1988.10.07 1988.07.20 1988.09.17 1\n",
      "1988.10.08 1988.07.21 1988.09.18 1\n",
      "1988.10.09 1988.07.22 1988.09.19 1\n",
      "1988.10.10 1988.07.23 1988.09.20 1\n",
      "1988.10.11 1988.07.24 1988.09.21 1\n",
      "1988.10.12 1988.07.25 1988.09.22 1\n",
      "1988.10.13 1988.07.26 1988.09.23 1\n",
      "1988.10.14 1988.07.27 1988.09.24 1\n",
      "1988.10.15 1988.07.28 1988.09.25 1\n",
      "1988.10.16 1988.07.29 1988.09.26 1\n",
      "1988.10.17 1988.07.30 1988.09.27 1\n",
      "1988.10.18 1988.07.31 1988.09.28 1\n",
      "1988.10.19 1988.08.01 1988.09.29 1\n",
      "1988.10.20 1988.08.02 1988.09.30 1\n",
      "1988.10.21 1988.08.03 1988.10.01 1\n",
      "1988.10.22 1988.08.04 1988.10.02 1\n",
      "1988.10.23 1988.08.05 1988.10.03 1\n",
      "1988.10.24 1988.08.06 1988.10.04 1\n",
      "1988.10.25 1988.08.07 1988.10.05 1\n",
      "1988.10.26 1988.08.08 1988.10.06 1\n",
      "1988.10.27 1988.08.09 1988.10.07 1\n",
      "1988.10.28 1988.08.10 1988.10.08 1\n",
      "1988.10.29 1988.08.11 1988.10.09 1\n",
      "1988.10.30 1988.08.12 1988.10.10 1\n",
      "1988.10.31 1988.08.13 1988.10.11 1\n",
      "1988.11.01 1988.08.14 1988.10.12 1\n",
      "1988.11.02 1988.08.15 1988.10.13 1\n",
      "1988.11.03 1988.08.16 1988.10.14 1\n",
      "1988.11.04 1988.08.17 1988.10.15 1\n",
      "1988.11.05 1988.08.18 1988.10.16 1\n",
      "1988.11.06 1988.08.19 1988.10.17 1\n",
      "1988.11.07 1988.08.20 1988.10.18 1\n",
      "1988.11.08 1988.08.21 1988.10.19 1\n",
      "1988.11.09 1988.08.22 1988.10.20 1\n",
      "1988.11.10 1988.08.23 1988.10.21 1\n",
      "1988.11.11 1988.08.24 1988.10.22 1\n",
      "1988.11.12 1988.08.25 1988.10.23 1\n",
      "1988.11.13 1988.08.26 1988.10.24 1\n",
      "1988.11.14 1988.08.27 1988.10.25 1\n",
      "1988.11.15 1988.08.28 1988.10.26 1\n",
      "1988.11.16 1988.08.29 1988.10.27 1\n",
      "1988.11.17 1988.08.30 1988.10.28 1\n",
      "1988.11.18 1988.08.31 1988.10.29 1\n",
      "1988.11.19 1988.09.01 1988.10.30 1\n",
      "1988.11.20 1988.09.02 1988.10.31 1\n",
      "1988.11.21 1988.09.03 1988.11.01 1\n",
      "1988.11.22 1988.09.04 1988.11.02 1\n",
      "1988.11.23 1988.09.05 1988.11.03 1\n",
      "1988.11.24 1988.09.06 1988.11.04 1\n",
      "1988.11.25 1988.09.07 1988.11.05 1\n",
      "1988.11.26 1988.09.08 1988.11.06 1\n",
      "1988.11.27 1988.09.09 1988.11.07 1\n",
      "1988.11.28 1988.09.10 1988.11.08 1\n",
      "1988.11.29 1988.09.11 1988.11.09 1\n",
      "1988.11.30 1988.09.12 1988.11.10 1\n",
      "1988.12.01 1988.09.13 1988.11.11 1\n",
      "1988.12.02 1988.09.14 1988.11.12 1\n",
      "1988.12.03 1988.09.15 1988.11.13 1\n",
      "1988.12.04 1988.09.16 1988.11.14 1\n",
      "1988.12.05 1988.09.17 1988.11.15 1\n",
      "1988.12.06 1988.09.18 1988.11.16 1\n",
      "1988.12.07 1988.09.19 1988.11.17 1\n",
      "1988.12.08 1988.09.20 1988.11.18 1\n",
      "1988.12.09 1988.09.21 1988.11.19 1\n",
      "1988.12.10 1988.09.22 1988.11.20 1\n",
      "1988.12.11 1988.09.23 1988.11.21 1\n",
      "1988.12.12 1988.09.24 1988.11.22 1\n",
      "1988.12.13 1988.09.25 1988.11.23 1\n",
      "1988.12.14 1988.09.26 1988.11.24 1\n",
      "1988.12.15 1988.09.27 1988.11.25 1\n",
      "1988.12.16 1988.09.28 1988.11.26 1\n",
      "1988.12.17 1988.09.29 1988.11.27 1\n",
      "1988.12.18 1988.09.30 1988.11.28 1\n",
      "1988.12.19 1988.10.01 1988.11.29 1\n",
      "1988.12.20 1988.10.02 1988.11.30 1\n",
      "1988.12.21 1988.10.03 1988.12.01 1\n",
      "1988.12.22 1988.10.04 1988.12.02 1\n",
      "1988.12.23 1988.10.05 1988.12.03 1\n",
      "1988.12.24 1988.10.06 1988.12.04 1\n",
      "1988.12.25 1988.10.07 1988.12.05 1\n",
      "1988.12.26 1988.10.08 1988.12.06 1\n",
      "1988.12.27 1988.10.09 1988.12.07 1\n",
      "1988.12.28 1988.10.10 1988.12.08 1\n",
      "1989.10.04 1989.07.17 1989.09.14 0\n",
      "1989.10.05 1989.07.18 1989.09.15 1\n",
      "1989.10.06 1989.07.19 1989.09.16 1\n",
      "1989.10.07 1989.07.20 1989.09.17 1\n",
      "1989.10.08 1989.07.21 1989.09.18 1\n",
      "1989.10.09 1989.07.22 1989.09.19 1\n",
      "1989.10.10 1989.07.23 1989.09.20 1\n",
      "1989.10.11 1989.07.24 1989.09.21 1\n",
      "1989.10.12 1989.07.25 1989.09.22 1\n",
      "1989.10.13 1989.07.26 1989.09.23 1\n",
      "1989.10.14 1989.07.27 1989.09.24 1\n",
      "1989.10.15 1989.07.28 1989.09.25 1\n",
      "1989.10.16 1989.07.29 1989.09.26 1\n",
      "1989.10.17 1989.07.30 1989.09.27 1\n",
      "1989.10.18 1989.07.31 1989.09.28 1\n",
      "1989.10.19 1989.08.01 1989.09.29 1\n",
      "1989.10.20 1989.08.02 1989.09.30 1\n",
      "1989.10.21 1989.08.03 1989.10.01 1\n",
      "1989.10.22 1989.08.04 1989.10.02 1\n",
      "1989.10.23 1989.08.05 1989.10.03 1\n",
      "1989.10.24 1989.08.06 1989.10.04 1\n",
      "1989.10.25 1989.08.07 1989.10.05 1\n",
      "1989.10.26 1989.08.08 1989.10.06 1\n",
      "1989.10.27 1989.08.09 1989.10.07 1\n",
      "1989.10.28 1989.08.10 1989.10.08 1\n",
      "1989.10.29 1989.08.11 1989.10.09 1\n",
      "1989.10.30 1989.08.12 1989.10.10 1\n",
      "1989.10.31 1989.08.13 1989.10.11 1\n",
      "1989.11.01 1989.08.14 1989.10.12 1\n",
      "1989.11.02 1989.08.15 1989.10.13 1\n",
      "1989.11.03 1989.08.16 1989.10.14 1\n",
      "1989.11.04 1989.08.17 1989.10.15 1\n",
      "1989.11.05 1989.08.18 1989.10.16 1\n",
      "1989.11.06 1989.08.19 1989.10.17 1\n",
      "1989.11.07 1989.08.20 1989.10.18 1\n",
      "1989.11.08 1989.08.21 1989.10.19 1\n",
      "1989.11.09 1989.08.22 1989.10.20 1\n",
      "1989.11.10 1989.08.23 1989.10.21 1\n",
      "1989.11.11 1989.08.24 1989.10.22 1\n",
      "1989.11.12 1989.08.25 1989.10.23 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1989.11.13 1989.08.26 1989.10.24 1\n",
      "1989.11.14 1989.08.27 1989.10.25 1\n",
      "1989.11.15 1989.08.28 1989.10.26 1\n",
      "1989.11.16 1989.08.29 1989.10.27 1\n",
      "1989.11.17 1989.08.30 1989.10.28 1\n",
      "1989.11.18 1989.08.31 1989.10.29 1\n",
      "1989.11.19 1989.09.01 1989.10.30 1\n",
      "1989.11.20 1989.09.02 1989.10.31 1\n",
      "1989.11.21 1989.09.03 1989.11.01 1\n",
      "1989.11.22 1989.09.04 1989.11.02 1\n",
      "1989.11.23 1989.09.05 1989.11.03 1\n",
      "1989.11.24 1989.09.06 1989.11.04 1\n",
      "1989.11.25 1989.09.07 1989.11.05 1\n",
      "1989.11.26 1989.09.08 1989.11.06 1\n",
      "1989.11.27 1989.09.09 1989.11.07 1\n",
      "1989.11.28 1989.09.10 1989.11.08 1\n",
      "1989.11.29 1989.09.11 1989.11.09 1\n",
      "1989.11.30 1989.09.12 1989.11.10 1\n",
      "1989.12.01 1989.09.13 1989.11.11 1\n",
      "1989.12.02 1989.09.14 1989.11.12 1\n",
      "1989.12.03 1989.09.15 1989.11.13 1\n",
      "1989.12.04 1989.09.16 1989.11.14 1\n",
      "1989.12.05 1989.09.17 1989.11.15 1\n",
      "1989.12.06 1989.09.18 1989.11.16 1\n",
      "1989.12.07 1989.09.19 1989.11.17 1\n",
      "1989.12.08 1989.09.20 1989.11.18 1\n",
      "1989.12.09 1989.09.21 1989.11.19 1\n",
      "1989.12.10 1989.09.22 1989.11.20 1\n",
      "1989.12.11 1989.09.23 1989.11.21 1\n",
      "1989.12.12 1989.09.24 1989.11.22 1\n",
      "1989.12.13 1989.09.25 1989.11.23 1\n",
      "1989.12.14 1989.09.26 1989.11.24 1\n",
      "1989.12.15 1989.09.27 1989.11.25 1\n",
      "1989.12.16 1989.09.28 1989.11.26 1\n",
      "1989.12.17 1989.09.29 1989.11.27 1\n",
      "1989.12.18 1989.09.30 1989.11.28 1\n",
      "1989.12.19 1989.10.01 1989.11.29 1\n",
      "1989.12.20 1989.10.02 1989.11.30 1\n",
      "1989.12.21 1989.10.03 1989.12.01 1\n",
      "1989.12.22 1989.10.04 1989.12.02 1\n",
      "1989.12.23 1989.10.05 1989.12.03 1\n",
      "1989.12.24 1989.10.06 1989.12.04 1\n",
      "1989.12.25 1989.10.07 1989.12.05 1\n",
      "1989.12.26 1989.10.08 1989.12.06 1\n",
      "1989.12.27 1989.10.09 1989.12.07 1\n",
      "1989.12.28 1989.10.10 1989.12.08 1\n",
      "1990.10.04 1990.07.17 1990.09.14 0\n",
      "1990.10.05 1990.07.18 1990.09.15 1\n",
      "1990.10.06 1990.07.19 1990.09.16 1\n",
      "1990.10.07 1990.07.20 1990.09.17 1\n",
      "1990.10.08 1990.07.21 1990.09.18 1\n",
      "1990.10.09 1990.07.22 1990.09.19 1\n",
      "1990.10.10 1990.07.23 1990.09.20 1\n",
      "1990.10.11 1990.07.24 1990.09.21 1\n",
      "1990.10.12 1990.07.25 1990.09.22 1\n",
      "1990.10.13 1990.07.26 1990.09.23 1\n",
      "1990.10.14 1990.07.27 1990.09.24 1\n",
      "1990.10.15 1990.07.28 1990.09.25 1\n",
      "1990.10.16 1990.07.29 1990.09.26 1\n",
      "1990.10.17 1990.07.30 1990.09.27 1\n",
      "1990.10.18 1990.07.31 1990.09.28 1\n",
      "1990.10.19 1990.08.01 1990.09.29 1\n",
      "1990.10.20 1990.08.02 1990.09.30 1\n",
      "1990.10.21 1990.08.03 1990.10.01 1\n",
      "1990.10.22 1990.08.04 1990.10.02 1\n",
      "1990.10.23 1990.08.05 1990.10.03 1\n",
      "1990.10.24 1990.08.06 1990.10.04 1\n",
      "1990.10.25 1990.08.07 1990.10.05 1\n",
      "1990.10.26 1990.08.08 1990.10.06 1\n",
      "1990.10.27 1990.08.09 1990.10.07 1\n",
      "1990.10.28 1990.08.10 1990.10.08 1\n",
      "1990.10.29 1990.08.11 1990.10.09 1\n",
      "1990.10.30 1990.08.12 1990.10.10 1\n",
      "1990.10.31 1990.08.13 1990.10.11 1\n",
      "1990.11.01 1990.08.14 1990.10.12 1\n",
      "1990.11.02 1990.08.15 1990.10.13 1\n",
      "1990.11.03 1990.08.16 1990.10.14 1\n",
      "1990.11.04 1990.08.17 1990.10.15 1\n",
      "1990.11.05 1990.08.18 1990.10.16 1\n",
      "1990.11.06 1990.08.19 1990.10.17 1\n",
      "1990.11.07 1990.08.20 1990.10.18 1\n",
      "1990.11.08 1990.08.21 1990.10.19 1\n",
      "1990.11.09 1990.08.22 1990.10.20 1\n",
      "1990.11.10 1990.08.23 1990.10.21 1\n",
      "1990.11.11 1990.08.24 1990.10.22 1\n",
      "1990.11.12 1990.08.25 1990.10.23 1\n",
      "1990.11.13 1990.08.26 1990.10.24 1\n",
      "1990.11.14 1990.08.27 1990.10.25 1\n",
      "1990.11.15 1990.08.28 1990.10.26 1\n",
      "1990.11.16 1990.08.29 1990.10.27 1\n",
      "1990.11.17 1990.08.30 1990.10.28 1\n",
      "1990.11.18 1990.08.31 1990.10.29 1\n",
      "1990.11.19 1990.09.01 1990.10.30 1\n",
      "1990.11.20 1990.09.02 1990.10.31 1\n",
      "1990.11.21 1990.09.03 1990.11.01 1\n",
      "1990.11.22 1990.09.04 1990.11.02 1\n",
      "1990.11.23 1990.09.05 1990.11.03 1\n",
      "1990.11.24 1990.09.06 1990.11.04 1\n",
      "1990.11.25 1990.09.07 1990.11.05 1\n",
      "1990.11.26 1990.09.08 1990.11.06 1\n",
      "1990.11.27 1990.09.09 1990.11.07 1\n",
      "1990.11.28 1990.09.10 1990.11.08 1\n",
      "1990.11.29 1990.09.11 1990.11.09 1\n",
      "1990.11.30 1990.09.12 1990.11.10 1\n",
      "1990.12.01 1990.09.13 1990.11.11 1\n",
      "1990.12.02 1990.09.14 1990.11.12 1\n",
      "1990.12.03 1990.09.15 1990.11.13 1\n",
      "1990.12.04 1990.09.16 1990.11.14 1\n",
      "1990.12.05 1990.09.17 1990.11.15 1\n",
      "1990.12.06 1990.09.18 1990.11.16 1\n",
      "1990.12.07 1990.09.19 1990.11.17 1\n",
      "1990.12.08 1990.09.20 1990.11.18 1\n",
      "1990.12.09 1990.09.21 1990.11.19 1\n",
      "1990.12.10 1990.09.22 1990.11.20 1\n",
      "1990.12.11 1990.09.23 1990.11.21 1\n",
      "1990.12.12 1990.09.24 1990.11.22 1\n",
      "1990.12.13 1990.09.25 1990.11.23 1\n",
      "1990.12.14 1990.09.26 1990.11.24 1\n",
      "1990.12.15 1990.09.27 1990.11.25 1\n",
      "1990.12.16 1990.09.28 1990.11.26 1\n",
      "1990.12.17 1990.09.29 1990.11.27 1\n",
      "1990.12.18 1990.09.30 1990.11.28 1\n",
      "1990.12.19 1990.10.01 1990.11.29 1\n",
      "1990.12.20 1990.10.02 1990.11.30 1\n",
      "1990.12.21 1990.10.03 1990.12.01 1\n",
      "1990.12.22 1990.10.04 1990.12.02 1\n",
      "1990.12.23 1990.10.05 1990.12.03 1\n",
      "1990.12.24 1990.10.06 1990.12.04 1\n",
      "1990.12.25 1990.10.07 1990.12.05 1\n",
      "1990.12.26 1990.10.08 1990.12.06 1\n",
      "1990.12.27 1990.10.09 1990.12.07 1\n",
      "1990.12.28 1990.10.10 1990.12.08 1\n",
      "1991.10.04 1991.07.17 1991.09.14 0\n",
      "1991.10.05 1991.07.18 1991.09.15 1\n",
      "1991.10.06 1991.07.19 1991.09.16 1\n",
      "1991.10.07 1991.07.20 1991.09.17 1\n",
      "1991.10.08 1991.07.21 1991.09.18 1\n",
      "1991.10.09 1991.07.22 1991.09.19 1\n",
      "1991.10.10 1991.07.23 1991.09.20 1\n",
      "1991.10.11 1991.07.24 1991.09.21 1\n",
      "1991.10.12 1991.07.25 1991.09.22 1\n",
      "1991.10.13 1991.07.26 1991.09.23 1\n",
      "1991.10.14 1991.07.27 1991.09.24 1\n",
      "1991.10.15 1991.07.28 1991.09.25 1\n",
      "1991.10.16 1991.07.29 1991.09.26 1\n",
      "1991.10.17 1991.07.30 1991.09.27 1\n",
      "1991.10.18 1991.07.31 1991.09.28 1\n",
      "1991.10.19 1991.08.01 1991.09.29 1\n",
      "1991.10.20 1991.08.02 1991.09.30 1\n",
      "1991.10.21 1991.08.03 1991.10.01 1\n",
      "1991.10.22 1991.08.04 1991.10.02 1\n",
      "1991.10.23 1991.08.05 1991.10.03 1\n",
      "1991.10.24 1991.08.06 1991.10.04 1\n",
      "1991.10.25 1991.08.07 1991.10.05 1\n",
      "1991.10.26 1991.08.08 1991.10.06 1\n",
      "1991.10.27 1991.08.09 1991.10.07 1\n",
      "1991.10.28 1991.08.10 1991.10.08 1\n",
      "1991.10.29 1991.08.11 1991.10.09 1\n",
      "1991.10.30 1991.08.12 1991.10.10 1\n",
      "1991.10.31 1991.08.13 1991.10.11 1\n",
      "1991.11.01 1991.08.14 1991.10.12 1\n",
      "1991.11.02 1991.08.15 1991.10.13 1\n",
      "1991.11.03 1991.08.16 1991.10.14 1\n",
      "1991.11.04 1991.08.17 1991.10.15 1\n",
      "1991.11.05 1991.08.18 1991.10.16 1\n",
      "1991.11.06 1991.08.19 1991.10.17 1\n",
      "1991.11.07 1991.08.20 1991.10.18 1\n",
      "1991.11.08 1991.08.21 1991.10.19 1\n",
      "1991.11.09 1991.08.22 1991.10.20 1\n",
      "1991.11.10 1991.08.23 1991.10.21 1\n",
      "1991.11.11 1991.08.24 1991.10.22 1\n",
      "1991.11.12 1991.08.25 1991.10.23 1\n",
      "1991.11.13 1991.08.26 1991.10.24 1\n",
      "1991.11.14 1991.08.27 1991.10.25 1\n",
      "1991.11.15 1991.08.28 1991.10.26 1\n",
      "1991.11.16 1991.08.29 1991.10.27 1\n",
      "1991.11.17 1991.08.30 1991.10.28 1\n",
      "1991.11.18 1991.08.31 1991.10.29 1\n",
      "1991.11.19 1991.09.01 1991.10.30 1\n",
      "1991.11.20 1991.09.02 1991.10.31 1\n",
      "1991.11.21 1991.09.03 1991.11.01 1\n",
      "1991.11.22 1991.09.04 1991.11.02 1\n",
      "1991.11.23 1991.09.05 1991.11.03 1\n",
      "1991.11.24 1991.09.06 1991.11.04 1\n",
      "1991.11.25 1991.09.07 1991.11.05 1\n",
      "1991.11.26 1991.09.08 1991.11.06 1\n",
      "1991.11.27 1991.09.09 1991.11.07 1\n",
      "1991.11.28 1991.09.10 1991.11.08 1\n",
      "1991.11.29 1991.09.11 1991.11.09 1\n",
      "1991.11.30 1991.09.12 1991.11.10 1\n",
      "1991.12.01 1991.09.13 1991.11.11 1\n",
      "1991.12.02 1991.09.14 1991.11.12 1\n",
      "1991.12.03 1991.09.15 1991.11.13 1\n",
      "1991.12.04 1991.09.16 1991.11.14 1\n",
      "1991.12.05 1991.09.17 1991.11.15 1\n",
      "1991.12.06 1991.09.18 1991.11.16 1\n",
      "1991.12.07 1991.09.19 1991.11.17 1\n",
      "1991.12.08 1991.09.20 1991.11.18 1\n",
      "1991.12.09 1991.09.21 1991.11.19 1\n",
      "1991.12.10 1991.09.22 1991.11.20 1\n",
      "1991.12.11 1991.09.23 1991.11.21 1\n",
      "1991.12.12 1991.09.24 1991.11.22 1\n",
      "1991.12.13 1991.09.25 1991.11.23 1\n",
      "1991.12.14 1991.09.26 1991.11.24 1\n",
      "1991.12.15 1991.09.27 1991.11.25 1\n",
      "1991.12.16 1991.09.28 1991.11.26 1\n",
      "1991.12.17 1991.09.29 1991.11.27 1\n",
      "1991.12.18 1991.09.30 1991.11.28 1\n",
      "1991.12.19 1991.10.01 1991.11.29 1\n",
      "1991.12.20 1991.10.02 1991.11.30 1\n",
      "1991.12.21 1991.10.03 1991.12.01 1\n",
      "1991.12.22 1991.10.04 1991.12.02 1\n",
      "1991.12.23 1991.10.05 1991.12.03 1\n",
      "1991.12.24 1991.10.06 1991.12.04 1\n",
      "1991.12.25 1991.10.07 1991.12.05 1\n",
      "1991.12.26 1991.10.08 1991.12.06 1\n",
      "1991.12.27 1991.10.09 1991.12.07 1\n",
      "1991.12.28 1991.10.10 1991.12.08 1\n",
      "1992.10.04 1992.07.17 1992.09.14 0\n",
      "1992.10.05 1992.07.18 1992.09.15 1\n",
      "1992.10.06 1992.07.19 1992.09.16 1\n",
      "1992.10.07 1992.07.20 1992.09.17 1\n",
      "1992.10.08 1992.07.21 1992.09.18 1\n",
      "1992.10.09 1992.07.22 1992.09.19 1\n",
      "1992.10.10 1992.07.23 1992.09.20 1\n",
      "1992.10.11 1992.07.24 1992.09.21 1\n",
      "1992.10.12 1992.07.25 1992.09.22 1\n",
      "1992.10.13 1992.07.26 1992.09.23 1\n",
      "1992.10.14 1992.07.27 1992.09.24 1\n",
      "1992.10.15 1992.07.28 1992.09.25 1\n",
      "1992.10.16 1992.07.29 1992.09.26 1\n",
      "1992.10.17 1992.07.30 1992.09.27 1\n",
      "1992.10.18 1992.07.31 1992.09.28 1\n",
      "1992.10.19 1992.08.01 1992.09.29 1\n",
      "1992.10.20 1992.08.02 1992.09.30 1\n",
      "1992.10.21 1992.08.03 1992.10.01 1\n",
      "1992.10.22 1992.08.04 1992.10.02 1\n",
      "1992.10.23 1992.08.05 1992.10.03 1\n",
      "1992.10.24 1992.08.06 1992.10.04 1\n",
      "1992.10.25 1992.08.07 1992.10.05 1\n",
      "1992.10.26 1992.08.08 1992.10.06 1\n",
      "1992.10.27 1992.08.09 1992.10.07 1\n",
      "1992.10.28 1992.08.10 1992.10.08 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1992.10.29 1992.08.11 1992.10.09 1\n",
      "1992.10.30 1992.08.12 1992.10.10 1\n",
      "1992.10.31 1992.08.13 1992.10.11 1\n",
      "1992.11.01 1992.08.14 1992.10.12 1\n",
      "1992.11.02 1992.08.15 1992.10.13 1\n",
      "1992.11.03 1992.08.16 1992.10.14 1\n",
      "1992.11.04 1992.08.17 1992.10.15 1\n",
      "1992.11.05 1992.08.18 1992.10.16 1\n",
      "1992.11.06 1992.08.19 1992.10.17 1\n",
      "1992.11.07 1992.08.20 1992.10.18 1\n",
      "1992.11.08 1992.08.21 1992.10.19 1\n",
      "1992.11.09 1992.08.22 1992.10.20 1\n",
      "1992.11.10 1992.08.23 1992.10.21 1\n",
      "1992.11.11 1992.08.24 1992.10.22 1\n",
      "1992.11.12 1992.08.25 1992.10.23 1\n",
      "1992.11.13 1992.08.26 1992.10.24 1\n",
      "1992.11.14 1992.08.27 1992.10.25 1\n",
      "1992.11.15 1992.08.28 1992.10.26 1\n",
      "1992.11.16 1992.08.29 1992.10.27 1\n",
      "1992.11.17 1992.08.30 1992.10.28 1\n",
      "1992.11.18 1992.08.31 1992.10.29 1\n",
      "1992.11.19 1992.09.01 1992.10.30 1\n",
      "1992.11.20 1992.09.02 1992.10.31 1\n",
      "1992.11.21 1992.09.03 1992.11.01 1\n",
      "1992.11.22 1992.09.04 1992.11.02 1\n",
      "1992.11.23 1992.09.05 1992.11.03 1\n",
      "1992.11.24 1992.09.06 1992.11.04 1\n",
      "1992.11.25 1992.09.07 1992.11.05 1\n",
      "1992.11.26 1992.09.08 1992.11.06 1\n",
      "1992.11.27 1992.09.09 1992.11.07 1\n",
      "1992.11.28 1992.09.10 1992.11.08 1\n",
      "1992.11.29 1992.09.11 1992.11.09 1\n",
      "1992.11.30 1992.09.12 1992.11.10 1\n",
      "1992.12.01 1992.09.13 1992.11.11 1\n",
      "1992.12.02 1992.09.14 1992.11.12 1\n",
      "1992.12.03 1992.09.15 1992.11.13 1\n",
      "1992.12.04 1992.09.16 1992.11.14 1\n",
      "1992.12.05 1992.09.17 1992.11.15 1\n",
      "1992.12.06 1992.09.18 1992.11.16 1\n",
      "1992.12.07 1992.09.19 1992.11.17 1\n",
      "1992.12.08 1992.09.20 1992.11.18 1\n",
      "1992.12.09 1992.09.21 1992.11.19 1\n",
      "1992.12.10 1992.09.22 1992.11.20 1\n",
      "1992.12.11 1992.09.23 1992.11.21 1\n",
      "1992.12.12 1992.09.24 1992.11.22 1\n",
      "1992.12.13 1992.09.25 1992.11.23 1\n",
      "1992.12.14 1992.09.26 1992.11.24 1\n",
      "1992.12.15 1992.09.27 1992.11.25 1\n",
      "1992.12.16 1992.09.28 1992.11.26 1\n",
      "1992.12.17 1992.09.29 1992.11.27 1\n",
      "1992.12.18 1992.09.30 1992.11.28 1\n",
      "1992.12.19 1992.10.01 1992.11.29 1\n",
      "1992.12.20 1992.10.02 1992.11.30 1\n",
      "1992.12.21 1992.10.03 1992.12.01 1\n",
      "1992.12.22 1992.10.04 1992.12.02 1\n",
      "1992.12.23 1992.10.05 1992.12.03 1\n",
      "1992.12.24 1992.10.06 1992.12.04 1\n",
      "1992.12.25 1992.10.07 1992.12.05 1\n",
      "1992.12.26 1992.10.08 1992.12.06 1\n",
      "1992.12.27 1992.10.09 1992.12.07 1\n",
      "1992.12.28 1992.10.10 1992.12.08 1\n",
      "1993.10.04 1993.07.17 1993.09.14 0\n",
      "1993.10.05 1993.07.18 1993.09.15 1\n",
      "1993.10.06 1993.07.19 1993.09.16 1\n",
      "1993.10.07 1993.07.20 1993.09.17 1\n",
      "1993.10.08 1993.07.21 1993.09.18 1\n",
      "1993.10.09 1993.07.22 1993.09.19 1\n",
      "1993.10.10 1993.07.23 1993.09.20 1\n",
      "1993.10.11 1993.07.24 1993.09.21 1\n",
      "1993.10.12 1993.07.25 1993.09.22 1\n",
      "1993.10.13 1993.07.26 1993.09.23 1\n",
      "1993.10.14 1993.07.27 1993.09.24 1\n",
      "1993.10.15 1993.07.28 1993.09.25 1\n",
      "1993.10.16 1993.07.29 1993.09.26 1\n",
      "1993.10.17 1993.07.30 1993.09.27 1\n",
      "1993.10.18 1993.07.31 1993.09.28 1\n",
      "1993.10.19 1993.08.01 1993.09.29 1\n",
      "1993.10.20 1993.08.02 1993.09.30 1\n",
      "1993.10.21 1993.08.03 1993.10.01 1\n",
      "1993.10.22 1993.08.04 1993.10.02 1\n",
      "1993.10.23 1993.08.05 1993.10.03 1\n",
      "1993.10.24 1993.08.06 1993.10.04 1\n",
      "1993.10.25 1993.08.07 1993.10.05 1\n",
      "1993.10.26 1993.08.08 1993.10.06 1\n",
      "1993.10.27 1993.08.09 1993.10.07 1\n",
      "1993.10.28 1993.08.10 1993.10.08 1\n",
      "1993.10.29 1993.08.11 1993.10.09 1\n",
      "1993.10.30 1993.08.12 1993.10.10 1\n",
      "1993.10.31 1993.08.13 1993.10.11 1\n",
      "1993.11.01 1993.08.14 1993.10.12 1\n",
      "1993.11.02 1993.08.15 1993.10.13 1\n",
      "1993.11.03 1993.08.16 1993.10.14 1\n",
      "1993.11.04 1993.08.17 1993.10.15 1\n",
      "1993.11.05 1993.08.18 1993.10.16 1\n",
      "1993.11.06 1993.08.19 1993.10.17 1\n",
      "1993.11.07 1993.08.20 1993.10.18 1\n",
      "1993.11.08 1993.08.21 1993.10.19 1\n",
      "1993.11.09 1993.08.22 1993.10.20 1\n",
      "1993.11.10 1993.08.23 1993.10.21 1\n",
      "1993.11.11 1993.08.24 1993.10.22 1\n",
      "1993.11.12 1993.08.25 1993.10.23 1\n",
      "1993.11.13 1993.08.26 1993.10.24 1\n",
      "1993.11.14 1993.08.27 1993.10.25 1\n",
      "1993.11.15 1993.08.28 1993.10.26 1\n",
      "1993.11.16 1993.08.29 1993.10.27 1\n",
      "1993.11.17 1993.08.30 1993.10.28 1\n",
      "1993.11.18 1993.08.31 1993.10.29 1\n",
      "1993.11.19 1993.09.01 1993.10.30 1\n",
      "1993.11.20 1993.09.02 1993.10.31 1\n",
      "1993.11.21 1993.09.03 1993.11.01 1\n",
      "1993.11.22 1993.09.04 1993.11.02 1\n",
      "1993.11.23 1993.09.05 1993.11.03 1\n",
      "1993.11.24 1993.09.06 1993.11.04 1\n",
      "1993.11.25 1993.09.07 1993.11.05 1\n",
      "1993.11.26 1993.09.08 1993.11.06 1\n",
      "1993.11.27 1993.09.09 1993.11.07 1\n",
      "1993.11.28 1993.09.10 1993.11.08 1\n",
      "1993.11.29 1993.09.11 1993.11.09 1\n",
      "1993.11.30 1993.09.12 1993.11.10 1\n",
      "1993.12.01 1993.09.13 1993.11.11 1\n",
      "1993.12.02 1993.09.14 1993.11.12 1\n",
      "1993.12.03 1993.09.15 1993.11.13 1\n",
      "1993.12.04 1993.09.16 1993.11.14 1\n",
      "1993.12.05 1993.09.17 1993.11.15 1\n",
      "1993.12.06 1993.09.18 1993.11.16 1\n",
      "1993.12.07 1993.09.19 1993.11.17 1\n",
      "1993.12.08 1993.09.20 1993.11.18 1\n",
      "1993.12.09 1993.09.21 1993.11.19 1\n",
      "1993.12.10 1993.09.22 1993.11.20 1\n",
      "1993.12.11 1993.09.23 1993.11.21 1\n",
      "1993.12.12 1993.09.24 1993.11.22 1\n",
      "1993.12.13 1993.09.25 1993.11.23 1\n",
      "1993.12.14 1993.09.26 1993.11.24 1\n",
      "1993.12.15 1993.09.27 1993.11.25 1\n",
      "1993.12.16 1993.09.28 1993.11.26 1\n",
      "1993.12.17 1993.09.29 1993.11.27 1\n",
      "1993.12.18 1993.09.30 1993.11.28 1\n",
      "1993.12.19 1993.10.01 1993.11.29 1\n",
      "1993.12.20 1993.10.02 1993.11.30 1\n",
      "1993.12.21 1993.10.03 1993.12.01 1\n",
      "1993.12.22 1993.10.04 1993.12.02 1\n",
      "1993.12.23 1993.10.05 1993.12.03 1\n",
      "1993.12.24 1993.10.06 1993.12.04 1\n",
      "1993.12.25 1993.10.07 1993.12.05 1\n",
      "1993.12.26 1993.10.08 1993.12.06 1\n",
      "1993.12.27 1993.10.09 1993.12.07 1\n",
      "1993.12.28 1993.10.10 1993.12.08 1\n",
      "1994.10.04 1994.07.17 1994.09.14 0\n",
      "1994.10.05 1994.07.18 1994.09.15 1\n",
      "1994.10.06 1994.07.19 1994.09.16 1\n",
      "1994.10.07 1994.07.20 1994.09.17 1\n",
      "1994.10.08 1994.07.21 1994.09.18 1\n",
      "1994.10.09 1994.07.22 1994.09.19 1\n",
      "1994.10.10 1994.07.23 1994.09.20 1\n",
      "1994.10.11 1994.07.24 1994.09.21 1\n",
      "1994.10.12 1994.07.25 1994.09.22 1\n",
      "1994.10.13 1994.07.26 1994.09.23 1\n",
      "1994.10.14 1994.07.27 1994.09.24 1\n",
      "1994.10.15 1994.07.28 1994.09.25 1\n",
      "1994.10.16 1994.07.29 1994.09.26 1\n",
      "1994.10.17 1994.07.30 1994.09.27 1\n",
      "1994.10.18 1994.07.31 1994.09.28 1\n",
      "1994.10.19 1994.08.01 1994.09.29 1\n",
      "1994.10.20 1994.08.02 1994.09.30 1\n",
      "1994.10.21 1994.08.03 1994.10.01 1\n",
      "1994.10.22 1994.08.04 1994.10.02 1\n",
      "1994.10.23 1994.08.05 1994.10.03 1\n",
      "1994.10.24 1994.08.06 1994.10.04 1\n",
      "1994.10.25 1994.08.07 1994.10.05 1\n",
      "1994.10.26 1994.08.08 1994.10.06 1\n",
      "1994.10.27 1994.08.09 1994.10.07 1\n",
      "1994.10.28 1994.08.10 1994.10.08 1\n",
      "1994.10.29 1994.08.11 1994.10.09 1\n",
      "1994.10.30 1994.08.12 1994.10.10 1\n",
      "1994.10.31 1994.08.13 1994.10.11 1\n",
      "1994.11.01 1994.08.14 1994.10.12 1\n",
      "1994.11.02 1994.08.15 1994.10.13 1\n",
      "1994.11.03 1994.08.16 1994.10.14 1\n",
      "1994.11.04 1994.08.17 1994.10.15 1\n",
      "1994.11.05 1994.08.18 1994.10.16 1\n",
      "1994.11.06 1994.08.19 1994.10.17 1\n",
      "1994.11.07 1994.08.20 1994.10.18 1\n",
      "1994.11.08 1994.08.21 1994.10.19 1\n",
      "1994.11.09 1994.08.22 1994.10.20 1\n",
      "1994.11.10 1994.08.23 1994.10.21 1\n",
      "1994.11.11 1994.08.24 1994.10.22 1\n",
      "1994.11.12 1994.08.25 1994.10.23 1\n",
      "1994.11.13 1994.08.26 1994.10.24 1\n",
      "1994.11.14 1994.08.27 1994.10.25 1\n",
      "1994.11.15 1994.08.28 1994.10.26 1\n",
      "1994.11.16 1994.08.29 1994.10.27 1\n",
      "1994.11.17 1994.08.30 1994.10.28 1\n",
      "1994.11.18 1994.08.31 1994.10.29 1\n",
      "1994.11.19 1994.09.01 1994.10.30 1\n",
      "1994.11.20 1994.09.02 1994.10.31 1\n",
      "1994.11.21 1994.09.03 1994.11.01 1\n",
      "1994.11.22 1994.09.04 1994.11.02 1\n",
      "1994.11.23 1994.09.05 1994.11.03 1\n",
      "1994.11.24 1994.09.06 1994.11.04 1\n",
      "1994.11.25 1994.09.07 1994.11.05 1\n",
      "1994.11.26 1994.09.08 1994.11.06 1\n",
      "1994.11.27 1994.09.09 1994.11.07 1\n",
      "1994.11.28 1994.09.10 1994.11.08 1\n",
      "1994.11.29 1994.09.11 1994.11.09 1\n",
      "1994.11.30 1994.09.12 1994.11.10 1\n",
      "1994.12.01 1994.09.13 1994.11.11 1\n",
      "1994.12.02 1994.09.14 1994.11.12 1\n",
      "1994.12.03 1994.09.15 1994.11.13 1\n",
      "1994.12.04 1994.09.16 1994.11.14 1\n",
      "1994.12.05 1994.09.17 1994.11.15 1\n",
      "1994.12.06 1994.09.18 1994.11.16 1\n",
      "1994.12.07 1994.09.19 1994.11.17 1\n",
      "1994.12.08 1994.09.20 1994.11.18 1\n",
      "1994.12.09 1994.09.21 1994.11.19 1\n",
      "1994.12.10 1994.09.22 1994.11.20 1\n",
      "1994.12.11 1994.09.23 1994.11.21 1\n",
      "1994.12.12 1994.09.24 1994.11.22 1\n",
      "1994.12.13 1994.09.25 1994.11.23 1\n",
      "1994.12.14 1994.09.26 1994.11.24 1\n",
      "1994.12.15 1994.09.27 1994.11.25 1\n",
      "1994.12.16 1994.09.28 1994.11.26 1\n",
      "1994.12.17 1994.09.29 1994.11.27 1\n",
      "1994.12.18 1994.09.30 1994.11.28 1\n",
      "1994.12.19 1994.10.01 1994.11.29 1\n",
      "1994.12.20 1994.10.02 1994.11.30 1\n",
      "1994.12.21 1994.10.03 1994.12.01 1\n",
      "1994.12.22 1994.10.04 1994.12.02 1\n",
      "1994.12.23 1994.10.05 1994.12.03 1\n",
      "1994.12.24 1994.10.06 1994.12.04 1\n",
      "1994.12.25 1994.10.07 1994.12.05 1\n",
      "1994.12.26 1994.10.08 1994.12.06 1\n",
      "1994.12.27 1994.10.09 1994.12.07 1\n",
      "1994.12.28 1994.10.10 1994.12.08 1\n",
      "1995.10.04 1995.07.17 1995.09.14 0\n",
      "1995.10.05 1995.07.18 1995.09.15 1\n",
      "1995.10.06 1995.07.19 1995.09.16 1\n",
      "1995.10.07 1995.07.20 1995.09.17 1\n",
      "1995.10.08 1995.07.21 1995.09.18 1\n",
      "1995.10.09 1995.07.22 1995.09.19 1\n",
      "1995.10.10 1995.07.23 1995.09.20 1\n",
      "1995.10.11 1995.07.24 1995.09.21 1\n",
      "1995.10.12 1995.07.25 1995.09.22 1\n",
      "1995.10.13 1995.07.26 1995.09.23 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1995.10.14 1995.07.27 1995.09.24 1\n",
      "1995.10.15 1995.07.28 1995.09.25 1\n",
      "1995.10.16 1995.07.29 1995.09.26 1\n",
      "1995.10.17 1995.07.30 1995.09.27 1\n",
      "1995.10.18 1995.07.31 1995.09.28 1\n",
      "1995.10.19 1995.08.01 1995.09.29 1\n",
      "1995.10.20 1995.08.02 1995.09.30 1\n",
      "1995.10.21 1995.08.03 1995.10.01 1\n",
      "1995.10.22 1995.08.04 1995.10.02 1\n",
      "1995.10.23 1995.08.05 1995.10.03 1\n",
      "1995.10.24 1995.08.06 1995.10.04 1\n",
      "1995.10.25 1995.08.07 1995.10.05 1\n",
      "1995.10.26 1995.08.08 1995.10.06 1\n",
      "1995.10.27 1995.08.09 1995.10.07 1\n",
      "1995.10.28 1995.08.10 1995.10.08 1\n",
      "1995.10.29 1995.08.11 1995.10.09 1\n",
      "1995.10.30 1995.08.12 1995.10.10 1\n",
      "1995.10.31 1995.08.13 1995.10.11 1\n",
      "1995.11.01 1995.08.14 1995.10.12 1\n",
      "1995.11.02 1995.08.15 1995.10.13 1\n",
      "1995.11.03 1995.08.16 1995.10.14 1\n",
      "1995.11.04 1995.08.17 1995.10.15 1\n",
      "1995.11.05 1995.08.18 1995.10.16 1\n",
      "1995.11.06 1995.08.19 1995.10.17 1\n",
      "1995.11.07 1995.08.20 1995.10.18 1\n",
      "1995.11.08 1995.08.21 1995.10.19 1\n",
      "1995.11.09 1995.08.22 1995.10.20 1\n",
      "1995.11.10 1995.08.23 1995.10.21 1\n",
      "1995.11.11 1995.08.24 1995.10.22 1\n",
      "1995.11.12 1995.08.25 1995.10.23 1\n",
      "1995.11.13 1995.08.26 1995.10.24 1\n",
      "1995.11.14 1995.08.27 1995.10.25 1\n",
      "1995.11.15 1995.08.28 1995.10.26 1\n",
      "1995.11.16 1995.08.29 1995.10.27 1\n",
      "1995.11.17 1995.08.30 1995.10.28 1\n",
      "1995.11.18 1995.08.31 1995.10.29 1\n",
      "1995.11.19 1995.09.01 1995.10.30 1\n",
      "1995.11.20 1995.09.02 1995.10.31 1\n",
      "1995.11.21 1995.09.03 1995.11.01 1\n",
      "1995.11.22 1995.09.04 1995.11.02 1\n",
      "1995.11.23 1995.09.05 1995.11.03 1\n",
      "1995.11.24 1995.09.06 1995.11.04 1\n",
      "1995.11.25 1995.09.07 1995.11.05 1\n",
      "1995.11.26 1995.09.08 1995.11.06 1\n",
      "1995.11.27 1995.09.09 1995.11.07 1\n",
      "1995.11.28 1995.09.10 1995.11.08 1\n",
      "1995.11.29 1995.09.11 1995.11.09 1\n",
      "1995.11.30 1995.09.12 1995.11.10 1\n",
      "1995.12.01 1995.09.13 1995.11.11 1\n",
      "1995.12.02 1995.09.14 1995.11.12 1\n",
      "1995.12.03 1995.09.15 1995.11.13 1\n",
      "1995.12.04 1995.09.16 1995.11.14 1\n",
      "1995.12.05 1995.09.17 1995.11.15 1\n",
      "1995.12.06 1995.09.18 1995.11.16 1\n",
      "1995.12.07 1995.09.19 1995.11.17 1\n",
      "1995.12.08 1995.09.20 1995.11.18 1\n",
      "1995.12.09 1995.09.21 1995.11.19 1\n",
      "1995.12.10 1995.09.22 1995.11.20 1\n",
      "1995.12.11 1995.09.23 1995.11.21 1\n",
      "1995.12.12 1995.09.24 1995.11.22 1\n",
      "1995.12.13 1995.09.25 1995.11.23 1\n",
      "1995.12.14 1995.09.26 1995.11.24 1\n",
      "1995.12.15 1995.09.27 1995.11.25 1\n",
      "1995.12.16 1995.09.28 1995.11.26 1\n",
      "1995.12.17 1995.09.29 1995.11.27 1\n",
      "1995.12.18 1995.09.30 1995.11.28 1\n",
      "1995.12.19 1995.10.01 1995.11.29 1\n",
      "1995.12.20 1995.10.02 1995.11.30 1\n",
      "1995.12.21 1995.10.03 1995.12.01 1\n",
      "1995.12.22 1995.10.04 1995.12.02 1\n",
      "1995.12.23 1995.10.05 1995.12.03 1\n",
      "1995.12.24 1995.10.06 1995.12.04 1\n",
      "1995.12.25 1995.10.07 1995.12.05 1\n",
      "1995.12.26 1995.10.08 1995.12.06 1\n",
      "1995.12.27 1995.10.09 1995.12.07 1\n",
      "1995.12.28 1995.10.10 1995.12.08 1\n",
      "1996.10.04 1996.07.17 1996.09.14 0\n",
      "1996.10.05 1996.07.18 1996.09.15 1\n",
      "1996.10.06 1996.07.19 1996.09.16 1\n",
      "1996.10.07 1996.07.20 1996.09.17 1\n",
      "1996.10.08 1996.07.21 1996.09.18 1\n",
      "1996.10.09 1996.07.22 1996.09.19 1\n",
      "1996.10.10 1996.07.23 1996.09.20 1\n",
      "1996.10.11 1996.07.24 1996.09.21 1\n",
      "1996.10.12 1996.07.25 1996.09.22 1\n",
      "1996.10.13 1996.07.26 1996.09.23 1\n",
      "1996.10.14 1996.07.27 1996.09.24 1\n",
      "1996.10.15 1996.07.28 1996.09.25 1\n",
      "1996.10.16 1996.07.29 1996.09.26 1\n",
      "1996.10.17 1996.07.30 1996.09.27 1\n",
      "1996.10.18 1996.07.31 1996.09.28 1\n",
      "1996.10.19 1996.08.01 1996.09.29 1\n",
      "1996.10.20 1996.08.02 1996.09.30 1\n",
      "1996.10.21 1996.08.03 1996.10.01 1\n",
      "1996.10.22 1996.08.04 1996.10.02 1\n",
      "1996.10.23 1996.08.05 1996.10.03 1\n",
      "1996.10.24 1996.08.06 1996.10.04 1\n",
      "1996.10.25 1996.08.07 1996.10.05 1\n",
      "1996.10.26 1996.08.08 1996.10.06 1\n",
      "1996.10.27 1996.08.09 1996.10.07 1\n",
      "1996.10.28 1996.08.10 1996.10.08 1\n",
      "1996.10.29 1996.08.11 1996.10.09 1\n",
      "1996.10.30 1996.08.12 1996.10.10 1\n",
      "1996.10.31 1996.08.13 1996.10.11 1\n",
      "1996.11.01 1996.08.14 1996.10.12 1\n",
      "1996.11.02 1996.08.15 1996.10.13 1\n",
      "1996.11.03 1996.08.16 1996.10.14 1\n",
      "1996.11.04 1996.08.17 1996.10.15 1\n",
      "1996.11.05 1996.08.18 1996.10.16 1\n",
      "1996.11.06 1996.08.19 1996.10.17 1\n",
      "1996.11.07 1996.08.20 1996.10.18 1\n",
      "1996.11.08 1996.08.21 1996.10.19 1\n",
      "1996.11.09 1996.08.22 1996.10.20 1\n",
      "1996.11.10 1996.08.23 1996.10.21 1\n",
      "1996.11.11 1996.08.24 1996.10.22 1\n",
      "1996.11.12 1996.08.25 1996.10.23 1\n",
      "1996.11.13 1996.08.26 1996.10.24 1\n",
      "1996.11.14 1996.08.27 1996.10.25 1\n",
      "1996.11.15 1996.08.28 1996.10.26 1\n",
      "1996.11.16 1996.08.29 1996.10.27 1\n",
      "1996.11.17 1996.08.30 1996.10.28 1\n",
      "1996.11.18 1996.08.31 1996.10.29 1\n",
      "1996.11.19 1996.09.01 1996.10.30 1\n",
      "1996.11.20 1996.09.02 1996.10.31 1\n",
      "1996.11.21 1996.09.03 1996.11.01 1\n",
      "1996.11.22 1996.09.04 1996.11.02 1\n",
      "1996.11.23 1996.09.05 1996.11.03 1\n",
      "1996.11.24 1996.09.06 1996.11.04 1\n",
      "1996.11.25 1996.09.07 1996.11.05 1\n",
      "1996.11.26 1996.09.08 1996.11.06 1\n",
      "1996.11.27 1996.09.09 1996.11.07 1\n",
      "1996.11.28 1996.09.10 1996.11.08 1\n",
      "1996.11.29 1996.09.11 1996.11.09 1\n",
      "1996.11.30 1996.09.12 1996.11.10 1\n",
      "1996.12.01 1996.09.13 1996.11.11 1\n",
      "1996.12.02 1996.09.14 1996.11.12 1\n",
      "1996.12.03 1996.09.15 1996.11.13 1\n",
      "1996.12.04 1996.09.16 1996.11.14 1\n",
      "1996.12.05 1996.09.17 1996.11.15 1\n",
      "1996.12.06 1996.09.18 1996.11.16 1\n",
      "1996.12.07 1996.09.19 1996.11.17 1\n",
      "1996.12.08 1996.09.20 1996.11.18 1\n",
      "1996.12.09 1996.09.21 1996.11.19 1\n",
      "1996.12.10 1996.09.22 1996.11.20 1\n",
      "1996.12.11 1996.09.23 1996.11.21 1\n",
      "1996.12.12 1996.09.24 1996.11.22 1\n",
      "1996.12.13 1996.09.25 1996.11.23 1\n",
      "1996.12.14 1996.09.26 1996.11.24 1\n",
      "1996.12.15 1996.09.27 1996.11.25 1\n",
      "1996.12.16 1996.09.28 1996.11.26 1\n",
      "1996.12.17 1996.09.29 1996.11.27 1\n",
      "1996.12.18 1996.09.30 1996.11.28 1\n",
      "1996.12.19 1996.10.01 1996.11.29 1\n",
      "1996.12.20 1996.10.02 1996.11.30 1\n",
      "1996.12.21 1996.10.03 1996.12.01 1\n",
      "1996.12.22 1996.10.04 1996.12.02 1\n",
      "1996.12.23 1996.10.05 1996.12.03 1\n",
      "1996.12.24 1996.10.06 1996.12.04 1\n",
      "1996.12.25 1996.10.07 1996.12.05 1\n",
      "1996.12.26 1996.10.08 1996.12.06 1\n",
      "1996.12.27 1996.10.09 1996.12.07 1\n",
      "1996.12.28 1996.10.10 1996.12.08 1\n",
      "1997.10.04 1997.07.17 1997.09.14 0\n",
      "1997.10.05 1997.07.18 1997.09.15 1\n",
      "1997.10.06 1997.07.19 1997.09.16 1\n",
      "1997.10.07 1997.07.20 1997.09.17 1\n",
      "1997.10.08 1997.07.21 1997.09.18 1\n",
      "1997.10.09 1997.07.22 1997.09.19 1\n",
      "1997.10.10 1997.07.23 1997.09.20 1\n",
      "1997.10.11 1997.07.24 1997.09.21 1\n",
      "1997.10.12 1997.07.25 1997.09.22 1\n",
      "1997.10.13 1997.07.26 1997.09.23 1\n",
      "1997.10.14 1997.07.27 1997.09.24 1\n",
      "1997.10.15 1997.07.28 1997.09.25 1\n",
      "1997.10.16 1997.07.29 1997.09.26 1\n",
      "1997.10.17 1997.07.30 1997.09.27 1\n",
      "1997.10.18 1997.07.31 1997.09.28 1\n",
      "1997.10.19 1997.08.01 1997.09.29 1\n",
      "1997.10.20 1997.08.02 1997.09.30 1\n",
      "1997.10.21 1997.08.03 1997.10.01 1\n",
      "1997.10.22 1997.08.04 1997.10.02 1\n",
      "1997.10.23 1997.08.05 1997.10.03 1\n",
      "1997.10.24 1997.08.06 1997.10.04 1\n",
      "1997.10.25 1997.08.07 1997.10.05 1\n",
      "1997.10.26 1997.08.08 1997.10.06 1\n",
      "1997.10.27 1997.08.09 1997.10.07 1\n",
      "1997.10.28 1997.08.10 1997.10.08 1\n",
      "1997.10.29 1997.08.11 1997.10.09 1\n",
      "1997.10.30 1997.08.12 1997.10.10 1\n",
      "1997.10.31 1997.08.13 1997.10.11 1\n",
      "1997.11.01 1997.08.14 1997.10.12 1\n",
      "1997.11.02 1997.08.15 1997.10.13 1\n",
      "1997.11.03 1997.08.16 1997.10.14 1\n",
      "1997.11.04 1997.08.17 1997.10.15 1\n",
      "1997.11.05 1997.08.18 1997.10.16 1\n",
      "1997.11.06 1997.08.19 1997.10.17 1\n",
      "1997.11.07 1997.08.20 1997.10.18 1\n",
      "1997.11.08 1997.08.21 1997.10.19 1\n",
      "1997.11.09 1997.08.22 1997.10.20 1\n",
      "1997.11.10 1997.08.23 1997.10.21 1\n",
      "1997.11.11 1997.08.24 1997.10.22 1\n",
      "1997.11.12 1997.08.25 1997.10.23 1\n",
      "1997.11.13 1997.08.26 1997.10.24 1\n",
      "1997.11.14 1997.08.27 1997.10.25 1\n",
      "1997.11.15 1997.08.28 1997.10.26 1\n",
      "1997.11.16 1997.08.29 1997.10.27 1\n",
      "1997.11.17 1997.08.30 1997.10.28 1\n",
      "1997.11.18 1997.08.31 1997.10.29 1\n",
      "1997.11.19 1997.09.01 1997.10.30 1\n",
      "1997.11.20 1997.09.02 1997.10.31 1\n",
      "1997.11.21 1997.09.03 1997.11.01 1\n",
      "1997.11.22 1997.09.04 1997.11.02 1\n",
      "1997.11.23 1997.09.05 1997.11.03 1\n",
      "1997.11.24 1997.09.06 1997.11.04 1\n",
      "1997.11.25 1997.09.07 1997.11.05 1\n",
      "1997.11.26 1997.09.08 1997.11.06 1\n",
      "1997.11.27 1997.09.09 1997.11.07 1\n",
      "1997.11.28 1997.09.10 1997.11.08 1\n",
      "1997.11.29 1997.09.11 1997.11.09 1\n",
      "1997.11.30 1997.09.12 1997.11.10 1\n",
      "1997.12.01 1997.09.13 1997.11.11 1\n",
      "1997.12.02 1997.09.14 1997.11.12 1\n",
      "1997.12.03 1997.09.15 1997.11.13 1\n",
      "1997.12.04 1997.09.16 1997.11.14 1\n",
      "1997.12.05 1997.09.17 1997.11.15 1\n",
      "1997.12.06 1997.09.18 1997.11.16 1\n",
      "1997.12.07 1997.09.19 1997.11.17 1\n",
      "1997.12.08 1997.09.20 1997.11.18 1\n",
      "1997.12.09 1997.09.21 1997.11.19 1\n",
      "1997.12.10 1997.09.22 1997.11.20 1\n",
      "1997.12.11 1997.09.23 1997.11.21 1\n",
      "1997.12.12 1997.09.24 1997.11.22 1\n",
      "1997.12.13 1997.09.25 1997.11.23 1\n",
      "1997.12.14 1997.09.26 1997.11.24 1\n",
      "1997.12.15 1997.09.27 1997.11.25 1\n",
      "1997.12.16 1997.09.28 1997.11.26 1\n",
      "1997.12.17 1997.09.29 1997.11.27 1\n",
      "1997.12.18 1997.09.30 1997.11.28 1\n",
      "1997.12.19 1997.10.01 1997.11.29 1\n",
      "1997.12.20 1997.10.02 1997.11.30 1\n",
      "1997.12.21 1997.10.03 1997.12.01 1\n",
      "1997.12.22 1997.10.04 1997.12.02 1\n",
      "1997.12.23 1997.10.05 1997.12.03 1\n",
      "1997.12.24 1997.10.06 1997.12.04 1\n",
      "1997.12.25 1997.10.07 1997.12.05 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1997.12.26 1997.10.08 1997.12.06 1\n",
      "1997.12.27 1997.10.09 1997.12.07 1\n",
      "1997.12.28 1997.10.10 1997.12.08 1\n",
      "1998.10.04 1998.07.17 1998.09.14 0\n",
      "1998.10.05 1998.07.18 1998.09.15 1\n",
      "1998.10.06 1998.07.19 1998.09.16 1\n",
      "1998.10.07 1998.07.20 1998.09.17 1\n",
      "1998.10.08 1998.07.21 1998.09.18 1\n",
      "1998.10.09 1998.07.22 1998.09.19 1\n",
      "1998.10.10 1998.07.23 1998.09.20 1\n",
      "1998.10.11 1998.07.24 1998.09.21 1\n",
      "1998.10.12 1998.07.25 1998.09.22 1\n",
      "1998.10.13 1998.07.26 1998.09.23 1\n",
      "1998.10.14 1998.07.27 1998.09.24 1\n",
      "1998.10.15 1998.07.28 1998.09.25 1\n",
      "1998.10.16 1998.07.29 1998.09.26 1\n",
      "1998.10.17 1998.07.30 1998.09.27 1\n",
      "1998.10.18 1998.07.31 1998.09.28 1\n",
      "1998.10.19 1998.08.01 1998.09.29 1\n",
      "1998.10.20 1998.08.02 1998.09.30 1\n",
      "1998.10.21 1998.08.03 1998.10.01 1\n",
      "1998.10.22 1998.08.04 1998.10.02 1\n",
      "1998.10.23 1998.08.05 1998.10.03 1\n",
      "1998.10.24 1998.08.06 1998.10.04 1\n",
      "1998.10.25 1998.08.07 1998.10.05 1\n",
      "1998.10.26 1998.08.08 1998.10.06 1\n",
      "1998.10.27 1998.08.09 1998.10.07 1\n",
      "1998.10.28 1998.08.10 1998.10.08 1\n",
      "1998.10.29 1998.08.11 1998.10.09 1\n",
      "1998.10.30 1998.08.12 1998.10.10 1\n",
      "1998.10.31 1998.08.13 1998.10.11 1\n",
      "1998.11.01 1998.08.14 1998.10.12 1\n",
      "1998.11.02 1998.08.15 1998.10.13 1\n",
      "1998.11.03 1998.08.16 1998.10.14 1\n",
      "1998.11.04 1998.08.17 1998.10.15 1\n",
      "1998.11.05 1998.08.18 1998.10.16 1\n",
      "1998.11.06 1998.08.19 1998.10.17 1\n",
      "1998.11.07 1998.08.20 1998.10.18 1\n",
      "1998.11.08 1998.08.21 1998.10.19 1\n",
      "1998.11.09 1998.08.22 1998.10.20 1\n",
      "1998.11.10 1998.08.23 1998.10.21 1\n",
      "1998.11.11 1998.08.24 1998.10.22 1\n",
      "1998.11.12 1998.08.25 1998.10.23 1\n",
      "1998.11.13 1998.08.26 1998.10.24 1\n",
      "1998.11.14 1998.08.27 1998.10.25 1\n",
      "1998.11.15 1998.08.28 1998.10.26 1\n",
      "1998.11.16 1998.08.29 1998.10.27 1\n",
      "1998.11.17 1998.08.30 1998.10.28 1\n",
      "1998.11.18 1998.08.31 1998.10.29 1\n",
      "1998.11.19 1998.09.01 1998.10.30 1\n",
      "1998.11.20 1998.09.02 1998.10.31 1\n",
      "1998.11.21 1998.09.03 1998.11.01 1\n",
      "1998.11.22 1998.09.04 1998.11.02 1\n",
      "1998.11.23 1998.09.05 1998.11.03 1\n",
      "1998.11.24 1998.09.06 1998.11.04 1\n",
      "1998.11.25 1998.09.07 1998.11.05 1\n",
      "1998.11.26 1998.09.08 1998.11.06 1\n",
      "1998.11.27 1998.09.09 1998.11.07 1\n",
      "1998.11.28 1998.09.10 1998.11.08 1\n",
      "1998.11.29 1998.09.11 1998.11.09 1\n",
      "1998.11.30 1998.09.12 1998.11.10 1\n",
      "1998.12.01 1998.09.13 1998.11.11 1\n",
      "1998.12.02 1998.09.14 1998.11.12 1\n",
      "1998.12.03 1998.09.15 1998.11.13 1\n",
      "1998.12.04 1998.09.16 1998.11.14 1\n",
      "1998.12.05 1998.09.17 1998.11.15 1\n",
      "1998.12.06 1998.09.18 1998.11.16 1\n",
      "1998.12.07 1998.09.19 1998.11.17 1\n",
      "1998.12.08 1998.09.20 1998.11.18 1\n",
      "1998.12.09 1998.09.21 1998.11.19 1\n",
      "1998.12.10 1998.09.22 1998.11.20 1\n",
      "1998.12.11 1998.09.23 1998.11.21 1\n",
      "1998.12.12 1998.09.24 1998.11.22 1\n",
      "1998.12.13 1998.09.25 1998.11.23 1\n",
      "1998.12.14 1998.09.26 1998.11.24 1\n",
      "1998.12.15 1998.09.27 1998.11.25 1\n",
      "1998.12.16 1998.09.28 1998.11.26 1\n",
      "1998.12.17 1998.09.29 1998.11.27 1\n",
      "1998.12.18 1998.09.30 1998.11.28 1\n",
      "1998.12.19 1998.10.01 1998.11.29 1\n",
      "1998.12.20 1998.10.02 1998.11.30 1\n",
      "1998.12.21 1998.10.03 1998.12.01 1\n",
      "1998.12.22 1998.10.04 1998.12.02 1\n",
      "1998.12.23 1998.10.05 1998.12.03 1\n",
      "1998.12.24 1998.10.06 1998.12.04 1\n",
      "1998.12.25 1998.10.07 1998.12.05 1\n",
      "1998.12.26 1998.10.08 1998.12.06 1\n",
      "1998.12.27 1998.10.09 1998.12.07 1\n",
      "1998.12.28 1998.10.10 1998.12.08 1\n",
      "1999.10.04 1999.07.17 1999.09.14 0\n",
      "1999.10.05 1999.07.18 1999.09.15 1\n",
      "1999.10.06 1999.07.19 1999.09.16 1\n",
      "1999.10.07 1999.07.20 1999.09.17 1\n",
      "1999.10.08 1999.07.21 1999.09.18 1\n",
      "1999.10.09 1999.07.22 1999.09.19 1\n",
      "1999.10.10 1999.07.23 1999.09.20 1\n",
      "1999.10.11 1999.07.24 1999.09.21 1\n",
      "1999.10.12 1999.07.25 1999.09.22 1\n",
      "1999.10.13 1999.07.26 1999.09.23 1\n",
      "1999.10.14 1999.07.27 1999.09.24 1\n",
      "1999.10.15 1999.07.28 1999.09.25 1\n",
      "1999.10.16 1999.07.29 1999.09.26 1\n",
      "1999.10.17 1999.07.30 1999.09.27 1\n",
      "1999.10.18 1999.07.31 1999.09.28 1\n",
      "1999.10.19 1999.08.01 1999.09.29 1\n",
      "1999.10.20 1999.08.02 1999.09.30 1\n",
      "1999.10.21 1999.08.03 1999.10.01 1\n",
      "1999.10.22 1999.08.04 1999.10.02 1\n",
      "1999.10.23 1999.08.05 1999.10.03 1\n",
      "1999.10.24 1999.08.06 1999.10.04 1\n",
      "1999.10.25 1999.08.07 1999.10.05 1\n",
      "1999.10.26 1999.08.08 1999.10.06 1\n",
      "1999.10.27 1999.08.09 1999.10.07 1\n",
      "1999.10.28 1999.08.10 1999.10.08 1\n",
      "1999.10.29 1999.08.11 1999.10.09 1\n",
      "1999.10.30 1999.08.12 1999.10.10 1\n",
      "1999.10.31 1999.08.13 1999.10.11 1\n",
      "1999.11.01 1999.08.14 1999.10.12 1\n",
      "1999.11.02 1999.08.15 1999.10.13 1\n",
      "1999.11.03 1999.08.16 1999.10.14 1\n",
      "1999.11.04 1999.08.17 1999.10.15 1\n",
      "1999.11.05 1999.08.18 1999.10.16 1\n",
      "1999.11.06 1999.08.19 1999.10.17 1\n",
      "1999.11.07 1999.08.20 1999.10.18 1\n",
      "1999.11.08 1999.08.21 1999.10.19 1\n",
      "1999.11.09 1999.08.22 1999.10.20 1\n",
      "1999.11.10 1999.08.23 1999.10.21 1\n",
      "1999.11.11 1999.08.24 1999.10.22 1\n",
      "1999.11.12 1999.08.25 1999.10.23 1\n",
      "1999.11.13 1999.08.26 1999.10.24 1\n",
      "1999.11.14 1999.08.27 1999.10.25 1\n",
      "1999.11.15 1999.08.28 1999.10.26 1\n",
      "1999.11.16 1999.08.29 1999.10.27 1\n",
      "1999.11.17 1999.08.30 1999.10.28 1\n",
      "1999.11.18 1999.08.31 1999.10.29 1\n",
      "1999.11.19 1999.09.01 1999.10.30 1\n",
      "1999.11.20 1999.09.02 1999.10.31 1\n",
      "1999.11.21 1999.09.03 1999.11.01 1\n",
      "1999.11.22 1999.09.04 1999.11.02 1\n",
      "1999.11.23 1999.09.05 1999.11.03 1\n",
      "1999.11.24 1999.09.06 1999.11.04 1\n",
      "1999.11.25 1999.09.07 1999.11.05 1\n",
      "1999.11.26 1999.09.08 1999.11.06 1\n",
      "1999.11.27 1999.09.09 1999.11.07 1\n",
      "1999.11.28 1999.09.10 1999.11.08 1\n",
      "1999.11.29 1999.09.11 1999.11.09 1\n",
      "1999.11.30 1999.09.12 1999.11.10 1\n",
      "1999.12.01 1999.09.13 1999.11.11 1\n",
      "1999.12.02 1999.09.14 1999.11.12 1\n",
      "1999.12.03 1999.09.15 1999.11.13 1\n",
      "1999.12.04 1999.09.16 1999.11.14 1\n",
      "1999.12.05 1999.09.17 1999.11.15 1\n",
      "1999.12.06 1999.09.18 1999.11.16 1\n",
      "1999.12.07 1999.09.19 1999.11.17 1\n",
      "1999.12.08 1999.09.20 1999.11.18 1\n",
      "1999.12.09 1999.09.21 1999.11.19 1\n",
      "1999.12.10 1999.09.22 1999.11.20 1\n",
      "1999.12.11 1999.09.23 1999.11.21 1\n",
      "1999.12.12 1999.09.24 1999.11.22 1\n",
      "1999.12.13 1999.09.25 1999.11.23 1\n",
      "1999.12.14 1999.09.26 1999.11.24 1\n",
      "1999.12.15 1999.09.27 1999.11.25 1\n",
      "1999.12.16 1999.09.28 1999.11.26 1\n",
      "1999.12.17 1999.09.29 1999.11.27 1\n",
      "1999.12.18 1999.09.30 1999.11.28 1\n",
      "1999.12.19 1999.10.01 1999.11.29 1\n",
      "1999.12.20 1999.10.02 1999.11.30 1\n",
      "1999.12.21 1999.10.03 1999.12.01 1\n",
      "1999.12.22 1999.10.04 1999.12.02 1\n",
      "1999.12.23 1999.10.05 1999.12.03 1\n",
      "1999.12.24 1999.10.06 1999.12.04 1\n",
      "1999.12.25 1999.10.07 1999.12.05 1\n",
      "1999.12.26 1999.10.08 1999.12.06 1\n",
      "1999.12.27 1999.10.09 1999.12.07 1\n",
      "1999.12.28 1999.10.10 1999.12.08 1\n",
      "2000.10.04 2000.07.17 2000.09.14 0\n",
      "2000.10.05 2000.07.18 2000.09.15 1\n",
      "2000.10.06 2000.07.19 2000.09.16 1\n",
      "2000.10.07 2000.07.20 2000.09.17 1\n",
      "2000.10.08 2000.07.21 2000.09.18 1\n",
      "2000.10.09 2000.07.22 2000.09.19 1\n",
      "2000.10.10 2000.07.23 2000.09.20 1\n",
      "2000.10.11 2000.07.24 2000.09.21 1\n",
      "2000.10.12 2000.07.25 2000.09.22 1\n",
      "2000.10.13 2000.07.26 2000.09.23 1\n",
      "2000.10.14 2000.07.27 2000.09.24 1\n",
      "2000.10.15 2000.07.28 2000.09.25 1\n",
      "2000.10.16 2000.07.29 2000.09.26 1\n",
      "2000.10.17 2000.07.30 2000.09.27 1\n",
      "2000.10.18 2000.07.31 2000.09.28 1\n",
      "2000.10.19 2000.08.01 2000.09.29 1\n",
      "2000.10.20 2000.08.02 2000.09.30 1\n",
      "2000.10.21 2000.08.03 2000.10.01 1\n",
      "2000.10.22 2000.08.04 2000.10.02 1\n",
      "2000.10.23 2000.08.05 2000.10.03 1\n",
      "2000.10.24 2000.08.06 2000.10.04 1\n",
      "2000.10.25 2000.08.07 2000.10.05 1\n",
      "2000.10.26 2000.08.08 2000.10.06 1\n",
      "2000.10.27 2000.08.09 2000.10.07 1\n",
      "2000.10.28 2000.08.10 2000.10.08 1\n",
      "2000.10.29 2000.08.11 2000.10.09 1\n",
      "2000.10.30 2000.08.12 2000.10.10 1\n",
      "2000.10.31 2000.08.13 2000.10.11 1\n",
      "2000.11.01 2000.08.14 2000.10.12 1\n",
      "2000.11.02 2000.08.15 2000.10.13 1\n",
      "2000.11.03 2000.08.16 2000.10.14 1\n",
      "2000.11.04 2000.08.17 2000.10.15 1\n",
      "2000.11.05 2000.08.18 2000.10.16 1\n",
      "2000.11.06 2000.08.19 2000.10.17 1\n",
      "2000.11.07 2000.08.20 2000.10.18 1\n",
      "2000.11.08 2000.08.21 2000.10.19 1\n",
      "2000.11.09 2000.08.22 2000.10.20 1\n",
      "2000.11.10 2000.08.23 2000.10.21 1\n",
      "2000.11.11 2000.08.24 2000.10.22 1\n",
      "2000.11.12 2000.08.25 2000.10.23 1\n",
      "2000.11.13 2000.08.26 2000.10.24 1\n",
      "2000.11.14 2000.08.27 2000.10.25 1\n",
      "2000.11.15 2000.08.28 2000.10.26 1\n",
      "2000.11.16 2000.08.29 2000.10.27 1\n",
      "2000.11.17 2000.08.30 2000.10.28 1\n",
      "2000.11.18 2000.08.31 2000.10.29 1\n",
      "2000.11.19 2000.09.01 2000.10.30 1\n",
      "2000.11.20 2000.09.02 2000.10.31 1\n",
      "2000.11.21 2000.09.03 2000.11.01 1\n",
      "2000.11.22 2000.09.04 2000.11.02 1\n",
      "2000.11.23 2000.09.05 2000.11.03 1\n",
      "2000.11.24 2000.09.06 2000.11.04 1\n",
      "2000.11.25 2000.09.07 2000.11.05 1\n",
      "2000.11.26 2000.09.08 2000.11.06 1\n",
      "2000.11.27 2000.09.09 2000.11.07 1\n",
      "2000.11.28 2000.09.10 2000.11.08 1\n",
      "2000.11.29 2000.09.11 2000.11.09 1\n",
      "2000.11.30 2000.09.12 2000.11.10 1\n",
      "2000.12.01 2000.09.13 2000.11.11 1\n",
      "2000.12.02 2000.09.14 2000.11.12 1\n",
      "2000.12.03 2000.09.15 2000.11.13 1\n",
      "2000.12.04 2000.09.16 2000.11.14 1\n",
      "2000.12.05 2000.09.17 2000.11.15 1\n",
      "2000.12.06 2000.09.18 2000.11.16 1\n",
      "2000.12.07 2000.09.19 2000.11.17 1\n",
      "2000.12.08 2000.09.20 2000.11.18 1\n",
      "2000.12.09 2000.09.21 2000.11.19 1\n",
      "2000.12.10 2000.09.22 2000.11.20 1\n",
      "2000.12.11 2000.09.23 2000.11.21 1\n",
      "2000.12.12 2000.09.24 2000.11.22 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000.12.13 2000.09.25 2000.11.23 1\n",
      "2000.12.14 2000.09.26 2000.11.24 1\n",
      "2000.12.15 2000.09.27 2000.11.25 1\n",
      "2000.12.16 2000.09.28 2000.11.26 1\n",
      "2000.12.17 2000.09.29 2000.11.27 1\n",
      "2000.12.18 2000.09.30 2000.11.28 1\n",
      "2000.12.19 2000.10.01 2000.11.29 1\n",
      "2000.12.20 2000.10.02 2000.11.30 1\n",
      "2000.12.21 2000.10.03 2000.12.01 1\n",
      "2000.12.22 2000.10.04 2000.12.02 1\n",
      "2000.12.23 2000.10.05 2000.12.03 1\n",
      "2000.12.24 2000.10.06 2000.12.04 1\n",
      "2000.12.25 2000.10.07 2000.12.05 1\n",
      "2000.12.26 2000.10.08 2000.12.06 1\n",
      "2000.12.27 2000.10.09 2000.12.07 1\n",
      "2000.12.28 2000.10.10 2000.12.08 1\n",
      "2001.10.04 2001.07.17 2001.09.14 0\n",
      "2001.10.05 2001.07.18 2001.09.15 1\n",
      "2001.10.06 2001.07.19 2001.09.16 1\n",
      "2001.10.07 2001.07.20 2001.09.17 1\n",
      "2001.10.08 2001.07.21 2001.09.18 1\n",
      "2001.10.09 2001.07.22 2001.09.19 1\n",
      "2001.10.10 2001.07.23 2001.09.20 1\n",
      "2001.10.11 2001.07.24 2001.09.21 1\n",
      "2001.10.12 2001.07.25 2001.09.22 1\n",
      "2001.10.13 2001.07.26 2001.09.23 1\n",
      "2001.10.14 2001.07.27 2001.09.24 1\n",
      "2001.10.15 2001.07.28 2001.09.25 1\n",
      "2001.10.16 2001.07.29 2001.09.26 1\n",
      "2001.10.17 2001.07.30 2001.09.27 1\n",
      "2001.10.18 2001.07.31 2001.09.28 1\n",
      "2001.10.19 2001.08.01 2001.09.29 1\n",
      "2001.10.20 2001.08.02 2001.09.30 1\n",
      "2001.10.21 2001.08.03 2001.10.01 1\n",
      "2001.10.22 2001.08.04 2001.10.02 1\n",
      "2001.10.23 2001.08.05 2001.10.03 1\n",
      "2001.10.24 2001.08.06 2001.10.04 1\n",
      "2001.10.25 2001.08.07 2001.10.05 1\n",
      "2001.10.26 2001.08.08 2001.10.06 1\n",
      "2001.10.27 2001.08.09 2001.10.07 1\n",
      "2001.10.28 2001.08.10 2001.10.08 1\n",
      "2001.10.29 2001.08.11 2001.10.09 1\n",
      "2001.10.30 2001.08.12 2001.10.10 1\n",
      "2001.10.31 2001.08.13 2001.10.11 1\n",
      "2001.11.01 2001.08.14 2001.10.12 1\n",
      "2001.11.02 2001.08.15 2001.10.13 1\n",
      "2001.11.03 2001.08.16 2001.10.14 1\n",
      "2001.11.04 2001.08.17 2001.10.15 1\n",
      "2001.11.05 2001.08.18 2001.10.16 1\n",
      "2001.11.06 2001.08.19 2001.10.17 1\n",
      "2001.11.07 2001.08.20 2001.10.18 1\n",
      "2001.11.08 2001.08.21 2001.10.19 1\n",
      "2001.11.09 2001.08.22 2001.10.20 1\n",
      "2001.11.10 2001.08.23 2001.10.21 1\n",
      "2001.11.11 2001.08.24 2001.10.22 1\n",
      "2001.11.12 2001.08.25 2001.10.23 1\n",
      "2001.11.13 2001.08.26 2001.10.24 1\n",
      "2001.11.14 2001.08.27 2001.10.25 1\n",
      "2001.11.15 2001.08.28 2001.10.26 1\n",
      "2001.11.16 2001.08.29 2001.10.27 1\n",
      "2001.11.17 2001.08.30 2001.10.28 1\n",
      "2001.11.18 2001.08.31 2001.10.29 1\n",
      "2001.11.19 2001.09.01 2001.10.30 1\n",
      "2001.11.20 2001.09.02 2001.10.31 1\n",
      "2001.11.21 2001.09.03 2001.11.01 1\n",
      "2001.11.22 2001.09.04 2001.11.02 1\n",
      "2001.11.23 2001.09.05 2001.11.03 1\n",
      "2001.11.24 2001.09.06 2001.11.04 1\n",
      "2001.11.25 2001.09.07 2001.11.05 1\n",
      "2001.11.26 2001.09.08 2001.11.06 1\n",
      "2001.11.27 2001.09.09 2001.11.07 1\n",
      "2001.11.28 2001.09.10 2001.11.08 1\n",
      "2001.11.29 2001.09.11 2001.11.09 1\n",
      "2001.11.30 2001.09.12 2001.11.10 1\n",
      "2001.12.01 2001.09.13 2001.11.11 1\n",
      "2001.12.02 2001.09.14 2001.11.12 1\n",
      "2001.12.03 2001.09.15 2001.11.13 1\n",
      "2001.12.04 2001.09.16 2001.11.14 1\n",
      "2001.12.05 2001.09.17 2001.11.15 1\n",
      "2001.12.06 2001.09.18 2001.11.16 1\n",
      "2001.12.07 2001.09.19 2001.11.17 1\n",
      "2001.12.08 2001.09.20 2001.11.18 1\n",
      "2001.12.09 2001.09.21 2001.11.19 1\n",
      "2001.12.10 2001.09.22 2001.11.20 1\n",
      "2001.12.11 2001.09.23 2001.11.21 1\n",
      "2001.12.12 2001.09.24 2001.11.22 1\n",
      "2001.12.13 2001.09.25 2001.11.23 1\n",
      "2001.12.14 2001.09.26 2001.11.24 1\n",
      "2001.12.15 2001.09.27 2001.11.25 1\n",
      "2001.12.16 2001.09.28 2001.11.26 1\n",
      "2001.12.17 2001.09.29 2001.11.27 1\n",
      "2001.12.18 2001.09.30 2001.11.28 1\n",
      "2001.12.19 2001.10.01 2001.11.29 1\n",
      "2001.12.20 2001.10.02 2001.11.30 1\n",
      "2001.12.21 2001.10.03 2001.12.01 1\n",
      "2001.12.22 2001.10.04 2001.12.02 1\n",
      "2001.12.23 2001.10.05 2001.12.03 1\n",
      "2001.12.24 2001.10.06 2001.12.04 1\n",
      "2001.12.25 2001.10.07 2001.12.05 1\n",
      "2001.12.26 2001.10.08 2001.12.06 1\n",
      "2001.12.27 2001.10.09 2001.12.07 1\n",
      "2001.12.28 2001.10.10 2001.12.08 1\n",
      "2002.10.04 2002.07.17 2002.09.14 0\n",
      "2002.10.05 2002.07.18 2002.09.15 1\n",
      "2002.10.06 2002.07.19 2002.09.16 1\n",
      "2002.10.07 2002.07.20 2002.09.17 1\n",
      "2002.10.08 2002.07.21 2002.09.18 1\n",
      "2002.10.09 2002.07.22 2002.09.19 1\n",
      "2002.10.10 2002.07.23 2002.09.20 1\n",
      "2002.10.11 2002.07.24 2002.09.21 1\n",
      "2002.10.12 2002.07.25 2002.09.22 1\n",
      "2002.10.13 2002.07.26 2002.09.23 1\n",
      "2002.10.14 2002.07.27 2002.09.24 1\n",
      "2002.10.15 2002.07.28 2002.09.25 1\n",
      "2002.10.16 2002.07.29 2002.09.26 1\n",
      "2002.10.17 2002.07.30 2002.09.27 1\n",
      "2002.10.18 2002.07.31 2002.09.28 1\n",
      "2002.10.19 2002.08.01 2002.09.29 1\n",
      "2002.10.20 2002.08.02 2002.09.30 1\n",
      "2002.10.21 2002.08.03 2002.10.01 1\n",
      "2002.10.22 2002.08.04 2002.10.02 1\n",
      "2002.10.23 2002.08.05 2002.10.03 1\n",
      "2002.10.24 2002.08.06 2002.10.04 1\n",
      "2002.10.25 2002.08.07 2002.10.05 1\n",
      "2002.10.26 2002.08.08 2002.10.06 1\n",
      "2002.10.27 2002.08.09 2002.10.07 1\n",
      "2002.10.28 2002.08.10 2002.10.08 1\n",
      "2002.10.29 2002.08.11 2002.10.09 1\n",
      "2002.10.30 2002.08.12 2002.10.10 1\n",
      "2002.10.31 2002.08.13 2002.10.11 1\n",
      "2002.11.01 2002.08.14 2002.10.12 1\n",
      "2002.11.02 2002.08.15 2002.10.13 1\n",
      "2002.11.03 2002.08.16 2002.10.14 1\n",
      "2002.11.04 2002.08.17 2002.10.15 1\n",
      "2002.11.05 2002.08.18 2002.10.16 1\n",
      "2002.11.06 2002.08.19 2002.10.17 1\n",
      "2002.11.07 2002.08.20 2002.10.18 1\n",
      "2002.11.08 2002.08.21 2002.10.19 1\n",
      "2002.11.09 2002.08.22 2002.10.20 1\n",
      "2002.11.10 2002.08.23 2002.10.21 1\n",
      "2002.11.11 2002.08.24 2002.10.22 1\n",
      "2002.11.12 2002.08.25 2002.10.23 1\n",
      "2002.11.13 2002.08.26 2002.10.24 1\n",
      "2002.11.14 2002.08.27 2002.10.25 1\n",
      "2002.11.15 2002.08.28 2002.10.26 1\n",
      "2002.11.16 2002.08.29 2002.10.27 1\n",
      "2002.11.17 2002.08.30 2002.10.28 1\n",
      "2002.11.18 2002.08.31 2002.10.29 1\n",
      "2002.11.19 2002.09.01 2002.10.30 1\n",
      "2002.11.20 2002.09.02 2002.10.31 1\n",
      "2002.11.21 2002.09.03 2002.11.01 1\n",
      "2002.11.22 2002.09.04 2002.11.02 1\n",
      "2002.11.23 2002.09.05 2002.11.03 1\n",
      "2002.11.24 2002.09.06 2002.11.04 1\n",
      "2002.11.25 2002.09.07 2002.11.05 1\n",
      "2002.11.26 2002.09.08 2002.11.06 1\n",
      "2002.11.27 2002.09.09 2002.11.07 1\n",
      "2002.11.28 2002.09.10 2002.11.08 1\n",
      "2002.11.29 2002.09.11 2002.11.09 1\n",
      "2002.11.30 2002.09.12 2002.11.10 1\n",
      "2002.12.01 2002.09.13 2002.11.11 1\n",
      "2002.12.02 2002.09.14 2002.11.12 1\n",
      "2002.12.03 2002.09.15 2002.11.13 1\n",
      "2002.12.04 2002.09.16 2002.11.14 1\n",
      "2002.12.05 2002.09.17 2002.11.15 1\n",
      "2002.12.06 2002.09.18 2002.11.16 1\n",
      "2002.12.07 2002.09.19 2002.11.17 1\n",
      "2002.12.08 2002.09.20 2002.11.18 1\n",
      "2002.12.09 2002.09.21 2002.11.19 1\n",
      "2002.12.10 2002.09.22 2002.11.20 1\n",
      "2002.12.11 2002.09.23 2002.11.21 1\n",
      "2002.12.12 2002.09.24 2002.11.22 1\n",
      "2002.12.13 2002.09.25 2002.11.23 1\n",
      "2002.12.14 2002.09.26 2002.11.24 1\n",
      "2002.12.15 2002.09.27 2002.11.25 1\n",
      "2002.12.16 2002.09.28 2002.11.26 1\n",
      "2002.12.17 2002.09.29 2002.11.27 1\n",
      "2002.12.18 2002.09.30 2002.11.28 1\n",
      "2002.12.19 2002.10.01 2002.11.29 1\n",
      "2002.12.20 2002.10.02 2002.11.30 1\n",
      "2002.12.21 2002.10.03 2002.12.01 1\n",
      "2002.12.22 2002.10.04 2002.12.02 1\n",
      "2002.12.23 2002.10.05 2002.12.03 1\n",
      "2002.12.24 2002.10.06 2002.12.04 1\n",
      "2002.12.25 2002.10.07 2002.12.05 1\n",
      "2002.12.26 2002.10.08 2002.12.06 1\n",
      "2002.12.27 2002.10.09 2002.12.07 1\n",
      "2002.12.28 2002.10.10 2002.12.08 1\n",
      "2003.10.04 2003.07.17 2003.09.14 0\n",
      "2003.10.05 2003.07.18 2003.09.15 1\n",
      "2003.10.06 2003.07.19 2003.09.16 1\n",
      "2003.10.07 2003.07.20 2003.09.17 1\n",
      "2003.10.08 2003.07.21 2003.09.18 1\n",
      "2003.10.09 2003.07.22 2003.09.19 1\n",
      "2003.10.10 2003.07.23 2003.09.20 1\n",
      "2003.10.11 2003.07.24 2003.09.21 1\n",
      "2003.10.12 2003.07.25 2003.09.22 1\n",
      "2003.10.13 2003.07.26 2003.09.23 1\n",
      "2003.10.14 2003.07.27 2003.09.24 1\n",
      "2003.10.15 2003.07.28 2003.09.25 1\n",
      "2003.10.16 2003.07.29 2003.09.26 1\n",
      "2003.10.17 2003.07.30 2003.09.27 1\n",
      "2003.10.18 2003.07.31 2003.09.28 1\n",
      "2003.10.19 2003.08.01 2003.09.29 1\n",
      "2003.10.20 2003.08.02 2003.09.30 1\n",
      "2003.10.21 2003.08.03 2003.10.01 1\n",
      "2003.10.22 2003.08.04 2003.10.02 1\n",
      "2003.10.23 2003.08.05 2003.10.03 1\n",
      "2003.10.24 2003.08.06 2003.10.04 1\n",
      "2003.10.25 2003.08.07 2003.10.05 1\n",
      "2003.10.26 2003.08.08 2003.10.06 1\n",
      "2003.10.27 2003.08.09 2003.10.07 1\n",
      "2003.10.28 2003.08.10 2003.10.08 1\n",
      "2003.10.29 2003.08.11 2003.10.09 1\n",
      "2003.10.30 2003.08.12 2003.10.10 1\n",
      "2003.10.31 2003.08.13 2003.10.11 1\n",
      "2003.11.01 2003.08.14 2003.10.12 1\n",
      "2003.11.02 2003.08.15 2003.10.13 1\n",
      "2003.11.03 2003.08.16 2003.10.14 1\n",
      "2003.11.04 2003.08.17 2003.10.15 1\n",
      "2003.11.05 2003.08.18 2003.10.16 1\n",
      "2003.11.06 2003.08.19 2003.10.17 1\n",
      "2003.11.07 2003.08.20 2003.10.18 1\n",
      "2003.11.08 2003.08.21 2003.10.19 1\n",
      "2003.11.09 2003.08.22 2003.10.20 1\n",
      "2003.11.10 2003.08.23 2003.10.21 1\n",
      "2003.11.11 2003.08.24 2003.10.22 1\n",
      "2003.11.12 2003.08.25 2003.10.23 1\n",
      "2003.11.13 2003.08.26 2003.10.24 1\n",
      "2003.11.14 2003.08.27 2003.10.25 1\n",
      "2003.11.15 2003.08.28 2003.10.26 1\n",
      "2003.11.16 2003.08.29 2003.10.27 1\n",
      "2003.11.17 2003.08.30 2003.10.28 1\n",
      "2003.11.18 2003.08.31 2003.10.29 1\n",
      "2003.11.19 2003.09.01 2003.10.30 1\n",
      "2003.11.20 2003.09.02 2003.10.31 1\n",
      "2003.11.21 2003.09.03 2003.11.01 1\n",
      "2003.11.22 2003.09.04 2003.11.02 1\n",
      "2003.11.23 2003.09.05 2003.11.03 1\n",
      "2003.11.24 2003.09.06 2003.11.04 1\n",
      "2003.11.25 2003.09.07 2003.11.05 1\n",
      "2003.11.26 2003.09.08 2003.11.06 1\n",
      "2003.11.27 2003.09.09 2003.11.07 1\n",
      "2003.11.28 2003.09.10 2003.11.08 1\n",
      "2003.11.29 2003.09.11 2003.11.09 1\n",
      "2003.11.30 2003.09.12 2003.11.10 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2003.12.01 2003.09.13 2003.11.11 1\n",
      "2003.12.02 2003.09.14 2003.11.12 1\n",
      "2003.12.03 2003.09.15 2003.11.13 1\n",
      "2003.12.04 2003.09.16 2003.11.14 1\n",
      "2003.12.05 2003.09.17 2003.11.15 1\n",
      "2003.12.06 2003.09.18 2003.11.16 1\n",
      "2003.12.07 2003.09.19 2003.11.17 1\n",
      "2003.12.08 2003.09.20 2003.11.18 1\n",
      "2003.12.09 2003.09.21 2003.11.19 1\n",
      "2003.12.10 2003.09.22 2003.11.20 1\n",
      "2003.12.11 2003.09.23 2003.11.21 1\n",
      "2003.12.12 2003.09.24 2003.11.22 1\n",
      "2003.12.13 2003.09.25 2003.11.23 1\n",
      "2003.12.14 2003.09.26 2003.11.24 1\n",
      "2003.12.15 2003.09.27 2003.11.25 1\n",
      "2003.12.16 2003.09.28 2003.11.26 1\n",
      "2003.12.17 2003.09.29 2003.11.27 1\n",
      "2003.12.18 2003.09.30 2003.11.28 1\n",
      "2003.12.19 2003.10.01 2003.11.29 1\n",
      "2003.12.20 2003.10.02 2003.11.30 1\n",
      "2003.12.21 2003.10.03 2003.12.01 1\n",
      "2003.12.22 2003.10.04 2003.12.02 1\n",
      "2003.12.23 2003.10.05 2003.12.03 1\n",
      "2003.12.24 2003.10.06 2003.12.04 1\n",
      "2003.12.25 2003.10.07 2003.12.05 1\n",
      "2003.12.26 2003.10.08 2003.12.06 1\n",
      "2003.12.27 2003.10.09 2003.12.07 1\n",
      "2003.12.28 2003.10.10 2003.12.08 1\n",
      "2004.10.04 2004.07.17 2004.09.14 0\n",
      "2004.10.05 2004.07.18 2004.09.15 1\n",
      "2004.10.06 2004.07.19 2004.09.16 1\n",
      "2004.10.07 2004.07.20 2004.09.17 1\n",
      "2004.10.08 2004.07.21 2004.09.18 1\n",
      "2004.10.09 2004.07.22 2004.09.19 1\n",
      "2004.10.10 2004.07.23 2004.09.20 1\n",
      "2004.10.11 2004.07.24 2004.09.21 1\n",
      "2004.10.12 2004.07.25 2004.09.22 1\n",
      "2004.10.13 2004.07.26 2004.09.23 1\n",
      "2004.10.14 2004.07.27 2004.09.24 1\n",
      "2004.10.15 2004.07.28 2004.09.25 1\n",
      "2004.10.16 2004.07.29 2004.09.26 1\n",
      "2004.10.17 2004.07.30 2004.09.27 1\n",
      "2004.10.18 2004.07.31 2004.09.28 1\n",
      "2004.10.19 2004.08.01 2004.09.29 1\n",
      "2004.10.20 2004.08.02 2004.09.30 1\n",
      "2004.10.21 2004.08.03 2004.10.01 1\n",
      "2004.10.22 2004.08.04 2004.10.02 1\n",
      "2004.10.23 2004.08.05 2004.10.03 1\n",
      "2004.10.24 2004.08.06 2004.10.04 1\n",
      "2004.10.25 2004.08.07 2004.10.05 1\n",
      "2004.10.26 2004.08.08 2004.10.06 1\n",
      "2004.10.27 2004.08.09 2004.10.07 1\n",
      "2004.10.28 2004.08.10 2004.10.08 1\n",
      "2004.10.29 2004.08.11 2004.10.09 1\n",
      "2004.10.30 2004.08.12 2004.10.10 1\n",
      "2004.10.31 2004.08.13 2004.10.11 1\n",
      "2004.11.01 2004.08.14 2004.10.12 1\n",
      "2004.11.02 2004.08.15 2004.10.13 1\n",
      "2004.11.03 2004.08.16 2004.10.14 1\n",
      "2004.11.04 2004.08.17 2004.10.15 1\n",
      "2004.11.05 2004.08.18 2004.10.16 1\n",
      "2004.11.06 2004.08.19 2004.10.17 1\n",
      "2004.11.07 2004.08.20 2004.10.18 1\n",
      "2004.11.08 2004.08.21 2004.10.19 1\n",
      "2004.11.09 2004.08.22 2004.10.20 1\n",
      "2004.11.10 2004.08.23 2004.10.21 1\n",
      "2004.11.11 2004.08.24 2004.10.22 1\n",
      "2004.11.12 2004.08.25 2004.10.23 1\n",
      "2004.11.13 2004.08.26 2004.10.24 1\n",
      "2004.11.14 2004.08.27 2004.10.25 1\n",
      "2004.11.15 2004.08.28 2004.10.26 1\n",
      "2004.11.16 2004.08.29 2004.10.27 1\n",
      "2004.11.17 2004.08.30 2004.10.28 1\n",
      "2004.11.18 2004.08.31 2004.10.29 1\n",
      "2004.11.19 2004.09.01 2004.10.30 1\n",
      "2004.11.20 2004.09.02 2004.10.31 1\n",
      "2004.11.21 2004.09.03 2004.11.01 1\n",
      "2004.11.22 2004.09.04 2004.11.02 1\n",
      "2004.11.23 2004.09.05 2004.11.03 1\n",
      "2004.11.24 2004.09.06 2004.11.04 1\n",
      "2004.11.25 2004.09.07 2004.11.05 1\n",
      "2004.11.26 2004.09.08 2004.11.06 1\n",
      "2004.11.27 2004.09.09 2004.11.07 1\n",
      "2004.11.28 2004.09.10 2004.11.08 1\n",
      "2004.11.29 2004.09.11 2004.11.09 1\n",
      "2004.11.30 2004.09.12 2004.11.10 1\n",
      "2004.12.01 2004.09.13 2004.11.11 1\n",
      "2004.12.02 2004.09.14 2004.11.12 1\n",
      "2004.12.03 2004.09.15 2004.11.13 1\n",
      "2004.12.04 2004.09.16 2004.11.14 1\n",
      "2004.12.05 2004.09.17 2004.11.15 1\n",
      "2004.12.06 2004.09.18 2004.11.16 1\n",
      "2004.12.07 2004.09.19 2004.11.17 1\n",
      "2004.12.08 2004.09.20 2004.11.18 1\n",
      "2004.12.09 2004.09.21 2004.11.19 1\n",
      "2004.12.10 2004.09.22 2004.11.20 1\n",
      "2004.12.11 2004.09.23 2004.11.21 1\n",
      "2004.12.12 2004.09.24 2004.11.22 1\n",
      "2004.12.13 2004.09.25 2004.11.23 1\n",
      "2004.12.14 2004.09.26 2004.11.24 1\n",
      "2004.12.15 2004.09.27 2004.11.25 1\n",
      "2004.12.16 2004.09.28 2004.11.26 1\n",
      "2004.12.17 2004.09.29 2004.11.27 1\n",
      "2004.12.18 2004.09.30 2004.11.28 1\n",
      "2004.12.19 2004.10.01 2004.11.29 1\n",
      "2004.12.20 2004.10.02 2004.11.30 1\n",
      "2004.12.21 2004.10.03 2004.12.01 1\n",
      "2004.12.22 2004.10.04 2004.12.02 1\n",
      "2004.12.23 2004.10.05 2004.12.03 1\n",
      "2004.12.24 2004.10.06 2004.12.04 1\n",
      "2004.12.25 2004.10.07 2004.12.05 1\n",
      "2004.12.26 2004.10.08 2004.12.06 1\n",
      "2004.12.27 2004.10.09 2004.12.07 1\n",
      "2004.12.28 2004.10.10 2004.12.08 1\n",
      "2005.10.04 2005.07.17 2005.09.14 0\n",
      "2005.10.05 2005.07.18 2005.09.15 1\n",
      "2005.10.06 2005.07.19 2005.09.16 1\n",
      "2005.10.07 2005.07.20 2005.09.17 1\n",
      "2005.10.08 2005.07.21 2005.09.18 1\n",
      "2005.10.09 2005.07.22 2005.09.19 1\n",
      "2005.10.10 2005.07.23 2005.09.20 1\n",
      "2005.10.11 2005.07.24 2005.09.21 1\n",
      "2005.10.12 2005.07.25 2005.09.22 1\n",
      "2005.10.13 2005.07.26 2005.09.23 1\n",
      "2005.10.14 2005.07.27 2005.09.24 1\n",
      "2005.10.15 2005.07.28 2005.09.25 1\n",
      "2005.10.16 2005.07.29 2005.09.26 1\n",
      "2005.10.17 2005.07.30 2005.09.27 1\n",
      "2005.10.18 2005.07.31 2005.09.28 1\n",
      "2005.10.19 2005.08.01 2005.09.29 1\n",
      "2005.10.20 2005.08.02 2005.09.30 1\n",
      "2005.10.21 2005.08.03 2005.10.01 1\n",
      "2005.10.22 2005.08.04 2005.10.02 1\n",
      "2005.10.23 2005.08.05 2005.10.03 1\n",
      "2005.10.24 2005.08.06 2005.10.04 1\n",
      "2005.10.25 2005.08.07 2005.10.05 1\n",
      "2005.10.26 2005.08.08 2005.10.06 1\n",
      "2005.10.27 2005.08.09 2005.10.07 1\n",
      "2005.10.28 2005.08.10 2005.10.08 1\n",
      "2005.10.29 2005.08.11 2005.10.09 1\n",
      "2005.10.30 2005.08.12 2005.10.10 1\n",
      "2005.10.31 2005.08.13 2005.10.11 1\n",
      "2005.11.01 2005.08.14 2005.10.12 1\n",
      "2005.11.02 2005.08.15 2005.10.13 1\n",
      "2005.11.03 2005.08.16 2005.10.14 1\n",
      "2005.11.04 2005.08.17 2005.10.15 1\n",
      "2005.11.05 2005.08.18 2005.10.16 1\n",
      "2005.11.06 2005.08.19 2005.10.17 1\n",
      "2005.11.07 2005.08.20 2005.10.18 1\n",
      "2005.11.08 2005.08.21 2005.10.19 1\n",
      "2005.11.09 2005.08.22 2005.10.20 1\n",
      "2005.11.10 2005.08.23 2005.10.21 1\n",
      "2005.11.11 2005.08.24 2005.10.22 1\n",
      "2005.11.12 2005.08.25 2005.10.23 1\n",
      "2005.11.13 2005.08.26 2005.10.24 1\n",
      "2005.11.14 2005.08.27 2005.10.25 1\n",
      "2005.11.15 2005.08.28 2005.10.26 1\n",
      "2005.11.16 2005.08.29 2005.10.27 1\n",
      "2005.11.17 2005.08.30 2005.10.28 1\n",
      "2005.11.18 2005.08.31 2005.10.29 1\n",
      "2005.11.19 2005.09.01 2005.10.30 1\n",
      "2005.11.20 2005.09.02 2005.10.31 1\n",
      "2005.11.21 2005.09.03 2005.11.01 1\n",
      "2005.11.22 2005.09.04 2005.11.02 1\n",
      "2005.11.23 2005.09.05 2005.11.03 1\n",
      "2005.11.24 2005.09.06 2005.11.04 1\n",
      "2005.11.25 2005.09.07 2005.11.05 1\n",
      "2005.11.26 2005.09.08 2005.11.06 1\n",
      "2005.11.27 2005.09.09 2005.11.07 1\n",
      "2005.11.28 2005.09.10 2005.11.08 1\n",
      "2005.11.29 2005.09.11 2005.11.09 1\n",
      "2005.11.30 2005.09.12 2005.11.10 1\n",
      "2005.12.01 2005.09.13 2005.11.11 1\n",
      "2005.12.02 2005.09.14 2005.11.12 1\n",
      "2005.12.03 2005.09.15 2005.11.13 1\n",
      "2005.12.04 2005.09.16 2005.11.14 1\n",
      "2005.12.05 2005.09.17 2005.11.15 1\n",
      "2005.12.06 2005.09.18 2005.11.16 1\n",
      "2005.12.07 2005.09.19 2005.11.17 1\n",
      "2005.12.08 2005.09.20 2005.11.18 1\n",
      "2005.12.09 2005.09.21 2005.11.19 1\n",
      "2005.12.10 2005.09.22 2005.11.20 1\n",
      "2005.12.11 2005.09.23 2005.11.21 1\n",
      "2005.12.12 2005.09.24 2005.11.22 1\n",
      "2005.12.13 2005.09.25 2005.11.23 1\n",
      "2005.12.14 2005.09.26 2005.11.24 1\n",
      "2005.12.15 2005.09.27 2005.11.25 1\n",
      "2005.12.16 2005.09.28 2005.11.26 1\n",
      "2005.12.17 2005.09.29 2005.11.27 1\n",
      "2005.12.18 2005.09.30 2005.11.28 1\n",
      "2005.12.19 2005.10.01 2005.11.29 1\n",
      "2005.12.20 2005.10.02 2005.11.30 1\n",
      "2005.12.21 2005.10.03 2005.12.01 1\n",
      "2005.12.22 2005.10.04 2005.12.02 1\n",
      "2005.12.23 2005.10.05 2005.12.03 1\n",
      "2005.12.24 2005.10.06 2005.12.04 1\n",
      "2005.12.25 2005.10.07 2005.12.05 1\n",
      "2005.12.26 2005.10.08 2005.12.06 1\n",
      "2005.12.27 2005.10.09 2005.12.07 1\n",
      "2005.12.28 2005.10.10 2005.12.08 1\n",
      "2006.10.04 2006.07.17 2006.09.14 0\n",
      "2006.10.05 2006.07.18 2006.09.15 1\n",
      "2006.10.06 2006.07.19 2006.09.16 1\n",
      "2006.10.07 2006.07.20 2006.09.17 1\n",
      "2006.10.08 2006.07.21 2006.09.18 1\n",
      "2006.10.09 2006.07.22 2006.09.19 1\n",
      "2006.10.10 2006.07.23 2006.09.20 1\n",
      "2006.10.11 2006.07.24 2006.09.21 1\n",
      "2006.10.12 2006.07.25 2006.09.22 1\n",
      "2006.10.13 2006.07.26 2006.09.23 1\n",
      "2006.10.14 2006.07.27 2006.09.24 1\n",
      "2006.10.15 2006.07.28 2006.09.25 1\n",
      "2006.10.16 2006.07.29 2006.09.26 1\n",
      "2006.10.17 2006.07.30 2006.09.27 1\n",
      "2006.10.18 2006.07.31 2006.09.28 1\n",
      "2006.10.19 2006.08.01 2006.09.29 1\n",
      "2006.10.20 2006.08.02 2006.09.30 1\n",
      "2006.10.21 2006.08.03 2006.10.01 1\n",
      "2006.10.22 2006.08.04 2006.10.02 1\n",
      "2006.10.23 2006.08.05 2006.10.03 1\n",
      "2006.10.24 2006.08.06 2006.10.04 1\n",
      "2006.10.25 2006.08.07 2006.10.05 1\n",
      "2006.10.26 2006.08.08 2006.10.06 1\n",
      "2006.10.27 2006.08.09 2006.10.07 1\n",
      "2006.10.28 2006.08.10 2006.10.08 1\n",
      "2006.10.29 2006.08.11 2006.10.09 1\n",
      "2006.10.30 2006.08.12 2006.10.10 1\n",
      "2006.10.31 2006.08.13 2006.10.11 1\n",
      "2006.11.01 2006.08.14 2006.10.12 1\n",
      "2006.11.02 2006.08.15 2006.10.13 1\n",
      "2006.11.03 2006.08.16 2006.10.14 1\n",
      "2006.11.04 2006.08.17 2006.10.15 1\n",
      "2006.11.05 2006.08.18 2006.10.16 1\n",
      "2006.11.06 2006.08.19 2006.10.17 1\n",
      "2006.11.07 2006.08.20 2006.10.18 1\n",
      "2006.11.08 2006.08.21 2006.10.19 1\n",
      "2006.11.09 2006.08.22 2006.10.20 1\n",
      "2006.11.10 2006.08.23 2006.10.21 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006.11.11 2006.08.24 2006.10.22 1\n",
      "2006.11.12 2006.08.25 2006.10.23 1\n",
      "2006.11.13 2006.08.26 2006.10.24 1\n",
      "2006.11.14 2006.08.27 2006.10.25 1\n",
      "2006.11.15 2006.08.28 2006.10.26 1\n",
      "2006.11.16 2006.08.29 2006.10.27 1\n",
      "2006.11.17 2006.08.30 2006.10.28 1\n",
      "2006.11.18 2006.08.31 2006.10.29 1\n",
      "2006.11.19 2006.09.01 2006.10.30 1\n",
      "2006.11.20 2006.09.02 2006.10.31 1\n",
      "2006.11.21 2006.09.03 2006.11.01 1\n",
      "2006.11.22 2006.09.04 2006.11.02 1\n",
      "2006.11.23 2006.09.05 2006.11.03 1\n",
      "2006.11.24 2006.09.06 2006.11.04 1\n",
      "2006.11.25 2006.09.07 2006.11.05 1\n",
      "2006.11.26 2006.09.08 2006.11.06 1\n",
      "2006.11.27 2006.09.09 2006.11.07 1\n",
      "2006.11.28 2006.09.10 2006.11.08 1\n",
      "2006.11.29 2006.09.11 2006.11.09 1\n",
      "2006.11.30 2006.09.12 2006.11.10 1\n",
      "2006.12.01 2006.09.13 2006.11.11 1\n",
      "2006.12.02 2006.09.14 2006.11.12 1\n",
      "2006.12.03 2006.09.15 2006.11.13 1\n",
      "2006.12.04 2006.09.16 2006.11.14 1\n",
      "2006.12.05 2006.09.17 2006.11.15 1\n",
      "2006.12.06 2006.09.18 2006.11.16 1\n",
      "2006.12.07 2006.09.19 2006.11.17 1\n",
      "2006.12.08 2006.09.20 2006.11.18 1\n",
      "2006.12.09 2006.09.21 2006.11.19 1\n",
      "2006.12.10 2006.09.22 2006.11.20 1\n",
      "2006.12.11 2006.09.23 2006.11.21 1\n",
      "2006.12.12 2006.09.24 2006.11.22 1\n",
      "2006.12.13 2006.09.25 2006.11.23 1\n",
      "2006.12.14 2006.09.26 2006.11.24 1\n",
      "2006.12.15 2006.09.27 2006.11.25 1\n",
      "2006.12.16 2006.09.28 2006.11.26 1\n",
      "2006.12.17 2006.09.29 2006.11.27 1\n",
      "2006.12.18 2006.09.30 2006.11.28 1\n",
      "2006.12.19 2006.10.01 2006.11.29 1\n",
      "2006.12.20 2006.10.02 2006.11.30 1\n",
      "2006.12.21 2006.10.03 2006.12.01 1\n",
      "2006.12.22 2006.10.04 2006.12.02 1\n",
      "2006.12.23 2006.10.05 2006.12.03 1\n",
      "2006.12.24 2006.10.06 2006.12.04 1\n",
      "2006.12.25 2006.10.07 2006.12.05 1\n",
      "2006.12.26 2006.10.08 2006.12.06 1\n",
      "2006.12.27 2006.10.09 2006.12.07 1\n",
      "2006.12.28 2006.10.10 2006.12.08 1\n",
      "2007.10.04 2007.07.17 2007.09.14 0\n",
      "2007.10.05 2007.07.18 2007.09.15 1\n",
      "2007.10.06 2007.07.19 2007.09.16 1\n",
      "2007.10.07 2007.07.20 2007.09.17 1\n",
      "2007.10.08 2007.07.21 2007.09.18 1\n",
      "2007.10.09 2007.07.22 2007.09.19 1\n",
      "2007.10.10 2007.07.23 2007.09.20 1\n",
      "2007.10.11 2007.07.24 2007.09.21 1\n",
      "2007.10.12 2007.07.25 2007.09.22 1\n",
      "2007.10.13 2007.07.26 2007.09.23 1\n",
      "2007.10.14 2007.07.27 2007.09.24 1\n",
      "2007.10.15 2007.07.28 2007.09.25 1\n",
      "2007.10.16 2007.07.29 2007.09.26 1\n",
      "2007.10.17 2007.07.30 2007.09.27 1\n",
      "2007.10.18 2007.07.31 2007.09.28 1\n",
      "2007.10.19 2007.08.01 2007.09.29 1\n",
      "2007.10.20 2007.08.02 2007.09.30 1\n",
      "2007.10.21 2007.08.03 2007.10.01 1\n",
      "2007.10.22 2007.08.04 2007.10.02 1\n",
      "2007.10.23 2007.08.05 2007.10.03 1\n",
      "2007.10.24 2007.08.06 2007.10.04 1\n",
      "2007.10.25 2007.08.07 2007.10.05 1\n",
      "2007.10.26 2007.08.08 2007.10.06 1\n",
      "2007.10.27 2007.08.09 2007.10.07 1\n",
      "2007.10.28 2007.08.10 2007.10.08 1\n",
      "2007.10.29 2007.08.11 2007.10.09 1\n",
      "2007.10.30 2007.08.12 2007.10.10 1\n",
      "2007.10.31 2007.08.13 2007.10.11 1\n",
      "2007.11.01 2007.08.14 2007.10.12 1\n",
      "2007.11.02 2007.08.15 2007.10.13 1\n",
      "2007.11.03 2007.08.16 2007.10.14 1\n",
      "2007.11.04 2007.08.17 2007.10.15 1\n",
      "2007.11.05 2007.08.18 2007.10.16 1\n",
      "2007.11.06 2007.08.19 2007.10.17 1\n",
      "2007.11.07 2007.08.20 2007.10.18 1\n",
      "2007.11.08 2007.08.21 2007.10.19 1\n",
      "2007.11.09 2007.08.22 2007.10.20 1\n",
      "2007.11.10 2007.08.23 2007.10.21 1\n",
      "2007.11.11 2007.08.24 2007.10.22 1\n",
      "2007.11.12 2007.08.25 2007.10.23 1\n",
      "2007.11.13 2007.08.26 2007.10.24 1\n",
      "2007.11.14 2007.08.27 2007.10.25 1\n",
      "2007.11.15 2007.08.28 2007.10.26 1\n",
      "2007.11.16 2007.08.29 2007.10.27 1\n",
      "2007.11.17 2007.08.30 2007.10.28 1\n",
      "2007.11.18 2007.08.31 2007.10.29 1\n",
      "2007.11.19 2007.09.01 2007.10.30 1\n",
      "2007.11.20 2007.09.02 2007.10.31 1\n",
      "2007.11.21 2007.09.03 2007.11.01 1\n",
      "2007.11.22 2007.09.04 2007.11.02 1\n",
      "2007.11.23 2007.09.05 2007.11.03 1\n",
      "2007.11.24 2007.09.06 2007.11.04 1\n",
      "2007.11.25 2007.09.07 2007.11.05 1\n",
      "2007.11.26 2007.09.08 2007.11.06 1\n",
      "2007.11.27 2007.09.09 2007.11.07 1\n",
      "2007.11.28 2007.09.10 2007.11.08 1\n",
      "2007.11.29 2007.09.11 2007.11.09 1\n",
      "2007.11.30 2007.09.12 2007.11.10 1\n",
      "2007.12.01 2007.09.13 2007.11.11 1\n",
      "2007.12.02 2007.09.14 2007.11.12 1\n",
      "2007.12.03 2007.09.15 2007.11.13 1\n",
      "2007.12.04 2007.09.16 2007.11.14 1\n",
      "2007.12.05 2007.09.17 2007.11.15 1\n",
      "2007.12.06 2007.09.18 2007.11.16 1\n",
      "2007.12.07 2007.09.19 2007.11.17 1\n",
      "2007.12.08 2007.09.20 2007.11.18 1\n",
      "2007.12.09 2007.09.21 2007.11.19 1\n",
      "2007.12.10 2007.09.22 2007.11.20 1\n",
      "2007.12.11 2007.09.23 2007.11.21 1\n",
      "2007.12.12 2007.09.24 2007.11.22 1\n",
      "2007.12.13 2007.09.25 2007.11.23 1\n",
      "2007.12.14 2007.09.26 2007.11.24 1\n",
      "2007.12.15 2007.09.27 2007.11.25 1\n",
      "2007.12.16 2007.09.28 2007.11.26 1\n",
      "2007.12.17 2007.09.29 2007.11.27 1\n",
      "2007.12.18 2007.09.30 2007.11.28 1\n",
      "2007.12.19 2007.10.01 2007.11.29 1\n",
      "2007.12.20 2007.10.02 2007.11.30 1\n",
      "2007.12.21 2007.10.03 2007.12.01 1\n",
      "2007.12.22 2007.10.04 2007.12.02 1\n",
      "2007.12.23 2007.10.05 2007.12.03 1\n",
      "2007.12.24 2007.10.06 2007.12.04 1\n",
      "2007.12.25 2007.10.07 2007.12.05 1\n",
      "2007.12.26 2007.10.08 2007.12.06 1\n",
      "2007.12.27 2007.10.09 2007.12.07 1\n",
      "2007.12.28 2007.10.10 2007.12.08 1\n",
      "2008.10.04 2008.07.17 2008.09.14 0\n",
      "2008.10.05 2008.07.18 2008.09.15 1\n",
      "2008.10.06 2008.07.19 2008.09.16 1\n",
      "2008.10.07 2008.07.20 2008.09.17 1\n",
      "2008.10.08 2008.07.21 2008.09.18 1\n",
      "2008.10.09 2008.07.22 2008.09.19 1\n",
      "2008.10.10 2008.07.23 2008.09.20 1\n",
      "2008.10.11 2008.07.24 2008.09.21 1\n",
      "2008.10.12 2008.07.25 2008.09.22 1\n",
      "2008.10.13 2008.07.26 2008.09.23 1\n",
      "2008.10.14 2008.07.27 2008.09.24 1\n",
      "2008.10.15 2008.07.28 2008.09.25 1\n",
      "2008.10.16 2008.07.29 2008.09.26 1\n",
      "2008.10.17 2008.07.30 2008.09.27 1\n",
      "2008.10.18 2008.07.31 2008.09.28 1\n",
      "2008.10.19 2008.08.01 2008.09.29 1\n",
      "2008.10.20 2008.08.02 2008.09.30 1\n",
      "2008.10.21 2008.08.03 2008.10.01 1\n",
      "2008.10.22 2008.08.04 2008.10.02 1\n",
      "2008.10.23 2008.08.05 2008.10.03 1\n",
      "2008.10.24 2008.08.06 2008.10.04 1\n",
      "2008.10.25 2008.08.07 2008.10.05 1\n",
      "2008.10.26 2008.08.08 2008.10.06 1\n",
      "2008.10.27 2008.08.09 2008.10.07 1\n",
      "2008.10.28 2008.08.10 2008.10.08 1\n",
      "2008.10.29 2008.08.11 2008.10.09 1\n",
      "2008.10.30 2008.08.12 2008.10.10 1\n",
      "2008.10.31 2008.08.13 2008.10.11 1\n",
      "2008.11.01 2008.08.14 2008.10.12 1\n",
      "2008.11.02 2008.08.15 2008.10.13 1\n",
      "2008.11.03 2008.08.16 2008.10.14 1\n",
      "2008.11.04 2008.08.17 2008.10.15 1\n",
      "2008.11.05 2008.08.18 2008.10.16 1\n",
      "2008.11.06 2008.08.19 2008.10.17 1\n",
      "2008.11.07 2008.08.20 2008.10.18 1\n",
      "2008.11.08 2008.08.21 2008.10.19 1\n",
      "2008.11.09 2008.08.22 2008.10.20 1\n",
      "2008.11.10 2008.08.23 2008.10.21 1\n",
      "2008.11.11 2008.08.24 2008.10.22 1\n",
      "2008.11.12 2008.08.25 2008.10.23 1\n",
      "2008.11.13 2008.08.26 2008.10.24 1\n",
      "2008.11.14 2008.08.27 2008.10.25 1\n",
      "2008.11.15 2008.08.28 2008.10.26 1\n",
      "2008.11.16 2008.08.29 2008.10.27 1\n",
      "2008.11.17 2008.08.30 2008.10.28 1\n",
      "2008.11.18 2008.08.31 2008.10.29 1\n",
      "2008.11.19 2008.09.01 2008.10.30 1\n",
      "2008.11.20 2008.09.02 2008.10.31 1\n",
      "2008.11.21 2008.09.03 2008.11.01 1\n",
      "2008.11.22 2008.09.04 2008.11.02 1\n",
      "2008.11.23 2008.09.05 2008.11.03 1\n",
      "2008.11.24 2008.09.06 2008.11.04 1\n",
      "2008.11.25 2008.09.07 2008.11.05 1\n",
      "2008.11.26 2008.09.08 2008.11.06 1\n",
      "2008.11.27 2008.09.09 2008.11.07 1\n",
      "2008.11.28 2008.09.10 2008.11.08 1\n",
      "2008.11.29 2008.09.11 2008.11.09 1\n",
      "2008.11.30 2008.09.12 2008.11.10 1\n",
      "2008.12.01 2008.09.13 2008.11.11 1\n",
      "2008.12.02 2008.09.14 2008.11.12 1\n",
      "2008.12.03 2008.09.15 2008.11.13 1\n",
      "2008.12.04 2008.09.16 2008.11.14 1\n",
      "2008.12.05 2008.09.17 2008.11.15 1\n",
      "2008.12.06 2008.09.18 2008.11.16 1\n",
      "2008.12.07 2008.09.19 2008.11.17 1\n",
      "2008.12.08 2008.09.20 2008.11.18 1\n",
      "2008.12.09 2008.09.21 2008.11.19 1\n",
      "2008.12.10 2008.09.22 2008.11.20 1\n",
      "2008.12.11 2008.09.23 2008.11.21 1\n",
      "2008.12.12 2008.09.24 2008.11.22 1\n",
      "2008.12.13 2008.09.25 2008.11.23 1\n",
      "2008.12.14 2008.09.26 2008.11.24 1\n",
      "2008.12.15 2008.09.27 2008.11.25 1\n",
      "2008.12.16 2008.09.28 2008.11.26 1\n",
      "2008.12.17 2008.09.29 2008.11.27 1\n",
      "2008.12.18 2008.09.30 2008.11.28 1\n",
      "2008.12.19 2008.10.01 2008.11.29 1\n",
      "2008.12.20 2008.10.02 2008.11.30 1\n",
      "2008.12.21 2008.10.03 2008.12.01 1\n",
      "2008.12.22 2008.10.04 2008.12.02 1\n",
      "2008.12.23 2008.10.05 2008.12.03 1\n",
      "2008.12.24 2008.10.06 2008.12.04 1\n",
      "2008.12.25 2008.10.07 2008.12.05 1\n",
      "2008.12.26 2008.10.08 2008.12.06 1\n",
      "2008.12.27 2008.10.09 2008.12.07 1\n",
      "2008.12.28 2008.10.10 2008.12.08 1\n",
      "2009.10.04 2009.07.17 2009.09.14 0\n",
      "2009.10.05 2009.07.18 2009.09.15 1\n",
      "2009.10.06 2009.07.19 2009.09.16 1\n",
      "2009.10.07 2009.07.20 2009.09.17 1\n",
      "2009.10.08 2009.07.21 2009.09.18 1\n",
      "2009.10.09 2009.07.22 2009.09.19 1\n",
      "2009.10.10 2009.07.23 2009.09.20 1\n",
      "2009.10.11 2009.07.24 2009.09.21 1\n",
      "2009.10.12 2009.07.25 2009.09.22 1\n",
      "2009.10.13 2009.07.26 2009.09.23 1\n",
      "2009.10.14 2009.07.27 2009.09.24 1\n",
      "2009.10.15 2009.07.28 2009.09.25 1\n",
      "2009.10.16 2009.07.29 2009.09.26 1\n",
      "2009.10.17 2009.07.30 2009.09.27 1\n",
      "2009.10.18 2009.07.31 2009.09.28 1\n",
      "2009.10.19 2009.08.01 2009.09.29 1\n",
      "2009.10.20 2009.08.02 2009.09.30 1\n",
      "2009.10.21 2009.08.03 2009.10.01 1\n",
      "2009.10.22 2009.08.04 2009.10.02 1\n",
      "2009.10.23 2009.08.05 2009.10.03 1\n",
      "2009.10.24 2009.08.06 2009.10.04 1\n",
      "2009.10.25 2009.08.07 2009.10.05 1\n",
      "2009.10.26 2009.08.08 2009.10.06 1\n",
      "2009.10.27 2009.08.09 2009.10.07 1\n",
      "2009.10.28 2009.08.10 2009.10.08 1\n",
      "2009.10.29 2009.08.11 2009.10.09 1\n",
      "2009.10.30 2009.08.12 2009.10.10 1\n",
      "2009.10.31 2009.08.13 2009.10.11 1\n",
      "2009.11.01 2009.08.14 2009.10.12 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009.11.02 2009.08.15 2009.10.13 1\n",
      "2009.11.03 2009.08.16 2009.10.14 1\n",
      "2009.11.04 2009.08.17 2009.10.15 1\n",
      "2009.11.05 2009.08.18 2009.10.16 1\n",
      "2009.11.06 2009.08.19 2009.10.17 1\n",
      "2009.11.07 2009.08.20 2009.10.18 1\n",
      "2009.11.08 2009.08.21 2009.10.19 1\n",
      "2009.11.09 2009.08.22 2009.10.20 1\n",
      "2009.11.10 2009.08.23 2009.10.21 1\n",
      "2009.11.11 2009.08.24 2009.10.22 1\n",
      "2009.11.12 2009.08.25 2009.10.23 1\n",
      "2009.11.13 2009.08.26 2009.10.24 1\n",
      "2009.11.14 2009.08.27 2009.10.25 1\n",
      "2009.11.15 2009.08.28 2009.10.26 1\n",
      "2009.11.16 2009.08.29 2009.10.27 1\n",
      "2009.11.17 2009.08.30 2009.10.28 1\n",
      "2009.11.18 2009.08.31 2009.10.29 1\n",
      "2009.11.19 2009.09.01 2009.10.30 1\n",
      "2009.11.20 2009.09.02 2009.10.31 1\n",
      "2009.11.21 2009.09.03 2009.11.01 1\n",
      "2009.11.22 2009.09.04 2009.11.02 1\n",
      "2009.11.23 2009.09.05 2009.11.03 1\n",
      "2009.11.24 2009.09.06 2009.11.04 1\n",
      "2009.11.25 2009.09.07 2009.11.05 1\n",
      "2009.11.26 2009.09.08 2009.11.06 1\n",
      "2009.11.27 2009.09.09 2009.11.07 1\n",
      "2009.11.28 2009.09.10 2009.11.08 1\n",
      "2009.11.29 2009.09.11 2009.11.09 1\n",
      "2009.11.30 2009.09.12 2009.11.10 1\n",
      "2009.12.01 2009.09.13 2009.11.11 1\n",
      "2009.12.02 2009.09.14 2009.11.12 1\n",
      "2009.12.03 2009.09.15 2009.11.13 1\n",
      "2009.12.04 2009.09.16 2009.11.14 1\n",
      "2009.12.05 2009.09.17 2009.11.15 1\n",
      "2009.12.06 2009.09.18 2009.11.16 1\n",
      "2009.12.07 2009.09.19 2009.11.17 1\n",
      "2009.12.08 2009.09.20 2009.11.18 1\n",
      "2009.12.09 2009.09.21 2009.11.19 1\n",
      "2009.12.10 2009.09.22 2009.11.20 1\n",
      "2009.12.11 2009.09.23 2009.11.21 1\n",
      "2009.12.12 2009.09.24 2009.11.22 1\n",
      "2009.12.13 2009.09.25 2009.11.23 1\n",
      "2009.12.14 2009.09.26 2009.11.24 1\n",
      "2009.12.15 2009.09.27 2009.11.25 1\n",
      "2009.12.16 2009.09.28 2009.11.26 1\n",
      "2009.12.17 2009.09.29 2009.11.27 1\n",
      "2009.12.18 2009.09.30 2009.11.28 1\n",
      "2009.12.19 2009.10.01 2009.11.29 1\n",
      "2009.12.20 2009.10.02 2009.11.30 1\n",
      "2009.12.21 2009.10.03 2009.12.01 1\n",
      "2009.12.22 2009.10.04 2009.12.02 1\n",
      "2009.12.23 2009.10.05 2009.12.03 1\n",
      "2009.12.24 2009.10.06 2009.12.04 1\n",
      "2009.12.25 2009.10.07 2009.12.05 1\n",
      "2009.12.26 2009.10.08 2009.12.06 1\n",
      "2009.12.27 2009.10.09 2009.12.07 1\n",
      "2009.12.28 2009.10.10 2009.12.08 1\n",
      "2010.10.04 2010.07.17 2010.09.14 0\n",
      "2010.10.05 2010.07.18 2010.09.15 1\n",
      "2010.10.06 2010.07.19 2010.09.16 1\n",
      "2010.10.07 2010.07.20 2010.09.17 1\n",
      "2010.10.08 2010.07.21 2010.09.18 1\n",
      "2010.10.09 2010.07.22 2010.09.19 1\n",
      "2010.10.10 2010.07.23 2010.09.20 1\n",
      "2010.10.11 2010.07.24 2010.09.21 1\n",
      "2010.10.12 2010.07.25 2010.09.22 1\n",
      "2010.10.13 2010.07.26 2010.09.23 1\n",
      "2010.10.14 2010.07.27 2010.09.24 1\n",
      "2010.10.15 2010.07.28 2010.09.25 1\n",
      "2010.10.16 2010.07.29 2010.09.26 1\n",
      "2010.10.17 2010.07.30 2010.09.27 1\n",
      "2010.10.18 2010.07.31 2010.09.28 1\n",
      "2010.10.19 2010.08.01 2010.09.29 1\n",
      "2010.10.20 2010.08.02 2010.09.30 1\n",
      "2010.10.21 2010.08.03 2010.10.01 1\n",
      "2010.10.22 2010.08.04 2010.10.02 1\n",
      "2010.10.23 2010.08.05 2010.10.03 1\n",
      "2010.10.24 2010.08.06 2010.10.04 1\n",
      "2010.10.25 2010.08.07 2010.10.05 1\n",
      "2010.10.26 2010.08.08 2010.10.06 1\n",
      "2010.10.27 2010.08.09 2010.10.07 1\n",
      "2010.10.28 2010.08.10 2010.10.08 1\n",
      "2010.10.29 2010.08.11 2010.10.09 1\n",
      "2010.10.30 2010.08.12 2010.10.10 1\n",
      "2010.10.31 2010.08.13 2010.10.11 1\n",
      "2010.11.01 2010.08.14 2010.10.12 1\n",
      "2010.11.02 2010.08.15 2010.10.13 1\n",
      "2010.11.03 2010.08.16 2010.10.14 1\n",
      "2010.11.04 2010.08.17 2010.10.15 1\n",
      "2010.11.05 2010.08.18 2010.10.16 1\n",
      "2010.11.06 2010.08.19 2010.10.17 1\n",
      "2010.11.07 2010.08.20 2010.10.18 1\n",
      "2010.11.08 2010.08.21 2010.10.19 1\n",
      "2010.11.09 2010.08.22 2010.10.20 1\n",
      "2010.11.10 2010.08.23 2010.10.21 1\n",
      "2010.11.11 2010.08.24 2010.10.22 1\n",
      "2010.11.12 2010.08.25 2010.10.23 1\n",
      "2010.11.13 2010.08.26 2010.10.24 1\n",
      "2010.11.14 2010.08.27 2010.10.25 1\n",
      "2010.11.15 2010.08.28 2010.10.26 1\n",
      "2010.11.16 2010.08.29 2010.10.27 1\n",
      "2010.11.17 2010.08.30 2010.10.28 1\n",
      "2010.11.18 2010.08.31 2010.10.29 1\n",
      "2010.11.19 2010.09.01 2010.10.30 1\n",
      "2010.11.20 2010.09.02 2010.10.31 1\n",
      "2010.11.21 2010.09.03 2010.11.01 1\n",
      "2010.11.22 2010.09.04 2010.11.02 1\n",
      "2010.11.23 2010.09.05 2010.11.03 1\n",
      "2010.11.24 2010.09.06 2010.11.04 1\n",
      "2010.11.25 2010.09.07 2010.11.05 1\n",
      "2010.11.26 2010.09.08 2010.11.06 1\n",
      "2010.11.27 2010.09.09 2010.11.07 1\n",
      "2010.11.28 2010.09.10 2010.11.08 1\n",
      "2010.11.29 2010.09.11 2010.11.09 1\n",
      "2010.11.30 2010.09.12 2010.11.10 1\n",
      "2010.12.01 2010.09.13 2010.11.11 1\n",
      "2010.12.02 2010.09.14 2010.11.12 1\n",
      "2010.12.03 2010.09.15 2010.11.13 1\n",
      "2010.12.04 2010.09.16 2010.11.14 1\n",
      "2010.12.05 2010.09.17 2010.11.15 1\n",
      "2010.12.06 2010.09.18 2010.11.16 1\n",
      "2010.12.07 2010.09.19 2010.11.17 1\n",
      "2010.12.08 2010.09.20 2010.11.18 1\n",
      "2010.12.09 2010.09.21 2010.11.19 1\n",
      "2010.12.10 2010.09.22 2010.11.20 1\n",
      "2010.12.11 2010.09.23 2010.11.21 1\n",
      "2010.12.12 2010.09.24 2010.11.22 1\n",
      "2010.12.13 2010.09.25 2010.11.23 1\n",
      "2010.12.14 2010.09.26 2010.11.24 1\n",
      "2010.12.15 2010.09.27 2010.11.25 1\n",
      "2010.12.16 2010.09.28 2010.11.26 1\n",
      "2010.12.17 2010.09.29 2010.11.27 1\n",
      "2010.12.18 2010.09.30 2010.11.28 1\n",
      "2010.12.19 2010.10.01 2010.11.29 1\n",
      "2010.12.20 2010.10.02 2010.11.30 1\n",
      "2010.12.21 2010.10.03 2010.12.01 1\n",
      "2010.12.22 2010.10.04 2010.12.02 1\n",
      "2010.12.23 2010.10.05 2010.12.03 1\n",
      "2010.12.24 2010.10.06 2010.12.04 1\n",
      "2010.12.25 2010.10.07 2010.12.05 1\n",
      "2010.12.26 2010.10.08 2010.12.06 1\n",
      "2010.12.27 2010.10.09 2010.12.07 1\n",
      "2010.12.28 2010.10.10 2010.12.08 1\n",
      "2011.10.04 2011.07.17 2011.09.14 0\n",
      "2011.10.05 2011.07.18 2011.09.15 1\n",
      "2011.10.06 2011.07.19 2011.09.16 1\n",
      "2011.10.07 2011.07.20 2011.09.17 1\n",
      "2011.10.08 2011.07.21 2011.09.18 1\n",
      "2011.10.09 2011.07.22 2011.09.19 1\n",
      "2011.10.10 2011.07.23 2011.09.20 1\n",
      "2011.10.11 2011.07.24 2011.09.21 1\n",
      "2011.10.12 2011.07.25 2011.09.22 1\n",
      "2011.10.13 2011.07.26 2011.09.23 1\n",
      "2011.10.14 2011.07.27 2011.09.24 1\n",
      "2011.10.15 2011.07.28 2011.09.25 1\n",
      "2011.10.16 2011.07.29 2011.09.26 1\n",
      "2011.10.17 2011.07.30 2011.09.27 1\n",
      "2011.10.18 2011.07.31 2011.09.28 1\n",
      "2011.10.19 2011.08.01 2011.09.29 1\n",
      "2011.10.20 2011.08.02 2011.09.30 1\n",
      "2011.10.21 2011.08.03 2011.10.01 1\n",
      "2011.10.22 2011.08.04 2011.10.02 1\n",
      "2011.10.23 2011.08.05 2011.10.03 1\n",
      "2011.10.24 2011.08.06 2011.10.04 1\n",
      "2011.10.25 2011.08.07 2011.10.05 1\n",
      "2011.10.26 2011.08.08 2011.10.06 1\n",
      "2011.10.27 2011.08.09 2011.10.07 1\n",
      "2011.10.28 2011.08.10 2011.10.08 1\n",
      "2011.10.29 2011.08.11 2011.10.09 1\n",
      "2011.10.30 2011.08.12 2011.10.10 1\n",
      "2011.10.31 2011.08.13 2011.10.11 1\n",
      "2011.11.01 2011.08.14 2011.10.12 1\n",
      "2011.11.02 2011.08.15 2011.10.13 1\n",
      "2011.11.03 2011.08.16 2011.10.14 1\n",
      "2011.11.04 2011.08.17 2011.10.15 1\n",
      "2011.11.05 2011.08.18 2011.10.16 1\n",
      "2011.11.06 2011.08.19 2011.10.17 1\n",
      "2011.11.07 2011.08.20 2011.10.18 1\n",
      "2011.11.08 2011.08.21 2011.10.19 1\n",
      "2011.11.09 2011.08.22 2011.10.20 1\n",
      "2011.11.10 2011.08.23 2011.10.21 1\n",
      "2011.11.11 2011.08.24 2011.10.22 1\n",
      "2011.11.12 2011.08.25 2011.10.23 1\n",
      "2011.11.13 2011.08.26 2011.10.24 1\n",
      "2011.11.14 2011.08.27 2011.10.25 1\n",
      "2011.11.15 2011.08.28 2011.10.26 1\n",
      "2011.11.16 2011.08.29 2011.10.27 1\n",
      "2011.11.17 2011.08.30 2011.10.28 1\n",
      "2011.11.18 2011.08.31 2011.10.29 1\n",
      "2011.11.19 2011.09.01 2011.10.30 1\n",
      "2011.11.20 2011.09.02 2011.10.31 1\n",
      "2011.11.21 2011.09.03 2011.11.01 1\n",
      "2011.11.22 2011.09.04 2011.11.02 1\n",
      "2011.11.23 2011.09.05 2011.11.03 1\n",
      "2011.11.24 2011.09.06 2011.11.04 1\n",
      "2011.11.25 2011.09.07 2011.11.05 1\n",
      "2011.11.26 2011.09.08 2011.11.06 1\n",
      "2011.11.27 2011.09.09 2011.11.07 1\n",
      "2011.11.28 2011.09.10 2011.11.08 1\n",
      "2011.11.29 2011.09.11 2011.11.09 1\n",
      "2011.11.30 2011.09.12 2011.11.10 1\n",
      "2011.12.01 2011.09.13 2011.11.11 1\n",
      "2011.12.02 2011.09.14 2011.11.12 1\n",
      "2011.12.03 2011.09.15 2011.11.13 1\n",
      "2011.12.04 2011.09.16 2011.11.14 1\n",
      "2011.12.05 2011.09.17 2011.11.15 1\n",
      "2011.12.06 2011.09.18 2011.11.16 1\n",
      "2011.12.07 2011.09.19 2011.11.17 1\n",
      "2011.12.08 2011.09.20 2011.11.18 1\n",
      "2011.12.09 2011.09.21 2011.11.19 1\n",
      "2011.12.10 2011.09.22 2011.11.20 1\n",
      "2011.12.11 2011.09.23 2011.11.21 1\n",
      "2011.12.12 2011.09.24 2011.11.22 1\n",
      "2011.12.13 2011.09.25 2011.11.23 1\n",
      "2011.12.14 2011.09.26 2011.11.24 1\n",
      "2011.12.15 2011.09.27 2011.11.25 1\n",
      "2011.12.16 2011.09.28 2011.11.26 1\n",
      "2011.12.17 2011.09.29 2011.11.27 1\n",
      "2011.12.18 2011.09.30 2011.11.28 1\n",
      "2011.12.19 2011.10.01 2011.11.29 1\n",
      "2011.12.20 2011.10.02 2011.11.30 1\n",
      "2011.12.21 2011.10.03 2011.12.01 1\n",
      "2011.12.22 2011.10.04 2011.12.02 1\n",
      "2011.12.23 2011.10.05 2011.12.03 1\n",
      "2011.12.24 2011.10.06 2011.12.04 1\n",
      "2011.12.25 2011.10.07 2011.12.05 1\n",
      "2011.12.26 2011.10.08 2011.12.06 1\n",
      "2011.12.27 2011.10.09 2011.12.07 1\n",
      "2011.12.28 2011.10.10 2011.12.08 1\n",
      "2012.10.04 2012.07.17 2012.09.14 0\n",
      "2012.10.05 2012.07.18 2012.09.15 1\n",
      "2012.10.06 2012.07.19 2012.09.16 1\n",
      "2012.10.07 2012.07.20 2012.09.17 1\n",
      "2012.10.08 2012.07.21 2012.09.18 1\n",
      "2012.10.09 2012.07.22 2012.09.19 1\n",
      "2012.10.10 2012.07.23 2012.09.20 1\n",
      "2012.10.11 2012.07.24 2012.09.21 1\n",
      "2012.10.12 2012.07.25 2012.09.22 1\n",
      "2012.10.13 2012.07.26 2012.09.23 1\n",
      "2012.10.14 2012.07.27 2012.09.24 1\n",
      "2012.10.15 2012.07.28 2012.09.25 1\n",
      "2012.10.16 2012.07.29 2012.09.26 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012.10.17 2012.07.30 2012.09.27 1\n",
      "2012.10.18 2012.07.31 2012.09.28 1\n",
      "2012.10.19 2012.08.01 2012.09.29 1\n",
      "2012.10.20 2012.08.02 2012.09.30 1\n",
      "2012.10.21 2012.08.03 2012.10.01 1\n",
      "2012.10.22 2012.08.04 2012.10.02 1\n",
      "2012.10.23 2012.08.05 2012.10.03 1\n",
      "2012.10.24 2012.08.06 2012.10.04 1\n",
      "2012.10.25 2012.08.07 2012.10.05 1\n",
      "2012.10.26 2012.08.08 2012.10.06 1\n",
      "2012.10.27 2012.08.09 2012.10.07 1\n",
      "2012.10.28 2012.08.10 2012.10.08 1\n",
      "2012.10.29 2012.08.11 2012.10.09 1\n",
      "2012.10.30 2012.08.12 2012.10.10 1\n",
      "2012.10.31 2012.08.13 2012.10.11 1\n",
      "2012.11.01 2012.08.14 2012.10.12 1\n",
      "2012.11.02 2012.08.15 2012.10.13 1\n",
      "2012.11.03 2012.08.16 2012.10.14 1\n",
      "2012.11.04 2012.08.17 2012.10.15 1\n",
      "2012.11.05 2012.08.18 2012.10.16 1\n",
      "2012.11.06 2012.08.19 2012.10.17 1\n",
      "2012.11.07 2012.08.20 2012.10.18 1\n",
      "2012.11.08 2012.08.21 2012.10.19 1\n",
      "2012.11.09 2012.08.22 2012.10.20 1\n",
      "2012.11.10 2012.08.23 2012.10.21 1\n",
      "2012.11.11 2012.08.24 2012.10.22 1\n",
      "2012.11.12 2012.08.25 2012.10.23 1\n",
      "2012.11.13 2012.08.26 2012.10.24 1\n",
      "2012.11.14 2012.08.27 2012.10.25 1\n",
      "2012.11.15 2012.08.28 2012.10.26 1\n",
      "2012.11.16 2012.08.29 2012.10.27 1\n",
      "2012.11.17 2012.08.30 2012.10.28 1\n",
      "2012.11.18 2012.08.31 2012.10.29 1\n",
      "2012.11.19 2012.09.01 2012.10.30 1\n",
      "2012.11.20 2012.09.02 2012.10.31 1\n",
      "2012.11.21 2012.09.03 2012.11.01 1\n",
      "2012.11.22 2012.09.04 2012.11.02 1\n",
      "2012.11.23 2012.09.05 2012.11.03 1\n",
      "2012.11.24 2012.09.06 2012.11.04 1\n",
      "2012.11.25 2012.09.07 2012.11.05 1\n",
      "2012.11.26 2012.09.08 2012.11.06 1\n",
      "2012.11.27 2012.09.09 2012.11.07 1\n",
      "2012.11.28 2012.09.10 2012.11.08 1\n",
      "2012.11.29 2012.09.11 2012.11.09 1\n",
      "2012.11.30 2012.09.12 2012.11.10 1\n",
      "2012.12.01 2012.09.13 2012.11.11 1\n",
      "2012.12.02 2012.09.14 2012.11.12 1\n",
      "2012.12.03 2012.09.15 2012.11.13 1\n",
      "2012.12.04 2012.09.16 2012.11.14 1\n",
      "2012.12.05 2012.09.17 2012.11.15 1\n",
      "2012.12.06 2012.09.18 2012.11.16 1\n",
      "2012.12.07 2012.09.19 2012.11.17 1\n",
      "2012.12.08 2012.09.20 2012.11.18 1\n",
      "2012.12.09 2012.09.21 2012.11.19 1\n",
      "2012.12.10 2012.09.22 2012.11.20 1\n",
      "2012.12.11 2012.09.23 2012.11.21 1\n",
      "2012.12.12 2012.09.24 2012.11.22 1\n",
      "2012.12.13 2012.09.25 2012.11.23 1\n",
      "2012.12.14 2012.09.26 2012.11.24 1\n",
      "2012.12.15 2012.09.27 2012.11.25 1\n",
      "2012.12.16 2012.09.28 2012.11.26 1\n",
      "2012.12.17 2012.09.29 2012.11.27 1\n",
      "2012.12.18 2012.09.30 2012.11.28 1\n",
      "2012.12.19 2012.10.01 2012.11.29 1\n",
      "2012.12.20 2012.10.02 2012.11.30 1\n",
      "2012.12.21 2012.10.03 2012.12.01 1\n",
      "2012.12.22 2012.10.04 2012.12.02 1\n",
      "2012.12.23 2012.10.05 2012.12.03 1\n",
      "2012.12.24 2012.10.06 2012.12.04 1\n",
      "2012.12.25 2012.10.07 2012.12.05 1\n",
      "2012.12.26 2012.10.08 2012.12.06 1\n",
      "2012.12.27 2012.10.09 2012.12.07 1\n",
      "2012.12.28 2012.10.10 2012.12.08 1\n",
      "2013.10.04 2013.07.17 2013.09.14 0\n",
      "2013.10.05 2013.07.18 2013.09.15 1\n",
      "2013.10.06 2013.07.19 2013.09.16 1\n",
      "2013.10.07 2013.07.20 2013.09.17 1\n",
      "2013.10.08 2013.07.21 2013.09.18 1\n",
      "2013.10.09 2013.07.22 2013.09.19 1\n",
      "2013.10.10 2013.07.23 2013.09.20 1\n",
      "2013.10.11 2013.07.24 2013.09.21 1\n",
      "2013.10.12 2013.07.25 2013.09.22 1\n",
      "2013.10.13 2013.07.26 2013.09.23 1\n",
      "2013.10.14 2013.07.27 2013.09.24 1\n",
      "2013.10.15 2013.07.28 2013.09.25 1\n",
      "2013.10.16 2013.07.29 2013.09.26 1\n",
      "2013.10.17 2013.07.30 2013.09.27 1\n",
      "2013.10.18 2013.07.31 2013.09.28 1\n",
      "2013.10.19 2013.08.01 2013.09.29 1\n",
      "2013.10.20 2013.08.02 2013.09.30 1\n",
      "2013.10.21 2013.08.03 2013.10.01 1\n",
      "2013.10.22 2013.08.04 2013.10.02 1\n",
      "2013.10.23 2013.08.05 2013.10.03 1\n",
      "2013.10.24 2013.08.06 2013.10.04 1\n",
      "2013.10.25 2013.08.07 2013.10.05 1\n",
      "2013.10.26 2013.08.08 2013.10.06 1\n",
      "2013.10.27 2013.08.09 2013.10.07 1\n",
      "2013.10.28 2013.08.10 2013.10.08 1\n",
      "2013.10.29 2013.08.11 2013.10.09 1\n",
      "2013.10.30 2013.08.12 2013.10.10 1\n",
      "2013.10.31 2013.08.13 2013.10.11 1\n",
      "2013.11.01 2013.08.14 2013.10.12 1\n",
      "2013.11.02 2013.08.15 2013.10.13 1\n",
      "2013.11.03 2013.08.16 2013.10.14 1\n",
      "2013.11.04 2013.08.17 2013.10.15 1\n",
      "2013.11.05 2013.08.18 2013.10.16 1\n",
      "2013.11.06 2013.08.19 2013.10.17 1\n",
      "2013.11.07 2013.08.20 2013.10.18 1\n",
      "2013.11.08 2013.08.21 2013.10.19 1\n",
      "2013.11.09 2013.08.22 2013.10.20 1\n",
      "2013.11.10 2013.08.23 2013.10.21 1\n",
      "2013.11.11 2013.08.24 2013.10.22 1\n",
      "2013.11.12 2013.08.25 2013.10.23 1\n",
      "2013.11.13 2013.08.26 2013.10.24 1\n",
      "2013.11.14 2013.08.27 2013.10.25 1\n",
      "2013.11.15 2013.08.28 2013.10.26 1\n",
      "2013.11.16 2013.08.29 2013.10.27 1\n",
      "2013.11.17 2013.08.30 2013.10.28 1\n",
      "2013.11.18 2013.08.31 2013.10.29 1\n",
      "2013.11.19 2013.09.01 2013.10.30 1\n",
      "2013.11.20 2013.09.02 2013.10.31 1\n",
      "2013.11.21 2013.09.03 2013.11.01 1\n",
      "2013.11.22 2013.09.04 2013.11.02 1\n",
      "2013.11.23 2013.09.05 2013.11.03 1\n",
      "2013.11.24 2013.09.06 2013.11.04 1\n",
      "2013.11.25 2013.09.07 2013.11.05 1\n",
      "2013.11.26 2013.09.08 2013.11.06 1\n",
      "2013.11.27 2013.09.09 2013.11.07 1\n",
      "2013.11.28 2013.09.10 2013.11.08 1\n",
      "2013.11.29 2013.09.11 2013.11.09 1\n",
      "2013.11.30 2013.09.12 2013.11.10 1\n",
      "2013.12.01 2013.09.13 2013.11.11 1\n",
      "2013.12.02 2013.09.14 2013.11.12 1\n",
      "2013.12.03 2013.09.15 2013.11.13 1\n",
      "2013.12.04 2013.09.16 2013.11.14 1\n",
      "2013.12.05 2013.09.17 2013.11.15 1\n",
      "2013.12.06 2013.09.18 2013.11.16 1\n",
      "2013.12.07 2013.09.19 2013.11.17 1\n",
      "2013.12.08 2013.09.20 2013.11.18 1\n",
      "2013.12.09 2013.09.21 2013.11.19 1\n",
      "2013.12.10 2013.09.22 2013.11.20 1\n",
      "2013.12.11 2013.09.23 2013.11.21 1\n",
      "2013.12.12 2013.09.24 2013.11.22 1\n",
      "2013.12.13 2013.09.25 2013.11.23 1\n",
      "2013.12.14 2013.09.26 2013.11.24 1\n",
      "2013.12.15 2013.09.27 2013.11.25 1\n",
      "2013.12.16 2013.09.28 2013.11.26 1\n",
      "2013.12.17 2013.09.29 2013.11.27 1\n",
      "2013.12.18 2013.09.30 2013.11.28 1\n",
      "2013.12.19 2013.10.01 2013.11.29 1\n",
      "2013.12.20 2013.10.02 2013.11.30 1\n",
      "2013.12.21 2013.10.03 2013.12.01 1\n",
      "2013.12.22 2013.10.04 2013.12.02 1\n",
      "2013.12.23 2013.10.05 2013.12.03 1\n",
      "2013.12.24 2013.10.06 2013.12.04 1\n",
      "2013.12.25 2013.10.07 2013.12.05 1\n",
      "2013.12.26 2013.10.08 2013.12.06 1\n",
      "2013.12.27 2013.10.09 2013.12.07 1\n",
      "2013.12.28 2013.10.10 2013.12.08 1\n",
      "2014.10.04 2014.07.17 2014.09.14 0\n",
      "2014.10.05 2014.07.18 2014.09.15 1\n",
      "2014.10.06 2014.07.19 2014.09.16 1\n",
      "2014.10.07 2014.07.20 2014.09.17 1\n",
      "2014.10.08 2014.07.21 2014.09.18 1\n",
      "2014.10.09 2014.07.22 2014.09.19 1\n",
      "2014.10.10 2014.07.23 2014.09.20 1\n",
      "2014.10.11 2014.07.24 2014.09.21 1\n",
      "2014.10.12 2014.07.25 2014.09.22 1\n",
      "2014.10.13 2014.07.26 2014.09.23 1\n",
      "2014.10.14 2014.07.27 2014.09.24 1\n",
      "2014.10.15 2014.07.28 2014.09.25 1\n",
      "2014.10.16 2014.07.29 2014.09.26 1\n",
      "2014.10.17 2014.07.30 2014.09.27 1\n",
      "2014.10.18 2014.07.31 2014.09.28 1\n",
      "2014.10.19 2014.08.01 2014.09.29 1\n",
      "2014.10.20 2014.08.02 2014.09.30 1\n",
      "2014.10.21 2014.08.03 2014.10.01 1\n",
      "2014.10.22 2014.08.04 2014.10.02 1\n",
      "2014.10.23 2014.08.05 2014.10.03 1\n",
      "2014.10.24 2014.08.06 2014.10.04 1\n",
      "2014.10.25 2014.08.07 2014.10.05 1\n",
      "2014.10.26 2014.08.08 2014.10.06 1\n",
      "2014.10.27 2014.08.09 2014.10.07 1\n",
      "2014.10.28 2014.08.10 2014.10.08 1\n",
      "2014.10.29 2014.08.11 2014.10.09 1\n",
      "2014.10.30 2014.08.12 2014.10.10 1\n",
      "2014.10.31 2014.08.13 2014.10.11 1\n",
      "2014.11.01 2014.08.14 2014.10.12 1\n",
      "2014.11.02 2014.08.15 2014.10.13 1\n",
      "2014.11.03 2014.08.16 2014.10.14 1\n",
      "2014.11.04 2014.08.17 2014.10.15 1\n",
      "2014.11.05 2014.08.18 2014.10.16 1\n",
      "2014.11.06 2014.08.19 2014.10.17 1\n",
      "2014.11.07 2014.08.20 2014.10.18 1\n",
      "2014.11.08 2014.08.21 2014.10.19 1\n",
      "2014.11.09 2014.08.22 2014.10.20 1\n",
      "2014.11.10 2014.08.23 2014.10.21 1\n",
      "2014.11.11 2014.08.24 2014.10.22 1\n",
      "2014.11.12 2014.08.25 2014.10.23 1\n",
      "2014.11.13 2014.08.26 2014.10.24 1\n",
      "2014.11.14 2014.08.27 2014.10.25 1\n",
      "2014.11.15 2014.08.28 2014.10.26 1\n",
      "2014.11.16 2014.08.29 2014.10.27 1\n",
      "2014.11.17 2014.08.30 2014.10.28 1\n",
      "2014.11.18 2014.08.31 2014.10.29 1\n",
      "2014.11.19 2014.09.01 2014.10.30 1\n",
      "2014.11.20 2014.09.02 2014.10.31 1\n",
      "2014.11.21 2014.09.03 2014.11.01 1\n",
      "2014.11.22 2014.09.04 2014.11.02 1\n",
      "2014.11.23 2014.09.05 2014.11.03 1\n",
      "2014.11.24 2014.09.06 2014.11.04 1\n",
      "2014.11.25 2014.09.07 2014.11.05 1\n",
      "2014.11.26 2014.09.08 2014.11.06 1\n",
      "2014.11.27 2014.09.09 2014.11.07 1\n",
      "2014.11.28 2014.09.10 2014.11.08 1\n",
      "2014.11.29 2014.09.11 2014.11.09 1\n",
      "2014.11.30 2014.09.12 2014.11.10 1\n",
      "2014.12.01 2014.09.13 2014.11.11 1\n",
      "2014.12.02 2014.09.14 2014.11.12 1\n",
      "2014.12.03 2014.09.15 2014.11.13 1\n",
      "2014.12.04 2014.09.16 2014.11.14 1\n",
      "2014.12.05 2014.09.17 2014.11.15 1\n",
      "2014.12.06 2014.09.18 2014.11.16 1\n",
      "2014.12.07 2014.09.19 2014.11.17 1\n",
      "2014.12.08 2014.09.20 2014.11.18 1\n",
      "2014.12.09 2014.09.21 2014.11.19 1\n",
      "2014.12.10 2014.09.22 2014.11.20 1\n",
      "2014.12.11 2014.09.23 2014.11.21 1\n",
      "2014.12.12 2014.09.24 2014.11.22 1\n",
      "2014.12.13 2014.09.25 2014.11.23 1\n",
      "2014.12.14 2014.09.26 2014.11.24 1\n",
      "2014.12.15 2014.09.27 2014.11.25 1\n",
      "2014.12.16 2014.09.28 2014.11.26 1\n",
      "2014.12.17 2014.09.29 2014.11.27 1\n",
      "2014.12.18 2014.09.30 2014.11.28 1\n",
      "2014.12.19 2014.10.01 2014.11.29 1\n",
      "2014.12.20 2014.10.02 2014.11.30 1\n",
      "2014.12.21 2014.10.03 2014.12.01 1\n",
      "2014.12.22 2014.10.04 2014.12.02 1\n",
      "2014.12.23 2014.10.05 2014.12.03 1\n",
      "2014.12.24 2014.10.06 2014.12.04 1\n",
      "2014.12.25 2014.10.07 2014.12.05 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014.12.26 2014.10.08 2014.12.06 1\n",
      "2014.12.27 2014.10.09 2014.12.07 1\n",
      "2014.12.28 2014.10.10 2014.12.08 1\n",
      "2015.10.04 2015.07.17 2015.09.14 0\n",
      "2015.10.05 2015.07.18 2015.09.15 1\n",
      "2015.10.06 2015.07.19 2015.09.16 1\n",
      "2015.10.07 2015.07.20 2015.09.17 1\n",
      "2015.10.08 2015.07.21 2015.09.18 1\n",
      "2015.10.09 2015.07.22 2015.09.19 1\n",
      "2015.10.10 2015.07.23 2015.09.20 1\n",
      "2015.10.11 2015.07.24 2015.09.21 1\n",
      "2015.10.12 2015.07.25 2015.09.22 1\n",
      "2015.10.13 2015.07.26 2015.09.23 1\n",
      "2015.10.14 2015.07.27 2015.09.24 1\n",
      "2015.10.15 2015.07.28 2015.09.25 1\n",
      "2015.10.16 2015.07.29 2015.09.26 1\n",
      "2015.10.17 2015.07.30 2015.09.27 1\n",
      "2015.10.18 2015.07.31 2015.09.28 1\n",
      "2015.10.19 2015.08.01 2015.09.29 1\n",
      "2015.10.20 2015.08.02 2015.09.30 1\n",
      "2015.10.21 2015.08.03 2015.10.01 1\n",
      "2015.10.22 2015.08.04 2015.10.02 1\n",
      "2015.10.23 2015.08.05 2015.10.03 1\n",
      "2015.10.24 2015.08.06 2015.10.04 1\n",
      "2015.10.25 2015.08.07 2015.10.05 1\n",
      "2015.10.26 2015.08.08 2015.10.06 1\n",
      "2015.10.27 2015.08.09 2015.10.07 1\n",
      "2015.10.28 2015.08.10 2015.10.08 1\n",
      "2015.10.29 2015.08.11 2015.10.09 1\n",
      "2015.10.30 2015.08.12 2015.10.10 1\n",
      "2015.10.31 2015.08.13 2015.10.11 1\n",
      "2015.11.01 2015.08.14 2015.10.12 1\n",
      "2015.11.02 2015.08.15 2015.10.13 1\n",
      "2015.11.03 2015.08.16 2015.10.14 1\n",
      "2015.11.04 2015.08.17 2015.10.15 1\n",
      "2015.11.05 2015.08.18 2015.10.16 1\n",
      "2015.11.06 2015.08.19 2015.10.17 1\n",
      "2015.11.07 2015.08.20 2015.10.18 1\n",
      "2015.11.08 2015.08.21 2015.10.19 1\n",
      "2015.11.09 2015.08.22 2015.10.20 1\n",
      "2015.11.10 2015.08.23 2015.10.21 1\n",
      "2015.11.11 2015.08.24 2015.10.22 1\n",
      "2015.11.12 2015.08.25 2015.10.23 1\n",
      "2015.11.13 2015.08.26 2015.10.24 1\n",
      "2015.11.14 2015.08.27 2015.10.25 1\n",
      "2015.11.15 2015.08.28 2015.10.26 1\n",
      "2015.11.16 2015.08.29 2015.10.27 1\n",
      "2015.11.17 2015.08.30 2015.10.28 1\n",
      "2015.11.18 2015.08.31 2015.10.29 1\n",
      "2015.11.19 2015.09.01 2015.10.30 1\n",
      "2015.11.20 2015.09.02 2015.10.31 1\n",
      "2015.11.21 2015.09.03 2015.11.01 1\n",
      "2015.11.22 2015.09.04 2015.11.02 1\n",
      "2015.11.23 2015.09.05 2015.11.03 1\n",
      "2015.11.24 2015.09.06 2015.11.04 1\n",
      "2015.11.25 2015.09.07 2015.11.05 1\n",
      "2015.11.26 2015.09.08 2015.11.06 1\n",
      "2015.11.27 2015.09.09 2015.11.07 1\n",
      "2015.11.28 2015.09.10 2015.11.08 1\n",
      "2015.11.29 2015.09.11 2015.11.09 1\n",
      "2015.11.30 2015.09.12 2015.11.10 1\n",
      "2015.12.01 2015.09.13 2015.11.11 1\n",
      "2015.12.02 2015.09.14 2015.11.12 1\n",
      "2015.12.03 2015.09.15 2015.11.13 1\n",
      "2015.12.04 2015.09.16 2015.11.14 1\n",
      "2015.12.05 2015.09.17 2015.11.15 1\n",
      "2015.12.06 2015.09.18 2015.11.16 1\n",
      "2015.12.07 2015.09.19 2015.11.17 1\n",
      "2015.12.08 2015.09.20 2015.11.18 1\n",
      "2015.12.09 2015.09.21 2015.11.19 1\n",
      "2015.12.10 2015.09.22 2015.11.20 1\n",
      "2015.12.11 2015.09.23 2015.11.21 1\n",
      "2015.12.12 2015.09.24 2015.11.22 1\n",
      "2015.12.13 2015.09.25 2015.11.23 1\n",
      "2015.12.14 2015.09.26 2015.11.24 1\n",
      "2015.12.15 2015.09.27 2015.11.25 1\n",
      "2015.12.16 2015.09.28 2015.11.26 1\n",
      "2015.12.17 2015.09.29 2015.11.27 1\n",
      "2015.12.18 2015.09.30 2015.11.28 1\n",
      "2015.12.19 2015.10.01 2015.11.29 1\n",
      "2015.12.20 2015.10.02 2015.11.30 1\n",
      "2015.12.21 2015.10.03 2015.12.01 1\n",
      "2015.12.22 2015.10.04 2015.12.02 1\n",
      "2015.12.23 2015.10.05 2015.12.03 1\n",
      "2015.12.24 2015.10.06 2015.12.04 1\n",
      "2015.12.25 2015.10.07 2015.12.05 1\n",
      "2015.12.26 2015.10.08 2015.12.06 1\n",
      "2015.12.27 2015.10.09 2015.12.07 1\n",
      "2015.12.28 2015.10.10 2015.12.08 1\n",
      "2016.10.04 2016.07.17 2016.09.14 0\n",
      "2016.10.05 2016.07.18 2016.09.15 1\n",
      "2016.10.06 2016.07.19 2016.09.16 1\n",
      "2016.10.07 2016.07.20 2016.09.17 1\n",
      "2016.10.08 2016.07.21 2016.09.18 1\n",
      "2016.10.09 2016.07.22 2016.09.19 1\n",
      "2016.10.10 2016.07.23 2016.09.20 1\n",
      "2016.10.11 2016.07.24 2016.09.21 1\n",
      "2016.10.12 2016.07.25 2016.09.22 1\n",
      "2016.10.13 2016.07.26 2016.09.23 1\n",
      "2016.10.14 2016.07.27 2016.09.24 1\n",
      "2016.10.15 2016.07.28 2016.09.25 1\n",
      "2016.10.16 2016.07.29 2016.09.26 1\n",
      "2016.10.17 2016.07.30 2016.09.27 1\n",
      "2016.10.18 2016.07.31 2016.09.28 1\n",
      "2016.10.19 2016.08.01 2016.09.29 1\n",
      "2016.10.20 2016.08.02 2016.09.30 1\n",
      "2016.10.21 2016.08.03 2016.10.01 1\n",
      "2016.10.22 2016.08.04 2016.10.02 1\n",
      "2016.10.23 2016.08.05 2016.10.03 1\n",
      "2016.10.24 2016.08.06 2016.10.04 1\n",
      "2016.10.25 2016.08.07 2016.10.05 1\n",
      "2016.10.26 2016.08.08 2016.10.06 1\n",
      "2016.10.27 2016.08.09 2016.10.07 1\n",
      "2016.10.28 2016.08.10 2016.10.08 1\n",
      "2016.10.29 2016.08.11 2016.10.09 1\n",
      "2016.10.30 2016.08.12 2016.10.10 1\n",
      "2016.10.31 2016.08.13 2016.10.11 1\n",
      "2016.11.01 2016.08.14 2016.10.12 1\n",
      "2016.11.02 2016.08.15 2016.10.13 1\n",
      "2016.11.03 2016.08.16 2016.10.14 1\n",
      "2016.11.04 2016.08.17 2016.10.15 1\n",
      "2016.11.05 2016.08.18 2016.10.16 1\n",
      "2016.11.06 2016.08.19 2016.10.17 1\n",
      "2016.11.07 2016.08.20 2016.10.18 1\n",
      "2016.11.08 2016.08.21 2016.10.19 1\n",
      "2016.11.09 2016.08.22 2016.10.20 1\n",
      "2016.11.10 2016.08.23 2016.10.21 1\n",
      "2016.11.11 2016.08.24 2016.10.22 1\n",
      "2016.11.12 2016.08.25 2016.10.23 1\n",
      "2016.11.13 2016.08.26 2016.10.24 1\n",
      "2016.11.14 2016.08.27 2016.10.25 1\n",
      "2016.11.15 2016.08.28 2016.10.26 1\n",
      "2016.11.16 2016.08.29 2016.10.27 1\n",
      "2016.11.17 2016.08.30 2016.10.28 1\n",
      "2016.11.18 2016.08.31 2016.10.29 1\n",
      "2016.11.19 2016.09.01 2016.10.30 1\n",
      "2016.11.20 2016.09.02 2016.10.31 1\n",
      "2016.11.21 2016.09.03 2016.11.01 1\n",
      "2016.11.22 2016.09.04 2016.11.02 1\n",
      "2016.11.23 2016.09.05 2016.11.03 1\n",
      "2016.11.24 2016.09.06 2016.11.04 1\n",
      "2016.11.25 2016.09.07 2016.11.05 1\n",
      "2016.11.26 2016.09.08 2016.11.06 1\n",
      "2016.11.27 2016.09.09 2016.11.07 1\n",
      "2016.11.28 2016.09.10 2016.11.08 1\n",
      "2016.11.29 2016.09.11 2016.11.09 1\n",
      "2016.11.30 2016.09.12 2016.11.10 1\n",
      "2016.12.01 2016.09.13 2016.11.11 1\n",
      "2016.12.02 2016.09.14 2016.11.12 1\n",
      "2016.12.03 2016.09.15 2016.11.13 1\n",
      "2016.12.04 2016.09.16 2016.11.14 1\n",
      "2016.12.05 2016.09.17 2016.11.15 1\n",
      "2016.12.06 2016.09.18 2016.11.16 1\n",
      "2016.12.07 2016.09.19 2016.11.17 1\n",
      "2016.12.08 2016.09.20 2016.11.18 1\n",
      "2016.12.09 2016.09.21 2016.11.19 1\n",
      "2016.12.10 2016.09.22 2016.11.20 1\n",
      "2016.12.11 2016.09.23 2016.11.21 1\n",
      "2016.12.12 2016.09.24 2016.11.22 1\n",
      "2016.12.13 2016.09.25 2016.11.23 1\n",
      "2016.12.14 2016.09.26 2016.11.24 1\n",
      "2016.12.15 2016.09.27 2016.11.25 1\n",
      "2016.12.16 2016.09.28 2016.11.26 1\n",
      "2016.12.17 2016.09.29 2016.11.27 1\n",
      "2016.12.18 2016.09.30 2016.11.28 1\n",
      "2016.12.19 2016.10.01 2016.11.29 1\n",
      "2016.12.20 2016.10.02 2016.11.30 1\n",
      "2016.12.21 2016.10.03 2016.12.01 1\n",
      "2016.12.22 2016.10.04 2016.12.02 1\n",
      "2016.12.23 2016.10.05 2016.12.03 1\n",
      "2016.12.24 2016.10.06 2016.12.04 1\n",
      "2016.12.25 2016.10.07 2016.12.05 1\n",
      "2016.12.26 2016.10.08 2016.12.06 1\n",
      "2016.12.27 2016.10.09 2016.12.07 1\n",
      "2016.12.28 2016.10.10 2016.12.08 1\n",
      "2017.10.04 2017.07.17 2017.09.14 0\n",
      "2017.10.05 2017.07.18 2017.09.15 1\n",
      "2017.10.06 2017.07.19 2017.09.16 1\n",
      "2017.10.07 2017.07.20 2017.09.17 1\n",
      "2017.10.08 2017.07.21 2017.09.18 1\n",
      "2017.10.09 2017.07.22 2017.09.19 1\n",
      "2017.10.10 2017.07.23 2017.09.20 1\n",
      "2017.10.11 2017.07.24 2017.09.21 1\n",
      "2017.10.12 2017.07.25 2017.09.22 1\n",
      "2017.10.13 2017.07.26 2017.09.23 1\n",
      "2017.10.14 2017.07.27 2017.09.24 1\n",
      "2017.10.15 2017.07.28 2017.09.25 1\n",
      "2017.10.16 2017.07.29 2017.09.26 1\n",
      "2017.10.17 2017.07.30 2017.09.27 1\n",
      "2017.10.18 2017.07.31 2017.09.28 1\n",
      "2017.10.19 2017.08.01 2017.09.29 1\n",
      "2017.10.20 2017.08.02 2017.09.30 1\n",
      "2017.10.21 2017.08.03 2017.10.01 1\n",
      "2017.10.22 2017.08.04 2017.10.02 1\n",
      "2017.10.23 2017.08.05 2017.10.03 1\n",
      "2017.10.24 2017.08.06 2017.10.04 1\n",
      "2017.10.25 2017.08.07 2017.10.05 1\n",
      "2017.10.26 2017.08.08 2017.10.06 1\n",
      "2017.10.27 2017.08.09 2017.10.07 1\n",
      "2017.10.28 2017.08.10 2017.10.08 1\n",
      "2017.10.29 2017.08.11 2017.10.09 1\n",
      "2017.10.30 2017.08.12 2017.10.10 1\n",
      "2017.10.31 2017.08.13 2017.10.11 1\n",
      "2017.11.01 2017.08.14 2017.10.12 1\n",
      "2017.11.02 2017.08.15 2017.10.13 1\n",
      "2017.11.03 2017.08.16 2017.10.14 1\n",
      "2017.11.04 2017.08.17 2017.10.15 1\n",
      "2017.11.05 2017.08.18 2017.10.16 1\n",
      "2017.11.06 2017.08.19 2017.10.17 1\n",
      "2017.11.07 2017.08.20 2017.10.18 1\n",
      "2017.11.08 2017.08.21 2017.10.19 1\n",
      "2017.11.09 2017.08.22 2017.10.20 1\n",
      "2017.11.10 2017.08.23 2017.10.21 1\n",
      "2017.11.11 2017.08.24 2017.10.22 1\n",
      "2017.11.12 2017.08.25 2017.10.23 1\n",
      "2017.11.13 2017.08.26 2017.10.24 1\n",
      "2017.11.14 2017.08.27 2017.10.25 1\n",
      "2017.11.15 2017.08.28 2017.10.26 1\n",
      "2017.11.16 2017.08.29 2017.10.27 1\n",
      "2017.11.17 2017.08.30 2017.10.28 1\n",
      "2017.11.18 2017.08.31 2017.10.29 1\n",
      "2017.11.19 2017.09.01 2017.10.30 1\n",
      "2017.11.20 2017.09.02 2017.10.31 1\n",
      "2017.11.21 2017.09.03 2017.11.01 1\n",
      "2017.11.22 2017.09.04 2017.11.02 1\n",
      "2017.11.23 2017.09.05 2017.11.03 1\n",
      "2017.11.24 2017.09.06 2017.11.04 1\n",
      "2017.11.25 2017.09.07 2017.11.05 1\n",
      "2017.11.26 2017.09.08 2017.11.06 1\n",
      "2017.11.27 2017.09.09 2017.11.07 1\n",
      "2017.11.28 2017.09.10 2017.11.08 1\n",
      "2017.11.29 2017.09.11 2017.11.09 1\n",
      "2017.11.30 2017.09.12 2017.11.10 1\n",
      "2017.12.01 2017.09.13 2017.11.11 1\n",
      "2017.12.02 2017.09.14 2017.11.12 1\n",
      "2017.12.03 2017.09.15 2017.11.13 1\n",
      "2017.12.04 2017.09.16 2017.11.14 1\n",
      "2017.12.05 2017.09.17 2017.11.15 1\n",
      "2017.12.06 2017.09.18 2017.11.16 1\n",
      "2017.12.07 2017.09.19 2017.11.17 1\n",
      "2017.12.08 2017.09.20 2017.11.18 1\n",
      "2017.12.09 2017.09.21 2017.11.19 1\n",
      "2017.12.10 2017.09.22 2017.11.20 1\n",
      "2017.12.11 2017.09.23 2017.11.21 1\n",
      "2017.12.12 2017.09.24 2017.11.22 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017.12.13 2017.09.25 2017.11.23 1\n",
      "2017.12.14 2017.09.26 2017.11.24 1\n",
      "2017.12.15 2017.09.27 2017.11.25 1\n",
      "2017.12.16 2017.09.28 2017.11.26 1\n",
      "2017.12.17 2017.09.29 2017.11.27 1\n",
      "2017.12.18 2017.09.30 2017.11.28 1\n",
      "2017.12.19 2017.10.01 2017.11.29 1\n",
      "2017.12.20 2017.10.02 2017.11.30 1\n",
      "2017.12.21 2017.10.03 2017.12.01 1\n",
      "2017.12.22 2017.10.04 2017.12.02 1\n",
      "2017.12.23 2017.10.05 2017.12.03 1\n",
      "2017.12.24 2017.10.06 2017.12.04 1\n",
      "2017.12.25 2017.10.07 2017.12.05 1\n",
      "2017.12.26 2017.10.08 2017.12.06 1\n",
      "2017.12.27 2017.10.09 2017.12.07 1\n",
      "2017.12.28 2017.10.10 2017.12.08 1\n",
      "2018.10.04 2018.07.17 2018.09.14 0\n",
      "2018.10.05 2018.07.18 2018.09.15 1\n",
      "2018.10.06 2018.07.19 2018.09.16 1\n",
      "2018.10.07 2018.07.20 2018.09.17 1\n",
      "2018.10.08 2018.07.21 2018.09.18 1\n",
      "2018.10.09 2018.07.22 2018.09.19 1\n",
      "2018.10.10 2018.07.23 2018.09.20 1\n",
      "2018.10.11 2018.07.24 2018.09.21 1\n",
      "2018.10.12 2018.07.25 2018.09.22 1\n",
      "2018.10.13 2018.07.26 2018.09.23 1\n",
      "2018.10.14 2018.07.27 2018.09.24 1\n",
      "2018.10.15 2018.07.28 2018.09.25 1\n",
      "2018.10.16 2018.07.29 2018.09.26 1\n",
      "2018.10.17 2018.07.30 2018.09.27 1\n",
      "2018.10.18 2018.07.31 2018.09.28 1\n",
      "2018.10.19 2018.08.01 2018.09.29 1\n",
      "2018.10.20 2018.08.02 2018.09.30 1\n",
      "2018.10.21 2018.08.03 2018.10.01 1\n",
      "2018.10.22 2018.08.04 2018.10.02 1\n",
      "2018.10.23 2018.08.05 2018.10.03 1\n",
      "2018.10.24 2018.08.06 2018.10.04 1\n",
      "2018.10.25 2018.08.07 2018.10.05 1\n",
      "2018.10.26 2018.08.08 2018.10.06 1\n",
      "2018.10.27 2018.08.09 2018.10.07 1\n",
      "2018.10.28 2018.08.10 2018.10.08 1\n",
      "2018.10.29 2018.08.11 2018.10.09 1\n",
      "2018.10.30 2018.08.12 2018.10.10 1\n",
      "2018.10.31 2018.08.13 2018.10.11 1\n",
      "2018.11.01 2018.08.14 2018.10.12 1\n",
      "2018.11.02 2018.08.15 2018.10.13 1\n",
      "2018.11.03 2018.08.16 2018.10.14 1\n",
      "2018.11.04 2018.08.17 2018.10.15 1\n",
      "2018.11.05 2018.08.18 2018.10.16 1\n",
      "2018.11.06 2018.08.19 2018.10.17 1\n",
      "2018.11.07 2018.08.20 2018.10.18 1\n",
      "2018.11.08 2018.08.21 2018.10.19 1\n",
      "2018.11.09 2018.08.22 2018.10.20 1\n",
      "2018.11.10 2018.08.23 2018.10.21 1\n",
      "2018.11.11 2018.08.24 2018.10.22 1\n",
      "2018.11.12 2018.08.25 2018.10.23 1\n",
      "2018.11.13 2018.08.26 2018.10.24 1\n",
      "2018.11.14 2018.08.27 2018.10.25 1\n",
      "2018.11.15 2018.08.28 2018.10.26 1\n",
      "2018.11.16 2018.08.29 2018.10.27 1\n",
      "2018.11.17 2018.08.30 2018.10.28 1\n",
      "2018.11.18 2018.08.31 2018.10.29 1\n",
      "2018.11.19 2018.09.01 2018.10.30 1\n",
      "2018.11.20 2018.09.02 2018.10.31 1\n",
      "2018.11.21 2018.09.03 2018.11.01 1\n",
      "2018.11.22 2018.09.04 2018.11.02 1\n",
      "2018.11.23 2018.09.05 2018.11.03 1\n",
      "2018.11.24 2018.09.06 2018.11.04 1\n",
      "2018.11.25 2018.09.07 2018.11.05 1\n",
      "2018.11.26 2018.09.08 2018.11.06 1\n",
      "2018.11.27 2018.09.09 2018.11.07 1\n",
      "2018.11.28 2018.09.10 2018.11.08 1\n",
      "2018.11.29 2018.09.11 2018.11.09 1\n",
      "2018.11.30 2018.09.12 2018.11.10 1\n",
      "2018.12.01 2018.09.13 2018.11.11 1\n",
      "2018.12.02 2018.09.14 2018.11.12 1\n",
      "2018.12.03 2018.09.15 2018.11.13 1\n",
      "2018.12.04 2018.09.16 2018.11.14 1\n",
      "2018.12.05 2018.09.17 2018.11.15 1\n",
      "2018.12.06 2018.09.18 2018.11.16 1\n",
      "2018.12.07 2018.09.19 2018.11.17 1\n",
      "2018.12.08 2018.09.20 2018.11.18 1\n",
      "2018.12.09 2018.09.21 2018.11.19 1\n",
      "2018.12.10 2018.09.22 2018.11.20 1\n",
      "2018.12.11 2018.09.23 2018.11.21 1\n",
      "2018.12.12 2018.09.24 2018.11.22 1\n",
      "2018.12.13 2018.09.25 2018.11.23 1\n",
      "2018.12.14 2018.09.26 2018.11.24 1\n",
      "2018.12.15 2018.09.27 2018.11.25 1\n",
      "2018.12.16 2018.09.28 2018.11.26 1\n",
      "2018.12.17 2018.09.29 2018.11.27 1\n",
      "2018.12.18 2018.09.30 2018.11.28 1\n",
      "2018.12.19 2018.10.01 2018.11.29 1\n",
      "2018.12.20 2018.10.02 2018.11.30 1\n",
      "2018.12.21 2018.10.03 2018.12.01 1\n",
      "2018.12.22 2018.10.04 2018.12.02 1\n",
      "2018.12.23 2018.10.05 2018.12.03 1\n",
      "2018.12.24 2018.10.06 2018.12.04 1\n",
      "2018.12.25 2018.10.07 2018.12.05 1\n",
      "2018.12.26 2018.10.08 2018.12.06 1\n",
      "2018.12.27 2018.10.09 2018.12.07 1\n",
      "2018.12.28 2018.10.10 2018.12.08 1\n",
      "2019.10.04 2019.07.17 2019.09.14 0\n",
      "2019.10.05 2019.07.18 2019.09.15 1\n",
      "2019.10.06 2019.07.19 2019.09.16 1\n",
      "2019.10.07 2019.07.20 2019.09.17 1\n",
      "2019.10.08 2019.07.21 2019.09.18 1\n",
      "2019.10.09 2019.07.22 2019.09.19 1\n",
      "2019.10.10 2019.07.23 2019.09.20 1\n",
      "2019.10.11 2019.07.24 2019.09.21 1\n",
      "2019.10.12 2019.07.25 2019.09.22 1\n",
      "2019.10.13 2019.07.26 2019.09.23 1\n",
      "2019.10.14 2019.07.27 2019.09.24 1\n",
      "2019.10.15 2019.07.28 2019.09.25 1\n",
      "2019.10.16 2019.07.29 2019.09.26 1\n",
      "2019.10.17 2019.07.30 2019.09.27 1\n",
      "2019.10.18 2019.07.31 2019.09.28 1\n",
      "2019.10.19 2019.08.01 2019.09.29 1\n",
      "2019.10.20 2019.08.02 2019.09.30 1\n",
      "2019.10.21 2019.08.03 2019.10.01 1\n",
      "2019.10.22 2019.08.04 2019.10.02 1\n",
      "2019.10.23 2019.08.05 2019.10.03 1\n",
      "2019.10.24 2019.08.06 2019.10.04 1\n",
      "2019.10.25 2019.08.07 2019.10.05 1\n",
      "2019.10.26 2019.08.08 2019.10.06 1\n",
      "2019.10.27 2019.08.09 2019.10.07 1\n",
      "2019.10.28 2019.08.10 2019.10.08 1\n",
      "2019.10.29 2019.08.11 2019.10.09 1\n",
      "2019.10.30 2019.08.12 2019.10.10 1\n",
      "2019.10.31 2019.08.13 2019.10.11 1\n",
      "2019.11.01 2019.08.14 2019.10.12 1\n",
      "2019.11.02 2019.08.15 2019.10.13 1\n",
      "2019.11.03 2019.08.16 2019.10.14 1\n",
      "2019.11.04 2019.08.17 2019.10.15 1\n",
      "2019.11.05 2019.08.18 2019.10.16 1\n",
      "2019.11.06 2019.08.19 2019.10.17 1\n",
      "2019.11.07 2019.08.20 2019.10.18 1\n",
      "2019.11.08 2019.08.21 2019.10.19 1\n",
      "2019.11.09 2019.08.22 2019.10.20 1\n",
      "2019.11.10 2019.08.23 2019.10.21 1\n",
      "2019.11.11 2019.08.24 2019.10.22 1\n",
      "2019.11.12 2019.08.25 2019.10.23 1\n",
      "2019.11.13 2019.08.26 2019.10.24 1\n",
      "2019.11.14 2019.08.27 2019.10.25 1\n",
      "2019.11.15 2019.08.28 2019.10.26 1\n",
      "2019.11.16 2019.08.29 2019.10.27 1\n",
      "2019.11.17 2019.08.30 2019.10.28 1\n",
      "2019.11.18 2019.08.31 2019.10.29 1\n",
      "2019.11.19 2019.09.01 2019.10.30 1\n",
      "2019.11.20 2019.09.02 2019.10.31 1\n",
      "2019.11.21 2019.09.03 2019.11.01 1\n",
      "2019.11.22 2019.09.04 2019.11.02 1\n",
      "2019.11.23 2019.09.05 2019.11.03 1\n",
      "2019.11.24 2019.09.06 2019.11.04 1\n",
      "2019.11.25 2019.09.07 2019.11.05 1\n",
      "2019.11.26 2019.09.08 2019.11.06 1\n",
      "2019.11.27 2019.09.09 2019.11.07 1\n",
      "2019.11.28 2019.09.10 2019.11.08 1\n",
      "2019.11.29 2019.09.11 2019.11.09 1\n",
      "2019.11.30 2019.09.12 2019.11.10 1\n",
      "2019.12.01 2019.09.13 2019.11.11 1\n",
      "2019.12.02 2019.09.14 2019.11.12 1\n",
      "2019.12.03 2019.09.15 2019.11.13 1\n",
      "2019.12.04 2019.09.16 2019.11.14 1\n",
      "2019.12.05 2019.09.17 2019.11.15 1\n",
      "2019.12.06 2019.09.18 2019.11.16 1\n",
      "2019.12.07 2019.09.19 2019.11.17 1\n",
      "2019.12.08 2019.09.20 2019.11.18 1\n",
      "2019.12.09 2019.09.21 2019.11.19 1\n",
      "2019.12.10 2019.09.22 2019.11.20 1\n",
      "2019.12.11 2019.09.23 2019.11.21 1\n",
      "2019.12.12 2019.09.24 2019.11.22 1\n",
      "2019.12.13 2019.09.25 2019.11.23 1\n",
      "2019.12.14 2019.09.26 2019.11.24 1\n",
      "2019.12.15 2019.09.27 2019.11.25 1\n",
      "2019.12.16 2019.09.28 2019.11.26 1\n",
      "2019.12.17 2019.09.29 2019.11.27 1\n",
      "2019.12.18 2019.09.30 2019.11.28 1\n",
      "2019.12.19 2019.10.01 2019.11.29 1\n",
      "2019.12.20 2019.10.02 2019.11.30 1\n",
      "2019.12.21 2019.10.03 2019.12.01 1\n",
      "2019.12.22 2019.10.04 2019.12.02 1\n",
      "2019.12.23 2019.10.05 2019.12.03 1\n",
      "2019.12.24 2019.10.06 2019.12.04 1\n",
      "2019.12.25 2019.10.07 2019.12.05 1\n",
      "2019.12.26 2019.10.08 2019.12.06 1\n",
      "2019.12.27 2019.10.09 2019.12.07 1\n",
      "2019.12.28 2019.10.10 2019.12.08 1\n",
      "2020.10.04 2020.07.17 2020.09.14 0\n",
      "2020.10.05 2020.07.18 2020.09.15 1\n",
      "2020.10.06 2020.07.19 2020.09.16 1\n",
      "2020.10.07 2020.07.20 2020.09.17 1\n",
      "2020.10.08 2020.07.21 2020.09.18 1\n",
      "2020.10.09 2020.07.22 2020.09.19 1\n",
      "2020.10.10 2020.07.23 2020.09.20 1\n",
      "2020.10.11 2020.07.24 2020.09.21 1\n",
      "2020.10.12 2020.07.25 2020.09.22 1\n",
      "2020.10.13 2020.07.26 2020.09.23 1\n",
      "2020.10.14 2020.07.27 2020.09.24 1\n",
      "2020.10.15 2020.07.28 2020.09.25 1\n",
      "2020.10.16 2020.07.29 2020.09.26 1\n",
      "2020.10.17 2020.07.30 2020.09.27 1\n",
      "2020.10.18 2020.07.31 2020.09.28 1\n",
      "2020.10.19 2020.08.01 2020.09.29 1\n",
      "2020.10.20 2020.08.02 2020.09.30 1\n",
      "2020.10.21 2020.08.03 2020.10.01 1\n",
      "2020.10.22 2020.08.04 2020.10.02 1\n",
      "2020.10.23 2020.08.05 2020.10.03 1\n",
      "2020.10.24 2020.08.06 2020.10.04 1\n",
      "2020.10.25 2020.08.07 2020.10.05 1\n",
      "2020.10.26 2020.08.08 2020.10.06 1\n",
      "2020.10.27 2020.08.09 2020.10.07 1\n",
      "2020.10.28 2020.08.10 2020.10.08 1\n",
      "2020.10.29 2020.08.11 2020.10.09 1\n",
      "2020.10.30 2020.08.12 2020.10.10 1\n",
      "2020.10.31 2020.08.13 2020.10.11 1\n",
      "2020.11.01 2020.08.14 2020.10.12 1\n",
      "2020.11.02 2020.08.15 2020.10.13 1\n",
      "2020.11.03 2020.08.16 2020.10.14 1\n",
      "2020.11.04 2020.08.17 2020.10.15 1\n",
      "2020.11.05 2020.08.18 2020.10.16 1\n",
      "2020.11.06 2020.08.19 2020.10.17 1\n",
      "2020.11.07 2020.08.20 2020.10.18 1\n",
      "2020.11.08 2020.08.21 2020.10.19 1\n",
      "2020.11.09 2020.08.22 2020.10.20 1\n",
      "2020.11.10 2020.08.23 2020.10.21 1\n",
      "2020.11.11 2020.08.24 2020.10.22 1\n",
      "2020.11.12 2020.08.25 2020.10.23 1\n",
      "2020.11.13 2020.08.26 2020.10.24 1\n",
      "2020.11.14 2020.08.27 2020.10.25 1\n",
      "2020.11.15 2020.08.28 2020.10.26 1\n",
      "2020.11.16 2020.08.29 2020.10.27 1\n",
      "2020.11.17 2020.08.30 2020.10.28 1\n",
      "2020.11.18 2020.08.31 2020.10.29 1\n",
      "2020.11.19 2020.09.01 2020.10.30 1\n",
      "2020.11.20 2020.09.02 2020.10.31 1\n",
      "2020.11.21 2020.09.03 2020.11.01 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020.11.22 2020.09.04 2020.11.02 1\n",
      "2020.11.23 2020.09.05 2020.11.03 1\n",
      "2020.11.24 2020.09.06 2020.11.04 1\n",
      "2020.11.25 2020.09.07 2020.11.05 1\n",
      "2020.11.26 2020.09.08 2020.11.06 1\n",
      "2020.11.27 2020.09.09 2020.11.07 1\n",
      "2020.11.28 2020.09.10 2020.11.08 1\n",
      "2020.11.29 2020.09.11 2020.11.09 1\n",
      "2020.11.30 2020.09.12 2020.11.10 1\n",
      "2020.12.01 2020.09.13 2020.11.11 1\n",
      "2020.12.02 2020.09.14 2020.11.12 1\n",
      "2020.12.03 2020.09.15 2020.11.13 1\n",
      "2020.12.04 2020.09.16 2020.11.14 1\n",
      "2020.12.05 2020.09.17 2020.11.15 1\n",
      "2020.12.06 2020.09.18 2020.11.16 1\n",
      "2020.12.07 2020.09.19 2020.11.17 1\n",
      "2020.12.08 2020.09.20 2020.11.18 1\n",
      "2020.12.09 2020.09.21 2020.11.19 1\n",
      "2020.12.10 2020.09.22 2020.11.20 1\n",
      "2020.12.11 2020.09.23 2020.11.21 1\n",
      "2020.12.12 2020.09.24 2020.11.22 1\n",
      "2020.12.13 2020.09.25 2020.11.23 1\n",
      "2020.12.14 2020.09.26 2020.11.24 1\n",
      "2020.12.15 2020.09.27 2020.11.25 1\n",
      "2020.12.16 2020.09.28 2020.11.26 1\n",
      "2020.12.17 2020.09.29 2020.11.27 1\n",
      "2020.12.18 2020.09.30 2020.11.28 1\n",
      "2020.12.19 2020.10.01 2020.11.29 1\n",
      "2020.12.20 2020.10.02 2020.11.30 1\n",
      "2020.12.21 2020.10.03 2020.12.01 1\n",
      "2020.12.22 2020.10.04 2020.12.02 1\n",
      "2020.12.23 2020.10.05 2020.12.03 1\n",
      "2020.12.24 2020.10.06 2020.12.04 1\n",
      "2020.12.25 2020.10.07 2020.12.05 1\n",
      "2020.12.26 2020.10.08 2020.12.06 1\n",
      "2020.12.27 2020.10.09 2020.12.07 1\n",
      "2020.12.28 2020.10.10 2020.12.08 1\n",
      "<xarray.Dataset>\n",
      "Dimensions:    (lag: 60, lat: 23, lon: 23, time: 3440)\n",
      "Coordinates:\n",
      "  * lag        (lag) int64 0 1 2 3 4 5 6 7 8 9 ... 50 51 52 53 54 55 56 57 58 59\n",
      "  * lat        (lat) float32 30.0 28.0 26.0 24.0 22.0 ... -8.0 -10.0 -12.0 -14.0\n",
      "  * lon        (lon) float32 16.0 18.0 20.0 22.0 24.0 ... 54.0 56.0 58.0 60.0\n",
      "    dayofyear  (time, lag) int64 198 199 200 201 202 203 ... 339 340 341 342 343\n",
      "  * time       (time) datetime64[ns] 1981-10-04 1981-10-05 ... 2020-12-28\n",
      "Data variables:\n",
      "    t2m        (time, lag, lat, lon) float32 -0.8605126 ... 1.1051866\n",
      "lat lon exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/cfc/s2s/s2s_ai_challenge/conda/envs/s2sai_2020/lib/python3.7/site-packages/xarray/core/nanops.py:142: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis=axis, dtype=dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:    (lat: 61, lon: 180, time: 14610)\n",
      "Coordinates:\n",
      "  * lat        (lat) float32 60.0 58.0 56.0 54.0 ... -54.0 -56.0 -58.0 -60.0\n",
      "  * lon        (lon) float32 -180.0 -178.0 -176.0 -174.0 ... 174.0 176.0 178.0\n",
      "  * time       (time) datetime64[ns] 1981-01-01 1981-01-02 ... 2020-12-31\n",
      "    dayofyear  (time) int64 1 2 3 4 5 6 7 8 ... 359 360 361 362 363 364 365 366\n",
      "Data variables:\n",
      "    sst        (time, lat, lon) float32 nan nan nan nan nan ... nan nan nan nan\n",
      "starting\n",
      "start target date 4 10 1981\n",
      "end target 28 12 2020\n",
      "1981.10.04 1981.07.17 1981.09.14 0\n",
      "1981.10.05 1981.07.18 1981.09.15 1\n",
      "1981.10.06 1981.07.19 1981.09.16 1\n",
      "1981.10.07 1981.07.20 1981.09.17 1\n",
      "1981.10.08 1981.07.21 1981.09.18 1\n",
      "1981.10.09 1981.07.22 1981.09.19 1\n",
      "1981.10.10 1981.07.23 1981.09.20 1\n",
      "1981.10.11 1981.07.24 1981.09.21 1\n",
      "1981.10.12 1981.07.25 1981.09.22 1\n",
      "1981.10.13 1981.07.26 1981.09.23 1\n",
      "1981.10.14 1981.07.27 1981.09.24 1\n",
      "1981.10.15 1981.07.28 1981.09.25 1\n",
      "1981.10.16 1981.07.29 1981.09.26 1\n",
      "1981.10.17 1981.07.30 1981.09.27 1\n",
      "1981.10.18 1981.07.31 1981.09.28 1\n",
      "1981.10.19 1981.08.01 1981.09.29 1\n",
      "1981.10.20 1981.08.02 1981.09.30 1\n",
      "1981.10.21 1981.08.03 1981.10.01 1\n",
      "1981.10.22 1981.08.04 1981.10.02 1\n",
      "1981.10.23 1981.08.05 1981.10.03 1\n",
      "1981.10.24 1981.08.06 1981.10.04 1\n",
      "1981.10.25 1981.08.07 1981.10.05 1\n",
      "1981.10.26 1981.08.08 1981.10.06 1\n",
      "1981.10.27 1981.08.09 1981.10.07 1\n",
      "1981.10.28 1981.08.10 1981.10.08 1\n",
      "1981.10.29 1981.08.11 1981.10.09 1\n",
      "1981.10.30 1981.08.12 1981.10.10 1\n",
      "1981.10.31 1981.08.13 1981.10.11 1\n",
      "1981.11.01 1981.08.14 1981.10.12 1\n",
      "1981.11.02 1981.08.15 1981.10.13 1\n",
      "1981.11.03 1981.08.16 1981.10.14 1\n",
      "1981.11.04 1981.08.17 1981.10.15 1\n",
      "1981.11.05 1981.08.18 1981.10.16 1\n",
      "1981.11.06 1981.08.19 1981.10.17 1\n",
      "1981.11.07 1981.08.20 1981.10.18 1\n",
      "1981.11.08 1981.08.21 1981.10.19 1\n",
      "1981.11.09 1981.08.22 1981.10.20 1\n",
      "1981.11.10 1981.08.23 1981.10.21 1\n",
      "1981.11.11 1981.08.24 1981.10.22 1\n",
      "1981.11.12 1981.08.25 1981.10.23 1\n",
      "1981.11.13 1981.08.26 1981.10.24 1\n",
      "1981.11.14 1981.08.27 1981.10.25 1\n",
      "1981.11.15 1981.08.28 1981.10.26 1\n",
      "1981.11.16 1981.08.29 1981.10.27 1\n",
      "1981.11.17 1981.08.30 1981.10.28 1\n",
      "1981.11.18 1981.08.31 1981.10.29 1\n",
      "1981.11.19 1981.09.01 1981.10.30 1\n",
      "1981.11.20 1981.09.02 1981.10.31 1\n",
      "1981.11.21 1981.09.03 1981.11.01 1\n",
      "1981.11.22 1981.09.04 1981.11.02 1\n",
      "1981.11.23 1981.09.05 1981.11.03 1\n",
      "1981.11.24 1981.09.06 1981.11.04 1\n",
      "1981.11.25 1981.09.07 1981.11.05 1\n",
      "1981.11.26 1981.09.08 1981.11.06 1\n",
      "1981.11.27 1981.09.09 1981.11.07 1\n",
      "1981.11.28 1981.09.10 1981.11.08 1\n",
      "1981.11.29 1981.09.11 1981.11.09 1\n",
      "1981.11.30 1981.09.12 1981.11.10 1\n",
      "1981.12.01 1981.09.13 1981.11.11 1\n",
      "1981.12.02 1981.09.14 1981.11.12 1\n",
      "1981.12.03 1981.09.15 1981.11.13 1\n",
      "1981.12.04 1981.09.16 1981.11.14 1\n",
      "1981.12.05 1981.09.17 1981.11.15 1\n",
      "1981.12.06 1981.09.18 1981.11.16 1\n",
      "1981.12.07 1981.09.19 1981.11.17 1\n",
      "1981.12.08 1981.09.20 1981.11.18 1\n",
      "1981.12.09 1981.09.21 1981.11.19 1\n",
      "1981.12.10 1981.09.22 1981.11.20 1\n",
      "1981.12.11 1981.09.23 1981.11.21 1\n",
      "1981.12.12 1981.09.24 1981.11.22 1\n",
      "1981.12.13 1981.09.25 1981.11.23 1\n",
      "1981.12.14 1981.09.26 1981.11.24 1\n",
      "1981.12.15 1981.09.27 1981.11.25 1\n",
      "1981.12.16 1981.09.28 1981.11.26 1\n",
      "1981.12.17 1981.09.29 1981.11.27 1\n",
      "1981.12.18 1981.09.30 1981.11.28 1\n",
      "1981.12.19 1981.10.01 1981.11.29 1\n",
      "1981.12.20 1981.10.02 1981.11.30 1\n",
      "1981.12.21 1981.10.03 1981.12.01 1\n",
      "1981.12.22 1981.10.04 1981.12.02 1\n",
      "1981.12.23 1981.10.05 1981.12.03 1\n",
      "1981.12.24 1981.10.06 1981.12.04 1\n",
      "1981.12.25 1981.10.07 1981.12.05 1\n",
      "1981.12.26 1981.10.08 1981.12.06 1\n",
      "1981.12.27 1981.10.09 1981.12.07 1\n",
      "1981.12.28 1981.10.10 1981.12.08 1\n",
      "1982.10.04 1982.07.17 1982.09.14 0\n",
      "1982.10.05 1982.07.18 1982.09.15 1\n",
      "1982.10.06 1982.07.19 1982.09.16 1\n",
      "1982.10.07 1982.07.20 1982.09.17 1\n",
      "1982.10.08 1982.07.21 1982.09.18 1\n",
      "1982.10.09 1982.07.22 1982.09.19 1\n",
      "1982.10.10 1982.07.23 1982.09.20 1\n",
      "1982.10.11 1982.07.24 1982.09.21 1\n",
      "1982.10.12 1982.07.25 1982.09.22 1\n",
      "1982.10.13 1982.07.26 1982.09.23 1\n",
      "1982.10.14 1982.07.27 1982.09.24 1\n",
      "1982.10.15 1982.07.28 1982.09.25 1\n",
      "1982.10.16 1982.07.29 1982.09.26 1\n",
      "1982.10.17 1982.07.30 1982.09.27 1\n",
      "1982.10.18 1982.07.31 1982.09.28 1\n",
      "1982.10.19 1982.08.01 1982.09.29 1\n",
      "1982.10.20 1982.08.02 1982.09.30 1\n",
      "1982.10.21 1982.08.03 1982.10.01 1\n",
      "1982.10.22 1982.08.04 1982.10.02 1\n",
      "1982.10.23 1982.08.05 1982.10.03 1\n",
      "1982.10.24 1982.08.06 1982.10.04 1\n",
      "1982.10.25 1982.08.07 1982.10.05 1\n",
      "1982.10.26 1982.08.08 1982.10.06 1\n",
      "1982.10.27 1982.08.09 1982.10.07 1\n",
      "1982.10.28 1982.08.10 1982.10.08 1\n",
      "1982.10.29 1982.08.11 1982.10.09 1\n",
      "1982.10.30 1982.08.12 1982.10.10 1\n",
      "1982.10.31 1982.08.13 1982.10.11 1\n",
      "1982.11.01 1982.08.14 1982.10.12 1\n",
      "1982.11.02 1982.08.15 1982.10.13 1\n",
      "1982.11.03 1982.08.16 1982.10.14 1\n",
      "1982.11.04 1982.08.17 1982.10.15 1\n",
      "1982.11.05 1982.08.18 1982.10.16 1\n",
      "1982.11.06 1982.08.19 1982.10.17 1\n",
      "1982.11.07 1982.08.20 1982.10.18 1\n",
      "1982.11.08 1982.08.21 1982.10.19 1\n",
      "1982.11.09 1982.08.22 1982.10.20 1\n",
      "1982.11.10 1982.08.23 1982.10.21 1\n",
      "1982.11.11 1982.08.24 1982.10.22 1\n",
      "1982.11.12 1982.08.25 1982.10.23 1\n",
      "1982.11.13 1982.08.26 1982.10.24 1\n",
      "1982.11.14 1982.08.27 1982.10.25 1\n",
      "1982.11.15 1982.08.28 1982.10.26 1\n",
      "1982.11.16 1982.08.29 1982.10.27 1\n",
      "1982.11.17 1982.08.30 1982.10.28 1\n",
      "1982.11.18 1982.08.31 1982.10.29 1\n",
      "1982.11.19 1982.09.01 1982.10.30 1\n",
      "1982.11.20 1982.09.02 1982.10.31 1\n",
      "1982.11.21 1982.09.03 1982.11.01 1\n",
      "1982.11.22 1982.09.04 1982.11.02 1\n",
      "1982.11.23 1982.09.05 1982.11.03 1\n",
      "1982.11.24 1982.09.06 1982.11.04 1\n",
      "1982.11.25 1982.09.07 1982.11.05 1\n",
      "1982.11.26 1982.09.08 1982.11.06 1\n",
      "1982.11.27 1982.09.09 1982.11.07 1\n",
      "1982.11.28 1982.09.10 1982.11.08 1\n",
      "1982.11.29 1982.09.11 1982.11.09 1\n",
      "1982.11.30 1982.09.12 1982.11.10 1\n",
      "1982.12.01 1982.09.13 1982.11.11 1\n",
      "1982.12.02 1982.09.14 1982.11.12 1\n",
      "1982.12.03 1982.09.15 1982.11.13 1\n",
      "1982.12.04 1982.09.16 1982.11.14 1\n",
      "1982.12.05 1982.09.17 1982.11.15 1\n",
      "1982.12.06 1982.09.18 1982.11.16 1\n",
      "1982.12.07 1982.09.19 1982.11.17 1\n",
      "1982.12.08 1982.09.20 1982.11.18 1\n",
      "1982.12.09 1982.09.21 1982.11.19 1\n",
      "1982.12.10 1982.09.22 1982.11.20 1\n",
      "1982.12.11 1982.09.23 1982.11.21 1\n",
      "1982.12.12 1982.09.24 1982.11.22 1\n",
      "1982.12.13 1982.09.25 1982.11.23 1\n",
      "1982.12.14 1982.09.26 1982.11.24 1\n",
      "1982.12.15 1982.09.27 1982.11.25 1\n",
      "1982.12.16 1982.09.28 1982.11.26 1\n",
      "1982.12.17 1982.09.29 1982.11.27 1\n",
      "1982.12.18 1982.09.30 1982.11.28 1\n",
      "1982.12.19 1982.10.01 1982.11.29 1\n",
      "1982.12.20 1982.10.02 1982.11.30 1\n",
      "1982.12.21 1982.10.03 1982.12.01 1\n",
      "1982.12.22 1982.10.04 1982.12.02 1\n",
      "1982.12.23 1982.10.05 1982.12.03 1\n",
      "1982.12.24 1982.10.06 1982.12.04 1\n",
      "1982.12.25 1982.10.07 1982.12.05 1\n",
      "1982.12.26 1982.10.08 1982.12.06 1\n",
      "1982.12.27 1982.10.09 1982.12.07 1\n",
      "1982.12.28 1982.10.10 1982.12.08 1\n",
      "1983.10.04 1983.07.17 1983.09.14 0\n",
      "1983.10.05 1983.07.18 1983.09.15 1\n",
      "1983.10.06 1983.07.19 1983.09.16 1\n",
      "1983.10.07 1983.07.20 1983.09.17 1\n",
      "1983.10.08 1983.07.21 1983.09.18 1\n",
      "1983.10.09 1983.07.22 1983.09.19 1\n",
      "1983.10.10 1983.07.23 1983.09.20 1\n",
      "1983.10.11 1983.07.24 1983.09.21 1\n",
      "1983.10.12 1983.07.25 1983.09.22 1\n",
      "1983.10.13 1983.07.26 1983.09.23 1\n",
      "1983.10.14 1983.07.27 1983.09.24 1\n",
      "1983.10.15 1983.07.28 1983.09.25 1\n",
      "1983.10.16 1983.07.29 1983.09.26 1\n",
      "1983.10.17 1983.07.30 1983.09.27 1\n",
      "1983.10.18 1983.07.31 1983.09.28 1\n",
      "1983.10.19 1983.08.01 1983.09.29 1\n",
      "1983.10.20 1983.08.02 1983.09.30 1\n",
      "1983.10.21 1983.08.03 1983.10.01 1\n",
      "1983.10.22 1983.08.04 1983.10.02 1\n",
      "1983.10.23 1983.08.05 1983.10.03 1\n",
      "1983.10.24 1983.08.06 1983.10.04 1\n",
      "1983.10.25 1983.08.07 1983.10.05 1\n",
      "1983.10.26 1983.08.08 1983.10.06 1\n",
      "1983.10.27 1983.08.09 1983.10.07 1\n",
      "1983.10.28 1983.08.10 1983.10.08 1\n",
      "1983.10.29 1983.08.11 1983.10.09 1\n",
      "1983.10.30 1983.08.12 1983.10.10 1\n",
      "1983.10.31 1983.08.13 1983.10.11 1\n",
      "1983.11.01 1983.08.14 1983.10.12 1\n",
      "1983.11.02 1983.08.15 1983.10.13 1\n",
      "1983.11.03 1983.08.16 1983.10.14 1\n",
      "1983.11.04 1983.08.17 1983.10.15 1\n",
      "1983.11.05 1983.08.18 1983.10.16 1\n",
      "1983.11.06 1983.08.19 1983.10.17 1\n",
      "1983.11.07 1983.08.20 1983.10.18 1\n",
      "1983.11.08 1983.08.21 1983.10.19 1\n",
      "1983.11.09 1983.08.22 1983.10.20 1\n",
      "1983.11.10 1983.08.23 1983.10.21 1\n",
      "1983.11.11 1983.08.24 1983.10.22 1\n",
      "1983.11.12 1983.08.25 1983.10.23 1\n",
      "1983.11.13 1983.08.26 1983.10.24 1\n",
      "1983.11.14 1983.08.27 1983.10.25 1\n",
      "1983.11.15 1983.08.28 1983.10.26 1\n",
      "1983.11.16 1983.08.29 1983.10.27 1\n",
      "1983.11.17 1983.08.30 1983.10.28 1\n",
      "1983.11.18 1983.08.31 1983.10.29 1\n",
      "1983.11.19 1983.09.01 1983.10.30 1\n",
      "1983.11.20 1983.09.02 1983.10.31 1\n",
      "1983.11.21 1983.09.03 1983.11.01 1\n",
      "1983.11.22 1983.09.04 1983.11.02 1\n",
      "1983.11.23 1983.09.05 1983.11.03 1\n",
      "1983.11.24 1983.09.06 1983.11.04 1\n",
      "1983.11.25 1983.09.07 1983.11.05 1\n",
      "1983.11.26 1983.09.08 1983.11.06 1\n",
      "1983.11.27 1983.09.09 1983.11.07 1\n",
      "1983.11.28 1983.09.10 1983.11.08 1\n",
      "1983.11.29 1983.09.11 1983.11.09 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1983.11.30 1983.09.12 1983.11.10 1\n",
      "1983.12.01 1983.09.13 1983.11.11 1\n",
      "1983.12.02 1983.09.14 1983.11.12 1\n",
      "1983.12.03 1983.09.15 1983.11.13 1\n",
      "1983.12.04 1983.09.16 1983.11.14 1\n",
      "1983.12.05 1983.09.17 1983.11.15 1\n",
      "1983.12.06 1983.09.18 1983.11.16 1\n",
      "1983.12.07 1983.09.19 1983.11.17 1\n",
      "1983.12.08 1983.09.20 1983.11.18 1\n",
      "1983.12.09 1983.09.21 1983.11.19 1\n",
      "1983.12.10 1983.09.22 1983.11.20 1\n",
      "1983.12.11 1983.09.23 1983.11.21 1\n",
      "1983.12.12 1983.09.24 1983.11.22 1\n",
      "1983.12.13 1983.09.25 1983.11.23 1\n",
      "1983.12.14 1983.09.26 1983.11.24 1\n",
      "1983.12.15 1983.09.27 1983.11.25 1\n",
      "1983.12.16 1983.09.28 1983.11.26 1\n",
      "1983.12.17 1983.09.29 1983.11.27 1\n",
      "1983.12.18 1983.09.30 1983.11.28 1\n",
      "1983.12.19 1983.10.01 1983.11.29 1\n",
      "1983.12.20 1983.10.02 1983.11.30 1\n",
      "1983.12.21 1983.10.03 1983.12.01 1\n",
      "1983.12.22 1983.10.04 1983.12.02 1\n",
      "1983.12.23 1983.10.05 1983.12.03 1\n",
      "1983.12.24 1983.10.06 1983.12.04 1\n",
      "1983.12.25 1983.10.07 1983.12.05 1\n",
      "1983.12.26 1983.10.08 1983.12.06 1\n",
      "1983.12.27 1983.10.09 1983.12.07 1\n",
      "1983.12.28 1983.10.10 1983.12.08 1\n",
      "1984.10.04 1984.07.17 1984.09.14 0\n",
      "1984.10.05 1984.07.18 1984.09.15 1\n",
      "1984.10.06 1984.07.19 1984.09.16 1\n",
      "1984.10.07 1984.07.20 1984.09.17 1\n",
      "1984.10.08 1984.07.21 1984.09.18 1\n",
      "1984.10.09 1984.07.22 1984.09.19 1\n",
      "1984.10.10 1984.07.23 1984.09.20 1\n",
      "1984.10.11 1984.07.24 1984.09.21 1\n",
      "1984.10.12 1984.07.25 1984.09.22 1\n",
      "1984.10.13 1984.07.26 1984.09.23 1\n",
      "1984.10.14 1984.07.27 1984.09.24 1\n",
      "1984.10.15 1984.07.28 1984.09.25 1\n",
      "1984.10.16 1984.07.29 1984.09.26 1\n",
      "1984.10.17 1984.07.30 1984.09.27 1\n",
      "1984.10.18 1984.07.31 1984.09.28 1\n",
      "1984.10.19 1984.08.01 1984.09.29 1\n",
      "1984.10.20 1984.08.02 1984.09.30 1\n",
      "1984.10.21 1984.08.03 1984.10.01 1\n",
      "1984.10.22 1984.08.04 1984.10.02 1\n",
      "1984.10.23 1984.08.05 1984.10.03 1\n",
      "1984.10.24 1984.08.06 1984.10.04 1\n",
      "1984.10.25 1984.08.07 1984.10.05 1\n",
      "1984.10.26 1984.08.08 1984.10.06 1\n",
      "1984.10.27 1984.08.09 1984.10.07 1\n",
      "1984.10.28 1984.08.10 1984.10.08 1\n",
      "1984.10.29 1984.08.11 1984.10.09 1\n",
      "1984.10.30 1984.08.12 1984.10.10 1\n",
      "1984.10.31 1984.08.13 1984.10.11 1\n",
      "1984.11.01 1984.08.14 1984.10.12 1\n",
      "1984.11.02 1984.08.15 1984.10.13 1\n",
      "1984.11.03 1984.08.16 1984.10.14 1\n",
      "1984.11.04 1984.08.17 1984.10.15 1\n",
      "1984.11.05 1984.08.18 1984.10.16 1\n",
      "1984.11.06 1984.08.19 1984.10.17 1\n",
      "1984.11.07 1984.08.20 1984.10.18 1\n",
      "1984.11.08 1984.08.21 1984.10.19 1\n",
      "1984.11.09 1984.08.22 1984.10.20 1\n",
      "1984.11.10 1984.08.23 1984.10.21 1\n",
      "1984.11.11 1984.08.24 1984.10.22 1\n",
      "1984.11.12 1984.08.25 1984.10.23 1\n",
      "1984.11.13 1984.08.26 1984.10.24 1\n",
      "1984.11.14 1984.08.27 1984.10.25 1\n",
      "1984.11.15 1984.08.28 1984.10.26 1\n",
      "1984.11.16 1984.08.29 1984.10.27 1\n",
      "1984.11.17 1984.08.30 1984.10.28 1\n",
      "1984.11.18 1984.08.31 1984.10.29 1\n",
      "1984.11.19 1984.09.01 1984.10.30 1\n",
      "1984.11.20 1984.09.02 1984.10.31 1\n",
      "1984.11.21 1984.09.03 1984.11.01 1\n",
      "1984.11.22 1984.09.04 1984.11.02 1\n",
      "1984.11.23 1984.09.05 1984.11.03 1\n",
      "1984.11.24 1984.09.06 1984.11.04 1\n",
      "1984.11.25 1984.09.07 1984.11.05 1\n",
      "1984.11.26 1984.09.08 1984.11.06 1\n",
      "1984.11.27 1984.09.09 1984.11.07 1\n",
      "1984.11.28 1984.09.10 1984.11.08 1\n",
      "1984.11.29 1984.09.11 1984.11.09 1\n",
      "1984.11.30 1984.09.12 1984.11.10 1\n",
      "1984.12.01 1984.09.13 1984.11.11 1\n",
      "1984.12.02 1984.09.14 1984.11.12 1\n",
      "1984.12.03 1984.09.15 1984.11.13 1\n",
      "1984.12.04 1984.09.16 1984.11.14 1\n",
      "1984.12.05 1984.09.17 1984.11.15 1\n",
      "1984.12.06 1984.09.18 1984.11.16 1\n",
      "1984.12.07 1984.09.19 1984.11.17 1\n",
      "1984.12.08 1984.09.20 1984.11.18 1\n",
      "1984.12.09 1984.09.21 1984.11.19 1\n",
      "1984.12.10 1984.09.22 1984.11.20 1\n",
      "1984.12.11 1984.09.23 1984.11.21 1\n",
      "1984.12.12 1984.09.24 1984.11.22 1\n",
      "1984.12.13 1984.09.25 1984.11.23 1\n",
      "1984.12.14 1984.09.26 1984.11.24 1\n",
      "1984.12.15 1984.09.27 1984.11.25 1\n",
      "1984.12.16 1984.09.28 1984.11.26 1\n",
      "1984.12.17 1984.09.29 1984.11.27 1\n",
      "1984.12.18 1984.09.30 1984.11.28 1\n",
      "1984.12.19 1984.10.01 1984.11.29 1\n",
      "1984.12.20 1984.10.02 1984.11.30 1\n",
      "1984.12.21 1984.10.03 1984.12.01 1\n",
      "1984.12.22 1984.10.04 1984.12.02 1\n",
      "1984.12.23 1984.10.05 1984.12.03 1\n",
      "1984.12.24 1984.10.06 1984.12.04 1\n",
      "1984.12.25 1984.10.07 1984.12.05 1\n",
      "1984.12.26 1984.10.08 1984.12.06 1\n",
      "1984.12.27 1984.10.09 1984.12.07 1\n",
      "1984.12.28 1984.10.10 1984.12.08 1\n",
      "1985.10.04 1985.07.17 1985.09.14 0\n",
      "1985.10.05 1985.07.18 1985.09.15 1\n",
      "1985.10.06 1985.07.19 1985.09.16 1\n",
      "1985.10.07 1985.07.20 1985.09.17 1\n",
      "1985.10.08 1985.07.21 1985.09.18 1\n",
      "1985.10.09 1985.07.22 1985.09.19 1\n",
      "1985.10.10 1985.07.23 1985.09.20 1\n",
      "1985.10.11 1985.07.24 1985.09.21 1\n",
      "1985.10.12 1985.07.25 1985.09.22 1\n",
      "1985.10.13 1985.07.26 1985.09.23 1\n",
      "1985.10.14 1985.07.27 1985.09.24 1\n",
      "1985.10.15 1985.07.28 1985.09.25 1\n",
      "1985.10.16 1985.07.29 1985.09.26 1\n",
      "1985.10.17 1985.07.30 1985.09.27 1\n",
      "1985.10.18 1985.07.31 1985.09.28 1\n",
      "1985.10.19 1985.08.01 1985.09.29 1\n",
      "1985.10.20 1985.08.02 1985.09.30 1\n",
      "1985.10.21 1985.08.03 1985.10.01 1\n",
      "1985.10.22 1985.08.04 1985.10.02 1\n",
      "1985.10.23 1985.08.05 1985.10.03 1\n",
      "1985.10.24 1985.08.06 1985.10.04 1\n",
      "1985.10.25 1985.08.07 1985.10.05 1\n",
      "1985.10.26 1985.08.08 1985.10.06 1\n",
      "1985.10.27 1985.08.09 1985.10.07 1\n",
      "1985.10.28 1985.08.10 1985.10.08 1\n",
      "1985.10.29 1985.08.11 1985.10.09 1\n",
      "1985.10.30 1985.08.12 1985.10.10 1\n",
      "1985.10.31 1985.08.13 1985.10.11 1\n",
      "1985.11.01 1985.08.14 1985.10.12 1\n",
      "1985.11.02 1985.08.15 1985.10.13 1\n",
      "1985.11.03 1985.08.16 1985.10.14 1\n",
      "1985.11.04 1985.08.17 1985.10.15 1\n",
      "1985.11.05 1985.08.18 1985.10.16 1\n",
      "1985.11.06 1985.08.19 1985.10.17 1\n",
      "1985.11.07 1985.08.20 1985.10.18 1\n",
      "1985.11.08 1985.08.21 1985.10.19 1\n",
      "1985.11.09 1985.08.22 1985.10.20 1\n",
      "1985.11.10 1985.08.23 1985.10.21 1\n",
      "1985.11.11 1985.08.24 1985.10.22 1\n",
      "1985.11.12 1985.08.25 1985.10.23 1\n",
      "1985.11.13 1985.08.26 1985.10.24 1\n",
      "1985.11.14 1985.08.27 1985.10.25 1\n",
      "1985.11.15 1985.08.28 1985.10.26 1\n",
      "1985.11.16 1985.08.29 1985.10.27 1\n",
      "1985.11.17 1985.08.30 1985.10.28 1\n",
      "1985.11.18 1985.08.31 1985.10.29 1\n",
      "1985.11.19 1985.09.01 1985.10.30 1\n",
      "1985.11.20 1985.09.02 1985.10.31 1\n",
      "1985.11.21 1985.09.03 1985.11.01 1\n",
      "1985.11.22 1985.09.04 1985.11.02 1\n",
      "1985.11.23 1985.09.05 1985.11.03 1\n",
      "1985.11.24 1985.09.06 1985.11.04 1\n",
      "1985.11.25 1985.09.07 1985.11.05 1\n",
      "1985.11.26 1985.09.08 1985.11.06 1\n",
      "1985.11.27 1985.09.09 1985.11.07 1\n",
      "1985.11.28 1985.09.10 1985.11.08 1\n",
      "1985.11.29 1985.09.11 1985.11.09 1\n",
      "1985.11.30 1985.09.12 1985.11.10 1\n",
      "1985.12.01 1985.09.13 1985.11.11 1\n",
      "1985.12.02 1985.09.14 1985.11.12 1\n",
      "1985.12.03 1985.09.15 1985.11.13 1\n",
      "1985.12.04 1985.09.16 1985.11.14 1\n",
      "1985.12.05 1985.09.17 1985.11.15 1\n",
      "1985.12.06 1985.09.18 1985.11.16 1\n",
      "1985.12.07 1985.09.19 1985.11.17 1\n",
      "1985.12.08 1985.09.20 1985.11.18 1\n",
      "1985.12.09 1985.09.21 1985.11.19 1\n",
      "1985.12.10 1985.09.22 1985.11.20 1\n",
      "1985.12.11 1985.09.23 1985.11.21 1\n",
      "1985.12.12 1985.09.24 1985.11.22 1\n",
      "1985.12.13 1985.09.25 1985.11.23 1\n",
      "1985.12.14 1985.09.26 1985.11.24 1\n",
      "1985.12.15 1985.09.27 1985.11.25 1\n",
      "1985.12.16 1985.09.28 1985.11.26 1\n",
      "1985.12.17 1985.09.29 1985.11.27 1\n",
      "1985.12.18 1985.09.30 1985.11.28 1\n",
      "1985.12.19 1985.10.01 1985.11.29 1\n",
      "1985.12.20 1985.10.02 1985.11.30 1\n",
      "1985.12.21 1985.10.03 1985.12.01 1\n",
      "1985.12.22 1985.10.04 1985.12.02 1\n",
      "1985.12.23 1985.10.05 1985.12.03 1\n",
      "1985.12.24 1985.10.06 1985.12.04 1\n",
      "1985.12.25 1985.10.07 1985.12.05 1\n",
      "1985.12.26 1985.10.08 1985.12.06 1\n",
      "1985.12.27 1985.10.09 1985.12.07 1\n",
      "1985.12.28 1985.10.10 1985.12.08 1\n",
      "1986.10.04 1986.07.17 1986.09.14 0\n",
      "1986.10.05 1986.07.18 1986.09.15 1\n",
      "1986.10.06 1986.07.19 1986.09.16 1\n",
      "1986.10.07 1986.07.20 1986.09.17 1\n",
      "1986.10.08 1986.07.21 1986.09.18 1\n",
      "1986.10.09 1986.07.22 1986.09.19 1\n",
      "1986.10.10 1986.07.23 1986.09.20 1\n",
      "1986.10.11 1986.07.24 1986.09.21 1\n",
      "1986.10.12 1986.07.25 1986.09.22 1\n",
      "1986.10.13 1986.07.26 1986.09.23 1\n",
      "1986.10.14 1986.07.27 1986.09.24 1\n",
      "1986.10.15 1986.07.28 1986.09.25 1\n",
      "1986.10.16 1986.07.29 1986.09.26 1\n",
      "1986.10.17 1986.07.30 1986.09.27 1\n",
      "1986.10.18 1986.07.31 1986.09.28 1\n",
      "1986.10.19 1986.08.01 1986.09.29 1\n",
      "1986.10.20 1986.08.02 1986.09.30 1\n",
      "1986.10.21 1986.08.03 1986.10.01 1\n",
      "1986.10.22 1986.08.04 1986.10.02 1\n",
      "1986.10.23 1986.08.05 1986.10.03 1\n",
      "1986.10.24 1986.08.06 1986.10.04 1\n",
      "1986.10.25 1986.08.07 1986.10.05 1\n",
      "1986.10.26 1986.08.08 1986.10.06 1\n",
      "1986.10.27 1986.08.09 1986.10.07 1\n",
      "1986.10.28 1986.08.10 1986.10.08 1\n",
      "1986.10.29 1986.08.11 1986.10.09 1\n",
      "1986.10.30 1986.08.12 1986.10.10 1\n",
      "1986.10.31 1986.08.13 1986.10.11 1\n",
      "1986.11.01 1986.08.14 1986.10.12 1\n",
      "1986.11.02 1986.08.15 1986.10.13 1\n",
      "1986.11.03 1986.08.16 1986.10.14 1\n",
      "1986.11.04 1986.08.17 1986.10.15 1\n",
      "1986.11.05 1986.08.18 1986.10.16 1\n",
      "1986.11.06 1986.08.19 1986.10.17 1\n",
      "1986.11.07 1986.08.20 1986.10.18 1\n",
      "1986.11.08 1986.08.21 1986.10.19 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1986.11.09 1986.08.22 1986.10.20 1\n",
      "1986.11.10 1986.08.23 1986.10.21 1\n",
      "1986.11.11 1986.08.24 1986.10.22 1\n",
      "1986.11.12 1986.08.25 1986.10.23 1\n",
      "1986.11.13 1986.08.26 1986.10.24 1\n",
      "1986.11.14 1986.08.27 1986.10.25 1\n",
      "1986.11.15 1986.08.28 1986.10.26 1\n",
      "1986.11.16 1986.08.29 1986.10.27 1\n",
      "1986.11.17 1986.08.30 1986.10.28 1\n",
      "1986.11.18 1986.08.31 1986.10.29 1\n",
      "1986.11.19 1986.09.01 1986.10.30 1\n",
      "1986.11.20 1986.09.02 1986.10.31 1\n",
      "1986.11.21 1986.09.03 1986.11.01 1\n",
      "1986.11.22 1986.09.04 1986.11.02 1\n",
      "1986.11.23 1986.09.05 1986.11.03 1\n",
      "1986.11.24 1986.09.06 1986.11.04 1\n",
      "1986.11.25 1986.09.07 1986.11.05 1\n",
      "1986.11.26 1986.09.08 1986.11.06 1\n",
      "1986.11.27 1986.09.09 1986.11.07 1\n",
      "1986.11.28 1986.09.10 1986.11.08 1\n",
      "1986.11.29 1986.09.11 1986.11.09 1\n",
      "1986.11.30 1986.09.12 1986.11.10 1\n",
      "1986.12.01 1986.09.13 1986.11.11 1\n",
      "1986.12.02 1986.09.14 1986.11.12 1\n",
      "1986.12.03 1986.09.15 1986.11.13 1\n",
      "1986.12.04 1986.09.16 1986.11.14 1\n",
      "1986.12.05 1986.09.17 1986.11.15 1\n",
      "1986.12.06 1986.09.18 1986.11.16 1\n",
      "1986.12.07 1986.09.19 1986.11.17 1\n",
      "1986.12.08 1986.09.20 1986.11.18 1\n",
      "1986.12.09 1986.09.21 1986.11.19 1\n",
      "1986.12.10 1986.09.22 1986.11.20 1\n",
      "1986.12.11 1986.09.23 1986.11.21 1\n",
      "1986.12.12 1986.09.24 1986.11.22 1\n",
      "1986.12.13 1986.09.25 1986.11.23 1\n",
      "1986.12.14 1986.09.26 1986.11.24 1\n",
      "1986.12.15 1986.09.27 1986.11.25 1\n",
      "1986.12.16 1986.09.28 1986.11.26 1\n",
      "1986.12.17 1986.09.29 1986.11.27 1\n",
      "1986.12.18 1986.09.30 1986.11.28 1\n",
      "1986.12.19 1986.10.01 1986.11.29 1\n",
      "1986.12.20 1986.10.02 1986.11.30 1\n",
      "1986.12.21 1986.10.03 1986.12.01 1\n",
      "1986.12.22 1986.10.04 1986.12.02 1\n",
      "1986.12.23 1986.10.05 1986.12.03 1\n",
      "1986.12.24 1986.10.06 1986.12.04 1\n",
      "1986.12.25 1986.10.07 1986.12.05 1\n",
      "1986.12.26 1986.10.08 1986.12.06 1\n",
      "1986.12.27 1986.10.09 1986.12.07 1\n",
      "1986.12.28 1986.10.10 1986.12.08 1\n",
      "1987.10.04 1987.07.17 1987.09.14 0\n",
      "1987.10.05 1987.07.18 1987.09.15 1\n",
      "1987.10.06 1987.07.19 1987.09.16 1\n",
      "1987.10.07 1987.07.20 1987.09.17 1\n",
      "1987.10.08 1987.07.21 1987.09.18 1\n",
      "1987.10.09 1987.07.22 1987.09.19 1\n",
      "1987.10.10 1987.07.23 1987.09.20 1\n",
      "1987.10.11 1987.07.24 1987.09.21 1\n",
      "1987.10.12 1987.07.25 1987.09.22 1\n",
      "1987.10.13 1987.07.26 1987.09.23 1\n",
      "1987.10.14 1987.07.27 1987.09.24 1\n",
      "1987.10.15 1987.07.28 1987.09.25 1\n",
      "1987.10.16 1987.07.29 1987.09.26 1\n",
      "1987.10.17 1987.07.30 1987.09.27 1\n",
      "1987.10.18 1987.07.31 1987.09.28 1\n",
      "1987.10.19 1987.08.01 1987.09.29 1\n",
      "1987.10.20 1987.08.02 1987.09.30 1\n",
      "1987.10.21 1987.08.03 1987.10.01 1\n",
      "1987.10.22 1987.08.04 1987.10.02 1\n",
      "1987.10.23 1987.08.05 1987.10.03 1\n",
      "1987.10.24 1987.08.06 1987.10.04 1\n",
      "1987.10.25 1987.08.07 1987.10.05 1\n",
      "1987.10.26 1987.08.08 1987.10.06 1\n",
      "1987.10.27 1987.08.09 1987.10.07 1\n",
      "1987.10.28 1987.08.10 1987.10.08 1\n",
      "1987.10.29 1987.08.11 1987.10.09 1\n",
      "1987.10.30 1987.08.12 1987.10.10 1\n",
      "1987.10.31 1987.08.13 1987.10.11 1\n",
      "1987.11.01 1987.08.14 1987.10.12 1\n",
      "1987.11.02 1987.08.15 1987.10.13 1\n",
      "1987.11.03 1987.08.16 1987.10.14 1\n",
      "1987.11.04 1987.08.17 1987.10.15 1\n",
      "1987.11.05 1987.08.18 1987.10.16 1\n",
      "1987.11.06 1987.08.19 1987.10.17 1\n",
      "1987.11.07 1987.08.20 1987.10.18 1\n",
      "1987.11.08 1987.08.21 1987.10.19 1\n",
      "1987.11.09 1987.08.22 1987.10.20 1\n",
      "1987.11.10 1987.08.23 1987.10.21 1\n",
      "1987.11.11 1987.08.24 1987.10.22 1\n",
      "1987.11.12 1987.08.25 1987.10.23 1\n",
      "1987.11.13 1987.08.26 1987.10.24 1\n",
      "1987.11.14 1987.08.27 1987.10.25 1\n",
      "1987.11.15 1987.08.28 1987.10.26 1\n",
      "1987.11.16 1987.08.29 1987.10.27 1\n",
      "1987.11.17 1987.08.30 1987.10.28 1\n",
      "1987.11.18 1987.08.31 1987.10.29 1\n",
      "1987.11.19 1987.09.01 1987.10.30 1\n",
      "1987.11.20 1987.09.02 1987.10.31 1\n",
      "1987.11.21 1987.09.03 1987.11.01 1\n",
      "1987.11.22 1987.09.04 1987.11.02 1\n",
      "1987.11.23 1987.09.05 1987.11.03 1\n",
      "1987.11.24 1987.09.06 1987.11.04 1\n",
      "1987.11.25 1987.09.07 1987.11.05 1\n",
      "1987.11.26 1987.09.08 1987.11.06 1\n",
      "1987.11.27 1987.09.09 1987.11.07 1\n",
      "1987.11.28 1987.09.10 1987.11.08 1\n",
      "1987.11.29 1987.09.11 1987.11.09 1\n",
      "1987.11.30 1987.09.12 1987.11.10 1\n",
      "1987.12.01 1987.09.13 1987.11.11 1\n",
      "1987.12.02 1987.09.14 1987.11.12 1\n",
      "1987.12.03 1987.09.15 1987.11.13 1\n",
      "1987.12.04 1987.09.16 1987.11.14 1\n",
      "1987.12.05 1987.09.17 1987.11.15 1\n",
      "1987.12.06 1987.09.18 1987.11.16 1\n",
      "1987.12.07 1987.09.19 1987.11.17 1\n",
      "1987.12.08 1987.09.20 1987.11.18 1\n",
      "1987.12.09 1987.09.21 1987.11.19 1\n",
      "1987.12.10 1987.09.22 1987.11.20 1\n",
      "1987.12.11 1987.09.23 1987.11.21 1\n",
      "1987.12.12 1987.09.24 1987.11.22 1\n",
      "1987.12.13 1987.09.25 1987.11.23 1\n",
      "1987.12.14 1987.09.26 1987.11.24 1\n",
      "1987.12.15 1987.09.27 1987.11.25 1\n",
      "1987.12.16 1987.09.28 1987.11.26 1\n",
      "1987.12.17 1987.09.29 1987.11.27 1\n",
      "1987.12.18 1987.09.30 1987.11.28 1\n",
      "1987.12.19 1987.10.01 1987.11.29 1\n",
      "1987.12.20 1987.10.02 1987.11.30 1\n",
      "1987.12.21 1987.10.03 1987.12.01 1\n",
      "1987.12.22 1987.10.04 1987.12.02 1\n",
      "1987.12.23 1987.10.05 1987.12.03 1\n",
      "1987.12.24 1987.10.06 1987.12.04 1\n",
      "1987.12.25 1987.10.07 1987.12.05 1\n",
      "1987.12.26 1987.10.08 1987.12.06 1\n",
      "1987.12.27 1987.10.09 1987.12.07 1\n",
      "1987.12.28 1987.10.10 1987.12.08 1\n",
      "1988.10.04 1988.07.17 1988.09.14 0\n",
      "1988.10.05 1988.07.18 1988.09.15 1\n",
      "1988.10.06 1988.07.19 1988.09.16 1\n",
      "1988.10.07 1988.07.20 1988.09.17 1\n",
      "1988.10.08 1988.07.21 1988.09.18 1\n",
      "1988.10.09 1988.07.22 1988.09.19 1\n",
      "1988.10.10 1988.07.23 1988.09.20 1\n",
      "1988.10.11 1988.07.24 1988.09.21 1\n",
      "1988.10.12 1988.07.25 1988.09.22 1\n",
      "1988.10.13 1988.07.26 1988.09.23 1\n",
      "1988.10.14 1988.07.27 1988.09.24 1\n",
      "1988.10.15 1988.07.28 1988.09.25 1\n",
      "1988.10.16 1988.07.29 1988.09.26 1\n",
      "1988.10.17 1988.07.30 1988.09.27 1\n",
      "1988.10.18 1988.07.31 1988.09.28 1\n",
      "1988.10.19 1988.08.01 1988.09.29 1\n",
      "1988.10.20 1988.08.02 1988.09.30 1\n",
      "1988.10.21 1988.08.03 1988.10.01 1\n",
      "1988.10.22 1988.08.04 1988.10.02 1\n",
      "1988.10.23 1988.08.05 1988.10.03 1\n",
      "1988.10.24 1988.08.06 1988.10.04 1\n",
      "1988.10.25 1988.08.07 1988.10.05 1\n",
      "1988.10.26 1988.08.08 1988.10.06 1\n",
      "1988.10.27 1988.08.09 1988.10.07 1\n",
      "1988.10.28 1988.08.10 1988.10.08 1\n",
      "1988.10.29 1988.08.11 1988.10.09 1\n",
      "1988.10.30 1988.08.12 1988.10.10 1\n",
      "1988.10.31 1988.08.13 1988.10.11 1\n",
      "1988.11.01 1988.08.14 1988.10.12 1\n",
      "1988.11.02 1988.08.15 1988.10.13 1\n",
      "1988.11.03 1988.08.16 1988.10.14 1\n",
      "1988.11.04 1988.08.17 1988.10.15 1\n",
      "1988.11.05 1988.08.18 1988.10.16 1\n",
      "1988.11.06 1988.08.19 1988.10.17 1\n",
      "1988.11.07 1988.08.20 1988.10.18 1\n",
      "1988.11.08 1988.08.21 1988.10.19 1\n",
      "1988.11.09 1988.08.22 1988.10.20 1\n",
      "1988.11.10 1988.08.23 1988.10.21 1\n",
      "1988.11.11 1988.08.24 1988.10.22 1\n",
      "1988.11.12 1988.08.25 1988.10.23 1\n",
      "1988.11.13 1988.08.26 1988.10.24 1\n",
      "1988.11.14 1988.08.27 1988.10.25 1\n",
      "1988.11.15 1988.08.28 1988.10.26 1\n",
      "1988.11.16 1988.08.29 1988.10.27 1\n",
      "1988.11.17 1988.08.30 1988.10.28 1\n",
      "1988.11.18 1988.08.31 1988.10.29 1\n",
      "1988.11.19 1988.09.01 1988.10.30 1\n",
      "1988.11.20 1988.09.02 1988.10.31 1\n",
      "1988.11.21 1988.09.03 1988.11.01 1\n",
      "1988.11.22 1988.09.04 1988.11.02 1\n",
      "1988.11.23 1988.09.05 1988.11.03 1\n",
      "1988.11.24 1988.09.06 1988.11.04 1\n",
      "1988.11.25 1988.09.07 1988.11.05 1\n",
      "1988.11.26 1988.09.08 1988.11.06 1\n",
      "1988.11.27 1988.09.09 1988.11.07 1\n",
      "1988.11.28 1988.09.10 1988.11.08 1\n",
      "1988.11.29 1988.09.11 1988.11.09 1\n",
      "1988.11.30 1988.09.12 1988.11.10 1\n",
      "1988.12.01 1988.09.13 1988.11.11 1\n",
      "1988.12.02 1988.09.14 1988.11.12 1\n",
      "1988.12.03 1988.09.15 1988.11.13 1\n",
      "1988.12.04 1988.09.16 1988.11.14 1\n",
      "1988.12.05 1988.09.17 1988.11.15 1\n",
      "1988.12.06 1988.09.18 1988.11.16 1\n",
      "1988.12.07 1988.09.19 1988.11.17 1\n",
      "1988.12.08 1988.09.20 1988.11.18 1\n",
      "1988.12.09 1988.09.21 1988.11.19 1\n",
      "1988.12.10 1988.09.22 1988.11.20 1\n",
      "1988.12.11 1988.09.23 1988.11.21 1\n",
      "1988.12.12 1988.09.24 1988.11.22 1\n",
      "1988.12.13 1988.09.25 1988.11.23 1\n",
      "1988.12.14 1988.09.26 1988.11.24 1\n",
      "1988.12.15 1988.09.27 1988.11.25 1\n",
      "1988.12.16 1988.09.28 1988.11.26 1\n",
      "1988.12.17 1988.09.29 1988.11.27 1\n",
      "1988.12.18 1988.09.30 1988.11.28 1\n",
      "1988.12.19 1988.10.01 1988.11.29 1\n",
      "1988.12.20 1988.10.02 1988.11.30 1\n",
      "1988.12.21 1988.10.03 1988.12.01 1\n",
      "1988.12.22 1988.10.04 1988.12.02 1\n",
      "1988.12.23 1988.10.05 1988.12.03 1\n",
      "1988.12.24 1988.10.06 1988.12.04 1\n",
      "1988.12.25 1988.10.07 1988.12.05 1\n",
      "1988.12.26 1988.10.08 1988.12.06 1\n",
      "1988.12.27 1988.10.09 1988.12.07 1\n",
      "1988.12.28 1988.10.10 1988.12.08 1\n",
      "1989.10.04 1989.07.17 1989.09.14 0\n",
      "1989.10.05 1989.07.18 1989.09.15 1\n",
      "1989.10.06 1989.07.19 1989.09.16 1\n",
      "1989.10.07 1989.07.20 1989.09.17 1\n",
      "1989.10.08 1989.07.21 1989.09.18 1\n",
      "1989.10.09 1989.07.22 1989.09.19 1\n",
      "1989.10.10 1989.07.23 1989.09.20 1\n",
      "1989.10.11 1989.07.24 1989.09.21 1\n",
      "1989.10.12 1989.07.25 1989.09.22 1\n",
      "1989.10.13 1989.07.26 1989.09.23 1\n",
      "1989.10.14 1989.07.27 1989.09.24 1\n",
      "1989.10.15 1989.07.28 1989.09.25 1\n",
      "1989.10.16 1989.07.29 1989.09.26 1\n",
      "1989.10.17 1989.07.30 1989.09.27 1\n",
      "1989.10.18 1989.07.31 1989.09.28 1\n",
      "1989.10.19 1989.08.01 1989.09.29 1\n",
      "1989.10.20 1989.08.02 1989.09.30 1\n",
      "1989.10.21 1989.08.03 1989.10.01 1\n",
      "1989.10.22 1989.08.04 1989.10.02 1\n",
      "1989.10.23 1989.08.05 1989.10.03 1\n",
      "1989.10.24 1989.08.06 1989.10.04 1\n",
      "1989.10.25 1989.08.07 1989.10.05 1\n",
      "1989.10.26 1989.08.08 1989.10.06 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1989.10.27 1989.08.09 1989.10.07 1\n",
      "1989.10.28 1989.08.10 1989.10.08 1\n",
      "1989.10.29 1989.08.11 1989.10.09 1\n",
      "1989.10.30 1989.08.12 1989.10.10 1\n",
      "1989.10.31 1989.08.13 1989.10.11 1\n",
      "1989.11.01 1989.08.14 1989.10.12 1\n",
      "1989.11.02 1989.08.15 1989.10.13 1\n",
      "1989.11.03 1989.08.16 1989.10.14 1\n",
      "1989.11.04 1989.08.17 1989.10.15 1\n",
      "1989.11.05 1989.08.18 1989.10.16 1\n",
      "1989.11.06 1989.08.19 1989.10.17 1\n",
      "1989.11.07 1989.08.20 1989.10.18 1\n",
      "1989.11.08 1989.08.21 1989.10.19 1\n",
      "1989.11.09 1989.08.22 1989.10.20 1\n",
      "1989.11.10 1989.08.23 1989.10.21 1\n",
      "1989.11.11 1989.08.24 1989.10.22 1\n",
      "1989.11.12 1989.08.25 1989.10.23 1\n",
      "1989.11.13 1989.08.26 1989.10.24 1\n",
      "1989.11.14 1989.08.27 1989.10.25 1\n",
      "1989.11.15 1989.08.28 1989.10.26 1\n",
      "1989.11.16 1989.08.29 1989.10.27 1\n",
      "1989.11.17 1989.08.30 1989.10.28 1\n",
      "1989.11.18 1989.08.31 1989.10.29 1\n",
      "1989.11.19 1989.09.01 1989.10.30 1\n",
      "1989.11.20 1989.09.02 1989.10.31 1\n",
      "1989.11.21 1989.09.03 1989.11.01 1\n",
      "1989.11.22 1989.09.04 1989.11.02 1\n",
      "1989.11.23 1989.09.05 1989.11.03 1\n",
      "1989.11.24 1989.09.06 1989.11.04 1\n",
      "1989.11.25 1989.09.07 1989.11.05 1\n",
      "1989.11.26 1989.09.08 1989.11.06 1\n",
      "1989.11.27 1989.09.09 1989.11.07 1\n",
      "1989.11.28 1989.09.10 1989.11.08 1\n",
      "1989.11.29 1989.09.11 1989.11.09 1\n",
      "1989.11.30 1989.09.12 1989.11.10 1\n",
      "1989.12.01 1989.09.13 1989.11.11 1\n",
      "1989.12.02 1989.09.14 1989.11.12 1\n",
      "1989.12.03 1989.09.15 1989.11.13 1\n",
      "1989.12.04 1989.09.16 1989.11.14 1\n",
      "1989.12.05 1989.09.17 1989.11.15 1\n",
      "1989.12.06 1989.09.18 1989.11.16 1\n",
      "1989.12.07 1989.09.19 1989.11.17 1\n",
      "1989.12.08 1989.09.20 1989.11.18 1\n",
      "1989.12.09 1989.09.21 1989.11.19 1\n",
      "1989.12.10 1989.09.22 1989.11.20 1\n",
      "1989.12.11 1989.09.23 1989.11.21 1\n",
      "1989.12.12 1989.09.24 1989.11.22 1\n",
      "1989.12.13 1989.09.25 1989.11.23 1\n",
      "1989.12.14 1989.09.26 1989.11.24 1\n",
      "1989.12.15 1989.09.27 1989.11.25 1\n",
      "1989.12.16 1989.09.28 1989.11.26 1\n",
      "1989.12.17 1989.09.29 1989.11.27 1\n",
      "1989.12.18 1989.09.30 1989.11.28 1\n",
      "1989.12.19 1989.10.01 1989.11.29 1\n",
      "1989.12.20 1989.10.02 1989.11.30 1\n",
      "1989.12.21 1989.10.03 1989.12.01 1\n",
      "1989.12.22 1989.10.04 1989.12.02 1\n",
      "1989.12.23 1989.10.05 1989.12.03 1\n",
      "1989.12.24 1989.10.06 1989.12.04 1\n",
      "1989.12.25 1989.10.07 1989.12.05 1\n",
      "1989.12.26 1989.10.08 1989.12.06 1\n",
      "1989.12.27 1989.10.09 1989.12.07 1\n",
      "1989.12.28 1989.10.10 1989.12.08 1\n",
      "1990.10.04 1990.07.17 1990.09.14 0\n",
      "1990.10.05 1990.07.18 1990.09.15 1\n",
      "1990.10.06 1990.07.19 1990.09.16 1\n",
      "1990.10.07 1990.07.20 1990.09.17 1\n",
      "1990.10.08 1990.07.21 1990.09.18 1\n",
      "1990.10.09 1990.07.22 1990.09.19 1\n",
      "1990.10.10 1990.07.23 1990.09.20 1\n",
      "1990.10.11 1990.07.24 1990.09.21 1\n",
      "1990.10.12 1990.07.25 1990.09.22 1\n",
      "1990.10.13 1990.07.26 1990.09.23 1\n",
      "1990.10.14 1990.07.27 1990.09.24 1\n",
      "1990.10.15 1990.07.28 1990.09.25 1\n",
      "1990.10.16 1990.07.29 1990.09.26 1\n",
      "1990.10.17 1990.07.30 1990.09.27 1\n",
      "1990.10.18 1990.07.31 1990.09.28 1\n",
      "1990.10.19 1990.08.01 1990.09.29 1\n",
      "1990.10.20 1990.08.02 1990.09.30 1\n",
      "1990.10.21 1990.08.03 1990.10.01 1\n",
      "1990.10.22 1990.08.04 1990.10.02 1\n",
      "1990.10.23 1990.08.05 1990.10.03 1\n",
      "1990.10.24 1990.08.06 1990.10.04 1\n",
      "1990.10.25 1990.08.07 1990.10.05 1\n",
      "1990.10.26 1990.08.08 1990.10.06 1\n",
      "1990.10.27 1990.08.09 1990.10.07 1\n",
      "1990.10.28 1990.08.10 1990.10.08 1\n",
      "1990.10.29 1990.08.11 1990.10.09 1\n",
      "1990.10.30 1990.08.12 1990.10.10 1\n",
      "1990.10.31 1990.08.13 1990.10.11 1\n",
      "1990.11.01 1990.08.14 1990.10.12 1\n",
      "1990.11.02 1990.08.15 1990.10.13 1\n",
      "1990.11.03 1990.08.16 1990.10.14 1\n",
      "1990.11.04 1990.08.17 1990.10.15 1\n",
      "1990.11.05 1990.08.18 1990.10.16 1\n",
      "1990.11.06 1990.08.19 1990.10.17 1\n",
      "1990.11.07 1990.08.20 1990.10.18 1\n",
      "1990.11.08 1990.08.21 1990.10.19 1\n",
      "1990.11.09 1990.08.22 1990.10.20 1\n",
      "1990.11.10 1990.08.23 1990.10.21 1\n",
      "1990.11.11 1990.08.24 1990.10.22 1\n",
      "1990.11.12 1990.08.25 1990.10.23 1\n",
      "1990.11.13 1990.08.26 1990.10.24 1\n",
      "1990.11.14 1990.08.27 1990.10.25 1\n",
      "1990.11.15 1990.08.28 1990.10.26 1\n",
      "1990.11.16 1990.08.29 1990.10.27 1\n",
      "1990.11.17 1990.08.30 1990.10.28 1\n",
      "1990.11.18 1990.08.31 1990.10.29 1\n",
      "1990.11.19 1990.09.01 1990.10.30 1\n",
      "1990.11.20 1990.09.02 1990.10.31 1\n",
      "1990.11.21 1990.09.03 1990.11.01 1\n",
      "1990.11.22 1990.09.04 1990.11.02 1\n",
      "1990.11.23 1990.09.05 1990.11.03 1\n",
      "1990.11.24 1990.09.06 1990.11.04 1\n",
      "1990.11.25 1990.09.07 1990.11.05 1\n",
      "1990.11.26 1990.09.08 1990.11.06 1\n",
      "1990.11.27 1990.09.09 1990.11.07 1\n",
      "1990.11.28 1990.09.10 1990.11.08 1\n",
      "1990.11.29 1990.09.11 1990.11.09 1\n",
      "1990.11.30 1990.09.12 1990.11.10 1\n",
      "1990.12.01 1990.09.13 1990.11.11 1\n",
      "1990.12.02 1990.09.14 1990.11.12 1\n",
      "1990.12.03 1990.09.15 1990.11.13 1\n",
      "1990.12.04 1990.09.16 1990.11.14 1\n",
      "1990.12.05 1990.09.17 1990.11.15 1\n",
      "1990.12.06 1990.09.18 1990.11.16 1\n",
      "1990.12.07 1990.09.19 1990.11.17 1\n",
      "1990.12.08 1990.09.20 1990.11.18 1\n",
      "1990.12.09 1990.09.21 1990.11.19 1\n",
      "1990.12.10 1990.09.22 1990.11.20 1\n",
      "1990.12.11 1990.09.23 1990.11.21 1\n",
      "1990.12.12 1990.09.24 1990.11.22 1\n",
      "1990.12.13 1990.09.25 1990.11.23 1\n",
      "1990.12.14 1990.09.26 1990.11.24 1\n",
      "1990.12.15 1990.09.27 1990.11.25 1\n",
      "1990.12.16 1990.09.28 1990.11.26 1\n",
      "1990.12.17 1990.09.29 1990.11.27 1\n",
      "1990.12.18 1990.09.30 1990.11.28 1\n",
      "1990.12.19 1990.10.01 1990.11.29 1\n",
      "1990.12.20 1990.10.02 1990.11.30 1\n",
      "1990.12.21 1990.10.03 1990.12.01 1\n",
      "1990.12.22 1990.10.04 1990.12.02 1\n",
      "1990.12.23 1990.10.05 1990.12.03 1\n",
      "1990.12.24 1990.10.06 1990.12.04 1\n",
      "1990.12.25 1990.10.07 1990.12.05 1\n",
      "1990.12.26 1990.10.08 1990.12.06 1\n",
      "1990.12.27 1990.10.09 1990.12.07 1\n",
      "1990.12.28 1990.10.10 1990.12.08 1\n",
      "1991.10.04 1991.07.17 1991.09.14 0\n",
      "1991.10.05 1991.07.18 1991.09.15 1\n",
      "1991.10.06 1991.07.19 1991.09.16 1\n",
      "1991.10.07 1991.07.20 1991.09.17 1\n",
      "1991.10.08 1991.07.21 1991.09.18 1\n",
      "1991.10.09 1991.07.22 1991.09.19 1\n",
      "1991.10.10 1991.07.23 1991.09.20 1\n",
      "1991.10.11 1991.07.24 1991.09.21 1\n",
      "1991.10.12 1991.07.25 1991.09.22 1\n",
      "1991.10.13 1991.07.26 1991.09.23 1\n",
      "1991.10.14 1991.07.27 1991.09.24 1\n",
      "1991.10.15 1991.07.28 1991.09.25 1\n",
      "1991.10.16 1991.07.29 1991.09.26 1\n",
      "1991.10.17 1991.07.30 1991.09.27 1\n",
      "1991.10.18 1991.07.31 1991.09.28 1\n",
      "1991.10.19 1991.08.01 1991.09.29 1\n",
      "1991.10.20 1991.08.02 1991.09.30 1\n",
      "1991.10.21 1991.08.03 1991.10.01 1\n",
      "1991.10.22 1991.08.04 1991.10.02 1\n",
      "1991.10.23 1991.08.05 1991.10.03 1\n",
      "1991.10.24 1991.08.06 1991.10.04 1\n",
      "1991.10.25 1991.08.07 1991.10.05 1\n",
      "1991.10.26 1991.08.08 1991.10.06 1\n",
      "1991.10.27 1991.08.09 1991.10.07 1\n",
      "1991.10.28 1991.08.10 1991.10.08 1\n",
      "1991.10.29 1991.08.11 1991.10.09 1\n",
      "1991.10.30 1991.08.12 1991.10.10 1\n",
      "1991.10.31 1991.08.13 1991.10.11 1\n",
      "1991.11.01 1991.08.14 1991.10.12 1\n",
      "1991.11.02 1991.08.15 1991.10.13 1\n",
      "1991.11.03 1991.08.16 1991.10.14 1\n",
      "1991.11.04 1991.08.17 1991.10.15 1\n",
      "1991.11.05 1991.08.18 1991.10.16 1\n",
      "1991.11.06 1991.08.19 1991.10.17 1\n",
      "1991.11.07 1991.08.20 1991.10.18 1\n",
      "1991.11.08 1991.08.21 1991.10.19 1\n",
      "1991.11.09 1991.08.22 1991.10.20 1\n",
      "1991.11.10 1991.08.23 1991.10.21 1\n",
      "1991.11.11 1991.08.24 1991.10.22 1\n",
      "1991.11.12 1991.08.25 1991.10.23 1\n",
      "1991.11.13 1991.08.26 1991.10.24 1\n",
      "1991.11.14 1991.08.27 1991.10.25 1\n",
      "1991.11.15 1991.08.28 1991.10.26 1\n",
      "1991.11.16 1991.08.29 1991.10.27 1\n",
      "1991.11.17 1991.08.30 1991.10.28 1\n",
      "1991.11.18 1991.08.31 1991.10.29 1\n",
      "1991.11.19 1991.09.01 1991.10.30 1\n",
      "1991.11.20 1991.09.02 1991.10.31 1\n",
      "1991.11.21 1991.09.03 1991.11.01 1\n",
      "1991.11.22 1991.09.04 1991.11.02 1\n",
      "1991.11.23 1991.09.05 1991.11.03 1\n",
      "1991.11.24 1991.09.06 1991.11.04 1\n",
      "1991.11.25 1991.09.07 1991.11.05 1\n",
      "1991.11.26 1991.09.08 1991.11.06 1\n",
      "1991.11.27 1991.09.09 1991.11.07 1\n",
      "1991.11.28 1991.09.10 1991.11.08 1\n",
      "1991.11.29 1991.09.11 1991.11.09 1\n",
      "1991.11.30 1991.09.12 1991.11.10 1\n",
      "1991.12.01 1991.09.13 1991.11.11 1\n",
      "1991.12.02 1991.09.14 1991.11.12 1\n",
      "1991.12.03 1991.09.15 1991.11.13 1\n",
      "1991.12.04 1991.09.16 1991.11.14 1\n",
      "1991.12.05 1991.09.17 1991.11.15 1\n",
      "1991.12.06 1991.09.18 1991.11.16 1\n",
      "1991.12.07 1991.09.19 1991.11.17 1\n",
      "1991.12.08 1991.09.20 1991.11.18 1\n",
      "1991.12.09 1991.09.21 1991.11.19 1\n",
      "1991.12.10 1991.09.22 1991.11.20 1\n",
      "1991.12.11 1991.09.23 1991.11.21 1\n",
      "1991.12.12 1991.09.24 1991.11.22 1\n",
      "1991.12.13 1991.09.25 1991.11.23 1\n",
      "1991.12.14 1991.09.26 1991.11.24 1\n",
      "1991.12.15 1991.09.27 1991.11.25 1\n",
      "1991.12.16 1991.09.28 1991.11.26 1\n",
      "1991.12.17 1991.09.29 1991.11.27 1\n",
      "1991.12.18 1991.09.30 1991.11.28 1\n",
      "1991.12.19 1991.10.01 1991.11.29 1\n",
      "1991.12.20 1991.10.02 1991.11.30 1\n",
      "1991.12.21 1991.10.03 1991.12.01 1\n",
      "1991.12.22 1991.10.04 1991.12.02 1\n",
      "1991.12.23 1991.10.05 1991.12.03 1\n",
      "1991.12.24 1991.10.06 1991.12.04 1\n",
      "1991.12.25 1991.10.07 1991.12.05 1\n",
      "1991.12.26 1991.10.08 1991.12.06 1\n",
      "1991.12.27 1991.10.09 1991.12.07 1\n",
      "1991.12.28 1991.10.10 1991.12.08 1\n",
      "1992.10.04 1992.07.17 1992.09.14 0\n",
      "1992.10.05 1992.07.18 1992.09.15 1\n",
      "1992.10.06 1992.07.19 1992.09.16 1\n",
      "1992.10.07 1992.07.20 1992.09.17 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1992.10.08 1992.07.21 1992.09.18 1\n",
      "1992.10.09 1992.07.22 1992.09.19 1\n",
      "1992.10.10 1992.07.23 1992.09.20 1\n",
      "1992.10.11 1992.07.24 1992.09.21 1\n",
      "1992.10.12 1992.07.25 1992.09.22 1\n",
      "1992.10.13 1992.07.26 1992.09.23 1\n",
      "1992.10.14 1992.07.27 1992.09.24 1\n",
      "1992.10.15 1992.07.28 1992.09.25 1\n",
      "1992.10.16 1992.07.29 1992.09.26 1\n",
      "1992.10.17 1992.07.30 1992.09.27 1\n",
      "1992.10.18 1992.07.31 1992.09.28 1\n",
      "1992.10.19 1992.08.01 1992.09.29 1\n",
      "1992.10.20 1992.08.02 1992.09.30 1\n",
      "1992.10.21 1992.08.03 1992.10.01 1\n",
      "1992.10.22 1992.08.04 1992.10.02 1\n",
      "1992.10.23 1992.08.05 1992.10.03 1\n",
      "1992.10.24 1992.08.06 1992.10.04 1\n",
      "1992.10.25 1992.08.07 1992.10.05 1\n",
      "1992.10.26 1992.08.08 1992.10.06 1\n",
      "1992.10.27 1992.08.09 1992.10.07 1\n",
      "1992.10.28 1992.08.10 1992.10.08 1\n",
      "1992.10.29 1992.08.11 1992.10.09 1\n",
      "1992.10.30 1992.08.12 1992.10.10 1\n",
      "1992.10.31 1992.08.13 1992.10.11 1\n",
      "1992.11.01 1992.08.14 1992.10.12 1\n",
      "1992.11.02 1992.08.15 1992.10.13 1\n",
      "1992.11.03 1992.08.16 1992.10.14 1\n",
      "1992.11.04 1992.08.17 1992.10.15 1\n",
      "1992.11.05 1992.08.18 1992.10.16 1\n",
      "1992.11.06 1992.08.19 1992.10.17 1\n",
      "1992.11.07 1992.08.20 1992.10.18 1\n",
      "1992.11.08 1992.08.21 1992.10.19 1\n",
      "1992.11.09 1992.08.22 1992.10.20 1\n",
      "1992.11.10 1992.08.23 1992.10.21 1\n",
      "1992.11.11 1992.08.24 1992.10.22 1\n",
      "1992.11.12 1992.08.25 1992.10.23 1\n",
      "1992.11.13 1992.08.26 1992.10.24 1\n",
      "1992.11.14 1992.08.27 1992.10.25 1\n",
      "1992.11.15 1992.08.28 1992.10.26 1\n",
      "1992.11.16 1992.08.29 1992.10.27 1\n",
      "1992.11.17 1992.08.30 1992.10.28 1\n",
      "1992.11.18 1992.08.31 1992.10.29 1\n",
      "1992.11.19 1992.09.01 1992.10.30 1\n",
      "1992.11.20 1992.09.02 1992.10.31 1\n",
      "1992.11.21 1992.09.03 1992.11.01 1\n",
      "1992.11.22 1992.09.04 1992.11.02 1\n",
      "1992.11.23 1992.09.05 1992.11.03 1\n",
      "1992.11.24 1992.09.06 1992.11.04 1\n",
      "1992.11.25 1992.09.07 1992.11.05 1\n",
      "1992.11.26 1992.09.08 1992.11.06 1\n",
      "1992.11.27 1992.09.09 1992.11.07 1\n",
      "1992.11.28 1992.09.10 1992.11.08 1\n",
      "1992.11.29 1992.09.11 1992.11.09 1\n",
      "1992.11.30 1992.09.12 1992.11.10 1\n",
      "1992.12.01 1992.09.13 1992.11.11 1\n",
      "1992.12.02 1992.09.14 1992.11.12 1\n",
      "1992.12.03 1992.09.15 1992.11.13 1\n",
      "1992.12.04 1992.09.16 1992.11.14 1\n",
      "1992.12.05 1992.09.17 1992.11.15 1\n",
      "1992.12.06 1992.09.18 1992.11.16 1\n",
      "1992.12.07 1992.09.19 1992.11.17 1\n",
      "1992.12.08 1992.09.20 1992.11.18 1\n",
      "1992.12.09 1992.09.21 1992.11.19 1\n",
      "1992.12.10 1992.09.22 1992.11.20 1\n",
      "1992.12.11 1992.09.23 1992.11.21 1\n",
      "1992.12.12 1992.09.24 1992.11.22 1\n",
      "1992.12.13 1992.09.25 1992.11.23 1\n",
      "1992.12.14 1992.09.26 1992.11.24 1\n",
      "1992.12.15 1992.09.27 1992.11.25 1\n",
      "1992.12.16 1992.09.28 1992.11.26 1\n",
      "1992.12.17 1992.09.29 1992.11.27 1\n",
      "1992.12.18 1992.09.30 1992.11.28 1\n",
      "1992.12.19 1992.10.01 1992.11.29 1\n",
      "1992.12.20 1992.10.02 1992.11.30 1\n",
      "1992.12.21 1992.10.03 1992.12.01 1\n",
      "1992.12.22 1992.10.04 1992.12.02 1\n",
      "1992.12.23 1992.10.05 1992.12.03 1\n",
      "1992.12.24 1992.10.06 1992.12.04 1\n",
      "1992.12.25 1992.10.07 1992.12.05 1\n",
      "1992.12.26 1992.10.08 1992.12.06 1\n",
      "1992.12.27 1992.10.09 1992.12.07 1\n",
      "1992.12.28 1992.10.10 1992.12.08 1\n",
      "1993.10.04 1993.07.17 1993.09.14 0\n",
      "1993.10.05 1993.07.18 1993.09.15 1\n",
      "1993.10.06 1993.07.19 1993.09.16 1\n",
      "1993.10.07 1993.07.20 1993.09.17 1\n",
      "1993.10.08 1993.07.21 1993.09.18 1\n",
      "1993.10.09 1993.07.22 1993.09.19 1\n",
      "1993.10.10 1993.07.23 1993.09.20 1\n",
      "1993.10.11 1993.07.24 1993.09.21 1\n",
      "1993.10.12 1993.07.25 1993.09.22 1\n",
      "1993.10.13 1993.07.26 1993.09.23 1\n",
      "1993.10.14 1993.07.27 1993.09.24 1\n",
      "1993.10.15 1993.07.28 1993.09.25 1\n",
      "1993.10.16 1993.07.29 1993.09.26 1\n",
      "1993.10.17 1993.07.30 1993.09.27 1\n",
      "1993.10.18 1993.07.31 1993.09.28 1\n",
      "1993.10.19 1993.08.01 1993.09.29 1\n",
      "1993.10.20 1993.08.02 1993.09.30 1\n",
      "1993.10.21 1993.08.03 1993.10.01 1\n",
      "1993.10.22 1993.08.04 1993.10.02 1\n",
      "1993.10.23 1993.08.05 1993.10.03 1\n",
      "1993.10.24 1993.08.06 1993.10.04 1\n",
      "1993.10.25 1993.08.07 1993.10.05 1\n",
      "1993.10.26 1993.08.08 1993.10.06 1\n",
      "1993.10.27 1993.08.09 1993.10.07 1\n",
      "1993.10.28 1993.08.10 1993.10.08 1\n",
      "1993.10.29 1993.08.11 1993.10.09 1\n",
      "1993.10.30 1993.08.12 1993.10.10 1\n",
      "1993.10.31 1993.08.13 1993.10.11 1\n",
      "1993.11.01 1993.08.14 1993.10.12 1\n",
      "1993.11.02 1993.08.15 1993.10.13 1\n",
      "1993.11.03 1993.08.16 1993.10.14 1\n",
      "1993.11.04 1993.08.17 1993.10.15 1\n",
      "1993.11.05 1993.08.18 1993.10.16 1\n",
      "1993.11.06 1993.08.19 1993.10.17 1\n",
      "1993.11.07 1993.08.20 1993.10.18 1\n",
      "1993.11.08 1993.08.21 1993.10.19 1\n",
      "1993.11.09 1993.08.22 1993.10.20 1\n",
      "1993.11.10 1993.08.23 1993.10.21 1\n",
      "1993.11.11 1993.08.24 1993.10.22 1\n",
      "1993.11.12 1993.08.25 1993.10.23 1\n",
      "1993.11.13 1993.08.26 1993.10.24 1\n",
      "1993.11.14 1993.08.27 1993.10.25 1\n",
      "1993.11.15 1993.08.28 1993.10.26 1\n",
      "1993.11.16 1993.08.29 1993.10.27 1\n",
      "1993.11.17 1993.08.30 1993.10.28 1\n",
      "1993.11.18 1993.08.31 1993.10.29 1\n",
      "1993.11.19 1993.09.01 1993.10.30 1\n",
      "1993.11.20 1993.09.02 1993.10.31 1\n",
      "1993.11.21 1993.09.03 1993.11.01 1\n",
      "1993.11.22 1993.09.04 1993.11.02 1\n",
      "1993.11.23 1993.09.05 1993.11.03 1\n",
      "1993.11.24 1993.09.06 1993.11.04 1\n",
      "1993.11.25 1993.09.07 1993.11.05 1\n",
      "1993.11.26 1993.09.08 1993.11.06 1\n",
      "1993.11.27 1993.09.09 1993.11.07 1\n",
      "1993.11.28 1993.09.10 1993.11.08 1\n",
      "1993.11.29 1993.09.11 1993.11.09 1\n",
      "1993.11.30 1993.09.12 1993.11.10 1\n",
      "1993.12.01 1993.09.13 1993.11.11 1\n",
      "1993.12.02 1993.09.14 1993.11.12 1\n",
      "1993.12.03 1993.09.15 1993.11.13 1\n",
      "1993.12.04 1993.09.16 1993.11.14 1\n",
      "1993.12.05 1993.09.17 1993.11.15 1\n",
      "1993.12.06 1993.09.18 1993.11.16 1\n",
      "1993.12.07 1993.09.19 1993.11.17 1\n",
      "1993.12.08 1993.09.20 1993.11.18 1\n",
      "1993.12.09 1993.09.21 1993.11.19 1\n",
      "1993.12.10 1993.09.22 1993.11.20 1\n",
      "1993.12.11 1993.09.23 1993.11.21 1\n",
      "1993.12.12 1993.09.24 1993.11.22 1\n",
      "1993.12.13 1993.09.25 1993.11.23 1\n",
      "1993.12.14 1993.09.26 1993.11.24 1\n",
      "1993.12.15 1993.09.27 1993.11.25 1\n",
      "1993.12.16 1993.09.28 1993.11.26 1\n",
      "1993.12.17 1993.09.29 1993.11.27 1\n",
      "1993.12.18 1993.09.30 1993.11.28 1\n",
      "1993.12.19 1993.10.01 1993.11.29 1\n",
      "1993.12.20 1993.10.02 1993.11.30 1\n",
      "1993.12.21 1993.10.03 1993.12.01 1\n",
      "1993.12.22 1993.10.04 1993.12.02 1\n",
      "1993.12.23 1993.10.05 1993.12.03 1\n",
      "1993.12.24 1993.10.06 1993.12.04 1\n",
      "1993.12.25 1993.10.07 1993.12.05 1\n",
      "1993.12.26 1993.10.08 1993.12.06 1\n",
      "1993.12.27 1993.10.09 1993.12.07 1\n",
      "1993.12.28 1993.10.10 1993.12.08 1\n",
      "1994.10.04 1994.07.17 1994.09.14 0\n",
      "1994.10.05 1994.07.18 1994.09.15 1\n",
      "1994.10.06 1994.07.19 1994.09.16 1\n",
      "1994.10.07 1994.07.20 1994.09.17 1\n",
      "1994.10.08 1994.07.21 1994.09.18 1\n",
      "1994.10.09 1994.07.22 1994.09.19 1\n",
      "1994.10.10 1994.07.23 1994.09.20 1\n",
      "1994.10.11 1994.07.24 1994.09.21 1\n",
      "1994.10.12 1994.07.25 1994.09.22 1\n",
      "1994.10.13 1994.07.26 1994.09.23 1\n",
      "1994.10.14 1994.07.27 1994.09.24 1\n",
      "1994.10.15 1994.07.28 1994.09.25 1\n",
      "1994.10.16 1994.07.29 1994.09.26 1\n",
      "1994.10.17 1994.07.30 1994.09.27 1\n",
      "1994.10.18 1994.07.31 1994.09.28 1\n",
      "1994.10.19 1994.08.01 1994.09.29 1\n",
      "1994.10.20 1994.08.02 1994.09.30 1\n",
      "1994.10.21 1994.08.03 1994.10.01 1\n",
      "1994.10.22 1994.08.04 1994.10.02 1\n",
      "1994.10.23 1994.08.05 1994.10.03 1\n",
      "1994.10.24 1994.08.06 1994.10.04 1\n",
      "1994.10.25 1994.08.07 1994.10.05 1\n",
      "1994.10.26 1994.08.08 1994.10.06 1\n",
      "1994.10.27 1994.08.09 1994.10.07 1\n",
      "1994.10.28 1994.08.10 1994.10.08 1\n",
      "1994.10.29 1994.08.11 1994.10.09 1\n",
      "1994.10.30 1994.08.12 1994.10.10 1\n",
      "1994.10.31 1994.08.13 1994.10.11 1\n",
      "1994.11.01 1994.08.14 1994.10.12 1\n",
      "1994.11.02 1994.08.15 1994.10.13 1\n",
      "1994.11.03 1994.08.16 1994.10.14 1\n",
      "1994.11.04 1994.08.17 1994.10.15 1\n",
      "1994.11.05 1994.08.18 1994.10.16 1\n",
      "1994.11.06 1994.08.19 1994.10.17 1\n",
      "1994.11.07 1994.08.20 1994.10.18 1\n",
      "1994.11.08 1994.08.21 1994.10.19 1\n",
      "1994.11.09 1994.08.22 1994.10.20 1\n",
      "1994.11.10 1994.08.23 1994.10.21 1\n",
      "1994.11.11 1994.08.24 1994.10.22 1\n",
      "1994.11.12 1994.08.25 1994.10.23 1\n",
      "1994.11.13 1994.08.26 1994.10.24 1\n",
      "1994.11.14 1994.08.27 1994.10.25 1\n",
      "1994.11.15 1994.08.28 1994.10.26 1\n",
      "1994.11.16 1994.08.29 1994.10.27 1\n",
      "1994.11.17 1994.08.30 1994.10.28 1\n",
      "1994.11.18 1994.08.31 1994.10.29 1\n",
      "1994.11.19 1994.09.01 1994.10.30 1\n",
      "1994.11.20 1994.09.02 1994.10.31 1\n",
      "1994.11.21 1994.09.03 1994.11.01 1\n",
      "1994.11.22 1994.09.04 1994.11.02 1\n",
      "1994.11.23 1994.09.05 1994.11.03 1\n",
      "1994.11.24 1994.09.06 1994.11.04 1\n",
      "1994.11.25 1994.09.07 1994.11.05 1\n",
      "1994.11.26 1994.09.08 1994.11.06 1\n",
      "1994.11.27 1994.09.09 1994.11.07 1\n",
      "1994.11.28 1994.09.10 1994.11.08 1\n",
      "1994.11.29 1994.09.11 1994.11.09 1\n",
      "1994.11.30 1994.09.12 1994.11.10 1\n",
      "1994.12.01 1994.09.13 1994.11.11 1\n",
      "1994.12.02 1994.09.14 1994.11.12 1\n",
      "1994.12.03 1994.09.15 1994.11.13 1\n",
      "1994.12.04 1994.09.16 1994.11.14 1\n",
      "1994.12.05 1994.09.17 1994.11.15 1\n",
      "1994.12.06 1994.09.18 1994.11.16 1\n",
      "1994.12.07 1994.09.19 1994.11.17 1\n",
      "1994.12.08 1994.09.20 1994.11.18 1\n",
      "1994.12.09 1994.09.21 1994.11.19 1\n",
      "1994.12.10 1994.09.22 1994.11.20 1\n",
      "1994.12.11 1994.09.23 1994.11.21 1\n",
      "1994.12.12 1994.09.24 1994.11.22 1\n",
      "1994.12.13 1994.09.25 1994.11.23 1\n",
      "1994.12.14 1994.09.26 1994.11.24 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1994.12.15 1994.09.27 1994.11.25 1\n",
      "1994.12.16 1994.09.28 1994.11.26 1\n",
      "1994.12.17 1994.09.29 1994.11.27 1\n",
      "1994.12.18 1994.09.30 1994.11.28 1\n",
      "1994.12.19 1994.10.01 1994.11.29 1\n",
      "1994.12.20 1994.10.02 1994.11.30 1\n",
      "1994.12.21 1994.10.03 1994.12.01 1\n",
      "1994.12.22 1994.10.04 1994.12.02 1\n",
      "1994.12.23 1994.10.05 1994.12.03 1\n",
      "1994.12.24 1994.10.06 1994.12.04 1\n",
      "1994.12.25 1994.10.07 1994.12.05 1\n",
      "1994.12.26 1994.10.08 1994.12.06 1\n",
      "1994.12.27 1994.10.09 1994.12.07 1\n",
      "1994.12.28 1994.10.10 1994.12.08 1\n",
      "1995.10.04 1995.07.17 1995.09.14 0\n",
      "1995.10.05 1995.07.18 1995.09.15 1\n",
      "1995.10.06 1995.07.19 1995.09.16 1\n",
      "1995.10.07 1995.07.20 1995.09.17 1\n",
      "1995.10.08 1995.07.21 1995.09.18 1\n",
      "1995.10.09 1995.07.22 1995.09.19 1\n",
      "1995.10.10 1995.07.23 1995.09.20 1\n",
      "1995.10.11 1995.07.24 1995.09.21 1\n",
      "1995.10.12 1995.07.25 1995.09.22 1\n",
      "1995.10.13 1995.07.26 1995.09.23 1\n",
      "1995.10.14 1995.07.27 1995.09.24 1\n",
      "1995.10.15 1995.07.28 1995.09.25 1\n",
      "1995.10.16 1995.07.29 1995.09.26 1\n",
      "1995.10.17 1995.07.30 1995.09.27 1\n",
      "1995.10.18 1995.07.31 1995.09.28 1\n",
      "1995.10.19 1995.08.01 1995.09.29 1\n",
      "1995.10.20 1995.08.02 1995.09.30 1\n",
      "1995.10.21 1995.08.03 1995.10.01 1\n",
      "1995.10.22 1995.08.04 1995.10.02 1\n",
      "1995.10.23 1995.08.05 1995.10.03 1\n",
      "1995.10.24 1995.08.06 1995.10.04 1\n",
      "1995.10.25 1995.08.07 1995.10.05 1\n",
      "1995.10.26 1995.08.08 1995.10.06 1\n",
      "1995.10.27 1995.08.09 1995.10.07 1\n",
      "1995.10.28 1995.08.10 1995.10.08 1\n",
      "1995.10.29 1995.08.11 1995.10.09 1\n",
      "1995.10.30 1995.08.12 1995.10.10 1\n",
      "1995.10.31 1995.08.13 1995.10.11 1\n",
      "1995.11.01 1995.08.14 1995.10.12 1\n",
      "1995.11.02 1995.08.15 1995.10.13 1\n",
      "1995.11.03 1995.08.16 1995.10.14 1\n",
      "1995.11.04 1995.08.17 1995.10.15 1\n",
      "1995.11.05 1995.08.18 1995.10.16 1\n",
      "1995.11.06 1995.08.19 1995.10.17 1\n",
      "1995.11.07 1995.08.20 1995.10.18 1\n",
      "1995.11.08 1995.08.21 1995.10.19 1\n",
      "1995.11.09 1995.08.22 1995.10.20 1\n",
      "1995.11.10 1995.08.23 1995.10.21 1\n",
      "1995.11.11 1995.08.24 1995.10.22 1\n",
      "1995.11.12 1995.08.25 1995.10.23 1\n",
      "1995.11.13 1995.08.26 1995.10.24 1\n",
      "1995.11.14 1995.08.27 1995.10.25 1\n",
      "1995.11.15 1995.08.28 1995.10.26 1\n",
      "1995.11.16 1995.08.29 1995.10.27 1\n",
      "1995.11.17 1995.08.30 1995.10.28 1\n",
      "1995.11.18 1995.08.31 1995.10.29 1\n",
      "1995.11.19 1995.09.01 1995.10.30 1\n",
      "1995.11.20 1995.09.02 1995.10.31 1\n",
      "1995.11.21 1995.09.03 1995.11.01 1\n",
      "1995.11.22 1995.09.04 1995.11.02 1\n",
      "1995.11.23 1995.09.05 1995.11.03 1\n",
      "1995.11.24 1995.09.06 1995.11.04 1\n",
      "1995.11.25 1995.09.07 1995.11.05 1\n",
      "1995.11.26 1995.09.08 1995.11.06 1\n",
      "1995.11.27 1995.09.09 1995.11.07 1\n",
      "1995.11.28 1995.09.10 1995.11.08 1\n",
      "1995.11.29 1995.09.11 1995.11.09 1\n",
      "1995.11.30 1995.09.12 1995.11.10 1\n",
      "1995.12.01 1995.09.13 1995.11.11 1\n",
      "1995.12.02 1995.09.14 1995.11.12 1\n",
      "1995.12.03 1995.09.15 1995.11.13 1\n",
      "1995.12.04 1995.09.16 1995.11.14 1\n",
      "1995.12.05 1995.09.17 1995.11.15 1\n",
      "1995.12.06 1995.09.18 1995.11.16 1\n",
      "1995.12.07 1995.09.19 1995.11.17 1\n",
      "1995.12.08 1995.09.20 1995.11.18 1\n",
      "1995.12.09 1995.09.21 1995.11.19 1\n",
      "1995.12.10 1995.09.22 1995.11.20 1\n",
      "1995.12.11 1995.09.23 1995.11.21 1\n",
      "1995.12.12 1995.09.24 1995.11.22 1\n",
      "1995.12.13 1995.09.25 1995.11.23 1\n",
      "1995.12.14 1995.09.26 1995.11.24 1\n",
      "1995.12.15 1995.09.27 1995.11.25 1\n",
      "1995.12.16 1995.09.28 1995.11.26 1\n",
      "1995.12.17 1995.09.29 1995.11.27 1\n",
      "1995.12.18 1995.09.30 1995.11.28 1\n",
      "1995.12.19 1995.10.01 1995.11.29 1\n",
      "1995.12.20 1995.10.02 1995.11.30 1\n",
      "1995.12.21 1995.10.03 1995.12.01 1\n",
      "1995.12.22 1995.10.04 1995.12.02 1\n",
      "1995.12.23 1995.10.05 1995.12.03 1\n",
      "1995.12.24 1995.10.06 1995.12.04 1\n",
      "1995.12.25 1995.10.07 1995.12.05 1\n",
      "1995.12.26 1995.10.08 1995.12.06 1\n",
      "1995.12.27 1995.10.09 1995.12.07 1\n",
      "1995.12.28 1995.10.10 1995.12.08 1\n",
      "1996.10.04 1996.07.17 1996.09.14 0\n",
      "1996.10.05 1996.07.18 1996.09.15 1\n",
      "1996.10.06 1996.07.19 1996.09.16 1\n",
      "1996.10.07 1996.07.20 1996.09.17 1\n",
      "1996.10.08 1996.07.21 1996.09.18 1\n",
      "1996.10.09 1996.07.22 1996.09.19 1\n",
      "1996.10.10 1996.07.23 1996.09.20 1\n",
      "1996.10.11 1996.07.24 1996.09.21 1\n",
      "1996.10.12 1996.07.25 1996.09.22 1\n",
      "1996.10.13 1996.07.26 1996.09.23 1\n",
      "1996.10.14 1996.07.27 1996.09.24 1\n",
      "1996.10.15 1996.07.28 1996.09.25 1\n",
      "1996.10.16 1996.07.29 1996.09.26 1\n",
      "1996.10.17 1996.07.30 1996.09.27 1\n",
      "1996.10.18 1996.07.31 1996.09.28 1\n",
      "1996.10.19 1996.08.01 1996.09.29 1\n",
      "1996.10.20 1996.08.02 1996.09.30 1\n",
      "1996.10.21 1996.08.03 1996.10.01 1\n",
      "1996.10.22 1996.08.04 1996.10.02 1\n",
      "1996.10.23 1996.08.05 1996.10.03 1\n",
      "1996.10.24 1996.08.06 1996.10.04 1\n",
      "1996.10.25 1996.08.07 1996.10.05 1\n",
      "1996.10.26 1996.08.08 1996.10.06 1\n",
      "1996.10.27 1996.08.09 1996.10.07 1\n",
      "1996.10.28 1996.08.10 1996.10.08 1\n",
      "1996.10.29 1996.08.11 1996.10.09 1\n",
      "1996.10.30 1996.08.12 1996.10.10 1\n",
      "1996.10.31 1996.08.13 1996.10.11 1\n",
      "1996.11.01 1996.08.14 1996.10.12 1\n",
      "1996.11.02 1996.08.15 1996.10.13 1\n",
      "1996.11.03 1996.08.16 1996.10.14 1\n",
      "1996.11.04 1996.08.17 1996.10.15 1\n",
      "1996.11.05 1996.08.18 1996.10.16 1\n",
      "1996.11.06 1996.08.19 1996.10.17 1\n",
      "1996.11.07 1996.08.20 1996.10.18 1\n",
      "1996.11.08 1996.08.21 1996.10.19 1\n",
      "1996.11.09 1996.08.22 1996.10.20 1\n",
      "1996.11.10 1996.08.23 1996.10.21 1\n",
      "1996.11.11 1996.08.24 1996.10.22 1\n",
      "1996.11.12 1996.08.25 1996.10.23 1\n",
      "1996.11.13 1996.08.26 1996.10.24 1\n",
      "1996.11.14 1996.08.27 1996.10.25 1\n",
      "1996.11.15 1996.08.28 1996.10.26 1\n",
      "1996.11.16 1996.08.29 1996.10.27 1\n",
      "1996.11.17 1996.08.30 1996.10.28 1\n",
      "1996.11.18 1996.08.31 1996.10.29 1\n",
      "1996.11.19 1996.09.01 1996.10.30 1\n",
      "1996.11.20 1996.09.02 1996.10.31 1\n",
      "1996.11.21 1996.09.03 1996.11.01 1\n",
      "1996.11.22 1996.09.04 1996.11.02 1\n",
      "1996.11.23 1996.09.05 1996.11.03 1\n",
      "1996.11.24 1996.09.06 1996.11.04 1\n",
      "1996.11.25 1996.09.07 1996.11.05 1\n",
      "1996.11.26 1996.09.08 1996.11.06 1\n",
      "1996.11.27 1996.09.09 1996.11.07 1\n",
      "1996.11.28 1996.09.10 1996.11.08 1\n",
      "1996.11.29 1996.09.11 1996.11.09 1\n",
      "1996.11.30 1996.09.12 1996.11.10 1\n",
      "1996.12.01 1996.09.13 1996.11.11 1\n",
      "1996.12.02 1996.09.14 1996.11.12 1\n",
      "1996.12.03 1996.09.15 1996.11.13 1\n",
      "1996.12.04 1996.09.16 1996.11.14 1\n",
      "1996.12.05 1996.09.17 1996.11.15 1\n",
      "1996.12.06 1996.09.18 1996.11.16 1\n",
      "1996.12.07 1996.09.19 1996.11.17 1\n",
      "1996.12.08 1996.09.20 1996.11.18 1\n",
      "1996.12.09 1996.09.21 1996.11.19 1\n",
      "1996.12.10 1996.09.22 1996.11.20 1\n",
      "1996.12.11 1996.09.23 1996.11.21 1\n",
      "1996.12.12 1996.09.24 1996.11.22 1\n",
      "1996.12.13 1996.09.25 1996.11.23 1\n",
      "1996.12.14 1996.09.26 1996.11.24 1\n",
      "1996.12.15 1996.09.27 1996.11.25 1\n",
      "1996.12.16 1996.09.28 1996.11.26 1\n",
      "1996.12.17 1996.09.29 1996.11.27 1\n",
      "1996.12.18 1996.09.30 1996.11.28 1\n",
      "1996.12.19 1996.10.01 1996.11.29 1\n",
      "1996.12.20 1996.10.02 1996.11.30 1\n",
      "1996.12.21 1996.10.03 1996.12.01 1\n",
      "1996.12.22 1996.10.04 1996.12.02 1\n",
      "1996.12.23 1996.10.05 1996.12.03 1\n",
      "1996.12.24 1996.10.06 1996.12.04 1\n",
      "1996.12.25 1996.10.07 1996.12.05 1\n",
      "1996.12.26 1996.10.08 1996.12.06 1\n",
      "1996.12.27 1996.10.09 1996.12.07 1\n",
      "1996.12.28 1996.10.10 1996.12.08 1\n",
      "1997.10.04 1997.07.17 1997.09.14 0\n",
      "1997.10.05 1997.07.18 1997.09.15 1\n",
      "1997.10.06 1997.07.19 1997.09.16 1\n",
      "1997.10.07 1997.07.20 1997.09.17 1\n",
      "1997.10.08 1997.07.21 1997.09.18 1\n",
      "1997.10.09 1997.07.22 1997.09.19 1\n",
      "1997.10.10 1997.07.23 1997.09.20 1\n",
      "1997.10.11 1997.07.24 1997.09.21 1\n",
      "1997.10.12 1997.07.25 1997.09.22 1\n",
      "1997.10.13 1997.07.26 1997.09.23 1\n",
      "1997.10.14 1997.07.27 1997.09.24 1\n",
      "1997.10.15 1997.07.28 1997.09.25 1\n",
      "1997.10.16 1997.07.29 1997.09.26 1\n",
      "1997.10.17 1997.07.30 1997.09.27 1\n",
      "1997.10.18 1997.07.31 1997.09.28 1\n",
      "1997.10.19 1997.08.01 1997.09.29 1\n",
      "1997.10.20 1997.08.02 1997.09.30 1\n",
      "1997.10.21 1997.08.03 1997.10.01 1\n",
      "1997.10.22 1997.08.04 1997.10.02 1\n",
      "1997.10.23 1997.08.05 1997.10.03 1\n",
      "1997.10.24 1997.08.06 1997.10.04 1\n",
      "1997.10.25 1997.08.07 1997.10.05 1\n",
      "1997.10.26 1997.08.08 1997.10.06 1\n",
      "1997.10.27 1997.08.09 1997.10.07 1\n",
      "1997.10.28 1997.08.10 1997.10.08 1\n",
      "1997.10.29 1997.08.11 1997.10.09 1\n",
      "1997.10.30 1997.08.12 1997.10.10 1\n",
      "1997.10.31 1997.08.13 1997.10.11 1\n",
      "1997.11.01 1997.08.14 1997.10.12 1\n",
      "1997.11.02 1997.08.15 1997.10.13 1\n",
      "1997.11.03 1997.08.16 1997.10.14 1\n",
      "1997.11.04 1997.08.17 1997.10.15 1\n",
      "1997.11.05 1997.08.18 1997.10.16 1\n",
      "1997.11.06 1997.08.19 1997.10.17 1\n",
      "1997.11.07 1997.08.20 1997.10.18 1\n",
      "1997.11.08 1997.08.21 1997.10.19 1\n",
      "1997.11.09 1997.08.22 1997.10.20 1\n",
      "1997.11.10 1997.08.23 1997.10.21 1\n",
      "1997.11.11 1997.08.24 1997.10.22 1\n",
      "1997.11.12 1997.08.25 1997.10.23 1\n",
      "1997.11.13 1997.08.26 1997.10.24 1\n",
      "1997.11.14 1997.08.27 1997.10.25 1\n",
      "1997.11.15 1997.08.28 1997.10.26 1\n",
      "1997.11.16 1997.08.29 1997.10.27 1\n",
      "1997.11.17 1997.08.30 1997.10.28 1\n",
      "1997.11.18 1997.08.31 1997.10.29 1\n",
      "1997.11.19 1997.09.01 1997.10.30 1\n",
      "1997.11.20 1997.09.02 1997.10.31 1\n",
      "1997.11.21 1997.09.03 1997.11.01 1\n",
      "1997.11.22 1997.09.04 1997.11.02 1\n",
      "1997.11.23 1997.09.05 1997.11.03 1\n",
      "1997.11.24 1997.09.06 1997.11.04 1\n",
      "1997.11.25 1997.09.07 1997.11.05 1\n",
      "1997.11.26 1997.09.08 1997.11.06 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1997.11.27 1997.09.09 1997.11.07 1\n",
      "1997.11.28 1997.09.10 1997.11.08 1\n",
      "1997.11.29 1997.09.11 1997.11.09 1\n",
      "1997.11.30 1997.09.12 1997.11.10 1\n",
      "1997.12.01 1997.09.13 1997.11.11 1\n",
      "1997.12.02 1997.09.14 1997.11.12 1\n",
      "1997.12.03 1997.09.15 1997.11.13 1\n",
      "1997.12.04 1997.09.16 1997.11.14 1\n",
      "1997.12.05 1997.09.17 1997.11.15 1\n",
      "1997.12.06 1997.09.18 1997.11.16 1\n",
      "1997.12.07 1997.09.19 1997.11.17 1\n",
      "1997.12.08 1997.09.20 1997.11.18 1\n",
      "1997.12.09 1997.09.21 1997.11.19 1\n",
      "1997.12.10 1997.09.22 1997.11.20 1\n",
      "1997.12.11 1997.09.23 1997.11.21 1\n",
      "1997.12.12 1997.09.24 1997.11.22 1\n",
      "1997.12.13 1997.09.25 1997.11.23 1\n",
      "1997.12.14 1997.09.26 1997.11.24 1\n",
      "1997.12.15 1997.09.27 1997.11.25 1\n",
      "1997.12.16 1997.09.28 1997.11.26 1\n",
      "1997.12.17 1997.09.29 1997.11.27 1\n",
      "1997.12.18 1997.09.30 1997.11.28 1\n",
      "1997.12.19 1997.10.01 1997.11.29 1\n",
      "1997.12.20 1997.10.02 1997.11.30 1\n",
      "1997.12.21 1997.10.03 1997.12.01 1\n",
      "1997.12.22 1997.10.04 1997.12.02 1\n",
      "1997.12.23 1997.10.05 1997.12.03 1\n",
      "1997.12.24 1997.10.06 1997.12.04 1\n",
      "1997.12.25 1997.10.07 1997.12.05 1\n",
      "1997.12.26 1997.10.08 1997.12.06 1\n",
      "1997.12.27 1997.10.09 1997.12.07 1\n",
      "1997.12.28 1997.10.10 1997.12.08 1\n",
      "1998.10.04 1998.07.17 1998.09.14 0\n",
      "1998.10.05 1998.07.18 1998.09.15 1\n",
      "1998.10.06 1998.07.19 1998.09.16 1\n",
      "1998.10.07 1998.07.20 1998.09.17 1\n",
      "1998.10.08 1998.07.21 1998.09.18 1\n",
      "1998.10.09 1998.07.22 1998.09.19 1\n",
      "1998.10.10 1998.07.23 1998.09.20 1\n",
      "1998.10.11 1998.07.24 1998.09.21 1\n",
      "1998.10.12 1998.07.25 1998.09.22 1\n",
      "1998.10.13 1998.07.26 1998.09.23 1\n",
      "1998.10.14 1998.07.27 1998.09.24 1\n",
      "1998.10.15 1998.07.28 1998.09.25 1\n",
      "1998.10.16 1998.07.29 1998.09.26 1\n",
      "1998.10.17 1998.07.30 1998.09.27 1\n",
      "1998.10.18 1998.07.31 1998.09.28 1\n",
      "1998.10.19 1998.08.01 1998.09.29 1\n",
      "1998.10.20 1998.08.02 1998.09.30 1\n",
      "1998.10.21 1998.08.03 1998.10.01 1\n",
      "1998.10.22 1998.08.04 1998.10.02 1\n",
      "1998.10.23 1998.08.05 1998.10.03 1\n",
      "1998.10.24 1998.08.06 1998.10.04 1\n",
      "1998.10.25 1998.08.07 1998.10.05 1\n",
      "1998.10.26 1998.08.08 1998.10.06 1\n",
      "1998.10.27 1998.08.09 1998.10.07 1\n",
      "1998.10.28 1998.08.10 1998.10.08 1\n",
      "1998.10.29 1998.08.11 1998.10.09 1\n",
      "1998.10.30 1998.08.12 1998.10.10 1\n",
      "1998.10.31 1998.08.13 1998.10.11 1\n",
      "1998.11.01 1998.08.14 1998.10.12 1\n",
      "1998.11.02 1998.08.15 1998.10.13 1\n",
      "1998.11.03 1998.08.16 1998.10.14 1\n",
      "1998.11.04 1998.08.17 1998.10.15 1\n",
      "1998.11.05 1998.08.18 1998.10.16 1\n",
      "1998.11.06 1998.08.19 1998.10.17 1\n",
      "1998.11.07 1998.08.20 1998.10.18 1\n",
      "1998.11.08 1998.08.21 1998.10.19 1\n",
      "1998.11.09 1998.08.22 1998.10.20 1\n",
      "1998.11.10 1998.08.23 1998.10.21 1\n",
      "1998.11.11 1998.08.24 1998.10.22 1\n",
      "1998.11.12 1998.08.25 1998.10.23 1\n",
      "1998.11.13 1998.08.26 1998.10.24 1\n",
      "1998.11.14 1998.08.27 1998.10.25 1\n",
      "1998.11.15 1998.08.28 1998.10.26 1\n",
      "1998.11.16 1998.08.29 1998.10.27 1\n",
      "1998.11.17 1998.08.30 1998.10.28 1\n",
      "1998.11.18 1998.08.31 1998.10.29 1\n",
      "1998.11.19 1998.09.01 1998.10.30 1\n",
      "1998.11.20 1998.09.02 1998.10.31 1\n",
      "1998.11.21 1998.09.03 1998.11.01 1\n",
      "1998.11.22 1998.09.04 1998.11.02 1\n",
      "1998.11.23 1998.09.05 1998.11.03 1\n",
      "1998.11.24 1998.09.06 1998.11.04 1\n",
      "1998.11.25 1998.09.07 1998.11.05 1\n",
      "1998.11.26 1998.09.08 1998.11.06 1\n",
      "1998.11.27 1998.09.09 1998.11.07 1\n",
      "1998.11.28 1998.09.10 1998.11.08 1\n",
      "1998.11.29 1998.09.11 1998.11.09 1\n",
      "1998.11.30 1998.09.12 1998.11.10 1\n",
      "1998.12.01 1998.09.13 1998.11.11 1\n",
      "1998.12.02 1998.09.14 1998.11.12 1\n",
      "1998.12.03 1998.09.15 1998.11.13 1\n",
      "1998.12.04 1998.09.16 1998.11.14 1\n",
      "1998.12.05 1998.09.17 1998.11.15 1\n",
      "1998.12.06 1998.09.18 1998.11.16 1\n",
      "1998.12.07 1998.09.19 1998.11.17 1\n",
      "1998.12.08 1998.09.20 1998.11.18 1\n",
      "1998.12.09 1998.09.21 1998.11.19 1\n",
      "1998.12.10 1998.09.22 1998.11.20 1\n",
      "1998.12.11 1998.09.23 1998.11.21 1\n",
      "1998.12.12 1998.09.24 1998.11.22 1\n",
      "1998.12.13 1998.09.25 1998.11.23 1\n",
      "1998.12.14 1998.09.26 1998.11.24 1\n",
      "1998.12.15 1998.09.27 1998.11.25 1\n",
      "1998.12.16 1998.09.28 1998.11.26 1\n",
      "1998.12.17 1998.09.29 1998.11.27 1\n",
      "1998.12.18 1998.09.30 1998.11.28 1\n",
      "1998.12.19 1998.10.01 1998.11.29 1\n",
      "1998.12.20 1998.10.02 1998.11.30 1\n",
      "1998.12.21 1998.10.03 1998.12.01 1\n",
      "1998.12.22 1998.10.04 1998.12.02 1\n",
      "1998.12.23 1998.10.05 1998.12.03 1\n",
      "1998.12.24 1998.10.06 1998.12.04 1\n",
      "1998.12.25 1998.10.07 1998.12.05 1\n",
      "1998.12.26 1998.10.08 1998.12.06 1\n",
      "1998.12.27 1998.10.09 1998.12.07 1\n",
      "1998.12.28 1998.10.10 1998.12.08 1\n",
      "1999.10.04 1999.07.17 1999.09.14 0\n",
      "1999.10.05 1999.07.18 1999.09.15 1\n",
      "1999.10.06 1999.07.19 1999.09.16 1\n",
      "1999.10.07 1999.07.20 1999.09.17 1\n",
      "1999.10.08 1999.07.21 1999.09.18 1\n",
      "1999.10.09 1999.07.22 1999.09.19 1\n",
      "1999.10.10 1999.07.23 1999.09.20 1\n",
      "1999.10.11 1999.07.24 1999.09.21 1\n",
      "1999.10.12 1999.07.25 1999.09.22 1\n",
      "1999.10.13 1999.07.26 1999.09.23 1\n",
      "1999.10.14 1999.07.27 1999.09.24 1\n",
      "1999.10.15 1999.07.28 1999.09.25 1\n",
      "1999.10.16 1999.07.29 1999.09.26 1\n",
      "1999.10.17 1999.07.30 1999.09.27 1\n",
      "1999.10.18 1999.07.31 1999.09.28 1\n",
      "1999.10.19 1999.08.01 1999.09.29 1\n",
      "1999.10.20 1999.08.02 1999.09.30 1\n",
      "1999.10.21 1999.08.03 1999.10.01 1\n",
      "1999.10.22 1999.08.04 1999.10.02 1\n",
      "1999.10.23 1999.08.05 1999.10.03 1\n",
      "1999.10.24 1999.08.06 1999.10.04 1\n",
      "1999.10.25 1999.08.07 1999.10.05 1\n",
      "1999.10.26 1999.08.08 1999.10.06 1\n",
      "1999.10.27 1999.08.09 1999.10.07 1\n",
      "1999.10.28 1999.08.10 1999.10.08 1\n",
      "1999.10.29 1999.08.11 1999.10.09 1\n",
      "1999.10.30 1999.08.12 1999.10.10 1\n",
      "1999.10.31 1999.08.13 1999.10.11 1\n",
      "1999.11.01 1999.08.14 1999.10.12 1\n",
      "1999.11.02 1999.08.15 1999.10.13 1\n",
      "1999.11.03 1999.08.16 1999.10.14 1\n",
      "1999.11.04 1999.08.17 1999.10.15 1\n",
      "1999.11.05 1999.08.18 1999.10.16 1\n",
      "1999.11.06 1999.08.19 1999.10.17 1\n",
      "1999.11.07 1999.08.20 1999.10.18 1\n",
      "1999.11.08 1999.08.21 1999.10.19 1\n",
      "1999.11.09 1999.08.22 1999.10.20 1\n",
      "1999.11.10 1999.08.23 1999.10.21 1\n",
      "1999.11.11 1999.08.24 1999.10.22 1\n",
      "1999.11.12 1999.08.25 1999.10.23 1\n",
      "1999.11.13 1999.08.26 1999.10.24 1\n",
      "1999.11.14 1999.08.27 1999.10.25 1\n",
      "1999.11.15 1999.08.28 1999.10.26 1\n",
      "1999.11.16 1999.08.29 1999.10.27 1\n",
      "1999.11.17 1999.08.30 1999.10.28 1\n",
      "1999.11.18 1999.08.31 1999.10.29 1\n",
      "1999.11.19 1999.09.01 1999.10.30 1\n",
      "1999.11.20 1999.09.02 1999.10.31 1\n",
      "1999.11.21 1999.09.03 1999.11.01 1\n",
      "1999.11.22 1999.09.04 1999.11.02 1\n",
      "1999.11.23 1999.09.05 1999.11.03 1\n",
      "1999.11.24 1999.09.06 1999.11.04 1\n",
      "1999.11.25 1999.09.07 1999.11.05 1\n",
      "1999.11.26 1999.09.08 1999.11.06 1\n",
      "1999.11.27 1999.09.09 1999.11.07 1\n",
      "1999.11.28 1999.09.10 1999.11.08 1\n",
      "1999.11.29 1999.09.11 1999.11.09 1\n",
      "1999.11.30 1999.09.12 1999.11.10 1\n",
      "1999.12.01 1999.09.13 1999.11.11 1\n",
      "1999.12.02 1999.09.14 1999.11.12 1\n",
      "1999.12.03 1999.09.15 1999.11.13 1\n",
      "1999.12.04 1999.09.16 1999.11.14 1\n",
      "1999.12.05 1999.09.17 1999.11.15 1\n",
      "1999.12.06 1999.09.18 1999.11.16 1\n",
      "1999.12.07 1999.09.19 1999.11.17 1\n",
      "1999.12.08 1999.09.20 1999.11.18 1\n",
      "1999.12.09 1999.09.21 1999.11.19 1\n",
      "1999.12.10 1999.09.22 1999.11.20 1\n",
      "1999.12.11 1999.09.23 1999.11.21 1\n",
      "1999.12.12 1999.09.24 1999.11.22 1\n",
      "1999.12.13 1999.09.25 1999.11.23 1\n",
      "1999.12.14 1999.09.26 1999.11.24 1\n",
      "1999.12.15 1999.09.27 1999.11.25 1\n",
      "1999.12.16 1999.09.28 1999.11.26 1\n",
      "1999.12.17 1999.09.29 1999.11.27 1\n",
      "1999.12.18 1999.09.30 1999.11.28 1\n",
      "1999.12.19 1999.10.01 1999.11.29 1\n",
      "1999.12.20 1999.10.02 1999.11.30 1\n",
      "1999.12.21 1999.10.03 1999.12.01 1\n",
      "1999.12.22 1999.10.04 1999.12.02 1\n",
      "1999.12.23 1999.10.05 1999.12.03 1\n",
      "1999.12.24 1999.10.06 1999.12.04 1\n",
      "1999.12.25 1999.10.07 1999.12.05 1\n",
      "1999.12.26 1999.10.08 1999.12.06 1\n",
      "1999.12.27 1999.10.09 1999.12.07 1\n",
      "1999.12.28 1999.10.10 1999.12.08 1\n",
      "2000.10.04 2000.07.17 2000.09.14 0\n",
      "2000.10.05 2000.07.18 2000.09.15 1\n",
      "2000.10.06 2000.07.19 2000.09.16 1\n",
      "2000.10.07 2000.07.20 2000.09.17 1\n",
      "2000.10.08 2000.07.21 2000.09.18 1\n",
      "2000.10.09 2000.07.22 2000.09.19 1\n",
      "2000.10.10 2000.07.23 2000.09.20 1\n",
      "2000.10.11 2000.07.24 2000.09.21 1\n",
      "2000.10.12 2000.07.25 2000.09.22 1\n",
      "2000.10.13 2000.07.26 2000.09.23 1\n",
      "2000.10.14 2000.07.27 2000.09.24 1\n",
      "2000.10.15 2000.07.28 2000.09.25 1\n",
      "2000.10.16 2000.07.29 2000.09.26 1\n",
      "2000.10.17 2000.07.30 2000.09.27 1\n",
      "2000.10.18 2000.07.31 2000.09.28 1\n",
      "2000.10.19 2000.08.01 2000.09.29 1\n",
      "2000.10.20 2000.08.02 2000.09.30 1\n",
      "2000.10.21 2000.08.03 2000.10.01 1\n",
      "2000.10.22 2000.08.04 2000.10.02 1\n",
      "2000.10.23 2000.08.05 2000.10.03 1\n",
      "2000.10.24 2000.08.06 2000.10.04 1\n",
      "2000.10.25 2000.08.07 2000.10.05 1\n",
      "2000.10.26 2000.08.08 2000.10.06 1\n",
      "2000.10.27 2000.08.09 2000.10.07 1\n",
      "2000.10.28 2000.08.10 2000.10.08 1\n",
      "2000.10.29 2000.08.11 2000.10.09 1\n",
      "2000.10.30 2000.08.12 2000.10.10 1\n",
      "2000.10.31 2000.08.13 2000.10.11 1\n",
      "2000.11.01 2000.08.14 2000.10.12 1\n",
      "2000.11.02 2000.08.15 2000.10.13 1\n",
      "2000.11.03 2000.08.16 2000.10.14 1\n",
      "2000.11.04 2000.08.17 2000.10.15 1\n",
      "2000.11.05 2000.08.18 2000.10.16 1\n",
      "2000.11.06 2000.08.19 2000.10.17 1\n",
      "2000.11.07 2000.08.20 2000.10.18 1\n",
      "2000.11.08 2000.08.21 2000.10.19 1\n",
      "2000.11.09 2000.08.22 2000.10.20 1\n",
      "2000.11.10 2000.08.23 2000.10.21 1\n",
      "2000.11.11 2000.08.24 2000.10.22 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000.11.12 2000.08.25 2000.10.23 1\n",
      "2000.11.13 2000.08.26 2000.10.24 1\n",
      "2000.11.14 2000.08.27 2000.10.25 1\n",
      "2000.11.15 2000.08.28 2000.10.26 1\n",
      "2000.11.16 2000.08.29 2000.10.27 1\n",
      "2000.11.17 2000.08.30 2000.10.28 1\n",
      "2000.11.18 2000.08.31 2000.10.29 1\n",
      "2000.11.19 2000.09.01 2000.10.30 1\n",
      "2000.11.20 2000.09.02 2000.10.31 1\n",
      "2000.11.21 2000.09.03 2000.11.01 1\n",
      "2000.11.22 2000.09.04 2000.11.02 1\n",
      "2000.11.23 2000.09.05 2000.11.03 1\n",
      "2000.11.24 2000.09.06 2000.11.04 1\n",
      "2000.11.25 2000.09.07 2000.11.05 1\n",
      "2000.11.26 2000.09.08 2000.11.06 1\n",
      "2000.11.27 2000.09.09 2000.11.07 1\n",
      "2000.11.28 2000.09.10 2000.11.08 1\n",
      "2000.11.29 2000.09.11 2000.11.09 1\n",
      "2000.11.30 2000.09.12 2000.11.10 1\n",
      "2000.12.01 2000.09.13 2000.11.11 1\n",
      "2000.12.02 2000.09.14 2000.11.12 1\n",
      "2000.12.03 2000.09.15 2000.11.13 1\n",
      "2000.12.04 2000.09.16 2000.11.14 1\n",
      "2000.12.05 2000.09.17 2000.11.15 1\n",
      "2000.12.06 2000.09.18 2000.11.16 1\n",
      "2000.12.07 2000.09.19 2000.11.17 1\n",
      "2000.12.08 2000.09.20 2000.11.18 1\n",
      "2000.12.09 2000.09.21 2000.11.19 1\n",
      "2000.12.10 2000.09.22 2000.11.20 1\n",
      "2000.12.11 2000.09.23 2000.11.21 1\n",
      "2000.12.12 2000.09.24 2000.11.22 1\n",
      "2000.12.13 2000.09.25 2000.11.23 1\n",
      "2000.12.14 2000.09.26 2000.11.24 1\n",
      "2000.12.15 2000.09.27 2000.11.25 1\n",
      "2000.12.16 2000.09.28 2000.11.26 1\n",
      "2000.12.17 2000.09.29 2000.11.27 1\n",
      "2000.12.18 2000.09.30 2000.11.28 1\n",
      "2000.12.19 2000.10.01 2000.11.29 1\n",
      "2000.12.20 2000.10.02 2000.11.30 1\n",
      "2000.12.21 2000.10.03 2000.12.01 1\n",
      "2000.12.22 2000.10.04 2000.12.02 1\n",
      "2000.12.23 2000.10.05 2000.12.03 1\n",
      "2000.12.24 2000.10.06 2000.12.04 1\n",
      "2000.12.25 2000.10.07 2000.12.05 1\n",
      "2000.12.26 2000.10.08 2000.12.06 1\n",
      "2000.12.27 2000.10.09 2000.12.07 1\n",
      "2000.12.28 2000.10.10 2000.12.08 1\n",
      "2001.10.04 2001.07.17 2001.09.14 0\n",
      "2001.10.05 2001.07.18 2001.09.15 1\n",
      "2001.10.06 2001.07.19 2001.09.16 1\n",
      "2001.10.07 2001.07.20 2001.09.17 1\n",
      "2001.10.08 2001.07.21 2001.09.18 1\n",
      "2001.10.09 2001.07.22 2001.09.19 1\n",
      "2001.10.10 2001.07.23 2001.09.20 1\n",
      "2001.10.11 2001.07.24 2001.09.21 1\n",
      "2001.10.12 2001.07.25 2001.09.22 1\n",
      "2001.10.13 2001.07.26 2001.09.23 1\n",
      "2001.10.14 2001.07.27 2001.09.24 1\n",
      "2001.10.15 2001.07.28 2001.09.25 1\n",
      "2001.10.16 2001.07.29 2001.09.26 1\n",
      "2001.10.17 2001.07.30 2001.09.27 1\n",
      "2001.10.18 2001.07.31 2001.09.28 1\n",
      "2001.10.19 2001.08.01 2001.09.29 1\n",
      "2001.10.20 2001.08.02 2001.09.30 1\n",
      "2001.10.21 2001.08.03 2001.10.01 1\n",
      "2001.10.22 2001.08.04 2001.10.02 1\n",
      "2001.10.23 2001.08.05 2001.10.03 1\n",
      "2001.10.24 2001.08.06 2001.10.04 1\n",
      "2001.10.25 2001.08.07 2001.10.05 1\n",
      "2001.10.26 2001.08.08 2001.10.06 1\n",
      "2001.10.27 2001.08.09 2001.10.07 1\n",
      "2001.10.28 2001.08.10 2001.10.08 1\n",
      "2001.10.29 2001.08.11 2001.10.09 1\n",
      "2001.10.30 2001.08.12 2001.10.10 1\n",
      "2001.10.31 2001.08.13 2001.10.11 1\n",
      "2001.11.01 2001.08.14 2001.10.12 1\n",
      "2001.11.02 2001.08.15 2001.10.13 1\n",
      "2001.11.03 2001.08.16 2001.10.14 1\n",
      "2001.11.04 2001.08.17 2001.10.15 1\n",
      "2001.11.05 2001.08.18 2001.10.16 1\n",
      "2001.11.06 2001.08.19 2001.10.17 1\n",
      "2001.11.07 2001.08.20 2001.10.18 1\n",
      "2001.11.08 2001.08.21 2001.10.19 1\n",
      "2001.11.09 2001.08.22 2001.10.20 1\n",
      "2001.11.10 2001.08.23 2001.10.21 1\n",
      "2001.11.11 2001.08.24 2001.10.22 1\n",
      "2001.11.12 2001.08.25 2001.10.23 1\n",
      "2001.11.13 2001.08.26 2001.10.24 1\n",
      "2001.11.14 2001.08.27 2001.10.25 1\n",
      "2001.11.15 2001.08.28 2001.10.26 1\n",
      "2001.11.16 2001.08.29 2001.10.27 1\n",
      "2001.11.17 2001.08.30 2001.10.28 1\n",
      "2001.11.18 2001.08.31 2001.10.29 1\n",
      "2001.11.19 2001.09.01 2001.10.30 1\n",
      "2001.11.20 2001.09.02 2001.10.31 1\n",
      "2001.11.21 2001.09.03 2001.11.01 1\n",
      "2001.11.22 2001.09.04 2001.11.02 1\n",
      "2001.11.23 2001.09.05 2001.11.03 1\n",
      "2001.11.24 2001.09.06 2001.11.04 1\n",
      "2001.11.25 2001.09.07 2001.11.05 1\n",
      "2001.11.26 2001.09.08 2001.11.06 1\n",
      "2001.11.27 2001.09.09 2001.11.07 1\n",
      "2001.11.28 2001.09.10 2001.11.08 1\n",
      "2001.11.29 2001.09.11 2001.11.09 1\n",
      "2001.11.30 2001.09.12 2001.11.10 1\n",
      "2001.12.01 2001.09.13 2001.11.11 1\n",
      "2001.12.02 2001.09.14 2001.11.12 1\n",
      "2001.12.03 2001.09.15 2001.11.13 1\n",
      "2001.12.04 2001.09.16 2001.11.14 1\n",
      "2001.12.05 2001.09.17 2001.11.15 1\n",
      "2001.12.06 2001.09.18 2001.11.16 1\n",
      "2001.12.07 2001.09.19 2001.11.17 1\n",
      "2001.12.08 2001.09.20 2001.11.18 1\n",
      "2001.12.09 2001.09.21 2001.11.19 1\n",
      "2001.12.10 2001.09.22 2001.11.20 1\n",
      "2001.12.11 2001.09.23 2001.11.21 1\n",
      "2001.12.12 2001.09.24 2001.11.22 1\n",
      "2001.12.13 2001.09.25 2001.11.23 1\n",
      "2001.12.14 2001.09.26 2001.11.24 1\n",
      "2001.12.15 2001.09.27 2001.11.25 1\n",
      "2001.12.16 2001.09.28 2001.11.26 1\n",
      "2001.12.17 2001.09.29 2001.11.27 1\n",
      "2001.12.18 2001.09.30 2001.11.28 1\n",
      "2001.12.19 2001.10.01 2001.11.29 1\n",
      "2001.12.20 2001.10.02 2001.11.30 1\n",
      "2001.12.21 2001.10.03 2001.12.01 1\n",
      "2001.12.22 2001.10.04 2001.12.02 1\n",
      "2001.12.23 2001.10.05 2001.12.03 1\n",
      "2001.12.24 2001.10.06 2001.12.04 1\n",
      "2001.12.25 2001.10.07 2001.12.05 1\n",
      "2001.12.26 2001.10.08 2001.12.06 1\n",
      "2001.12.27 2001.10.09 2001.12.07 1\n",
      "2001.12.28 2001.10.10 2001.12.08 1\n",
      "2002.10.04 2002.07.17 2002.09.14 0\n",
      "2002.10.05 2002.07.18 2002.09.15 1\n",
      "2002.10.06 2002.07.19 2002.09.16 1\n",
      "2002.10.07 2002.07.20 2002.09.17 1\n",
      "2002.10.08 2002.07.21 2002.09.18 1\n",
      "2002.10.09 2002.07.22 2002.09.19 1\n",
      "2002.10.10 2002.07.23 2002.09.20 1\n",
      "2002.10.11 2002.07.24 2002.09.21 1\n",
      "2002.10.12 2002.07.25 2002.09.22 1\n",
      "2002.10.13 2002.07.26 2002.09.23 1\n",
      "2002.10.14 2002.07.27 2002.09.24 1\n",
      "2002.10.15 2002.07.28 2002.09.25 1\n",
      "2002.10.16 2002.07.29 2002.09.26 1\n",
      "2002.10.17 2002.07.30 2002.09.27 1\n",
      "2002.10.18 2002.07.31 2002.09.28 1\n",
      "2002.10.19 2002.08.01 2002.09.29 1\n",
      "2002.10.20 2002.08.02 2002.09.30 1\n",
      "2002.10.21 2002.08.03 2002.10.01 1\n",
      "2002.10.22 2002.08.04 2002.10.02 1\n",
      "2002.10.23 2002.08.05 2002.10.03 1\n",
      "2002.10.24 2002.08.06 2002.10.04 1\n",
      "2002.10.25 2002.08.07 2002.10.05 1\n",
      "2002.10.26 2002.08.08 2002.10.06 1\n",
      "2002.10.27 2002.08.09 2002.10.07 1\n",
      "2002.10.28 2002.08.10 2002.10.08 1\n",
      "2002.10.29 2002.08.11 2002.10.09 1\n",
      "2002.10.30 2002.08.12 2002.10.10 1\n",
      "2002.10.31 2002.08.13 2002.10.11 1\n",
      "2002.11.01 2002.08.14 2002.10.12 1\n",
      "2002.11.02 2002.08.15 2002.10.13 1\n",
      "2002.11.03 2002.08.16 2002.10.14 1\n",
      "2002.11.04 2002.08.17 2002.10.15 1\n",
      "2002.11.05 2002.08.18 2002.10.16 1\n",
      "2002.11.06 2002.08.19 2002.10.17 1\n",
      "2002.11.07 2002.08.20 2002.10.18 1\n",
      "2002.11.08 2002.08.21 2002.10.19 1\n",
      "2002.11.09 2002.08.22 2002.10.20 1\n",
      "2002.11.10 2002.08.23 2002.10.21 1\n",
      "2002.11.11 2002.08.24 2002.10.22 1\n",
      "2002.11.12 2002.08.25 2002.10.23 1\n",
      "2002.11.13 2002.08.26 2002.10.24 1\n",
      "2002.11.14 2002.08.27 2002.10.25 1\n",
      "2002.11.15 2002.08.28 2002.10.26 1\n",
      "2002.11.16 2002.08.29 2002.10.27 1\n",
      "2002.11.17 2002.08.30 2002.10.28 1\n",
      "2002.11.18 2002.08.31 2002.10.29 1\n",
      "2002.11.19 2002.09.01 2002.10.30 1\n",
      "2002.11.20 2002.09.02 2002.10.31 1\n",
      "2002.11.21 2002.09.03 2002.11.01 1\n",
      "2002.11.22 2002.09.04 2002.11.02 1\n",
      "2002.11.23 2002.09.05 2002.11.03 1\n",
      "2002.11.24 2002.09.06 2002.11.04 1\n",
      "2002.11.25 2002.09.07 2002.11.05 1\n",
      "2002.11.26 2002.09.08 2002.11.06 1\n",
      "2002.11.27 2002.09.09 2002.11.07 1\n",
      "2002.11.28 2002.09.10 2002.11.08 1\n",
      "2002.11.29 2002.09.11 2002.11.09 1\n",
      "2002.11.30 2002.09.12 2002.11.10 1\n",
      "2002.12.01 2002.09.13 2002.11.11 1\n",
      "2002.12.02 2002.09.14 2002.11.12 1\n",
      "2002.12.03 2002.09.15 2002.11.13 1\n",
      "2002.12.04 2002.09.16 2002.11.14 1\n",
      "2002.12.05 2002.09.17 2002.11.15 1\n",
      "2002.12.06 2002.09.18 2002.11.16 1\n",
      "2002.12.07 2002.09.19 2002.11.17 1\n",
      "2002.12.08 2002.09.20 2002.11.18 1\n",
      "2002.12.09 2002.09.21 2002.11.19 1\n",
      "2002.12.10 2002.09.22 2002.11.20 1\n",
      "2002.12.11 2002.09.23 2002.11.21 1\n",
      "2002.12.12 2002.09.24 2002.11.22 1\n",
      "2002.12.13 2002.09.25 2002.11.23 1\n",
      "2002.12.14 2002.09.26 2002.11.24 1\n",
      "2002.12.15 2002.09.27 2002.11.25 1\n",
      "2002.12.16 2002.09.28 2002.11.26 1\n",
      "2002.12.17 2002.09.29 2002.11.27 1\n",
      "2002.12.18 2002.09.30 2002.11.28 1\n",
      "2002.12.19 2002.10.01 2002.11.29 1\n",
      "2002.12.20 2002.10.02 2002.11.30 1\n",
      "2002.12.21 2002.10.03 2002.12.01 1\n",
      "2002.12.22 2002.10.04 2002.12.02 1\n",
      "2002.12.23 2002.10.05 2002.12.03 1\n",
      "2002.12.24 2002.10.06 2002.12.04 1\n",
      "2002.12.25 2002.10.07 2002.12.05 1\n",
      "2002.12.26 2002.10.08 2002.12.06 1\n",
      "2002.12.27 2002.10.09 2002.12.07 1\n",
      "2002.12.28 2002.10.10 2002.12.08 1\n",
      "2003.10.04 2003.07.17 2003.09.14 0\n",
      "2003.10.05 2003.07.18 2003.09.15 1\n",
      "2003.10.06 2003.07.19 2003.09.16 1\n",
      "2003.10.07 2003.07.20 2003.09.17 1\n",
      "2003.10.08 2003.07.21 2003.09.18 1\n",
      "2003.10.09 2003.07.22 2003.09.19 1\n",
      "2003.10.10 2003.07.23 2003.09.20 1\n",
      "2003.10.11 2003.07.24 2003.09.21 1\n",
      "2003.10.12 2003.07.25 2003.09.22 1\n",
      "2003.10.13 2003.07.26 2003.09.23 1\n",
      "2003.10.14 2003.07.27 2003.09.24 1\n",
      "2003.10.15 2003.07.28 2003.09.25 1\n",
      "2003.10.16 2003.07.29 2003.09.26 1\n",
      "2003.10.17 2003.07.30 2003.09.27 1\n",
      "2003.10.18 2003.07.31 2003.09.28 1\n",
      "2003.10.19 2003.08.01 2003.09.29 1\n",
      "2003.10.20 2003.08.02 2003.09.30 1\n",
      "2003.10.21 2003.08.03 2003.10.01 1\n",
      "2003.10.22 2003.08.04 2003.10.02 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2003.10.23 2003.08.05 2003.10.03 1\n",
      "2003.10.24 2003.08.06 2003.10.04 1\n",
      "2003.10.25 2003.08.07 2003.10.05 1\n",
      "2003.10.26 2003.08.08 2003.10.06 1\n",
      "2003.10.27 2003.08.09 2003.10.07 1\n",
      "2003.10.28 2003.08.10 2003.10.08 1\n",
      "2003.10.29 2003.08.11 2003.10.09 1\n",
      "2003.10.30 2003.08.12 2003.10.10 1\n",
      "2003.10.31 2003.08.13 2003.10.11 1\n",
      "2003.11.01 2003.08.14 2003.10.12 1\n",
      "2003.11.02 2003.08.15 2003.10.13 1\n",
      "2003.11.03 2003.08.16 2003.10.14 1\n",
      "2003.11.04 2003.08.17 2003.10.15 1\n",
      "2003.11.05 2003.08.18 2003.10.16 1\n",
      "2003.11.06 2003.08.19 2003.10.17 1\n",
      "2003.11.07 2003.08.20 2003.10.18 1\n",
      "2003.11.08 2003.08.21 2003.10.19 1\n",
      "2003.11.09 2003.08.22 2003.10.20 1\n",
      "2003.11.10 2003.08.23 2003.10.21 1\n",
      "2003.11.11 2003.08.24 2003.10.22 1\n",
      "2003.11.12 2003.08.25 2003.10.23 1\n",
      "2003.11.13 2003.08.26 2003.10.24 1\n",
      "2003.11.14 2003.08.27 2003.10.25 1\n",
      "2003.11.15 2003.08.28 2003.10.26 1\n",
      "2003.11.16 2003.08.29 2003.10.27 1\n",
      "2003.11.17 2003.08.30 2003.10.28 1\n",
      "2003.11.18 2003.08.31 2003.10.29 1\n",
      "2003.11.19 2003.09.01 2003.10.30 1\n",
      "2003.11.20 2003.09.02 2003.10.31 1\n",
      "2003.11.21 2003.09.03 2003.11.01 1\n",
      "2003.11.22 2003.09.04 2003.11.02 1\n",
      "2003.11.23 2003.09.05 2003.11.03 1\n",
      "2003.11.24 2003.09.06 2003.11.04 1\n",
      "2003.11.25 2003.09.07 2003.11.05 1\n",
      "2003.11.26 2003.09.08 2003.11.06 1\n",
      "2003.11.27 2003.09.09 2003.11.07 1\n",
      "2003.11.28 2003.09.10 2003.11.08 1\n",
      "2003.11.29 2003.09.11 2003.11.09 1\n",
      "2003.11.30 2003.09.12 2003.11.10 1\n",
      "2003.12.01 2003.09.13 2003.11.11 1\n",
      "2003.12.02 2003.09.14 2003.11.12 1\n",
      "2003.12.03 2003.09.15 2003.11.13 1\n",
      "2003.12.04 2003.09.16 2003.11.14 1\n",
      "2003.12.05 2003.09.17 2003.11.15 1\n",
      "2003.12.06 2003.09.18 2003.11.16 1\n",
      "2003.12.07 2003.09.19 2003.11.17 1\n",
      "2003.12.08 2003.09.20 2003.11.18 1\n",
      "2003.12.09 2003.09.21 2003.11.19 1\n",
      "2003.12.10 2003.09.22 2003.11.20 1\n",
      "2003.12.11 2003.09.23 2003.11.21 1\n",
      "2003.12.12 2003.09.24 2003.11.22 1\n",
      "2003.12.13 2003.09.25 2003.11.23 1\n",
      "2003.12.14 2003.09.26 2003.11.24 1\n",
      "2003.12.15 2003.09.27 2003.11.25 1\n",
      "2003.12.16 2003.09.28 2003.11.26 1\n",
      "2003.12.17 2003.09.29 2003.11.27 1\n",
      "2003.12.18 2003.09.30 2003.11.28 1\n",
      "2003.12.19 2003.10.01 2003.11.29 1\n",
      "2003.12.20 2003.10.02 2003.11.30 1\n",
      "2003.12.21 2003.10.03 2003.12.01 1\n",
      "2003.12.22 2003.10.04 2003.12.02 1\n",
      "2003.12.23 2003.10.05 2003.12.03 1\n",
      "2003.12.24 2003.10.06 2003.12.04 1\n",
      "2003.12.25 2003.10.07 2003.12.05 1\n",
      "2003.12.26 2003.10.08 2003.12.06 1\n",
      "2003.12.27 2003.10.09 2003.12.07 1\n",
      "2003.12.28 2003.10.10 2003.12.08 1\n",
      "2004.10.04 2004.07.17 2004.09.14 0\n",
      "2004.10.05 2004.07.18 2004.09.15 1\n",
      "2004.10.06 2004.07.19 2004.09.16 1\n",
      "2004.10.07 2004.07.20 2004.09.17 1\n",
      "2004.10.08 2004.07.21 2004.09.18 1\n",
      "2004.10.09 2004.07.22 2004.09.19 1\n",
      "2004.10.10 2004.07.23 2004.09.20 1\n",
      "2004.10.11 2004.07.24 2004.09.21 1\n",
      "2004.10.12 2004.07.25 2004.09.22 1\n",
      "2004.10.13 2004.07.26 2004.09.23 1\n",
      "2004.10.14 2004.07.27 2004.09.24 1\n",
      "2004.10.15 2004.07.28 2004.09.25 1\n",
      "2004.10.16 2004.07.29 2004.09.26 1\n",
      "2004.10.17 2004.07.30 2004.09.27 1\n",
      "2004.10.18 2004.07.31 2004.09.28 1\n",
      "2004.10.19 2004.08.01 2004.09.29 1\n",
      "2004.10.20 2004.08.02 2004.09.30 1\n",
      "2004.10.21 2004.08.03 2004.10.01 1\n",
      "2004.10.22 2004.08.04 2004.10.02 1\n",
      "2004.10.23 2004.08.05 2004.10.03 1\n",
      "2004.10.24 2004.08.06 2004.10.04 1\n",
      "2004.10.25 2004.08.07 2004.10.05 1\n",
      "2004.10.26 2004.08.08 2004.10.06 1\n",
      "2004.10.27 2004.08.09 2004.10.07 1\n",
      "2004.10.28 2004.08.10 2004.10.08 1\n",
      "2004.10.29 2004.08.11 2004.10.09 1\n",
      "2004.10.30 2004.08.12 2004.10.10 1\n",
      "2004.10.31 2004.08.13 2004.10.11 1\n",
      "2004.11.01 2004.08.14 2004.10.12 1\n",
      "2004.11.02 2004.08.15 2004.10.13 1\n",
      "2004.11.03 2004.08.16 2004.10.14 1\n",
      "2004.11.04 2004.08.17 2004.10.15 1\n",
      "2004.11.05 2004.08.18 2004.10.16 1\n",
      "2004.11.06 2004.08.19 2004.10.17 1\n",
      "2004.11.07 2004.08.20 2004.10.18 1\n",
      "2004.11.08 2004.08.21 2004.10.19 1\n",
      "2004.11.09 2004.08.22 2004.10.20 1\n",
      "2004.11.10 2004.08.23 2004.10.21 1\n",
      "2004.11.11 2004.08.24 2004.10.22 1\n",
      "2004.11.12 2004.08.25 2004.10.23 1\n",
      "2004.11.13 2004.08.26 2004.10.24 1\n",
      "2004.11.14 2004.08.27 2004.10.25 1\n",
      "2004.11.15 2004.08.28 2004.10.26 1\n",
      "2004.11.16 2004.08.29 2004.10.27 1\n",
      "2004.11.17 2004.08.30 2004.10.28 1\n",
      "2004.11.18 2004.08.31 2004.10.29 1\n",
      "2004.11.19 2004.09.01 2004.10.30 1\n",
      "2004.11.20 2004.09.02 2004.10.31 1\n",
      "2004.11.21 2004.09.03 2004.11.01 1\n",
      "2004.11.22 2004.09.04 2004.11.02 1\n",
      "2004.11.23 2004.09.05 2004.11.03 1\n",
      "2004.11.24 2004.09.06 2004.11.04 1\n",
      "2004.11.25 2004.09.07 2004.11.05 1\n",
      "2004.11.26 2004.09.08 2004.11.06 1\n",
      "2004.11.27 2004.09.09 2004.11.07 1\n",
      "2004.11.28 2004.09.10 2004.11.08 1\n",
      "2004.11.29 2004.09.11 2004.11.09 1\n",
      "2004.11.30 2004.09.12 2004.11.10 1\n",
      "2004.12.01 2004.09.13 2004.11.11 1\n",
      "2004.12.02 2004.09.14 2004.11.12 1\n",
      "2004.12.03 2004.09.15 2004.11.13 1\n",
      "2004.12.04 2004.09.16 2004.11.14 1\n",
      "2004.12.05 2004.09.17 2004.11.15 1\n",
      "2004.12.06 2004.09.18 2004.11.16 1\n",
      "2004.12.07 2004.09.19 2004.11.17 1\n",
      "2004.12.08 2004.09.20 2004.11.18 1\n",
      "2004.12.09 2004.09.21 2004.11.19 1\n",
      "2004.12.10 2004.09.22 2004.11.20 1\n",
      "2004.12.11 2004.09.23 2004.11.21 1\n",
      "2004.12.12 2004.09.24 2004.11.22 1\n",
      "2004.12.13 2004.09.25 2004.11.23 1\n",
      "2004.12.14 2004.09.26 2004.11.24 1\n",
      "2004.12.15 2004.09.27 2004.11.25 1\n",
      "2004.12.16 2004.09.28 2004.11.26 1\n",
      "2004.12.17 2004.09.29 2004.11.27 1\n",
      "2004.12.18 2004.09.30 2004.11.28 1\n",
      "2004.12.19 2004.10.01 2004.11.29 1\n",
      "2004.12.20 2004.10.02 2004.11.30 1\n",
      "2004.12.21 2004.10.03 2004.12.01 1\n",
      "2004.12.22 2004.10.04 2004.12.02 1\n",
      "2004.12.23 2004.10.05 2004.12.03 1\n",
      "2004.12.24 2004.10.06 2004.12.04 1\n",
      "2004.12.25 2004.10.07 2004.12.05 1\n",
      "2004.12.26 2004.10.08 2004.12.06 1\n",
      "2004.12.27 2004.10.09 2004.12.07 1\n",
      "2004.12.28 2004.10.10 2004.12.08 1\n",
      "2005.10.04 2005.07.17 2005.09.14 0\n",
      "2005.10.05 2005.07.18 2005.09.15 1\n",
      "2005.10.06 2005.07.19 2005.09.16 1\n",
      "2005.10.07 2005.07.20 2005.09.17 1\n",
      "2005.10.08 2005.07.21 2005.09.18 1\n",
      "2005.10.09 2005.07.22 2005.09.19 1\n",
      "2005.10.10 2005.07.23 2005.09.20 1\n",
      "2005.10.11 2005.07.24 2005.09.21 1\n",
      "2005.10.12 2005.07.25 2005.09.22 1\n",
      "2005.10.13 2005.07.26 2005.09.23 1\n",
      "2005.10.14 2005.07.27 2005.09.24 1\n",
      "2005.10.15 2005.07.28 2005.09.25 1\n",
      "2005.10.16 2005.07.29 2005.09.26 1\n",
      "2005.10.17 2005.07.30 2005.09.27 1\n",
      "2005.10.18 2005.07.31 2005.09.28 1\n",
      "2005.10.19 2005.08.01 2005.09.29 1\n",
      "2005.10.20 2005.08.02 2005.09.30 1\n",
      "2005.10.21 2005.08.03 2005.10.01 1\n",
      "2005.10.22 2005.08.04 2005.10.02 1\n",
      "2005.10.23 2005.08.05 2005.10.03 1\n",
      "2005.10.24 2005.08.06 2005.10.04 1\n",
      "2005.10.25 2005.08.07 2005.10.05 1\n",
      "2005.10.26 2005.08.08 2005.10.06 1\n",
      "2005.10.27 2005.08.09 2005.10.07 1\n",
      "2005.10.28 2005.08.10 2005.10.08 1\n",
      "2005.10.29 2005.08.11 2005.10.09 1\n",
      "2005.10.30 2005.08.12 2005.10.10 1\n",
      "2005.10.31 2005.08.13 2005.10.11 1\n",
      "2005.11.01 2005.08.14 2005.10.12 1\n",
      "2005.11.02 2005.08.15 2005.10.13 1\n",
      "2005.11.03 2005.08.16 2005.10.14 1\n",
      "2005.11.04 2005.08.17 2005.10.15 1\n",
      "2005.11.05 2005.08.18 2005.10.16 1\n",
      "2005.11.06 2005.08.19 2005.10.17 1\n",
      "2005.11.07 2005.08.20 2005.10.18 1\n",
      "2005.11.08 2005.08.21 2005.10.19 1\n",
      "2005.11.09 2005.08.22 2005.10.20 1\n",
      "2005.11.10 2005.08.23 2005.10.21 1\n",
      "2005.11.11 2005.08.24 2005.10.22 1\n",
      "2005.11.12 2005.08.25 2005.10.23 1\n",
      "2005.11.13 2005.08.26 2005.10.24 1\n",
      "2005.11.14 2005.08.27 2005.10.25 1\n",
      "2005.11.15 2005.08.28 2005.10.26 1\n",
      "2005.11.16 2005.08.29 2005.10.27 1\n",
      "2005.11.17 2005.08.30 2005.10.28 1\n",
      "2005.11.18 2005.08.31 2005.10.29 1\n",
      "2005.11.19 2005.09.01 2005.10.30 1\n",
      "2005.11.20 2005.09.02 2005.10.31 1\n",
      "2005.11.21 2005.09.03 2005.11.01 1\n",
      "2005.11.22 2005.09.04 2005.11.02 1\n",
      "2005.11.23 2005.09.05 2005.11.03 1\n",
      "2005.11.24 2005.09.06 2005.11.04 1\n",
      "2005.11.25 2005.09.07 2005.11.05 1\n",
      "2005.11.26 2005.09.08 2005.11.06 1\n",
      "2005.11.27 2005.09.09 2005.11.07 1\n",
      "2005.11.28 2005.09.10 2005.11.08 1\n",
      "2005.11.29 2005.09.11 2005.11.09 1\n",
      "2005.11.30 2005.09.12 2005.11.10 1\n",
      "2005.12.01 2005.09.13 2005.11.11 1\n",
      "2005.12.02 2005.09.14 2005.11.12 1\n",
      "2005.12.03 2005.09.15 2005.11.13 1\n",
      "2005.12.04 2005.09.16 2005.11.14 1\n",
      "2005.12.05 2005.09.17 2005.11.15 1\n",
      "2005.12.06 2005.09.18 2005.11.16 1\n",
      "2005.12.07 2005.09.19 2005.11.17 1\n",
      "2005.12.08 2005.09.20 2005.11.18 1\n",
      "2005.12.09 2005.09.21 2005.11.19 1\n",
      "2005.12.10 2005.09.22 2005.11.20 1\n",
      "2005.12.11 2005.09.23 2005.11.21 1\n",
      "2005.12.12 2005.09.24 2005.11.22 1\n",
      "2005.12.13 2005.09.25 2005.11.23 1\n",
      "2005.12.14 2005.09.26 2005.11.24 1\n",
      "2005.12.15 2005.09.27 2005.11.25 1\n",
      "2005.12.16 2005.09.28 2005.11.26 1\n",
      "2005.12.17 2005.09.29 2005.11.27 1\n",
      "2005.12.18 2005.09.30 2005.11.28 1\n",
      "2005.12.19 2005.10.01 2005.11.29 1\n",
      "2005.12.20 2005.10.02 2005.11.30 1\n",
      "2005.12.21 2005.10.03 2005.12.01 1\n",
      "2005.12.22 2005.10.04 2005.12.02 1\n",
      "2005.12.23 2005.10.05 2005.12.03 1\n",
      "2005.12.24 2005.10.06 2005.12.04 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2005.12.25 2005.10.07 2005.12.05 1\n",
      "2005.12.26 2005.10.08 2005.12.06 1\n",
      "2005.12.27 2005.10.09 2005.12.07 1\n",
      "2005.12.28 2005.10.10 2005.12.08 1\n",
      "2006.10.04 2006.07.17 2006.09.14 0\n",
      "2006.10.05 2006.07.18 2006.09.15 1\n",
      "2006.10.06 2006.07.19 2006.09.16 1\n",
      "2006.10.07 2006.07.20 2006.09.17 1\n",
      "2006.10.08 2006.07.21 2006.09.18 1\n",
      "2006.10.09 2006.07.22 2006.09.19 1\n",
      "2006.10.10 2006.07.23 2006.09.20 1\n",
      "2006.10.11 2006.07.24 2006.09.21 1\n",
      "2006.10.12 2006.07.25 2006.09.22 1\n",
      "2006.10.13 2006.07.26 2006.09.23 1\n",
      "2006.10.14 2006.07.27 2006.09.24 1\n",
      "2006.10.15 2006.07.28 2006.09.25 1\n",
      "2006.10.16 2006.07.29 2006.09.26 1\n",
      "2006.10.17 2006.07.30 2006.09.27 1\n",
      "2006.10.18 2006.07.31 2006.09.28 1\n",
      "2006.10.19 2006.08.01 2006.09.29 1\n",
      "2006.10.20 2006.08.02 2006.09.30 1\n",
      "2006.10.21 2006.08.03 2006.10.01 1\n",
      "2006.10.22 2006.08.04 2006.10.02 1\n",
      "2006.10.23 2006.08.05 2006.10.03 1\n",
      "2006.10.24 2006.08.06 2006.10.04 1\n",
      "2006.10.25 2006.08.07 2006.10.05 1\n",
      "2006.10.26 2006.08.08 2006.10.06 1\n",
      "2006.10.27 2006.08.09 2006.10.07 1\n",
      "2006.10.28 2006.08.10 2006.10.08 1\n",
      "2006.10.29 2006.08.11 2006.10.09 1\n",
      "2006.10.30 2006.08.12 2006.10.10 1\n",
      "2006.10.31 2006.08.13 2006.10.11 1\n",
      "2006.11.01 2006.08.14 2006.10.12 1\n",
      "2006.11.02 2006.08.15 2006.10.13 1\n",
      "2006.11.03 2006.08.16 2006.10.14 1\n",
      "2006.11.04 2006.08.17 2006.10.15 1\n",
      "2006.11.05 2006.08.18 2006.10.16 1\n",
      "2006.11.06 2006.08.19 2006.10.17 1\n",
      "2006.11.07 2006.08.20 2006.10.18 1\n",
      "2006.11.08 2006.08.21 2006.10.19 1\n",
      "2006.11.09 2006.08.22 2006.10.20 1\n",
      "2006.11.10 2006.08.23 2006.10.21 1\n",
      "2006.11.11 2006.08.24 2006.10.22 1\n",
      "2006.11.12 2006.08.25 2006.10.23 1\n",
      "2006.11.13 2006.08.26 2006.10.24 1\n",
      "2006.11.14 2006.08.27 2006.10.25 1\n",
      "2006.11.15 2006.08.28 2006.10.26 1\n",
      "2006.11.16 2006.08.29 2006.10.27 1\n",
      "2006.11.17 2006.08.30 2006.10.28 1\n",
      "2006.11.18 2006.08.31 2006.10.29 1\n",
      "2006.11.19 2006.09.01 2006.10.30 1\n",
      "2006.11.20 2006.09.02 2006.10.31 1\n",
      "2006.11.21 2006.09.03 2006.11.01 1\n",
      "2006.11.22 2006.09.04 2006.11.02 1\n",
      "2006.11.23 2006.09.05 2006.11.03 1\n",
      "2006.11.24 2006.09.06 2006.11.04 1\n",
      "2006.11.25 2006.09.07 2006.11.05 1\n",
      "2006.11.26 2006.09.08 2006.11.06 1\n",
      "2006.11.27 2006.09.09 2006.11.07 1\n",
      "2006.11.28 2006.09.10 2006.11.08 1\n",
      "2006.11.29 2006.09.11 2006.11.09 1\n",
      "2006.11.30 2006.09.12 2006.11.10 1\n",
      "2006.12.01 2006.09.13 2006.11.11 1\n",
      "2006.12.02 2006.09.14 2006.11.12 1\n",
      "2006.12.03 2006.09.15 2006.11.13 1\n",
      "2006.12.04 2006.09.16 2006.11.14 1\n",
      "2006.12.05 2006.09.17 2006.11.15 1\n",
      "2006.12.06 2006.09.18 2006.11.16 1\n",
      "2006.12.07 2006.09.19 2006.11.17 1\n",
      "2006.12.08 2006.09.20 2006.11.18 1\n",
      "2006.12.09 2006.09.21 2006.11.19 1\n",
      "2006.12.10 2006.09.22 2006.11.20 1\n",
      "2006.12.11 2006.09.23 2006.11.21 1\n",
      "2006.12.12 2006.09.24 2006.11.22 1\n",
      "2006.12.13 2006.09.25 2006.11.23 1\n",
      "2006.12.14 2006.09.26 2006.11.24 1\n",
      "2006.12.15 2006.09.27 2006.11.25 1\n",
      "2006.12.16 2006.09.28 2006.11.26 1\n",
      "2006.12.17 2006.09.29 2006.11.27 1\n",
      "2006.12.18 2006.09.30 2006.11.28 1\n",
      "2006.12.19 2006.10.01 2006.11.29 1\n",
      "2006.12.20 2006.10.02 2006.11.30 1\n",
      "2006.12.21 2006.10.03 2006.12.01 1\n",
      "2006.12.22 2006.10.04 2006.12.02 1\n",
      "2006.12.23 2006.10.05 2006.12.03 1\n",
      "2006.12.24 2006.10.06 2006.12.04 1\n",
      "2006.12.25 2006.10.07 2006.12.05 1\n",
      "2006.12.26 2006.10.08 2006.12.06 1\n",
      "2006.12.27 2006.10.09 2006.12.07 1\n",
      "2006.12.28 2006.10.10 2006.12.08 1\n",
      "2007.10.04 2007.07.17 2007.09.14 0\n",
      "2007.10.05 2007.07.18 2007.09.15 1\n",
      "2007.10.06 2007.07.19 2007.09.16 1\n",
      "2007.10.07 2007.07.20 2007.09.17 1\n",
      "2007.10.08 2007.07.21 2007.09.18 1\n",
      "2007.10.09 2007.07.22 2007.09.19 1\n",
      "2007.10.10 2007.07.23 2007.09.20 1\n",
      "2007.10.11 2007.07.24 2007.09.21 1\n",
      "2007.10.12 2007.07.25 2007.09.22 1\n",
      "2007.10.13 2007.07.26 2007.09.23 1\n",
      "2007.10.14 2007.07.27 2007.09.24 1\n",
      "2007.10.15 2007.07.28 2007.09.25 1\n",
      "2007.10.16 2007.07.29 2007.09.26 1\n",
      "2007.10.17 2007.07.30 2007.09.27 1\n",
      "2007.10.18 2007.07.31 2007.09.28 1\n",
      "2007.10.19 2007.08.01 2007.09.29 1\n",
      "2007.10.20 2007.08.02 2007.09.30 1\n",
      "2007.10.21 2007.08.03 2007.10.01 1\n",
      "2007.10.22 2007.08.04 2007.10.02 1\n",
      "2007.10.23 2007.08.05 2007.10.03 1\n",
      "2007.10.24 2007.08.06 2007.10.04 1\n",
      "2007.10.25 2007.08.07 2007.10.05 1\n",
      "2007.10.26 2007.08.08 2007.10.06 1\n",
      "2007.10.27 2007.08.09 2007.10.07 1\n",
      "2007.10.28 2007.08.10 2007.10.08 1\n",
      "2007.10.29 2007.08.11 2007.10.09 1\n",
      "2007.10.30 2007.08.12 2007.10.10 1\n",
      "2007.10.31 2007.08.13 2007.10.11 1\n",
      "2007.11.01 2007.08.14 2007.10.12 1\n",
      "2007.11.02 2007.08.15 2007.10.13 1\n",
      "2007.11.03 2007.08.16 2007.10.14 1\n",
      "2007.11.04 2007.08.17 2007.10.15 1\n",
      "2007.11.05 2007.08.18 2007.10.16 1\n",
      "2007.11.06 2007.08.19 2007.10.17 1\n",
      "2007.11.07 2007.08.20 2007.10.18 1\n",
      "2007.11.08 2007.08.21 2007.10.19 1\n",
      "2007.11.09 2007.08.22 2007.10.20 1\n",
      "2007.11.10 2007.08.23 2007.10.21 1\n",
      "2007.11.11 2007.08.24 2007.10.22 1\n",
      "2007.11.12 2007.08.25 2007.10.23 1\n",
      "2007.11.13 2007.08.26 2007.10.24 1\n",
      "2007.11.14 2007.08.27 2007.10.25 1\n",
      "2007.11.15 2007.08.28 2007.10.26 1\n",
      "2007.11.16 2007.08.29 2007.10.27 1\n",
      "2007.11.17 2007.08.30 2007.10.28 1\n",
      "2007.11.18 2007.08.31 2007.10.29 1\n",
      "2007.11.19 2007.09.01 2007.10.30 1\n",
      "2007.11.20 2007.09.02 2007.10.31 1\n",
      "2007.11.21 2007.09.03 2007.11.01 1\n",
      "2007.11.22 2007.09.04 2007.11.02 1\n",
      "2007.11.23 2007.09.05 2007.11.03 1\n",
      "2007.11.24 2007.09.06 2007.11.04 1\n",
      "2007.11.25 2007.09.07 2007.11.05 1\n",
      "2007.11.26 2007.09.08 2007.11.06 1\n",
      "2007.11.27 2007.09.09 2007.11.07 1\n",
      "2007.11.28 2007.09.10 2007.11.08 1\n",
      "2007.11.29 2007.09.11 2007.11.09 1\n",
      "2007.11.30 2007.09.12 2007.11.10 1\n",
      "2007.12.01 2007.09.13 2007.11.11 1\n",
      "2007.12.02 2007.09.14 2007.11.12 1\n",
      "2007.12.03 2007.09.15 2007.11.13 1\n",
      "2007.12.04 2007.09.16 2007.11.14 1\n",
      "2007.12.05 2007.09.17 2007.11.15 1\n",
      "2007.12.06 2007.09.18 2007.11.16 1\n",
      "2007.12.07 2007.09.19 2007.11.17 1\n",
      "2007.12.08 2007.09.20 2007.11.18 1\n",
      "2007.12.09 2007.09.21 2007.11.19 1\n",
      "2007.12.10 2007.09.22 2007.11.20 1\n",
      "2007.12.11 2007.09.23 2007.11.21 1\n",
      "2007.12.12 2007.09.24 2007.11.22 1\n",
      "2007.12.13 2007.09.25 2007.11.23 1\n",
      "2007.12.14 2007.09.26 2007.11.24 1\n",
      "2007.12.15 2007.09.27 2007.11.25 1\n",
      "2007.12.16 2007.09.28 2007.11.26 1\n",
      "2007.12.17 2007.09.29 2007.11.27 1\n",
      "2007.12.18 2007.09.30 2007.11.28 1\n",
      "2007.12.19 2007.10.01 2007.11.29 1\n",
      "2007.12.20 2007.10.02 2007.11.30 1\n",
      "2007.12.21 2007.10.03 2007.12.01 1\n",
      "2007.12.22 2007.10.04 2007.12.02 1\n",
      "2007.12.23 2007.10.05 2007.12.03 1\n",
      "2007.12.24 2007.10.06 2007.12.04 1\n",
      "2007.12.25 2007.10.07 2007.12.05 1\n",
      "2007.12.26 2007.10.08 2007.12.06 1\n",
      "2007.12.27 2007.10.09 2007.12.07 1\n",
      "2007.12.28 2007.10.10 2007.12.08 1\n",
      "2008.10.04 2008.07.17 2008.09.14 0\n",
      "2008.10.05 2008.07.18 2008.09.15 1\n",
      "2008.10.06 2008.07.19 2008.09.16 1\n",
      "2008.10.07 2008.07.20 2008.09.17 1\n",
      "2008.10.08 2008.07.21 2008.09.18 1\n",
      "2008.10.09 2008.07.22 2008.09.19 1\n",
      "2008.10.10 2008.07.23 2008.09.20 1\n",
      "2008.10.11 2008.07.24 2008.09.21 1\n",
      "2008.10.12 2008.07.25 2008.09.22 1\n",
      "2008.10.13 2008.07.26 2008.09.23 1\n",
      "2008.10.14 2008.07.27 2008.09.24 1\n",
      "2008.10.15 2008.07.28 2008.09.25 1\n",
      "2008.10.16 2008.07.29 2008.09.26 1\n",
      "2008.10.17 2008.07.30 2008.09.27 1\n",
      "2008.10.18 2008.07.31 2008.09.28 1\n",
      "2008.10.19 2008.08.01 2008.09.29 1\n",
      "2008.10.20 2008.08.02 2008.09.30 1\n",
      "2008.10.21 2008.08.03 2008.10.01 1\n",
      "2008.10.22 2008.08.04 2008.10.02 1\n",
      "2008.10.23 2008.08.05 2008.10.03 1\n",
      "2008.10.24 2008.08.06 2008.10.04 1\n",
      "2008.10.25 2008.08.07 2008.10.05 1\n",
      "2008.10.26 2008.08.08 2008.10.06 1\n",
      "2008.10.27 2008.08.09 2008.10.07 1\n",
      "2008.10.28 2008.08.10 2008.10.08 1\n",
      "2008.10.29 2008.08.11 2008.10.09 1\n",
      "2008.10.30 2008.08.12 2008.10.10 1\n",
      "2008.10.31 2008.08.13 2008.10.11 1\n",
      "2008.11.01 2008.08.14 2008.10.12 1\n",
      "2008.11.02 2008.08.15 2008.10.13 1\n",
      "2008.11.03 2008.08.16 2008.10.14 1\n",
      "2008.11.04 2008.08.17 2008.10.15 1\n",
      "2008.11.05 2008.08.18 2008.10.16 1\n",
      "2008.11.06 2008.08.19 2008.10.17 1\n",
      "2008.11.07 2008.08.20 2008.10.18 1\n",
      "2008.11.08 2008.08.21 2008.10.19 1\n",
      "2008.11.09 2008.08.22 2008.10.20 1\n",
      "2008.11.10 2008.08.23 2008.10.21 1\n",
      "2008.11.11 2008.08.24 2008.10.22 1\n",
      "2008.11.12 2008.08.25 2008.10.23 1\n",
      "2008.11.13 2008.08.26 2008.10.24 1\n",
      "2008.11.14 2008.08.27 2008.10.25 1\n",
      "2008.11.15 2008.08.28 2008.10.26 1\n",
      "2008.11.16 2008.08.29 2008.10.27 1\n",
      "2008.11.17 2008.08.30 2008.10.28 1\n",
      "2008.11.18 2008.08.31 2008.10.29 1\n",
      "2008.11.19 2008.09.01 2008.10.30 1\n",
      "2008.11.20 2008.09.02 2008.10.31 1\n",
      "2008.11.21 2008.09.03 2008.11.01 1\n",
      "2008.11.22 2008.09.04 2008.11.02 1\n",
      "2008.11.23 2008.09.05 2008.11.03 1\n",
      "2008.11.24 2008.09.06 2008.11.04 1\n",
      "2008.11.25 2008.09.07 2008.11.05 1\n",
      "2008.11.26 2008.09.08 2008.11.06 1\n",
      "2008.11.27 2008.09.09 2008.11.07 1\n",
      "2008.11.28 2008.09.10 2008.11.08 1\n",
      "2008.11.29 2008.09.11 2008.11.09 1\n",
      "2008.11.30 2008.09.12 2008.11.10 1\n",
      "2008.12.01 2008.09.13 2008.11.11 1\n",
      "2008.12.02 2008.09.14 2008.11.12 1\n",
      "2008.12.03 2008.09.15 2008.11.13 1\n",
      "2008.12.04 2008.09.16 2008.11.14 1\n",
      "2008.12.05 2008.09.17 2008.11.15 1\n",
      "2008.12.06 2008.09.18 2008.11.16 1\n",
      "2008.12.07 2008.09.19 2008.11.17 1\n",
      "2008.12.08 2008.09.20 2008.11.18 1\n",
      "2008.12.09 2008.09.21 2008.11.19 1\n",
      "2008.12.10 2008.09.22 2008.11.20 1\n",
      "2008.12.11 2008.09.23 2008.11.21 1\n",
      "2008.12.12 2008.09.24 2008.11.22 1\n",
      "2008.12.13 2008.09.25 2008.11.23 1\n",
      "2008.12.14 2008.09.26 2008.11.24 1\n",
      "2008.12.15 2008.09.27 2008.11.25 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2008.12.16 2008.09.28 2008.11.26 1\n",
      "2008.12.17 2008.09.29 2008.11.27 1\n",
      "2008.12.18 2008.09.30 2008.11.28 1\n",
      "2008.12.19 2008.10.01 2008.11.29 1\n",
      "2008.12.20 2008.10.02 2008.11.30 1\n",
      "2008.12.21 2008.10.03 2008.12.01 1\n",
      "2008.12.22 2008.10.04 2008.12.02 1\n",
      "2008.12.23 2008.10.05 2008.12.03 1\n",
      "2008.12.24 2008.10.06 2008.12.04 1\n",
      "2008.12.25 2008.10.07 2008.12.05 1\n",
      "2008.12.26 2008.10.08 2008.12.06 1\n",
      "2008.12.27 2008.10.09 2008.12.07 1\n",
      "2008.12.28 2008.10.10 2008.12.08 1\n",
      "2009.10.04 2009.07.17 2009.09.14 0\n",
      "2009.10.05 2009.07.18 2009.09.15 1\n",
      "2009.10.06 2009.07.19 2009.09.16 1\n",
      "2009.10.07 2009.07.20 2009.09.17 1\n",
      "2009.10.08 2009.07.21 2009.09.18 1\n",
      "2009.10.09 2009.07.22 2009.09.19 1\n",
      "2009.10.10 2009.07.23 2009.09.20 1\n",
      "2009.10.11 2009.07.24 2009.09.21 1\n",
      "2009.10.12 2009.07.25 2009.09.22 1\n",
      "2009.10.13 2009.07.26 2009.09.23 1\n",
      "2009.10.14 2009.07.27 2009.09.24 1\n",
      "2009.10.15 2009.07.28 2009.09.25 1\n",
      "2009.10.16 2009.07.29 2009.09.26 1\n",
      "2009.10.17 2009.07.30 2009.09.27 1\n",
      "2009.10.18 2009.07.31 2009.09.28 1\n",
      "2009.10.19 2009.08.01 2009.09.29 1\n",
      "2009.10.20 2009.08.02 2009.09.30 1\n",
      "2009.10.21 2009.08.03 2009.10.01 1\n",
      "2009.10.22 2009.08.04 2009.10.02 1\n",
      "2009.10.23 2009.08.05 2009.10.03 1\n",
      "2009.10.24 2009.08.06 2009.10.04 1\n",
      "2009.10.25 2009.08.07 2009.10.05 1\n",
      "2009.10.26 2009.08.08 2009.10.06 1\n",
      "2009.10.27 2009.08.09 2009.10.07 1\n",
      "2009.10.28 2009.08.10 2009.10.08 1\n",
      "2009.10.29 2009.08.11 2009.10.09 1\n",
      "2009.10.30 2009.08.12 2009.10.10 1\n",
      "2009.10.31 2009.08.13 2009.10.11 1\n",
      "2009.11.01 2009.08.14 2009.10.12 1\n",
      "2009.11.02 2009.08.15 2009.10.13 1\n",
      "2009.11.03 2009.08.16 2009.10.14 1\n",
      "2009.11.04 2009.08.17 2009.10.15 1\n",
      "2009.11.05 2009.08.18 2009.10.16 1\n",
      "2009.11.06 2009.08.19 2009.10.17 1\n",
      "2009.11.07 2009.08.20 2009.10.18 1\n",
      "2009.11.08 2009.08.21 2009.10.19 1\n",
      "2009.11.09 2009.08.22 2009.10.20 1\n",
      "2009.11.10 2009.08.23 2009.10.21 1\n",
      "2009.11.11 2009.08.24 2009.10.22 1\n",
      "2009.11.12 2009.08.25 2009.10.23 1\n",
      "2009.11.13 2009.08.26 2009.10.24 1\n",
      "2009.11.14 2009.08.27 2009.10.25 1\n",
      "2009.11.15 2009.08.28 2009.10.26 1\n",
      "2009.11.16 2009.08.29 2009.10.27 1\n",
      "2009.11.17 2009.08.30 2009.10.28 1\n",
      "2009.11.18 2009.08.31 2009.10.29 1\n",
      "2009.11.19 2009.09.01 2009.10.30 1\n",
      "2009.11.20 2009.09.02 2009.10.31 1\n",
      "2009.11.21 2009.09.03 2009.11.01 1\n",
      "2009.11.22 2009.09.04 2009.11.02 1\n",
      "2009.11.23 2009.09.05 2009.11.03 1\n",
      "2009.11.24 2009.09.06 2009.11.04 1\n",
      "2009.11.25 2009.09.07 2009.11.05 1\n",
      "2009.11.26 2009.09.08 2009.11.06 1\n",
      "2009.11.27 2009.09.09 2009.11.07 1\n",
      "2009.11.28 2009.09.10 2009.11.08 1\n",
      "2009.11.29 2009.09.11 2009.11.09 1\n",
      "2009.11.30 2009.09.12 2009.11.10 1\n",
      "2009.12.01 2009.09.13 2009.11.11 1\n",
      "2009.12.02 2009.09.14 2009.11.12 1\n",
      "2009.12.03 2009.09.15 2009.11.13 1\n",
      "2009.12.04 2009.09.16 2009.11.14 1\n",
      "2009.12.05 2009.09.17 2009.11.15 1\n",
      "2009.12.06 2009.09.18 2009.11.16 1\n",
      "2009.12.07 2009.09.19 2009.11.17 1\n",
      "2009.12.08 2009.09.20 2009.11.18 1\n",
      "2009.12.09 2009.09.21 2009.11.19 1\n",
      "2009.12.10 2009.09.22 2009.11.20 1\n",
      "2009.12.11 2009.09.23 2009.11.21 1\n",
      "2009.12.12 2009.09.24 2009.11.22 1\n",
      "2009.12.13 2009.09.25 2009.11.23 1\n",
      "2009.12.14 2009.09.26 2009.11.24 1\n",
      "2009.12.15 2009.09.27 2009.11.25 1\n",
      "2009.12.16 2009.09.28 2009.11.26 1\n",
      "2009.12.17 2009.09.29 2009.11.27 1\n",
      "2009.12.18 2009.09.30 2009.11.28 1\n",
      "2009.12.19 2009.10.01 2009.11.29 1\n",
      "2009.12.20 2009.10.02 2009.11.30 1\n",
      "2009.12.21 2009.10.03 2009.12.01 1\n",
      "2009.12.22 2009.10.04 2009.12.02 1\n",
      "2009.12.23 2009.10.05 2009.12.03 1\n",
      "2009.12.24 2009.10.06 2009.12.04 1\n",
      "2009.12.25 2009.10.07 2009.12.05 1\n",
      "2009.12.26 2009.10.08 2009.12.06 1\n",
      "2009.12.27 2009.10.09 2009.12.07 1\n",
      "2009.12.28 2009.10.10 2009.12.08 1\n",
      "2010.10.04 2010.07.17 2010.09.14 0\n",
      "2010.10.05 2010.07.18 2010.09.15 1\n",
      "2010.10.06 2010.07.19 2010.09.16 1\n",
      "2010.10.07 2010.07.20 2010.09.17 1\n",
      "2010.10.08 2010.07.21 2010.09.18 1\n",
      "2010.10.09 2010.07.22 2010.09.19 1\n",
      "2010.10.10 2010.07.23 2010.09.20 1\n",
      "2010.10.11 2010.07.24 2010.09.21 1\n",
      "2010.10.12 2010.07.25 2010.09.22 1\n",
      "2010.10.13 2010.07.26 2010.09.23 1\n",
      "2010.10.14 2010.07.27 2010.09.24 1\n",
      "2010.10.15 2010.07.28 2010.09.25 1\n",
      "2010.10.16 2010.07.29 2010.09.26 1\n",
      "2010.10.17 2010.07.30 2010.09.27 1\n",
      "2010.10.18 2010.07.31 2010.09.28 1\n",
      "2010.10.19 2010.08.01 2010.09.29 1\n",
      "2010.10.20 2010.08.02 2010.09.30 1\n",
      "2010.10.21 2010.08.03 2010.10.01 1\n",
      "2010.10.22 2010.08.04 2010.10.02 1\n",
      "2010.10.23 2010.08.05 2010.10.03 1\n",
      "2010.10.24 2010.08.06 2010.10.04 1\n",
      "2010.10.25 2010.08.07 2010.10.05 1\n",
      "2010.10.26 2010.08.08 2010.10.06 1\n",
      "2010.10.27 2010.08.09 2010.10.07 1\n",
      "2010.10.28 2010.08.10 2010.10.08 1\n",
      "2010.10.29 2010.08.11 2010.10.09 1\n",
      "2010.10.30 2010.08.12 2010.10.10 1\n",
      "2010.10.31 2010.08.13 2010.10.11 1\n",
      "2010.11.01 2010.08.14 2010.10.12 1\n",
      "2010.11.02 2010.08.15 2010.10.13 1\n",
      "2010.11.03 2010.08.16 2010.10.14 1\n",
      "2010.11.04 2010.08.17 2010.10.15 1\n",
      "2010.11.05 2010.08.18 2010.10.16 1\n",
      "2010.11.06 2010.08.19 2010.10.17 1\n",
      "2010.11.07 2010.08.20 2010.10.18 1\n",
      "2010.11.08 2010.08.21 2010.10.19 1\n",
      "2010.11.09 2010.08.22 2010.10.20 1\n",
      "2010.11.10 2010.08.23 2010.10.21 1\n",
      "2010.11.11 2010.08.24 2010.10.22 1\n",
      "2010.11.12 2010.08.25 2010.10.23 1\n",
      "2010.11.13 2010.08.26 2010.10.24 1\n",
      "2010.11.14 2010.08.27 2010.10.25 1\n",
      "2010.11.15 2010.08.28 2010.10.26 1\n",
      "2010.11.16 2010.08.29 2010.10.27 1\n",
      "2010.11.17 2010.08.30 2010.10.28 1\n",
      "2010.11.18 2010.08.31 2010.10.29 1\n",
      "2010.11.19 2010.09.01 2010.10.30 1\n",
      "2010.11.20 2010.09.02 2010.10.31 1\n",
      "2010.11.21 2010.09.03 2010.11.01 1\n",
      "2010.11.22 2010.09.04 2010.11.02 1\n",
      "2010.11.23 2010.09.05 2010.11.03 1\n",
      "2010.11.24 2010.09.06 2010.11.04 1\n",
      "2010.11.25 2010.09.07 2010.11.05 1\n",
      "2010.11.26 2010.09.08 2010.11.06 1\n",
      "2010.11.27 2010.09.09 2010.11.07 1\n",
      "2010.11.28 2010.09.10 2010.11.08 1\n",
      "2010.11.29 2010.09.11 2010.11.09 1\n",
      "2010.11.30 2010.09.12 2010.11.10 1\n",
      "2010.12.01 2010.09.13 2010.11.11 1\n",
      "2010.12.02 2010.09.14 2010.11.12 1\n",
      "2010.12.03 2010.09.15 2010.11.13 1\n",
      "2010.12.04 2010.09.16 2010.11.14 1\n",
      "2010.12.05 2010.09.17 2010.11.15 1\n",
      "2010.12.06 2010.09.18 2010.11.16 1\n",
      "2010.12.07 2010.09.19 2010.11.17 1\n",
      "2010.12.08 2010.09.20 2010.11.18 1\n",
      "2010.12.09 2010.09.21 2010.11.19 1\n",
      "2010.12.10 2010.09.22 2010.11.20 1\n",
      "2010.12.11 2010.09.23 2010.11.21 1\n",
      "2010.12.12 2010.09.24 2010.11.22 1\n",
      "2010.12.13 2010.09.25 2010.11.23 1\n",
      "2010.12.14 2010.09.26 2010.11.24 1\n",
      "2010.12.15 2010.09.27 2010.11.25 1\n",
      "2010.12.16 2010.09.28 2010.11.26 1\n",
      "2010.12.17 2010.09.29 2010.11.27 1\n",
      "2010.12.18 2010.09.30 2010.11.28 1\n",
      "2010.12.19 2010.10.01 2010.11.29 1\n",
      "2010.12.20 2010.10.02 2010.11.30 1\n",
      "2010.12.21 2010.10.03 2010.12.01 1\n",
      "2010.12.22 2010.10.04 2010.12.02 1\n",
      "2010.12.23 2010.10.05 2010.12.03 1\n",
      "2010.12.24 2010.10.06 2010.12.04 1\n",
      "2010.12.25 2010.10.07 2010.12.05 1\n",
      "2010.12.26 2010.10.08 2010.12.06 1\n",
      "2010.12.27 2010.10.09 2010.12.07 1\n",
      "2010.12.28 2010.10.10 2010.12.08 1\n",
      "2011.10.04 2011.07.17 2011.09.14 0\n",
      "2011.10.05 2011.07.18 2011.09.15 1\n",
      "2011.10.06 2011.07.19 2011.09.16 1\n",
      "2011.10.07 2011.07.20 2011.09.17 1\n",
      "2011.10.08 2011.07.21 2011.09.18 1\n",
      "2011.10.09 2011.07.22 2011.09.19 1\n",
      "2011.10.10 2011.07.23 2011.09.20 1\n",
      "2011.10.11 2011.07.24 2011.09.21 1\n",
      "2011.10.12 2011.07.25 2011.09.22 1\n",
      "2011.10.13 2011.07.26 2011.09.23 1\n",
      "2011.10.14 2011.07.27 2011.09.24 1\n",
      "2011.10.15 2011.07.28 2011.09.25 1\n",
      "2011.10.16 2011.07.29 2011.09.26 1\n",
      "2011.10.17 2011.07.30 2011.09.27 1\n",
      "2011.10.18 2011.07.31 2011.09.28 1\n",
      "2011.10.19 2011.08.01 2011.09.29 1\n",
      "2011.10.20 2011.08.02 2011.09.30 1\n",
      "2011.10.21 2011.08.03 2011.10.01 1\n",
      "2011.10.22 2011.08.04 2011.10.02 1\n",
      "2011.10.23 2011.08.05 2011.10.03 1\n",
      "2011.10.24 2011.08.06 2011.10.04 1\n",
      "2011.10.25 2011.08.07 2011.10.05 1\n",
      "2011.10.26 2011.08.08 2011.10.06 1\n",
      "2011.10.27 2011.08.09 2011.10.07 1\n",
      "2011.10.28 2011.08.10 2011.10.08 1\n",
      "2011.10.29 2011.08.11 2011.10.09 1\n",
      "2011.10.30 2011.08.12 2011.10.10 1\n",
      "2011.10.31 2011.08.13 2011.10.11 1\n",
      "2011.11.01 2011.08.14 2011.10.12 1\n",
      "2011.11.02 2011.08.15 2011.10.13 1\n",
      "2011.11.03 2011.08.16 2011.10.14 1\n",
      "2011.11.04 2011.08.17 2011.10.15 1\n",
      "2011.11.05 2011.08.18 2011.10.16 1\n",
      "2011.11.06 2011.08.19 2011.10.17 1\n",
      "2011.11.07 2011.08.20 2011.10.18 1\n",
      "2011.11.08 2011.08.21 2011.10.19 1\n",
      "2011.11.09 2011.08.22 2011.10.20 1\n",
      "2011.11.10 2011.08.23 2011.10.21 1\n",
      "2011.11.11 2011.08.24 2011.10.22 1\n",
      "2011.11.12 2011.08.25 2011.10.23 1\n",
      "2011.11.13 2011.08.26 2011.10.24 1\n",
      "2011.11.14 2011.08.27 2011.10.25 1\n",
      "2011.11.15 2011.08.28 2011.10.26 1\n",
      "2011.11.16 2011.08.29 2011.10.27 1\n",
      "2011.11.17 2011.08.30 2011.10.28 1\n",
      "2011.11.18 2011.08.31 2011.10.29 1\n",
      "2011.11.19 2011.09.01 2011.10.30 1\n",
      "2011.11.20 2011.09.02 2011.10.31 1\n",
      "2011.11.21 2011.09.03 2011.11.01 1\n",
      "2011.11.22 2011.09.04 2011.11.02 1\n",
      "2011.11.23 2011.09.05 2011.11.03 1\n",
      "2011.11.24 2011.09.06 2011.11.04 1\n",
      "2011.11.25 2011.09.07 2011.11.05 1\n",
      "2011.11.26 2011.09.08 2011.11.06 1\n",
      "2011.11.27 2011.09.09 2011.11.07 1\n",
      "2011.11.28 2011.09.10 2011.11.08 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011.11.29 2011.09.11 2011.11.09 1\n",
      "2011.11.30 2011.09.12 2011.11.10 1\n",
      "2011.12.01 2011.09.13 2011.11.11 1\n",
      "2011.12.02 2011.09.14 2011.11.12 1\n",
      "2011.12.03 2011.09.15 2011.11.13 1\n",
      "2011.12.04 2011.09.16 2011.11.14 1\n",
      "2011.12.05 2011.09.17 2011.11.15 1\n",
      "2011.12.06 2011.09.18 2011.11.16 1\n",
      "2011.12.07 2011.09.19 2011.11.17 1\n",
      "2011.12.08 2011.09.20 2011.11.18 1\n",
      "2011.12.09 2011.09.21 2011.11.19 1\n",
      "2011.12.10 2011.09.22 2011.11.20 1\n",
      "2011.12.11 2011.09.23 2011.11.21 1\n",
      "2011.12.12 2011.09.24 2011.11.22 1\n",
      "2011.12.13 2011.09.25 2011.11.23 1\n",
      "2011.12.14 2011.09.26 2011.11.24 1\n",
      "2011.12.15 2011.09.27 2011.11.25 1\n",
      "2011.12.16 2011.09.28 2011.11.26 1\n",
      "2011.12.17 2011.09.29 2011.11.27 1\n",
      "2011.12.18 2011.09.30 2011.11.28 1\n",
      "2011.12.19 2011.10.01 2011.11.29 1\n",
      "2011.12.20 2011.10.02 2011.11.30 1\n",
      "2011.12.21 2011.10.03 2011.12.01 1\n",
      "2011.12.22 2011.10.04 2011.12.02 1\n",
      "2011.12.23 2011.10.05 2011.12.03 1\n",
      "2011.12.24 2011.10.06 2011.12.04 1\n",
      "2011.12.25 2011.10.07 2011.12.05 1\n",
      "2011.12.26 2011.10.08 2011.12.06 1\n",
      "2011.12.27 2011.10.09 2011.12.07 1\n",
      "2011.12.28 2011.10.10 2011.12.08 1\n",
      "2012.10.04 2012.07.17 2012.09.14 0\n",
      "2012.10.05 2012.07.18 2012.09.15 1\n",
      "2012.10.06 2012.07.19 2012.09.16 1\n",
      "2012.10.07 2012.07.20 2012.09.17 1\n",
      "2012.10.08 2012.07.21 2012.09.18 1\n",
      "2012.10.09 2012.07.22 2012.09.19 1\n",
      "2012.10.10 2012.07.23 2012.09.20 1\n",
      "2012.10.11 2012.07.24 2012.09.21 1\n",
      "2012.10.12 2012.07.25 2012.09.22 1\n",
      "2012.10.13 2012.07.26 2012.09.23 1\n",
      "2012.10.14 2012.07.27 2012.09.24 1\n",
      "2012.10.15 2012.07.28 2012.09.25 1\n",
      "2012.10.16 2012.07.29 2012.09.26 1\n",
      "2012.10.17 2012.07.30 2012.09.27 1\n",
      "2012.10.18 2012.07.31 2012.09.28 1\n",
      "2012.10.19 2012.08.01 2012.09.29 1\n",
      "2012.10.20 2012.08.02 2012.09.30 1\n",
      "2012.10.21 2012.08.03 2012.10.01 1\n",
      "2012.10.22 2012.08.04 2012.10.02 1\n",
      "2012.10.23 2012.08.05 2012.10.03 1\n",
      "2012.10.24 2012.08.06 2012.10.04 1\n",
      "2012.10.25 2012.08.07 2012.10.05 1\n",
      "2012.10.26 2012.08.08 2012.10.06 1\n",
      "2012.10.27 2012.08.09 2012.10.07 1\n",
      "2012.10.28 2012.08.10 2012.10.08 1\n",
      "2012.10.29 2012.08.11 2012.10.09 1\n",
      "2012.10.30 2012.08.12 2012.10.10 1\n",
      "2012.10.31 2012.08.13 2012.10.11 1\n",
      "2012.11.01 2012.08.14 2012.10.12 1\n",
      "2012.11.02 2012.08.15 2012.10.13 1\n",
      "2012.11.03 2012.08.16 2012.10.14 1\n",
      "2012.11.04 2012.08.17 2012.10.15 1\n",
      "2012.11.05 2012.08.18 2012.10.16 1\n",
      "2012.11.06 2012.08.19 2012.10.17 1\n",
      "2012.11.07 2012.08.20 2012.10.18 1\n",
      "2012.11.08 2012.08.21 2012.10.19 1\n",
      "2012.11.09 2012.08.22 2012.10.20 1\n",
      "2012.11.10 2012.08.23 2012.10.21 1\n",
      "2012.11.11 2012.08.24 2012.10.22 1\n",
      "2012.11.12 2012.08.25 2012.10.23 1\n",
      "2012.11.13 2012.08.26 2012.10.24 1\n",
      "2012.11.14 2012.08.27 2012.10.25 1\n",
      "2012.11.15 2012.08.28 2012.10.26 1\n",
      "2012.11.16 2012.08.29 2012.10.27 1\n",
      "2012.11.17 2012.08.30 2012.10.28 1\n",
      "2012.11.18 2012.08.31 2012.10.29 1\n",
      "2012.11.19 2012.09.01 2012.10.30 1\n",
      "2012.11.20 2012.09.02 2012.10.31 1\n",
      "2012.11.21 2012.09.03 2012.11.01 1\n",
      "2012.11.22 2012.09.04 2012.11.02 1\n",
      "2012.11.23 2012.09.05 2012.11.03 1\n",
      "2012.11.24 2012.09.06 2012.11.04 1\n",
      "2012.11.25 2012.09.07 2012.11.05 1\n",
      "2012.11.26 2012.09.08 2012.11.06 1\n",
      "2012.11.27 2012.09.09 2012.11.07 1\n",
      "2012.11.28 2012.09.10 2012.11.08 1\n",
      "2012.11.29 2012.09.11 2012.11.09 1\n",
      "2012.11.30 2012.09.12 2012.11.10 1\n",
      "2012.12.01 2012.09.13 2012.11.11 1\n",
      "2012.12.02 2012.09.14 2012.11.12 1\n",
      "2012.12.03 2012.09.15 2012.11.13 1\n",
      "2012.12.04 2012.09.16 2012.11.14 1\n",
      "2012.12.05 2012.09.17 2012.11.15 1\n",
      "2012.12.06 2012.09.18 2012.11.16 1\n",
      "2012.12.07 2012.09.19 2012.11.17 1\n",
      "2012.12.08 2012.09.20 2012.11.18 1\n",
      "2012.12.09 2012.09.21 2012.11.19 1\n",
      "2012.12.10 2012.09.22 2012.11.20 1\n",
      "2012.12.11 2012.09.23 2012.11.21 1\n",
      "2012.12.12 2012.09.24 2012.11.22 1\n",
      "2012.12.13 2012.09.25 2012.11.23 1\n",
      "2012.12.14 2012.09.26 2012.11.24 1\n",
      "2012.12.15 2012.09.27 2012.11.25 1\n",
      "2012.12.16 2012.09.28 2012.11.26 1\n",
      "2012.12.17 2012.09.29 2012.11.27 1\n",
      "2012.12.18 2012.09.30 2012.11.28 1\n",
      "2012.12.19 2012.10.01 2012.11.29 1\n",
      "2012.12.20 2012.10.02 2012.11.30 1\n",
      "2012.12.21 2012.10.03 2012.12.01 1\n",
      "2012.12.22 2012.10.04 2012.12.02 1\n",
      "2012.12.23 2012.10.05 2012.12.03 1\n",
      "2012.12.24 2012.10.06 2012.12.04 1\n",
      "2012.12.25 2012.10.07 2012.12.05 1\n",
      "2012.12.26 2012.10.08 2012.12.06 1\n",
      "2012.12.27 2012.10.09 2012.12.07 1\n",
      "2012.12.28 2012.10.10 2012.12.08 1\n",
      "2013.10.04 2013.07.17 2013.09.14 0\n",
      "2013.10.05 2013.07.18 2013.09.15 1\n",
      "2013.10.06 2013.07.19 2013.09.16 1\n",
      "2013.10.07 2013.07.20 2013.09.17 1\n",
      "2013.10.08 2013.07.21 2013.09.18 1\n",
      "2013.10.09 2013.07.22 2013.09.19 1\n",
      "2013.10.10 2013.07.23 2013.09.20 1\n",
      "2013.10.11 2013.07.24 2013.09.21 1\n",
      "2013.10.12 2013.07.25 2013.09.22 1\n",
      "2013.10.13 2013.07.26 2013.09.23 1\n",
      "2013.10.14 2013.07.27 2013.09.24 1\n",
      "2013.10.15 2013.07.28 2013.09.25 1\n",
      "2013.10.16 2013.07.29 2013.09.26 1\n",
      "2013.10.17 2013.07.30 2013.09.27 1\n",
      "2013.10.18 2013.07.31 2013.09.28 1\n",
      "2013.10.19 2013.08.01 2013.09.29 1\n",
      "2013.10.20 2013.08.02 2013.09.30 1\n",
      "2013.10.21 2013.08.03 2013.10.01 1\n",
      "2013.10.22 2013.08.04 2013.10.02 1\n",
      "2013.10.23 2013.08.05 2013.10.03 1\n",
      "2013.10.24 2013.08.06 2013.10.04 1\n",
      "2013.10.25 2013.08.07 2013.10.05 1\n",
      "2013.10.26 2013.08.08 2013.10.06 1\n",
      "2013.10.27 2013.08.09 2013.10.07 1\n",
      "2013.10.28 2013.08.10 2013.10.08 1\n",
      "2013.10.29 2013.08.11 2013.10.09 1\n",
      "2013.10.30 2013.08.12 2013.10.10 1\n",
      "2013.10.31 2013.08.13 2013.10.11 1\n",
      "2013.11.01 2013.08.14 2013.10.12 1\n",
      "2013.11.02 2013.08.15 2013.10.13 1\n",
      "2013.11.03 2013.08.16 2013.10.14 1\n",
      "2013.11.04 2013.08.17 2013.10.15 1\n",
      "2013.11.05 2013.08.18 2013.10.16 1\n",
      "2013.11.06 2013.08.19 2013.10.17 1\n",
      "2013.11.07 2013.08.20 2013.10.18 1\n",
      "2013.11.08 2013.08.21 2013.10.19 1\n",
      "2013.11.09 2013.08.22 2013.10.20 1\n",
      "2013.11.10 2013.08.23 2013.10.21 1\n",
      "2013.11.11 2013.08.24 2013.10.22 1\n",
      "2013.11.12 2013.08.25 2013.10.23 1\n",
      "2013.11.13 2013.08.26 2013.10.24 1\n",
      "2013.11.14 2013.08.27 2013.10.25 1\n",
      "2013.11.15 2013.08.28 2013.10.26 1\n",
      "2013.11.16 2013.08.29 2013.10.27 1\n",
      "2013.11.17 2013.08.30 2013.10.28 1\n",
      "2013.11.18 2013.08.31 2013.10.29 1\n",
      "2013.11.19 2013.09.01 2013.10.30 1\n",
      "2013.11.20 2013.09.02 2013.10.31 1\n",
      "2013.11.21 2013.09.03 2013.11.01 1\n",
      "2013.11.22 2013.09.04 2013.11.02 1\n",
      "2013.11.23 2013.09.05 2013.11.03 1\n",
      "2013.11.24 2013.09.06 2013.11.04 1\n",
      "2013.11.25 2013.09.07 2013.11.05 1\n",
      "2013.11.26 2013.09.08 2013.11.06 1\n",
      "2013.11.27 2013.09.09 2013.11.07 1\n",
      "2013.11.28 2013.09.10 2013.11.08 1\n",
      "2013.11.29 2013.09.11 2013.11.09 1\n",
      "2013.11.30 2013.09.12 2013.11.10 1\n",
      "2013.12.01 2013.09.13 2013.11.11 1\n",
      "2013.12.02 2013.09.14 2013.11.12 1\n",
      "2013.12.03 2013.09.15 2013.11.13 1\n",
      "2013.12.04 2013.09.16 2013.11.14 1\n",
      "2013.12.05 2013.09.17 2013.11.15 1\n",
      "2013.12.06 2013.09.18 2013.11.16 1\n",
      "2013.12.07 2013.09.19 2013.11.17 1\n",
      "2013.12.08 2013.09.20 2013.11.18 1\n",
      "2013.12.09 2013.09.21 2013.11.19 1\n",
      "2013.12.10 2013.09.22 2013.11.20 1\n",
      "2013.12.11 2013.09.23 2013.11.21 1\n",
      "2013.12.12 2013.09.24 2013.11.22 1\n",
      "2013.12.13 2013.09.25 2013.11.23 1\n",
      "2013.12.14 2013.09.26 2013.11.24 1\n",
      "2013.12.15 2013.09.27 2013.11.25 1\n",
      "2013.12.16 2013.09.28 2013.11.26 1\n",
      "2013.12.17 2013.09.29 2013.11.27 1\n",
      "2013.12.18 2013.09.30 2013.11.28 1\n",
      "2013.12.19 2013.10.01 2013.11.29 1\n",
      "2013.12.20 2013.10.02 2013.11.30 1\n",
      "2013.12.21 2013.10.03 2013.12.01 1\n",
      "2013.12.22 2013.10.04 2013.12.02 1\n",
      "2013.12.23 2013.10.05 2013.12.03 1\n",
      "2013.12.24 2013.10.06 2013.12.04 1\n",
      "2013.12.25 2013.10.07 2013.12.05 1\n",
      "2013.12.26 2013.10.08 2013.12.06 1\n",
      "2013.12.27 2013.10.09 2013.12.07 1\n",
      "2013.12.28 2013.10.10 2013.12.08 1\n",
      "2014.10.04 2014.07.17 2014.09.14 0\n",
      "2014.10.05 2014.07.18 2014.09.15 1\n",
      "2014.10.06 2014.07.19 2014.09.16 1\n",
      "2014.10.07 2014.07.20 2014.09.17 1\n",
      "2014.10.08 2014.07.21 2014.09.18 1\n",
      "2014.10.09 2014.07.22 2014.09.19 1\n",
      "2014.10.10 2014.07.23 2014.09.20 1\n",
      "2014.10.11 2014.07.24 2014.09.21 1\n",
      "2014.10.12 2014.07.25 2014.09.22 1\n",
      "2014.10.13 2014.07.26 2014.09.23 1\n",
      "2014.10.14 2014.07.27 2014.09.24 1\n",
      "2014.10.15 2014.07.28 2014.09.25 1\n",
      "2014.10.16 2014.07.29 2014.09.26 1\n",
      "2014.10.17 2014.07.30 2014.09.27 1\n",
      "2014.10.18 2014.07.31 2014.09.28 1\n",
      "2014.10.19 2014.08.01 2014.09.29 1\n",
      "2014.10.20 2014.08.02 2014.09.30 1\n",
      "2014.10.21 2014.08.03 2014.10.01 1\n",
      "2014.10.22 2014.08.04 2014.10.02 1\n",
      "2014.10.23 2014.08.05 2014.10.03 1\n",
      "2014.10.24 2014.08.06 2014.10.04 1\n",
      "2014.10.25 2014.08.07 2014.10.05 1\n",
      "2014.10.26 2014.08.08 2014.10.06 1\n",
      "2014.10.27 2014.08.09 2014.10.07 1\n",
      "2014.10.28 2014.08.10 2014.10.08 1\n",
      "2014.10.29 2014.08.11 2014.10.09 1\n",
      "2014.10.30 2014.08.12 2014.10.10 1\n",
      "2014.10.31 2014.08.13 2014.10.11 1\n",
      "2014.11.01 2014.08.14 2014.10.12 1\n",
      "2014.11.02 2014.08.15 2014.10.13 1\n",
      "2014.11.03 2014.08.16 2014.10.14 1\n",
      "2014.11.04 2014.08.17 2014.10.15 1\n",
      "2014.11.05 2014.08.18 2014.10.16 1\n",
      "2014.11.06 2014.08.19 2014.10.17 1\n",
      "2014.11.07 2014.08.20 2014.10.18 1\n",
      "2014.11.08 2014.08.21 2014.10.19 1\n",
      "2014.11.09 2014.08.22 2014.10.20 1\n",
      "2014.11.10 2014.08.23 2014.10.21 1\n",
      "2014.11.11 2014.08.24 2014.10.22 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014.11.12 2014.08.25 2014.10.23 1\n",
      "2014.11.13 2014.08.26 2014.10.24 1\n",
      "2014.11.14 2014.08.27 2014.10.25 1\n",
      "2014.11.15 2014.08.28 2014.10.26 1\n",
      "2014.11.16 2014.08.29 2014.10.27 1\n",
      "2014.11.17 2014.08.30 2014.10.28 1\n",
      "2014.11.18 2014.08.31 2014.10.29 1\n",
      "2014.11.19 2014.09.01 2014.10.30 1\n",
      "2014.11.20 2014.09.02 2014.10.31 1\n",
      "2014.11.21 2014.09.03 2014.11.01 1\n",
      "2014.11.22 2014.09.04 2014.11.02 1\n",
      "2014.11.23 2014.09.05 2014.11.03 1\n",
      "2014.11.24 2014.09.06 2014.11.04 1\n",
      "2014.11.25 2014.09.07 2014.11.05 1\n",
      "2014.11.26 2014.09.08 2014.11.06 1\n",
      "2014.11.27 2014.09.09 2014.11.07 1\n",
      "2014.11.28 2014.09.10 2014.11.08 1\n",
      "2014.11.29 2014.09.11 2014.11.09 1\n",
      "2014.11.30 2014.09.12 2014.11.10 1\n",
      "2014.12.01 2014.09.13 2014.11.11 1\n",
      "2014.12.02 2014.09.14 2014.11.12 1\n",
      "2014.12.03 2014.09.15 2014.11.13 1\n",
      "2014.12.04 2014.09.16 2014.11.14 1\n",
      "2014.12.05 2014.09.17 2014.11.15 1\n",
      "2014.12.06 2014.09.18 2014.11.16 1\n",
      "2014.12.07 2014.09.19 2014.11.17 1\n",
      "2014.12.08 2014.09.20 2014.11.18 1\n",
      "2014.12.09 2014.09.21 2014.11.19 1\n",
      "2014.12.10 2014.09.22 2014.11.20 1\n",
      "2014.12.11 2014.09.23 2014.11.21 1\n",
      "2014.12.12 2014.09.24 2014.11.22 1\n",
      "2014.12.13 2014.09.25 2014.11.23 1\n",
      "2014.12.14 2014.09.26 2014.11.24 1\n",
      "2014.12.15 2014.09.27 2014.11.25 1\n",
      "2014.12.16 2014.09.28 2014.11.26 1\n",
      "2014.12.17 2014.09.29 2014.11.27 1\n",
      "2014.12.18 2014.09.30 2014.11.28 1\n",
      "2014.12.19 2014.10.01 2014.11.29 1\n",
      "2014.12.20 2014.10.02 2014.11.30 1\n",
      "2014.12.21 2014.10.03 2014.12.01 1\n",
      "2014.12.22 2014.10.04 2014.12.02 1\n",
      "2014.12.23 2014.10.05 2014.12.03 1\n",
      "2014.12.24 2014.10.06 2014.12.04 1\n",
      "2014.12.25 2014.10.07 2014.12.05 1\n",
      "2014.12.26 2014.10.08 2014.12.06 1\n",
      "2014.12.27 2014.10.09 2014.12.07 1\n",
      "2014.12.28 2014.10.10 2014.12.08 1\n",
      "2015.10.04 2015.07.17 2015.09.14 0\n",
      "2015.10.05 2015.07.18 2015.09.15 1\n",
      "2015.10.06 2015.07.19 2015.09.16 1\n",
      "2015.10.07 2015.07.20 2015.09.17 1\n",
      "2015.10.08 2015.07.21 2015.09.18 1\n",
      "2015.10.09 2015.07.22 2015.09.19 1\n",
      "2015.10.10 2015.07.23 2015.09.20 1\n",
      "2015.10.11 2015.07.24 2015.09.21 1\n",
      "2015.10.12 2015.07.25 2015.09.22 1\n",
      "2015.10.13 2015.07.26 2015.09.23 1\n",
      "2015.10.14 2015.07.27 2015.09.24 1\n",
      "2015.10.15 2015.07.28 2015.09.25 1\n",
      "2015.10.16 2015.07.29 2015.09.26 1\n",
      "2015.10.17 2015.07.30 2015.09.27 1\n",
      "2015.10.18 2015.07.31 2015.09.28 1\n",
      "2015.10.19 2015.08.01 2015.09.29 1\n",
      "2015.10.20 2015.08.02 2015.09.30 1\n",
      "2015.10.21 2015.08.03 2015.10.01 1\n",
      "2015.10.22 2015.08.04 2015.10.02 1\n",
      "2015.10.23 2015.08.05 2015.10.03 1\n",
      "2015.10.24 2015.08.06 2015.10.04 1\n",
      "2015.10.25 2015.08.07 2015.10.05 1\n",
      "2015.10.26 2015.08.08 2015.10.06 1\n",
      "2015.10.27 2015.08.09 2015.10.07 1\n",
      "2015.10.28 2015.08.10 2015.10.08 1\n",
      "2015.10.29 2015.08.11 2015.10.09 1\n",
      "2015.10.30 2015.08.12 2015.10.10 1\n",
      "2015.10.31 2015.08.13 2015.10.11 1\n",
      "2015.11.01 2015.08.14 2015.10.12 1\n",
      "2015.11.02 2015.08.15 2015.10.13 1\n",
      "2015.11.03 2015.08.16 2015.10.14 1\n",
      "2015.11.04 2015.08.17 2015.10.15 1\n",
      "2015.11.05 2015.08.18 2015.10.16 1\n",
      "2015.11.06 2015.08.19 2015.10.17 1\n",
      "2015.11.07 2015.08.20 2015.10.18 1\n",
      "2015.11.08 2015.08.21 2015.10.19 1\n",
      "2015.11.09 2015.08.22 2015.10.20 1\n",
      "2015.11.10 2015.08.23 2015.10.21 1\n",
      "2015.11.11 2015.08.24 2015.10.22 1\n",
      "2015.11.12 2015.08.25 2015.10.23 1\n",
      "2015.11.13 2015.08.26 2015.10.24 1\n",
      "2015.11.14 2015.08.27 2015.10.25 1\n",
      "2015.11.15 2015.08.28 2015.10.26 1\n",
      "2015.11.16 2015.08.29 2015.10.27 1\n",
      "2015.11.17 2015.08.30 2015.10.28 1\n",
      "2015.11.18 2015.08.31 2015.10.29 1\n",
      "2015.11.19 2015.09.01 2015.10.30 1\n",
      "2015.11.20 2015.09.02 2015.10.31 1\n",
      "2015.11.21 2015.09.03 2015.11.01 1\n",
      "2015.11.22 2015.09.04 2015.11.02 1\n",
      "2015.11.23 2015.09.05 2015.11.03 1\n",
      "2015.11.24 2015.09.06 2015.11.04 1\n",
      "2015.11.25 2015.09.07 2015.11.05 1\n",
      "2015.11.26 2015.09.08 2015.11.06 1\n",
      "2015.11.27 2015.09.09 2015.11.07 1\n",
      "2015.11.28 2015.09.10 2015.11.08 1\n",
      "2015.11.29 2015.09.11 2015.11.09 1\n",
      "2015.11.30 2015.09.12 2015.11.10 1\n",
      "2015.12.01 2015.09.13 2015.11.11 1\n",
      "2015.12.02 2015.09.14 2015.11.12 1\n",
      "2015.12.03 2015.09.15 2015.11.13 1\n",
      "2015.12.04 2015.09.16 2015.11.14 1\n",
      "2015.12.05 2015.09.17 2015.11.15 1\n",
      "2015.12.06 2015.09.18 2015.11.16 1\n",
      "2015.12.07 2015.09.19 2015.11.17 1\n",
      "2015.12.08 2015.09.20 2015.11.18 1\n",
      "2015.12.09 2015.09.21 2015.11.19 1\n",
      "2015.12.10 2015.09.22 2015.11.20 1\n",
      "2015.12.11 2015.09.23 2015.11.21 1\n",
      "2015.12.12 2015.09.24 2015.11.22 1\n",
      "2015.12.13 2015.09.25 2015.11.23 1\n",
      "2015.12.14 2015.09.26 2015.11.24 1\n",
      "2015.12.15 2015.09.27 2015.11.25 1\n",
      "2015.12.16 2015.09.28 2015.11.26 1\n",
      "2015.12.17 2015.09.29 2015.11.27 1\n",
      "2015.12.18 2015.09.30 2015.11.28 1\n",
      "2015.12.19 2015.10.01 2015.11.29 1\n",
      "2015.12.20 2015.10.02 2015.11.30 1\n",
      "2015.12.21 2015.10.03 2015.12.01 1\n",
      "2015.12.22 2015.10.04 2015.12.02 1\n",
      "2015.12.23 2015.10.05 2015.12.03 1\n",
      "2015.12.24 2015.10.06 2015.12.04 1\n",
      "2015.12.25 2015.10.07 2015.12.05 1\n",
      "2015.12.26 2015.10.08 2015.12.06 1\n",
      "2015.12.27 2015.10.09 2015.12.07 1\n",
      "2015.12.28 2015.10.10 2015.12.08 1\n",
      "2016.10.04 2016.07.17 2016.09.14 0\n",
      "2016.10.05 2016.07.18 2016.09.15 1\n",
      "2016.10.06 2016.07.19 2016.09.16 1\n",
      "2016.10.07 2016.07.20 2016.09.17 1\n",
      "2016.10.08 2016.07.21 2016.09.18 1\n",
      "2016.10.09 2016.07.22 2016.09.19 1\n",
      "2016.10.10 2016.07.23 2016.09.20 1\n",
      "2016.10.11 2016.07.24 2016.09.21 1\n",
      "2016.10.12 2016.07.25 2016.09.22 1\n",
      "2016.10.13 2016.07.26 2016.09.23 1\n",
      "2016.10.14 2016.07.27 2016.09.24 1\n",
      "2016.10.15 2016.07.28 2016.09.25 1\n",
      "2016.10.16 2016.07.29 2016.09.26 1\n",
      "2016.10.17 2016.07.30 2016.09.27 1\n",
      "2016.10.18 2016.07.31 2016.09.28 1\n",
      "2016.10.19 2016.08.01 2016.09.29 1\n",
      "2016.10.20 2016.08.02 2016.09.30 1\n",
      "2016.10.21 2016.08.03 2016.10.01 1\n",
      "2016.10.22 2016.08.04 2016.10.02 1\n",
      "2016.10.23 2016.08.05 2016.10.03 1\n",
      "2016.10.24 2016.08.06 2016.10.04 1\n",
      "2016.10.25 2016.08.07 2016.10.05 1\n",
      "2016.10.26 2016.08.08 2016.10.06 1\n",
      "2016.10.27 2016.08.09 2016.10.07 1\n",
      "2016.10.28 2016.08.10 2016.10.08 1\n",
      "2016.10.29 2016.08.11 2016.10.09 1\n",
      "2016.10.30 2016.08.12 2016.10.10 1\n",
      "2016.10.31 2016.08.13 2016.10.11 1\n",
      "2016.11.01 2016.08.14 2016.10.12 1\n",
      "2016.11.02 2016.08.15 2016.10.13 1\n",
      "2016.11.03 2016.08.16 2016.10.14 1\n",
      "2016.11.04 2016.08.17 2016.10.15 1\n",
      "2016.11.05 2016.08.18 2016.10.16 1\n",
      "2016.11.06 2016.08.19 2016.10.17 1\n",
      "2016.11.07 2016.08.20 2016.10.18 1\n",
      "2016.11.08 2016.08.21 2016.10.19 1\n",
      "2016.11.09 2016.08.22 2016.10.20 1\n",
      "2016.11.10 2016.08.23 2016.10.21 1\n",
      "2016.11.11 2016.08.24 2016.10.22 1\n",
      "2016.11.12 2016.08.25 2016.10.23 1\n",
      "2016.11.13 2016.08.26 2016.10.24 1\n",
      "2016.11.14 2016.08.27 2016.10.25 1\n",
      "2016.11.15 2016.08.28 2016.10.26 1\n",
      "2016.11.16 2016.08.29 2016.10.27 1\n",
      "2016.11.17 2016.08.30 2016.10.28 1\n",
      "2016.11.18 2016.08.31 2016.10.29 1\n",
      "2016.11.19 2016.09.01 2016.10.30 1\n",
      "2016.11.20 2016.09.02 2016.10.31 1\n",
      "2016.11.21 2016.09.03 2016.11.01 1\n",
      "2016.11.22 2016.09.04 2016.11.02 1\n",
      "2016.11.23 2016.09.05 2016.11.03 1\n",
      "2016.11.24 2016.09.06 2016.11.04 1\n",
      "2016.11.25 2016.09.07 2016.11.05 1\n",
      "2016.11.26 2016.09.08 2016.11.06 1\n",
      "2016.11.27 2016.09.09 2016.11.07 1\n",
      "2016.11.28 2016.09.10 2016.11.08 1\n",
      "2016.11.29 2016.09.11 2016.11.09 1\n",
      "2016.11.30 2016.09.12 2016.11.10 1\n",
      "2016.12.01 2016.09.13 2016.11.11 1\n",
      "2016.12.02 2016.09.14 2016.11.12 1\n",
      "2016.12.03 2016.09.15 2016.11.13 1\n",
      "2016.12.04 2016.09.16 2016.11.14 1\n",
      "2016.12.05 2016.09.17 2016.11.15 1\n",
      "2016.12.06 2016.09.18 2016.11.16 1\n",
      "2016.12.07 2016.09.19 2016.11.17 1\n",
      "2016.12.08 2016.09.20 2016.11.18 1\n",
      "2016.12.09 2016.09.21 2016.11.19 1\n",
      "2016.12.10 2016.09.22 2016.11.20 1\n",
      "2016.12.11 2016.09.23 2016.11.21 1\n",
      "2016.12.12 2016.09.24 2016.11.22 1\n",
      "2016.12.13 2016.09.25 2016.11.23 1\n",
      "2016.12.14 2016.09.26 2016.11.24 1\n",
      "2016.12.15 2016.09.27 2016.11.25 1\n",
      "2016.12.16 2016.09.28 2016.11.26 1\n",
      "2016.12.17 2016.09.29 2016.11.27 1\n",
      "2016.12.18 2016.09.30 2016.11.28 1\n",
      "2016.12.19 2016.10.01 2016.11.29 1\n",
      "2016.12.20 2016.10.02 2016.11.30 1\n",
      "2016.12.21 2016.10.03 2016.12.01 1\n",
      "2016.12.22 2016.10.04 2016.12.02 1\n",
      "2016.12.23 2016.10.05 2016.12.03 1\n",
      "2016.12.24 2016.10.06 2016.12.04 1\n",
      "2016.12.25 2016.10.07 2016.12.05 1\n",
      "2016.12.26 2016.10.08 2016.12.06 1\n",
      "2016.12.27 2016.10.09 2016.12.07 1\n",
      "2016.12.28 2016.10.10 2016.12.08 1\n",
      "2017.10.04 2017.07.17 2017.09.14 0\n",
      "2017.10.05 2017.07.18 2017.09.15 1\n",
      "2017.10.06 2017.07.19 2017.09.16 1\n",
      "2017.10.07 2017.07.20 2017.09.17 1\n",
      "2017.10.08 2017.07.21 2017.09.18 1\n",
      "2017.10.09 2017.07.22 2017.09.19 1\n",
      "2017.10.10 2017.07.23 2017.09.20 1\n",
      "2017.10.11 2017.07.24 2017.09.21 1\n",
      "2017.10.12 2017.07.25 2017.09.22 1\n",
      "2017.10.13 2017.07.26 2017.09.23 1\n",
      "2017.10.14 2017.07.27 2017.09.24 1\n",
      "2017.10.15 2017.07.28 2017.09.25 1\n",
      "2017.10.16 2017.07.29 2017.09.26 1\n",
      "2017.10.17 2017.07.30 2017.09.27 1\n",
      "2017.10.18 2017.07.31 2017.09.28 1\n",
      "2017.10.19 2017.08.01 2017.09.29 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017.10.20 2017.08.02 2017.09.30 1\n",
      "2017.10.21 2017.08.03 2017.10.01 1\n",
      "2017.10.22 2017.08.04 2017.10.02 1\n",
      "2017.10.23 2017.08.05 2017.10.03 1\n",
      "2017.10.24 2017.08.06 2017.10.04 1\n",
      "2017.10.25 2017.08.07 2017.10.05 1\n",
      "2017.10.26 2017.08.08 2017.10.06 1\n",
      "2017.10.27 2017.08.09 2017.10.07 1\n",
      "2017.10.28 2017.08.10 2017.10.08 1\n",
      "2017.10.29 2017.08.11 2017.10.09 1\n",
      "2017.10.30 2017.08.12 2017.10.10 1\n",
      "2017.10.31 2017.08.13 2017.10.11 1\n",
      "2017.11.01 2017.08.14 2017.10.12 1\n",
      "2017.11.02 2017.08.15 2017.10.13 1\n",
      "2017.11.03 2017.08.16 2017.10.14 1\n",
      "2017.11.04 2017.08.17 2017.10.15 1\n",
      "2017.11.05 2017.08.18 2017.10.16 1\n",
      "2017.11.06 2017.08.19 2017.10.17 1\n",
      "2017.11.07 2017.08.20 2017.10.18 1\n",
      "2017.11.08 2017.08.21 2017.10.19 1\n",
      "2017.11.09 2017.08.22 2017.10.20 1\n",
      "2017.11.10 2017.08.23 2017.10.21 1\n",
      "2017.11.11 2017.08.24 2017.10.22 1\n",
      "2017.11.12 2017.08.25 2017.10.23 1\n",
      "2017.11.13 2017.08.26 2017.10.24 1\n",
      "2017.11.14 2017.08.27 2017.10.25 1\n",
      "2017.11.15 2017.08.28 2017.10.26 1\n",
      "2017.11.16 2017.08.29 2017.10.27 1\n",
      "2017.11.17 2017.08.30 2017.10.28 1\n",
      "2017.11.18 2017.08.31 2017.10.29 1\n",
      "2017.11.19 2017.09.01 2017.10.30 1\n",
      "2017.11.20 2017.09.02 2017.10.31 1\n",
      "2017.11.21 2017.09.03 2017.11.01 1\n",
      "2017.11.22 2017.09.04 2017.11.02 1\n",
      "2017.11.23 2017.09.05 2017.11.03 1\n",
      "2017.11.24 2017.09.06 2017.11.04 1\n",
      "2017.11.25 2017.09.07 2017.11.05 1\n",
      "2017.11.26 2017.09.08 2017.11.06 1\n",
      "2017.11.27 2017.09.09 2017.11.07 1\n",
      "2017.11.28 2017.09.10 2017.11.08 1\n",
      "2017.11.29 2017.09.11 2017.11.09 1\n",
      "2017.11.30 2017.09.12 2017.11.10 1\n",
      "2017.12.01 2017.09.13 2017.11.11 1\n",
      "2017.12.02 2017.09.14 2017.11.12 1\n",
      "2017.12.03 2017.09.15 2017.11.13 1\n",
      "2017.12.04 2017.09.16 2017.11.14 1\n",
      "2017.12.05 2017.09.17 2017.11.15 1\n",
      "2017.12.06 2017.09.18 2017.11.16 1\n",
      "2017.12.07 2017.09.19 2017.11.17 1\n",
      "2017.12.08 2017.09.20 2017.11.18 1\n",
      "2017.12.09 2017.09.21 2017.11.19 1\n",
      "2017.12.10 2017.09.22 2017.11.20 1\n",
      "2017.12.11 2017.09.23 2017.11.21 1\n",
      "2017.12.12 2017.09.24 2017.11.22 1\n",
      "2017.12.13 2017.09.25 2017.11.23 1\n",
      "2017.12.14 2017.09.26 2017.11.24 1\n",
      "2017.12.15 2017.09.27 2017.11.25 1\n",
      "2017.12.16 2017.09.28 2017.11.26 1\n",
      "2017.12.17 2017.09.29 2017.11.27 1\n",
      "2017.12.18 2017.09.30 2017.11.28 1\n",
      "2017.12.19 2017.10.01 2017.11.29 1\n",
      "2017.12.20 2017.10.02 2017.11.30 1\n",
      "2017.12.21 2017.10.03 2017.12.01 1\n",
      "2017.12.22 2017.10.04 2017.12.02 1\n",
      "2017.12.23 2017.10.05 2017.12.03 1\n",
      "2017.12.24 2017.10.06 2017.12.04 1\n",
      "2017.12.25 2017.10.07 2017.12.05 1\n",
      "2017.12.26 2017.10.08 2017.12.06 1\n",
      "2017.12.27 2017.10.09 2017.12.07 1\n",
      "2017.12.28 2017.10.10 2017.12.08 1\n",
      "2018.10.04 2018.07.17 2018.09.14 0\n",
      "2018.10.05 2018.07.18 2018.09.15 1\n",
      "2018.10.06 2018.07.19 2018.09.16 1\n",
      "2018.10.07 2018.07.20 2018.09.17 1\n",
      "2018.10.08 2018.07.21 2018.09.18 1\n",
      "2018.10.09 2018.07.22 2018.09.19 1\n",
      "2018.10.10 2018.07.23 2018.09.20 1\n",
      "2018.10.11 2018.07.24 2018.09.21 1\n",
      "2018.10.12 2018.07.25 2018.09.22 1\n",
      "2018.10.13 2018.07.26 2018.09.23 1\n",
      "2018.10.14 2018.07.27 2018.09.24 1\n",
      "2018.10.15 2018.07.28 2018.09.25 1\n",
      "2018.10.16 2018.07.29 2018.09.26 1\n",
      "2018.10.17 2018.07.30 2018.09.27 1\n",
      "2018.10.18 2018.07.31 2018.09.28 1\n",
      "2018.10.19 2018.08.01 2018.09.29 1\n",
      "2018.10.20 2018.08.02 2018.09.30 1\n",
      "2018.10.21 2018.08.03 2018.10.01 1\n",
      "2018.10.22 2018.08.04 2018.10.02 1\n",
      "2018.10.23 2018.08.05 2018.10.03 1\n",
      "2018.10.24 2018.08.06 2018.10.04 1\n",
      "2018.10.25 2018.08.07 2018.10.05 1\n",
      "2018.10.26 2018.08.08 2018.10.06 1\n",
      "2018.10.27 2018.08.09 2018.10.07 1\n",
      "2018.10.28 2018.08.10 2018.10.08 1\n",
      "2018.10.29 2018.08.11 2018.10.09 1\n",
      "2018.10.30 2018.08.12 2018.10.10 1\n",
      "2018.10.31 2018.08.13 2018.10.11 1\n",
      "2018.11.01 2018.08.14 2018.10.12 1\n",
      "2018.11.02 2018.08.15 2018.10.13 1\n",
      "2018.11.03 2018.08.16 2018.10.14 1\n",
      "2018.11.04 2018.08.17 2018.10.15 1\n",
      "2018.11.05 2018.08.18 2018.10.16 1\n",
      "2018.11.06 2018.08.19 2018.10.17 1\n",
      "2018.11.07 2018.08.20 2018.10.18 1\n",
      "2018.11.08 2018.08.21 2018.10.19 1\n",
      "2018.11.09 2018.08.22 2018.10.20 1\n",
      "2018.11.10 2018.08.23 2018.10.21 1\n",
      "2018.11.11 2018.08.24 2018.10.22 1\n",
      "2018.11.12 2018.08.25 2018.10.23 1\n",
      "2018.11.13 2018.08.26 2018.10.24 1\n",
      "2018.11.14 2018.08.27 2018.10.25 1\n",
      "2018.11.15 2018.08.28 2018.10.26 1\n",
      "2018.11.16 2018.08.29 2018.10.27 1\n",
      "2018.11.17 2018.08.30 2018.10.28 1\n",
      "2018.11.18 2018.08.31 2018.10.29 1\n",
      "2018.11.19 2018.09.01 2018.10.30 1\n",
      "2018.11.20 2018.09.02 2018.10.31 1\n",
      "2018.11.21 2018.09.03 2018.11.01 1\n",
      "2018.11.22 2018.09.04 2018.11.02 1\n",
      "2018.11.23 2018.09.05 2018.11.03 1\n",
      "2018.11.24 2018.09.06 2018.11.04 1\n",
      "2018.11.25 2018.09.07 2018.11.05 1\n",
      "2018.11.26 2018.09.08 2018.11.06 1\n",
      "2018.11.27 2018.09.09 2018.11.07 1\n",
      "2018.11.28 2018.09.10 2018.11.08 1\n",
      "2018.11.29 2018.09.11 2018.11.09 1\n",
      "2018.11.30 2018.09.12 2018.11.10 1\n",
      "2018.12.01 2018.09.13 2018.11.11 1\n",
      "2018.12.02 2018.09.14 2018.11.12 1\n",
      "2018.12.03 2018.09.15 2018.11.13 1\n",
      "2018.12.04 2018.09.16 2018.11.14 1\n",
      "2018.12.05 2018.09.17 2018.11.15 1\n",
      "2018.12.06 2018.09.18 2018.11.16 1\n",
      "2018.12.07 2018.09.19 2018.11.17 1\n",
      "2018.12.08 2018.09.20 2018.11.18 1\n",
      "2018.12.09 2018.09.21 2018.11.19 1\n",
      "2018.12.10 2018.09.22 2018.11.20 1\n",
      "2018.12.11 2018.09.23 2018.11.21 1\n",
      "2018.12.12 2018.09.24 2018.11.22 1\n",
      "2018.12.13 2018.09.25 2018.11.23 1\n",
      "2018.12.14 2018.09.26 2018.11.24 1\n",
      "2018.12.15 2018.09.27 2018.11.25 1\n",
      "2018.12.16 2018.09.28 2018.11.26 1\n",
      "2018.12.17 2018.09.29 2018.11.27 1\n",
      "2018.12.18 2018.09.30 2018.11.28 1\n",
      "2018.12.19 2018.10.01 2018.11.29 1\n",
      "2018.12.20 2018.10.02 2018.11.30 1\n",
      "2018.12.21 2018.10.03 2018.12.01 1\n",
      "2018.12.22 2018.10.04 2018.12.02 1\n",
      "2018.12.23 2018.10.05 2018.12.03 1\n",
      "2018.12.24 2018.10.06 2018.12.04 1\n",
      "2018.12.25 2018.10.07 2018.12.05 1\n",
      "2018.12.26 2018.10.08 2018.12.06 1\n",
      "2018.12.27 2018.10.09 2018.12.07 1\n",
      "2018.12.28 2018.10.10 2018.12.08 1\n",
      "2019.10.04 2019.07.17 2019.09.14 0\n",
      "2019.10.05 2019.07.18 2019.09.15 1\n",
      "2019.10.06 2019.07.19 2019.09.16 1\n",
      "2019.10.07 2019.07.20 2019.09.17 1\n",
      "2019.10.08 2019.07.21 2019.09.18 1\n",
      "2019.10.09 2019.07.22 2019.09.19 1\n",
      "2019.10.10 2019.07.23 2019.09.20 1\n",
      "2019.10.11 2019.07.24 2019.09.21 1\n",
      "2019.10.12 2019.07.25 2019.09.22 1\n",
      "2019.10.13 2019.07.26 2019.09.23 1\n",
      "2019.10.14 2019.07.27 2019.09.24 1\n",
      "2019.10.15 2019.07.28 2019.09.25 1\n",
      "2019.10.16 2019.07.29 2019.09.26 1\n",
      "2019.10.17 2019.07.30 2019.09.27 1\n",
      "2019.10.18 2019.07.31 2019.09.28 1\n",
      "2019.10.19 2019.08.01 2019.09.29 1\n",
      "2019.10.20 2019.08.02 2019.09.30 1\n",
      "2019.10.21 2019.08.03 2019.10.01 1\n",
      "2019.10.22 2019.08.04 2019.10.02 1\n",
      "2019.10.23 2019.08.05 2019.10.03 1\n",
      "2019.10.24 2019.08.06 2019.10.04 1\n",
      "2019.10.25 2019.08.07 2019.10.05 1\n",
      "2019.10.26 2019.08.08 2019.10.06 1\n",
      "2019.10.27 2019.08.09 2019.10.07 1\n",
      "2019.10.28 2019.08.10 2019.10.08 1\n",
      "2019.10.29 2019.08.11 2019.10.09 1\n",
      "2019.10.30 2019.08.12 2019.10.10 1\n",
      "2019.10.31 2019.08.13 2019.10.11 1\n",
      "2019.11.01 2019.08.14 2019.10.12 1\n",
      "2019.11.02 2019.08.15 2019.10.13 1\n",
      "2019.11.03 2019.08.16 2019.10.14 1\n",
      "2019.11.04 2019.08.17 2019.10.15 1\n",
      "2019.11.05 2019.08.18 2019.10.16 1\n",
      "2019.11.06 2019.08.19 2019.10.17 1\n",
      "2019.11.07 2019.08.20 2019.10.18 1\n",
      "2019.11.08 2019.08.21 2019.10.19 1\n",
      "2019.11.09 2019.08.22 2019.10.20 1\n",
      "2019.11.10 2019.08.23 2019.10.21 1\n",
      "2019.11.11 2019.08.24 2019.10.22 1\n",
      "2019.11.12 2019.08.25 2019.10.23 1\n",
      "2019.11.13 2019.08.26 2019.10.24 1\n",
      "2019.11.14 2019.08.27 2019.10.25 1\n",
      "2019.11.15 2019.08.28 2019.10.26 1\n",
      "2019.11.16 2019.08.29 2019.10.27 1\n",
      "2019.11.17 2019.08.30 2019.10.28 1\n",
      "2019.11.18 2019.08.31 2019.10.29 1\n",
      "2019.11.19 2019.09.01 2019.10.30 1\n",
      "2019.11.20 2019.09.02 2019.10.31 1\n",
      "2019.11.21 2019.09.03 2019.11.01 1\n",
      "2019.11.22 2019.09.04 2019.11.02 1\n",
      "2019.11.23 2019.09.05 2019.11.03 1\n",
      "2019.11.24 2019.09.06 2019.11.04 1\n",
      "2019.11.25 2019.09.07 2019.11.05 1\n",
      "2019.11.26 2019.09.08 2019.11.06 1\n",
      "2019.11.27 2019.09.09 2019.11.07 1\n",
      "2019.11.28 2019.09.10 2019.11.08 1\n",
      "2019.11.29 2019.09.11 2019.11.09 1\n",
      "2019.11.30 2019.09.12 2019.11.10 1\n",
      "2019.12.01 2019.09.13 2019.11.11 1\n",
      "2019.12.02 2019.09.14 2019.11.12 1\n",
      "2019.12.03 2019.09.15 2019.11.13 1\n",
      "2019.12.04 2019.09.16 2019.11.14 1\n",
      "2019.12.05 2019.09.17 2019.11.15 1\n",
      "2019.12.06 2019.09.18 2019.11.16 1\n",
      "2019.12.07 2019.09.19 2019.11.17 1\n",
      "2019.12.08 2019.09.20 2019.11.18 1\n",
      "2019.12.09 2019.09.21 2019.11.19 1\n",
      "2019.12.10 2019.09.22 2019.11.20 1\n",
      "2019.12.11 2019.09.23 2019.11.21 1\n",
      "2019.12.12 2019.09.24 2019.11.22 1\n",
      "2019.12.13 2019.09.25 2019.11.23 1\n",
      "2019.12.14 2019.09.26 2019.11.24 1\n",
      "2019.12.15 2019.09.27 2019.11.25 1\n",
      "2019.12.16 2019.09.28 2019.11.26 1\n",
      "2019.12.17 2019.09.29 2019.11.27 1\n",
      "2019.12.18 2019.09.30 2019.11.28 1\n",
      "2019.12.19 2019.10.01 2019.11.29 1\n",
      "2019.12.20 2019.10.02 2019.11.30 1\n",
      "2019.12.21 2019.10.03 2019.12.01 1\n",
      "2019.12.22 2019.10.04 2019.12.02 1\n",
      "2019.12.23 2019.10.05 2019.12.03 1\n",
      "2019.12.24 2019.10.06 2019.12.04 1\n",
      "2019.12.25 2019.10.07 2019.12.05 1\n",
      "2019.12.26 2019.10.08 2019.12.06 1\n",
      "2019.12.27 2019.10.09 2019.12.07 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019.12.28 2019.10.10 2019.12.08 1\n",
      "2020.10.04 2020.07.17 2020.09.14 0\n",
      "2020.10.05 2020.07.18 2020.09.15 1\n",
      "2020.10.06 2020.07.19 2020.09.16 1\n",
      "2020.10.07 2020.07.20 2020.09.17 1\n",
      "2020.10.08 2020.07.21 2020.09.18 1\n",
      "2020.10.09 2020.07.22 2020.09.19 1\n",
      "2020.10.10 2020.07.23 2020.09.20 1\n",
      "2020.10.11 2020.07.24 2020.09.21 1\n",
      "2020.10.12 2020.07.25 2020.09.22 1\n",
      "2020.10.13 2020.07.26 2020.09.23 1\n",
      "2020.10.14 2020.07.27 2020.09.24 1\n",
      "2020.10.15 2020.07.28 2020.09.25 1\n",
      "2020.10.16 2020.07.29 2020.09.26 1\n",
      "2020.10.17 2020.07.30 2020.09.27 1\n",
      "2020.10.18 2020.07.31 2020.09.28 1\n",
      "2020.10.19 2020.08.01 2020.09.29 1\n",
      "2020.10.20 2020.08.02 2020.09.30 1\n",
      "2020.10.21 2020.08.03 2020.10.01 1\n",
      "2020.10.22 2020.08.04 2020.10.02 1\n",
      "2020.10.23 2020.08.05 2020.10.03 1\n",
      "2020.10.24 2020.08.06 2020.10.04 1\n",
      "2020.10.25 2020.08.07 2020.10.05 1\n",
      "2020.10.26 2020.08.08 2020.10.06 1\n",
      "2020.10.27 2020.08.09 2020.10.07 1\n",
      "2020.10.28 2020.08.10 2020.10.08 1\n",
      "2020.10.29 2020.08.11 2020.10.09 1\n",
      "2020.10.30 2020.08.12 2020.10.10 1\n",
      "2020.10.31 2020.08.13 2020.10.11 1\n",
      "2020.11.01 2020.08.14 2020.10.12 1\n",
      "2020.11.02 2020.08.15 2020.10.13 1\n",
      "2020.11.03 2020.08.16 2020.10.14 1\n",
      "2020.11.04 2020.08.17 2020.10.15 1\n",
      "2020.11.05 2020.08.18 2020.10.16 1\n",
      "2020.11.06 2020.08.19 2020.10.17 1\n",
      "2020.11.07 2020.08.20 2020.10.18 1\n",
      "2020.11.08 2020.08.21 2020.10.19 1\n",
      "2020.11.09 2020.08.22 2020.10.20 1\n",
      "2020.11.10 2020.08.23 2020.10.21 1\n",
      "2020.11.11 2020.08.24 2020.10.22 1\n",
      "2020.11.12 2020.08.25 2020.10.23 1\n",
      "2020.11.13 2020.08.26 2020.10.24 1\n",
      "2020.11.14 2020.08.27 2020.10.25 1\n",
      "2020.11.15 2020.08.28 2020.10.26 1\n",
      "2020.11.16 2020.08.29 2020.10.27 1\n",
      "2020.11.17 2020.08.30 2020.10.28 1\n",
      "2020.11.18 2020.08.31 2020.10.29 1\n",
      "2020.11.19 2020.09.01 2020.10.30 1\n",
      "2020.11.20 2020.09.02 2020.10.31 1\n",
      "2020.11.21 2020.09.03 2020.11.01 1\n",
      "2020.11.22 2020.09.04 2020.11.02 1\n",
      "2020.11.23 2020.09.05 2020.11.03 1\n",
      "2020.11.24 2020.09.06 2020.11.04 1\n",
      "2020.11.25 2020.09.07 2020.11.05 1\n",
      "2020.11.26 2020.09.08 2020.11.06 1\n",
      "2020.11.27 2020.09.09 2020.11.07 1\n",
      "2020.11.28 2020.09.10 2020.11.08 1\n",
      "2020.11.29 2020.09.11 2020.11.09 1\n",
      "2020.11.30 2020.09.12 2020.11.10 1\n",
      "2020.12.01 2020.09.13 2020.11.11 1\n",
      "2020.12.02 2020.09.14 2020.11.12 1\n",
      "2020.12.03 2020.09.15 2020.11.13 1\n",
      "2020.12.04 2020.09.16 2020.11.14 1\n",
      "2020.12.05 2020.09.17 2020.11.15 1\n",
      "2020.12.06 2020.09.18 2020.11.16 1\n",
      "2020.12.07 2020.09.19 2020.11.17 1\n",
      "2020.12.08 2020.09.20 2020.11.18 1\n",
      "2020.12.09 2020.09.21 2020.11.19 1\n",
      "2020.12.10 2020.09.22 2020.11.20 1\n",
      "2020.12.11 2020.09.23 2020.11.21 1\n",
      "2020.12.12 2020.09.24 2020.11.22 1\n",
      "2020.12.13 2020.09.25 2020.11.23 1\n",
      "2020.12.14 2020.09.26 2020.11.24 1\n",
      "2020.12.15 2020.09.27 2020.11.25 1\n",
      "2020.12.16 2020.09.28 2020.11.26 1\n",
      "2020.12.17 2020.09.29 2020.11.27 1\n",
      "2020.12.18 2020.09.30 2020.11.28 1\n",
      "2020.12.19 2020.10.01 2020.11.29 1\n",
      "2020.12.20 2020.10.02 2020.11.30 1\n",
      "2020.12.21 2020.10.03 2020.12.01 1\n",
      "2020.12.22 2020.10.04 2020.12.02 1\n",
      "2020.12.23 2020.10.05 2020.12.03 1\n",
      "2020.12.24 2020.10.06 2020.12.04 1\n",
      "2020.12.25 2020.10.07 2020.12.05 1\n",
      "2020.12.26 2020.10.08 2020.12.06 1\n",
      "2020.12.27 2020.10.09 2020.12.07 1\n",
      "2020.12.28 2020.10.10 2020.12.08 1\n",
      "<xarray.Dataset>\n",
      "Dimensions:    (lag: 60, lat: 61, lon: 180, time: 3440)\n",
      "Coordinates:\n",
      "  * lag        (lag) int64 0 1 2 3 4 5 6 7 8 9 ... 50 51 52 53 54 55 56 57 58 59\n",
      "  * lat        (lat) float32 60.0 58.0 56.0 54.0 ... -54.0 -56.0 -58.0 -60.0\n",
      "  * lon        (lon) float32 -180.0 -178.0 -176.0 -174.0 ... 174.0 176.0 178.0\n",
      "    dayofyear  (time, lag) int64 198 199 200 201 202 203 ... 339 340 341 342 343\n",
      "  * time       (time) datetime64[ns] 1981-10-04 1981-10-05 ... 2020-12-28\n",
      "Data variables:\n",
      "    sst        (time, lag, lat, lon) float32 -0.36218587 ... 0.24122316\n"
     ]
    }
   ],
   "source": [
    "# Predictor data preprocessing\n",
    "# can select the values and region you want by changing the parameters\n",
    "from preprocess import get_principle_components_and_EOFs\n",
    "\n",
    "# Run the function\n",
    "s_target_date='04-10-1981'\n",
    "e_target_date='28-12-2020' # end of year - approximatelly 14 days\n",
    "\n",
    "# if eof is 15 days centered then rw_1=7 # If eof is 15 days NOT-centered then rw_1=15\n",
    "rw_1 = 3 # half eof smoothing\n",
    "lead_time = 14 # days until valid_time starts\n",
    "rw = 7 # data full smoothing value (gets half in function), because the data are centered, otherwise set to 0\n",
    "\n",
    "# LSTM timestep\n",
    "ntimestep = 60 # lags to consider for the lstm\n",
    "target_len = len(tp_target) # len(tp_target)\n",
    "\n",
    "drop_OND_years = [1970, 1971, 1972, 1973, 1974] # fake years to drop because we dont want to do any drop currently\n",
    "\n",
    "root_data = '/s2s_nobackup/zhengwu/workshop/'\n",
    "file_vars = ['ERA5_t2m', 'sst']\n",
    "header_vars = ['t2m', 'sst']\n",
    "output_vars = [1]*2\n",
    "\n",
    "# select regions for the individual predictor\n",
    "lon_slices = [[15,60],[-180,180],[-180,180]]\n",
    "lat_slices = [[30,-15],[60,-60],[-20,50]]\n",
    "\n",
    "predictors = []\n",
    "for file_var, header_var, lon_slice, lat_slice in zip (file_vars, header_vars, lon_slices, lat_slices):\n",
    "    if file_var == 'era5_olr':\n",
    "        file = xr.open_dataset(root_data+file_var+'_1950_2021_daily_1deg_tropics.nc')\n",
    "        #print('olr')\n",
    "    else:\n",
    "        file = xr.open_dataset(root_data+file_var+'_1959-2021_1_12_daily_2.0deg.nc')\n",
    "        #print(header_var)\n",
    "\n",
    "    if \"longitude\" in file.coords:\n",
    "        file = file.rename({\"longitude\": \"lon\",\"latitude\": \"lat\"})\n",
    "\n",
    "    assert \"lat\" in file.coords\n",
    "    assert \"lon\" in file.coords\n",
    "    print('lat lon exist')\n",
    "    \n",
    "    # select region\n",
    "    var_dim = file.sel(lon=slice(lon_slice[0],lon_slice[1]),lat=slice(lat_slice[0],lat_slice[1]))\n",
    "    \n",
    "    \n",
    "    var_series = var_dim.sel(time=var_dim.time.dt.year.isin([np.arange(SYY,EYY+1)])).rolling(time=7, center=True).mean(skipna=True)\n",
    "    var_series = var_series.sel(time=var_series.time.dt.year.isin(all_years))\n",
    "\n",
    "    # remove climatology\n",
    "    var_anom_series = var_series.groupby(\"time.dayofyear\") - var_series.groupby(\"time.dayofyear\").mean(\"time\",skipna=True)\n",
    "    var_norm_series = var_anom_series.groupby(\"time.dayofyear\") / var_anom_series.groupby(\"time.dayofyear\").std(\"time\",skipna=True)\n",
    "    print(var_norm_series)\n",
    "    \n",
    "    predictor_array=sel_train_data_lead(var_norm_series, target_len, s_target_date, e_target_date,\n",
    "                    rw_1, lead_time, rw, ntimestep, drop_OND_years)\n",
    "    if file_var == 'ERA5_t2m':\n",
    "        output_vars[0] = predictor_array\n",
    "    elif file_var == 'sst':\n",
    "        output_vars[1] = predictor_array\n",
    "    print(predictor_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<title>Show/Hide data repr</title>\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<title>Show/Hide attributes</title>\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt, dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2 {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><div class='xr-wrap'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-36cf3df6-fffd-4777-bd46-91eb654464fa' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-36cf3df6-fffd-4777-bd46-91eb654464fa' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span class='xr-has-index'>lag</span>: 60</li><li><span class='xr-has-index'>lat</span>: 23</li><li><span class='xr-has-index'>lon</span>: 23</li><li><span class='xr-has-index'>time</span>: 3440</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-05ac009c-8ae8-4dbb-bf97-1b7de5eb2cb9' class='xr-section-summary-in' type='checkbox'  checked><label for='section-05ac009c-8ae8-4dbb-bf97-1b7de5eb2cb9' class='xr-section-summary' >Coordinates: <span>(5)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>lag</span></div><div class='xr-var-dims'>(lag)</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>0 1 2 3 4 5 6 ... 54 55 56 57 58 59</div><input id='attrs-053ec2c0-5211-4638-b22e-eda192fe5f10' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-053ec2c0-5211-4638-b22e-eda192fe5f10' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-b6531aa2-a7a3-4203-841c-02887d8305b1' class='xr-var-data-in' type='checkbox'><label for='data-b6531aa2-a7a3-4203-841c-02887d8305b1' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><pre class='xr-var-data'>array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
       "       54, 55, 56, 57, 58, 59])</pre></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>lat</span></div><div class='xr-var-dims'>(lat)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>30.0 28.0 26.0 ... -12.0 -14.0</div><input id='attrs-3fa19ca0-c46f-4732-ac8d-ab4968b3217f' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-3fa19ca0-c46f-4732-ac8d-ab4968b3217f' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-7b952e7f-c0df-4d49-b792-08a08ad5e868' class='xr-var-data-in' type='checkbox'><label for='data-7b952e7f-c0df-4d49-b792-08a08ad5e868' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>standard_name :</span></dt><dd>latitude</dd><dt><span>long_name :</span></dt><dd>latitude</dd><dt><span>units :</span></dt><dd>degrees_north</dd><dt><span>axis :</span></dt><dd>Y</dd></dl></div><pre class='xr-var-data'>array([ 30.,  28.,  26.,  24.,  22.,  20.,  18.,  16.,  14.,  12.,  10.,   8.,\n",
       "         6.,   4.,   2.,   0.,  -2.,  -4.,  -6.,  -8., -10., -12., -14.],\n",
       "      dtype=float32)</pre></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>lon</span></div><div class='xr-var-dims'>(lon)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>16.0 18.0 20.0 ... 56.0 58.0 60.0</div><input id='attrs-3d6b45b9-4a0d-47d0-99fc-08c68f7a6616' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-3d6b45b9-4a0d-47d0-99fc-08c68f7a6616' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-e789ba06-7327-4a3f-b591-dfa68fdbe7cb' class='xr-var-data-in' type='checkbox'><label for='data-e789ba06-7327-4a3f-b591-dfa68fdbe7cb' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>standard_name :</span></dt><dd>longitude</dd><dt><span>long_name :</span></dt><dd>longitude</dd><dt><span>units :</span></dt><dd>degrees_east</dd><dt><span>axis :</span></dt><dd>X</dd></dl></div><pre class='xr-var-data'>array([16., 18., 20., 22., 24., 26., 28., 30., 32., 34., 36., 38., 40., 42.,\n",
       "       44., 46., 48., 50., 52., 54., 56., 58., 60.], dtype=float32)</pre></li><li class='xr-var-item'><div class='xr-var-name'><span>dayofyear</span></div><div class='xr-var-dims'>(time, lag)</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>198 199 200 201 ... 340 341 342 343</div><input id='attrs-f84e9782-88f6-44d3-9826-f63df9266f3b' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-f84e9782-88f6-44d3-9826-f63df9266f3b' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-7522aa88-4edd-41c8-9c36-2f43acf6c048' class='xr-var-data-in' type='checkbox'><label for='data-7522aa88-4edd-41c8-9c36-2f43acf6c048' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><pre class='xr-var-data'>array([[198, 199, 200, ..., 255, 256, 257],\n",
       "       [199, 200, 201, ..., 256, 257, 258],\n",
       "       [200, 201, 202, ..., 257, 258, 259],\n",
       "       ...,\n",
       "       [282, 283, 284, ..., 339, 340, 341],\n",
       "       [283, 284, 285, ..., 340, 341, 342],\n",
       "       [284, 285, 286, ..., 341, 342, 343]])</pre></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>time</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>1981-10-04 ... 2020-12-28</div><input id='attrs-e7b30a8f-41da-4929-81a7-fc4f528dea9c' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-e7b30a8f-41da-4929-81a7-fc4f528dea9c' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-b6075ee4-f795-48b6-822f-931158dfdda2' class='xr-var-data-in' type='checkbox'><label for='data-b6075ee4-f795-48b6-822f-931158dfdda2' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><pre class='xr-var-data'>array([&#x27;1981-10-04T00:00:00.000000000&#x27;, &#x27;1981-10-05T00:00:00.000000000&#x27;,\n",
       "       &#x27;1981-10-06T00:00:00.000000000&#x27;, ..., &#x27;2020-12-26T00:00:00.000000000&#x27;,\n",
       "       &#x27;2020-12-27T00:00:00.000000000&#x27;, &#x27;2020-12-28T00:00:00.000000000&#x27;],\n",
       "      dtype=&#x27;datetime64[ns]&#x27;)</pre></li></ul></div></li><li class='xr-section-item'><input id='section-ed1ac23b-9557-440b-9a48-e4c07bd984b1' class='xr-section-summary-in' type='checkbox'  checked><label for='section-ed1ac23b-9557-440b-9a48-e4c07bd984b1' class='xr-section-summary' >Data variables: <span>(1)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>t2m</span></div><div class='xr-var-dims'>(time, lag, lat, lon)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>-0.8605126 ... 1.1051866</div><input id='attrs-158112fa-fc60-4249-876c-f296a148fec2' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-158112fa-fc60-4249-876c-f296a148fec2' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-902a44ee-46b9-42ae-a769-ce026925d7da' class='xr-var-data-in' type='checkbox'><label for='data-902a44ee-46b9-42ae-a769-ce026925d7da' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><pre class='xr-var-data'>array([[[[-0.8605126 , -0.79783314, -0.606578  , ...,  0.05445895,\n",
       "           0.37713176,  0.5986276 ],\n",
       "         [-0.44121322, -0.45384833, -0.13056332, ...,  0.66614765,\n",
       "           0.89954036,  0.51175606],\n",
       "         [ 0.6240379 ,  0.08519142,  0.3719526 , ..., -0.19872057,\n",
       "           0.18958545,  0.0072465 ],\n",
       "         ...,\n",
       "         [-0.01939776, -0.34053606, -0.20958117, ...,  0.71832055,\n",
       "           1.204662  ,  0.8116611 ],\n",
       "         [ 0.45271853,  0.160351  , -0.16028725, ...,  0.5761695 ,\n",
       "           0.52563006,  0.7990062 ],\n",
       "         [ 0.4001447 ,  0.28325427, -0.25999206, ...,  0.5177923 ,\n",
       "          -0.1933661 , -0.61607736]],\n",
       "\n",
       "        [[-1.0540637 , -1.0684607 , -0.9539297 , ...,  0.05925299,\n",
       "           0.30715337,  0.6448296 ],\n",
       "         [-0.85953176, -0.9874389 , -0.85064703, ...,  0.14274462,\n",
       "           0.8087015 ,  0.5788005 ],\n",
       "         [ 0.08448735, -0.577983  , -0.2207507 , ..., -0.11993224,\n",
       "          -0.07929302, -0.08202551],\n",
       "         ...,\n",
       "         [-0.41998735, -0.70641345, -0.5782891 , ...,  1.0136825 ,\n",
       "           1.2327603 ,  0.739735  ],\n",
       "         [ 0.00698978, -0.21952818, -0.5869084 , ...,  0.96273845,\n",
       "           1.0105714 ,  1.1083597 ],\n",
       "         [-0.04313533, -0.14166173, -0.72930586, ...,  0.5729407 ,\n",
       "           0.05311228, -0.30956778]],\n",
       "\n",
       "        [[-1.1206665 , -1.2590227 , -1.1694227 , ..., -0.04233282,\n",
       "           0.19304277,  0.6256641 ],\n",
       "         [-1.1767457 , -1.2915941 , -1.1911428 , ..., -0.13076371,\n",
       "           0.77208465,  0.6255089 ],\n",
       "         [-0.43503723, -1.1447091 , -0.73901135, ..., -0.24677244,\n",
       "           0.01391528, -0.00956803],\n",
       "         ...,\n",
       "         [-0.59950316, -0.87976664, -0.7750564 , ...,  1.1380696 ,\n",
       "           1.4933487 ,  0.99007213],\n",
       "         [-0.30717084, -0.591711  , -0.79864347, ...,  0.9727316 ,\n",
       "           1.0163428 ,  1.2202325 ],\n",
       "         [-0.45287177, -0.4930628 , -1.0195061 , ...,  0.73959833,\n",
       "           0.17794877, -0.04728315]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.0172151 , -1.3629551 , -1.6279955 , ..., -1.088483  ,\n",
       "          -1.0297513 , -0.8351488 ],\n",
       "         [-1.3096007 , -1.4591134 , -1.5043869 , ..., -1.3652195 ,\n",
       "          -1.5991248 , -1.1485631 ],\n",
       "         [-1.277171  , -1.455228  , -1.2899752 , ..., -0.42057523,\n",
       "          -1.290234  , -1.3965833 ],\n",
       "         ...,\n",
       "         [ 0.5428179 ,  0.01780348, -0.61328894, ..., -0.67721266,\n",
       "          -0.8532396 , -0.7155773 ],\n",
       "         [-0.1506248 , -0.86782   , -1.5891861 , ..., -0.4002144 ,\n",
       "          -0.26143122, -0.57688624],\n",
       "         [-1.1228468 , -1.1692866 , -1.490525  , ...,  0.20560805,\n",
       "           0.04856438, -0.32353318]],\n",
       "\n",
       "        [[-0.7298372 , -1.2521594 , -1.656098  , ..., -0.7012567 ,\n",
       "          -0.6564347 , -0.33521506],\n",
       "         [-1.163496  , -1.3238921 , -1.4599313 , ..., -1.0792527 ,\n",
       "          -1.243919  , -0.6551099 ],\n",
       "         [-1.3488357 , -1.5058638 , -1.3299968 , ..., -0.549295  ,\n",
       "          -1.3895332 , -1.3501482 ],\n",
       "         ...,\n",
       "         [ 0.85003716,  0.12264695, -0.3808005 , ..., -0.6228825 ,\n",
       "          -0.8823811 , -0.71023154],\n",
       "         [ 0.24082574, -0.46891922, -0.9342927 , ..., -0.43148705,\n",
       "          -0.54429317, -0.78091025],\n",
       "         [-0.8647539 , -0.77538466, -0.98570687, ...,  0.02027408,\n",
       "          -0.07318967, -0.5116448 ]],\n",
       "\n",
       "        [[-0.5335817 , -1.1710082 , -1.5096895 , ..., -0.3827639 ,\n",
       "          -0.2817722 ,  0.01469781],\n",
       "         [-0.82266134, -0.99162054, -1.2038313 , ..., -0.83488303,\n",
       "          -0.972958  , -0.22933783],\n",
       "         [-1.1547663 , -1.2454494 , -1.0547537 , ..., -0.75679517,\n",
       "          -1.5676597 , -1.4415462 ],\n",
       "         ...,\n",
       "         [ 1.0642325 ,  0.26009896, -0.15060881, ..., -0.9323371 ,\n",
       "          -1.0943121 , -0.8693487 ],\n",
       "         [ 0.40514416, -0.23971099, -0.6661539 , ..., -0.5299456 ,\n",
       "          -0.6943807 , -0.9536308 ],\n",
       "         [-0.5902448 , -0.480672  , -0.7934444 , ...,  0.0197039 ,\n",
       "          -0.09818255, -0.48227888]]],\n",
       "\n",
       "\n",
       "       [[[-1.0540637 , -1.0684607 , -0.9539297 , ...,  0.05925299,\n",
       "           0.30715337,  0.6448296 ],\n",
       "         [-0.85953176, -0.9874389 , -0.85064703, ...,  0.14274462,\n",
       "           0.8087015 ,  0.5788005 ],\n",
       "         [ 0.08448735, -0.577983  , -0.2207507 , ..., -0.11993224,\n",
       "          -0.07929302, -0.08202551],\n",
       "         ...,\n",
       "         [-0.41998735, -0.70641345, -0.5782891 , ...,  1.0136825 ,\n",
       "           1.2327603 ,  0.739735  ],\n",
       "         [ 0.00698978, -0.21952818, -0.5869084 , ...,  0.96273845,\n",
       "           1.0105714 ,  1.1083597 ],\n",
       "         [-0.04313533, -0.14166173, -0.72930586, ...,  0.5729407 ,\n",
       "           0.05311228, -0.30956778]],\n",
       "\n",
       "        [[-1.1206665 , -1.2590227 , -1.1694227 , ..., -0.04233282,\n",
       "           0.19304277,  0.6256641 ],\n",
       "         [-1.1767457 , -1.2915941 , -1.1911428 , ..., -0.13076371,\n",
       "           0.77208465,  0.6255089 ],\n",
       "         [-0.43503723, -1.1447091 , -0.73901135, ..., -0.24677244,\n",
       "           0.01391528, -0.00956803],\n",
       "         ...,\n",
       "         [-0.59950316, -0.87976664, -0.7750564 , ...,  1.1380696 ,\n",
       "           1.4933487 ,  0.99007213],\n",
       "         [-0.30717084, -0.591711  , -0.79864347, ...,  0.9727316 ,\n",
       "           1.0163428 ,  1.2202325 ],\n",
       "         [-0.45287177, -0.4930628 , -1.0195061 , ...,  0.73959833,\n",
       "           0.17794877, -0.04728315]],\n",
       "\n",
       "        [[-1.0220225 , -1.2399082 , -1.2318797 , ..., -0.25864527,\n",
       "           0.03706228,  0.53063315],\n",
       "         [-1.1505646 , -1.2724237 , -1.2101529 , ..., -0.15407065,\n",
       "           0.7922034 ,  0.492414  ],\n",
       "         [-0.6500403 , -1.2606572 , -0.94736546, ..., -0.20729806,\n",
       "           0.2502232 ,  0.11809924],\n",
       "         ...,\n",
       "         [-0.63576794, -0.9641429 , -0.8752123 , ...,  1.0898422 ,\n",
       "           1.4722618 ,  0.9369664 ],\n",
       "         [-0.3523185 , -0.75150543, -0.95407325, ...,  0.8485745 ,\n",
       "           0.8558142 ,  1.0958719 ],\n",
       "         [-0.75962245, -0.7629691 , -1.162085  , ...,  0.85084116,\n",
       "           0.44443053,  0.19248931]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.7298372 , -1.2521594 , -1.656098  , ..., -0.7012567 ,\n",
       "          -0.6564347 , -0.33521506],\n",
       "         [-1.163496  , -1.3238921 , -1.4599313 , ..., -1.0792527 ,\n",
       "          -1.243919  , -0.6551099 ],\n",
       "         [-1.3488357 , -1.5058638 , -1.3299968 , ..., -0.549295  ,\n",
       "          -1.3895332 , -1.3501482 ],\n",
       "         ...,\n",
       "         [ 0.85003716,  0.12264695, -0.3808005 , ..., -0.6228825 ,\n",
       "          -0.8823811 , -0.71023154],\n",
       "         [ 0.24082574, -0.46891922, -0.9342927 , ..., -0.43148705,\n",
       "          -0.54429317, -0.78091025],\n",
       "         [-0.8647539 , -0.77538466, -0.98570687, ...,  0.02027408,\n",
       "          -0.07318967, -0.5116448 ]],\n",
       "\n",
       "        [[-0.5335817 , -1.1710082 , -1.5096895 , ..., -0.3827639 ,\n",
       "          -0.2817722 ,  0.01469781],\n",
       "         [-0.82266134, -0.99162054, -1.2038313 , ..., -0.83488303,\n",
       "          -0.972958  , -0.22933783],\n",
       "         [-1.1547663 , -1.2454494 , -1.0547537 , ..., -0.75679517,\n",
       "          -1.5676597 , -1.4415462 ],\n",
       "         ...,\n",
       "         [ 1.0642325 ,  0.26009896, -0.15060881, ..., -0.9323371 ,\n",
       "          -1.0943121 , -0.8693487 ],\n",
       "         [ 0.40514416, -0.23971099, -0.6661539 , ..., -0.5299456 ,\n",
       "          -0.6943807 , -0.9536308 ],\n",
       "         [-0.5902448 , -0.480672  , -0.7934444 , ...,  0.0197039 ,\n",
       "          -0.09818255, -0.48227888]],\n",
       "\n",
       "        [[-0.5808045 , -1.1865805 , -1.464581  , ..., -0.07659892,\n",
       "           0.1032098 ,  0.409277  ],\n",
       "         [-0.63291544, -0.87613606, -1.0461739 , ..., -0.49642307,\n",
       "          -0.6780919 ,  0.15281244],\n",
       "         [-0.9005717 , -0.94723904, -0.77186066, ..., -0.9312322 ,\n",
       "          -1.6307833 , -1.3312302 ],\n",
       "         ...,\n",
       "         [ 0.9758422 ,  0.20342313, -0.0312782 , ..., -1.031403  ,\n",
       "          -1.0618565 , -0.7541908 ],\n",
       "         [ 0.5503647 , -0.07357553, -0.4087056 , ..., -0.5105041 ,\n",
       "          -0.6220225 , -0.84152406],\n",
       "         [-0.27450842, -0.26937735, -0.563996  , ..., -0.14165193,\n",
       "          -0.23084725, -0.48029283]]],\n",
       "\n",
       "\n",
       "       [[[-1.1206665 , -1.2590227 , -1.1694227 , ..., -0.04233282,\n",
       "           0.19304277,  0.6256641 ],\n",
       "         [-1.1767457 , -1.2915941 , -1.1911428 , ..., -0.13076371,\n",
       "           0.77208465,  0.6255089 ],\n",
       "         [-0.43503723, -1.1447091 , -0.73901135, ..., -0.24677244,\n",
       "           0.01391528, -0.00956803],\n",
       "         ...,\n",
       "         [-0.59950316, -0.87976664, -0.7750564 , ...,  1.1380696 ,\n",
       "           1.4933487 ,  0.99007213],\n",
       "         [-0.30717084, -0.591711  , -0.79864347, ...,  0.9727316 ,\n",
       "           1.0163428 ,  1.2202325 ],\n",
       "         [-0.45287177, -0.4930628 , -1.0195061 , ...,  0.73959833,\n",
       "           0.17794877, -0.04728315]],\n",
       "\n",
       "        [[-1.0220225 , -1.2399082 , -1.2318797 , ..., -0.25864527,\n",
       "           0.03706228,  0.53063315],\n",
       "         [-1.1505646 , -1.2724237 , -1.2101529 , ..., -0.15407065,\n",
       "           0.7922034 ,  0.492414  ],\n",
       "         [-0.6500403 , -1.2606572 , -0.94736546, ..., -0.20729806,\n",
       "           0.2502232 ,  0.11809924],\n",
       "         ...,\n",
       "         [-0.63576794, -0.9641429 , -0.8752123 , ...,  1.0898422 ,\n",
       "           1.4722618 ,  0.9369664 ],\n",
       "         [-0.3523185 , -0.75150543, -0.95407325, ...,  0.8485745 ,\n",
       "           0.8558142 ,  1.0958719 ],\n",
       "         [-0.75962245, -0.7629691 , -1.162085  , ...,  0.85084116,\n",
       "           0.44443053,  0.19248931]],\n",
       "\n",
       "        [[-1.135769  , -1.3264133 , -1.2872076 , ..., -0.55161357,\n",
       "          -0.19763228,  0.2184126 ],\n",
       "         [-1.1968588 , -1.3140357 , -1.233461  , ...,  0.09514258,\n",
       "           0.6984468 ,  0.24999088],\n",
       "         [-0.8004519 , -1.3030584 , -1.0045067 , ...,  0.08829477,\n",
       "           0.81592095,  0.6556279 ],\n",
       "         ...,\n",
       "         [-0.6586539 , -1.0580524 , -0.86480296, ...,  0.7741716 ,\n",
       "           1.1890684 ,  0.83717245],\n",
       "         [-0.49140796, -0.78549904, -0.9466101 , ...,  0.96382433,\n",
       "           0.6444519 ,  0.84400135],\n",
       "         [-0.9367499 , -0.87557805, -1.1940122 , ...,  0.9094423 ,\n",
       "           0.4554536 ,  0.31394267]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.5335817 , -1.1710082 , -1.5096895 , ..., -0.3827639 ,\n",
       "          -0.2817722 ,  0.01469781],\n",
       "         [-0.82266134, -0.99162054, -1.2038313 , ..., -0.83488303,\n",
       "          -0.972958  , -0.22933783],\n",
       "         [-1.1547663 , -1.2454494 , -1.0547537 , ..., -0.75679517,\n",
       "          -1.5676597 , -1.4415462 ],\n",
       "         ...,\n",
       "         [ 1.0642325 ,  0.26009896, -0.15060881, ..., -0.9323371 ,\n",
       "          -1.0943121 , -0.8693487 ],\n",
       "         [ 0.40514416, -0.23971099, -0.6661539 , ..., -0.5299456 ,\n",
       "          -0.6943807 , -0.9536308 ],\n",
       "         [-0.5902448 , -0.480672  , -0.7934444 , ...,  0.0197039 ,\n",
       "          -0.09818255, -0.48227888]],\n",
       "\n",
       "        [[-0.5808045 , -1.1865805 , -1.464581  , ..., -0.07659892,\n",
       "           0.1032098 ,  0.409277  ],\n",
       "         [-0.63291544, -0.87613606, -1.0461739 , ..., -0.49642307,\n",
       "          -0.6780919 ,  0.15281244],\n",
       "         [-0.9005717 , -0.94723904, -0.77186066, ..., -0.9312322 ,\n",
       "          -1.6307833 , -1.3312302 ],\n",
       "         ...,\n",
       "         [ 0.9758422 ,  0.20342313, -0.0312782 , ..., -1.031403  ,\n",
       "          -1.0618565 , -0.7541908 ],\n",
       "         [ 0.5503647 , -0.07357553, -0.4087056 , ..., -0.5105041 ,\n",
       "          -0.6220225 , -0.84152406],\n",
       "         [-0.27450842, -0.26937735, -0.563996  , ..., -0.14165193,\n",
       "          -0.23084725, -0.48029283]],\n",
       "\n",
       "        [[-0.81707674, -1.3628061 , -1.5767183 , ...,  0.22125861,\n",
       "           0.40592316,  0.70701355],\n",
       "         [-0.6311462 , -0.9424456 , -1.0591725 , ..., -0.1488283 ,\n",
       "          -0.43622988,  0.42919117],\n",
       "         [-0.7289066 , -0.761867  , -0.659379  , ..., -1.0077916 ,\n",
       "          -1.5295005 , -1.1649517 ],\n",
       "         ...,\n",
       "         [ 0.8378489 ,  0.2595344 ,  0.18350448, ..., -0.93290913,\n",
       "          -0.93898046, -0.7891024 ],\n",
       "         [ 0.5876229 ,  0.12141598, -0.2442898 , ..., -0.58286476,\n",
       "          -0.67636794, -0.77369946],\n",
       "         [-0.10913767, -0.07055724, -0.2311853 , ..., -0.34072238,\n",
       "          -0.26147792, -0.40463594]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ 1.0869044 ,  1.1305515 ,  0.8964728 , ..., -0.1992589 ,\n",
       "           0.00628913, -0.07543794],\n",
       "         [ 1.110314  ,  0.88377976,  0.8287674 , ...,  0.66350514,\n",
       "           0.561669  ,  0.30231285],\n",
       "         [ 0.8332799 ,  0.7056926 ,  0.7006638 , ...,  1.3457131 ,\n",
       "           0.6278992 ,  1.0804474 ],\n",
       "         ...,\n",
       "         [ 0.9489849 ,  1.6820757 ,  1.6235616 , ...,  2.6585317 ,\n",
       "           2.3200872 ,  2.4245906 ],\n",
       "         [ 1.0636503 ,  1.4418532 ,  1.1551633 , ...,  2.489939  ,\n",
       "           2.8595028 ,  2.8983788 ],\n",
       "         [ 1.7212958 ,  1.4567899 ,  1.6195607 , ...,  2.2122319 ,\n",
       "           2.28775   ,  2.3680449 ]],\n",
       "\n",
       "        [[ 1.0073677 ,  0.9911809 ,  0.4985873 , ..., -0.78548455,\n",
       "          -0.468867  , -0.48161817],\n",
       "         [ 0.98616654,  0.7780646 ,  0.50405264, ...,  0.41252872,\n",
       "           0.12179888, -0.13391313],\n",
       "         [ 0.67029107,  0.50172013,  0.4808511 , ...,  1.2274828 ,\n",
       "           0.6355633 ,  1.0278362 ],\n",
       "         ...,\n",
       "         [ 0.8757177 ,  1.3182167 ,  1.1721811 , ...,  2.5754158 ,\n",
       "           2.118912  ,  2.1848004 ],\n",
       "         [ 1.0877335 ,  1.5199239 ,  0.99688333, ...,  2.5301988 ,\n",
       "           2.7364001 ,  2.768329  ],\n",
       "         [ 1.6906872 ,  1.5234073 ,  1.611475  , ...,  2.1823354 ,\n",
       "           2.243826  ,  2.2695339 ]],\n",
       "\n",
       "        [[ 0.508041  ,  0.5821548 ,  0.1491826 , ..., -1.3275528 ,\n",
       "          -0.9865133 , -0.8835642 ],\n",
       "         [ 0.67170393,  0.4951124 ,  0.3235854 , ..., -0.08619801,\n",
       "          -0.49589765, -0.72476095],\n",
       "         [ 0.5710169 ,  0.38835567,  0.39801618, ...,  0.9775782 ,\n",
       "           0.5404703 ,  0.6919967 ],\n",
       "         ...,\n",
       "         [ 0.65097916,  1.0560848 ,  0.7887768 , ...,  2.4869037 ,\n",
       "           2.0747986 ,  2.073656  ],\n",
       "         [ 1.0432525 ,  1.4454001 ,  0.78482443, ...,  2.4988616 ,\n",
       "           2.607407  ,  2.5544999 ],\n",
       "         [ 1.5854417 ,  1.4433268 ,  1.2234704 , ...,  2.0652318 ,\n",
       "           2.1641378 ,  2.1254005 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.70034134,  0.9547727 ,  0.9153288 , ...,  1.091371  ,\n",
       "           0.7018205 ,  1.0748413 ],\n",
       "         [ 0.4888539 ,  1.1762468 ,  1.3329039 , ...,  1.8408223 ,\n",
       "           1.3298681 ,  1.3097111 ],\n",
       "         [ 0.9862304 ,  1.579508  ,  1.5782657 , ...,  1.4435136 ,\n",
       "           1.7860277 ,  1.6612458 ],\n",
       "         ...,\n",
       "         [-0.61543965,  0.0106005 ,  0.1457372 , ...,  1.2201451 ,\n",
       "           1.3644809 ,  1.4034055 ],\n",
       "         [ 1.1431377 ,  0.6638969 ,  0.23378982, ...,  0.9938454 ,\n",
       "           1.3850793 ,  1.0691516 ],\n",
       "         [ 2.2842016 ,  1.5077722 , -0.09279341, ...,  0.93249345,\n",
       "           1.2790875 ,  1.3725072 ]],\n",
       "\n",
       "        [[ 0.47236323,  0.72127837,  0.45619267, ...,  0.82270324,\n",
       "           0.4580007 ,  0.8004402 ],\n",
       "         [ 0.5715142 ,  1.2872931 ,  1.3430691 , ...,  1.5601009 ,\n",
       "           1.0837805 ,  0.9001898 ],\n",
       "         [ 1.336429  ,  1.876855  ,  2.0374048 , ...,  1.3508455 ,\n",
       "           1.8881859 ,  1.6897153 ],\n",
       "         ...,\n",
       "         [-0.71289396,  0.41257912,  0.6686567 , ...,  1.1717373 ,\n",
       "           1.4107544 ,  1.3448795 ],\n",
       "         [ 1.304572  ,  0.8829155 ,  0.55312276, ...,  1.0816978 ,\n",
       "           1.5850843 ,  1.356293  ],\n",
       "         [ 2.599532  ,  1.6684345 , -0.00390746, ...,  0.8323841 ,\n",
       "           1.3578513 ,  1.3402768 ]],\n",
       "\n",
       "        [[ 0.40162632,  0.73377013,  0.4403963 , ...,  0.54226685,\n",
       "           0.16733967,  0.48278525],\n",
       "         [ 0.6641616 ,  1.4623272 ,  1.5046885 , ...,  1.2760824 ,\n",
       "           0.72763526,  0.4976023 ],\n",
       "         [ 1.6814953 ,  2.167573  ,  2.4430356 , ...,  1.1442178 ,\n",
       "           1.8221885 ,  1.6189034 ],\n",
       "         ...,\n",
       "         [-0.93782127,  0.3722686 ,  0.833304  , ...,  0.94000787,\n",
       "           1.2942857 ,  1.2150481 ],\n",
       "         [ 1.208697  ,  0.7896122 ,  0.5132715 , ...,  0.8612002 ,\n",
       "           1.5552503 ,  0.9468954 ],\n",
       "         [ 2.6000419 ,  1.429841  , -0.26363823, ...,  0.8599842 ,\n",
       "           1.2363102 ,  1.1699326 ]]],\n",
       "\n",
       "\n",
       "       [[[ 1.0073677 ,  0.9911809 ,  0.4985873 , ..., -0.78548455,\n",
       "          -0.468867  , -0.48161817],\n",
       "         [ 0.98616654,  0.7780646 ,  0.50405264, ...,  0.41252872,\n",
       "           0.12179888, -0.13391313],\n",
       "         [ 0.67029107,  0.50172013,  0.4808511 , ...,  1.2274828 ,\n",
       "           0.6355633 ,  1.0278362 ],\n",
       "         ...,\n",
       "         [ 0.8757177 ,  1.3182167 ,  1.1721811 , ...,  2.5754158 ,\n",
       "           2.118912  ,  2.1848004 ],\n",
       "         [ 1.0877335 ,  1.5199239 ,  0.99688333, ...,  2.5301988 ,\n",
       "           2.7364001 ,  2.768329  ],\n",
       "         [ 1.6906872 ,  1.5234073 ,  1.611475  , ...,  2.1823354 ,\n",
       "           2.243826  ,  2.2695339 ]],\n",
       "\n",
       "        [[ 0.508041  ,  0.5821548 ,  0.1491826 , ..., -1.3275528 ,\n",
       "          -0.9865133 , -0.8835642 ],\n",
       "         [ 0.67170393,  0.4951124 ,  0.3235854 , ..., -0.08619801,\n",
       "          -0.49589765, -0.72476095],\n",
       "         [ 0.5710169 ,  0.38835567,  0.39801618, ...,  0.9775782 ,\n",
       "           0.5404703 ,  0.6919967 ],\n",
       "         ...,\n",
       "         [ 0.65097916,  1.0560848 ,  0.7887768 , ...,  2.4869037 ,\n",
       "           2.0747986 ,  2.073656  ],\n",
       "         [ 1.0432525 ,  1.4454001 ,  0.78482443, ...,  2.4988616 ,\n",
       "           2.607407  ,  2.5544999 ],\n",
       "         [ 1.5854417 ,  1.4433268 ,  1.2234704 , ...,  2.0652318 ,\n",
       "           2.1641378 ,  2.1254005 ]],\n",
       "\n",
       "        [[ 0.19364023,  0.16981706, -0.1523782 , ..., -1.8228209 ,\n",
       "          -1.3503281 , -1.1766304 ],\n",
       "         [ 0.4299526 ,  0.2092577 ,  0.22642754, ..., -0.6004711 ,\n",
       "          -1.0847259 , -1.1419872 ],\n",
       "         [ 0.6494129 ,  0.41462764,  0.4791174 , ...,  0.6853284 ,\n",
       "           0.24430951,  0.31362045],\n",
       "         ...,\n",
       "         [ 0.7997284 ,  0.8917634 ,  0.39756945, ...,  2.1044996 ,\n",
       "           1.7479944 ,  1.8252748 ],\n",
       "         [ 1.1651704 ,  1.1648666 ,  0.45964295, ...,  2.2144268 ,\n",
       "           2.5064747 ,  2.3936448 ],\n",
       "         [ 1.4225763 ,  1.2505597 ,  0.84116924, ...,  1.9236319 ,\n",
       "           2.1020591 ,  2.0438101 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.47236323,  0.72127837,  0.45619267, ...,  0.82270324,\n",
       "           0.4580007 ,  0.8004402 ],\n",
       "         [ 0.5715142 ,  1.2872931 ,  1.3430691 , ...,  1.5601009 ,\n",
       "           1.0837805 ,  0.9001898 ],\n",
       "         [ 1.336429  ,  1.876855  ,  2.0374048 , ...,  1.3508455 ,\n",
       "           1.8881859 ,  1.6897153 ],\n",
       "         ...,\n",
       "         [-0.71289396,  0.41257912,  0.6686567 , ...,  1.1717373 ,\n",
       "           1.4107544 ,  1.3448795 ],\n",
       "         [ 1.304572  ,  0.8829155 ,  0.55312276, ...,  1.0816978 ,\n",
       "           1.5850843 ,  1.356293  ],\n",
       "         [ 2.599532  ,  1.6684345 , -0.00390746, ...,  0.8323841 ,\n",
       "           1.3578513 ,  1.3402768 ]],\n",
       "\n",
       "        [[ 0.40162632,  0.73377013,  0.4403963 , ...,  0.54226685,\n",
       "           0.16733967,  0.48278525],\n",
       "         [ 0.6641616 ,  1.4623272 ,  1.5046885 , ...,  1.2760824 ,\n",
       "           0.72763526,  0.4976023 ],\n",
       "         [ 1.6814953 ,  2.167573  ,  2.4430356 , ...,  1.1442178 ,\n",
       "           1.8221885 ,  1.6189034 ],\n",
       "         ...,\n",
       "         [-0.93782127,  0.3722686 ,  0.833304  , ...,  0.94000787,\n",
       "           1.2942857 ,  1.2150481 ],\n",
       "         [ 1.208697  ,  0.7896122 ,  0.5132715 , ...,  0.8612002 ,\n",
       "           1.5552503 ,  0.9468954 ],\n",
       "         [ 2.6000419 ,  1.429841  , -0.26363823, ...,  0.8599842 ,\n",
       "           1.2363102 ,  1.1699326 ]],\n",
       "\n",
       "        [[ 0.26196292,  0.5124657 ,  0.31413126, ...,  0.14089659,\n",
       "          -0.1600226 ,  0.15665966],\n",
       "         [ 0.66027665,  1.360105  ,  1.4213519 , ...,  0.93226475,\n",
       "           0.35487217,  0.11562636],\n",
       "         [ 1.837171  ,  2.4999602 ,  2.7772257 , ...,  1.0083396 ,\n",
       "           1.6296394 ,  1.4390591 ],\n",
       "         ...,\n",
       "         [-0.4225709 ,  0.750057  ,  0.7793196 , ...,  0.8344229 ,\n",
       "           1.0396608 ,  1.0153043 ],\n",
       "         [ 1.4657922 ,  0.8650145 ,  0.5639864 , ...,  0.8571089 ,\n",
       "           1.5010394 ,  0.9136061 ],\n",
       "         [ 2.9211009 ,  1.4014115 , -0.31420928, ...,  0.78046626,\n",
       "           1.2145818 ,  1.1869069 ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.508041  ,  0.5821548 ,  0.1491826 , ..., -1.3275528 ,\n",
       "          -0.9865133 , -0.8835642 ],\n",
       "         [ 0.67170393,  0.4951124 ,  0.3235854 , ..., -0.08619801,\n",
       "          -0.49589765, -0.72476095],\n",
       "         [ 0.5710169 ,  0.38835567,  0.39801618, ...,  0.9775782 ,\n",
       "           0.5404703 ,  0.6919967 ],\n",
       "         ...,\n",
       "         [ 0.65097916,  1.0560848 ,  0.7887768 , ...,  2.4869037 ,\n",
       "           2.0747986 ,  2.073656  ],\n",
       "         [ 1.0432525 ,  1.4454001 ,  0.78482443, ...,  2.4988616 ,\n",
       "           2.607407  ,  2.5544999 ],\n",
       "         [ 1.5854417 ,  1.4433268 ,  1.2234704 , ...,  2.0652318 ,\n",
       "           2.1641378 ,  2.1254005 ]],\n",
       "\n",
       "        [[ 0.19364023,  0.16981706, -0.1523782 , ..., -1.8228209 ,\n",
       "          -1.3503281 , -1.1766304 ],\n",
       "         [ 0.4299526 ,  0.2092577 ,  0.22642754, ..., -0.6004711 ,\n",
       "          -1.0847259 , -1.1419872 ],\n",
       "         [ 0.6494129 ,  0.41462764,  0.4791174 , ...,  0.6853284 ,\n",
       "           0.24430951,  0.31362045],\n",
       "         ...,\n",
       "         [ 0.7997284 ,  0.8917634 ,  0.39756945, ...,  2.1044996 ,\n",
       "           1.7479944 ,  1.8252748 ],\n",
       "         [ 1.1651704 ,  1.1648666 ,  0.45964295, ...,  2.2144268 ,\n",
       "           2.5064747 ,  2.3936448 ],\n",
       "         [ 1.4225763 ,  1.2505597 ,  0.84116924, ...,  1.9236319 ,\n",
       "           2.1020591 ,  2.0438101 ]],\n",
       "\n",
       "        [[ 0.42752326,  0.32731155, -0.01698755, ..., -2.27391   ,\n",
       "          -1.7189991 , -1.4954467 ],\n",
       "         [ 0.62331843,  0.34362507,  0.3427075 , ..., -1.1185154 ,\n",
       "          -1.5635662 , -1.5487063 ],\n",
       "         [ 0.8321454 ,  0.4979862 ,  0.5387601 , ...,  0.43077138,\n",
       "           0.01910488,  0.03264743],\n",
       "         ...,\n",
       "         [ 0.83607745,  0.80329955,  0.5519279 , ...,  1.9706329 ,\n",
       "           1.8221029 ,  2.0499287 ],\n",
       "         [ 1.0206984 ,  1.0847541 ,  0.47264215, ...,  2.1858912 ,\n",
       "           2.3727822 ,  2.3144114 ],\n",
       "         [ 1.0881684 ,  0.8966607 ,  0.4242183 , ...,  1.8305855 ,\n",
       "           2.0264695 ,  1.9136915 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.40162632,  0.73377013,  0.4403963 , ...,  0.54226685,\n",
       "           0.16733967,  0.48278525],\n",
       "         [ 0.6641616 ,  1.4623272 ,  1.5046885 , ...,  1.2760824 ,\n",
       "           0.72763526,  0.4976023 ],\n",
       "         [ 1.6814953 ,  2.167573  ,  2.4430356 , ...,  1.1442178 ,\n",
       "           1.8221885 ,  1.6189034 ],\n",
       "         ...,\n",
       "         [-0.93782127,  0.3722686 ,  0.833304  , ...,  0.94000787,\n",
       "           1.2942857 ,  1.2150481 ],\n",
       "         [ 1.208697  ,  0.7896122 ,  0.5132715 , ...,  0.8612002 ,\n",
       "           1.5552503 ,  0.9468954 ],\n",
       "         [ 2.6000419 ,  1.429841  , -0.26363823, ...,  0.8599842 ,\n",
       "           1.2363102 ,  1.1699326 ]],\n",
       "\n",
       "        [[ 0.26196292,  0.5124657 ,  0.31413126, ...,  0.14089659,\n",
       "          -0.1600226 ,  0.15665966],\n",
       "         [ 0.66027665,  1.360105  ,  1.4213519 , ...,  0.93226475,\n",
       "           0.35487217,  0.11562636],\n",
       "         [ 1.837171  ,  2.4999602 ,  2.7772257 , ...,  1.0083396 ,\n",
       "           1.6296394 ,  1.4390591 ],\n",
       "         ...,\n",
       "         [-0.4225709 ,  0.750057  ,  0.7793196 , ...,  0.8344229 ,\n",
       "           1.0396608 ,  1.0153043 ],\n",
       "         [ 1.4657922 ,  0.8650145 ,  0.5639864 , ...,  0.8571089 ,\n",
       "           1.5010394 ,  0.9136061 ],\n",
       "         [ 2.9211009 ,  1.4014115 , -0.31420928, ...,  0.78046626,\n",
       "           1.2145818 ,  1.1869069 ]],\n",
       "\n",
       "        [[ 0.20157848,  0.4420023 ,  0.11819386, ..., -0.08976054,\n",
       "          -0.38081098, -0.13370612],\n",
       "         [ 0.6418804 ,  1.1360885 ,  1.1560873 , ...,  0.5561986 ,\n",
       "          -0.03371998, -0.103088  ],\n",
       "         [ 1.8366168 ,  2.3817947 ,  2.7101753 , ...,  0.8532404 ,\n",
       "           1.1913111 ,  1.0983983 ],\n",
       "         ...,\n",
       "         [-0.14444453,  1.0560288 ,  1.0537266 , ...,  0.49084634,\n",
       "           0.95069665,  0.9882844 ],\n",
       "         [ 1.4934555 ,  0.86099696,  0.8280824 , ...,  0.7866972 ,\n",
       "           1.5490162 ,  1.0054905 ],\n",
       "         [ 3.1716354 ,  1.558469  ,  0.00568835, ...,  0.6508757 ,\n",
       "           1.1706222 ,  1.1051866 ]]]], dtype=float32)</pre></li></ul></div></li><li class='xr-section-item'><input id='section-bd5e3e62-e2d2-4b77-8843-80096417c18f' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-bd5e3e62-e2d2-4b77-8843-80096417c18f' class='xr-section-summary'  title='Expand/collapse section'>Attributes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:    (lag: 60, lat: 23, lon: 23, time: 3440)\n",
       "Coordinates:\n",
       "  * lag        (lag) int64 0 1 2 3 4 5 6 7 8 9 ... 50 51 52 53 54 55 56 57 58 59\n",
       "  * lat        (lat) float32 30.0 28.0 26.0 24.0 22.0 ... -8.0 -10.0 -12.0 -14.0\n",
       "  * lon        (lon) float32 16.0 18.0 20.0 22.0 24.0 ... 54.0 56.0 58.0 60.0\n",
       "    dayofyear  (time, lag) int64 198 199 200 201 202 203 ... 339 340 341 342 343\n",
       "  * time       (time) datetime64[ns] 1981-10-04 1981-10-05 ... 2020-12-28\n",
       "Data variables:\n",
       "    t2m        (time, lag, lat, lon) float32 -0.8605126 ... 1.1051866"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_vars[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<title>Show/Hide data repr</title>\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<title>Show/Hide attributes</title>\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt, dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2 {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><div class='xr-wrap'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-98937536-106d-42d3-aa0a-c965e0cb0ca6' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-98937536-106d-42d3-aa0a-c965e0cb0ca6' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span class='xr-has-index'>lag</span>: 60</li><li><span class='xr-has-index'>lat</span>: 61</li><li><span class='xr-has-index'>lon</span>: 180</li><li><span class='xr-has-index'>time</span>: 3440</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-a2b3bb8e-cd17-4333-be91-986827658cc9' class='xr-section-summary-in' type='checkbox'  checked><label for='section-a2b3bb8e-cd17-4333-be91-986827658cc9' class='xr-section-summary' >Coordinates: <span>(5)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>lag</span></div><div class='xr-var-dims'>(lag)</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>0 1 2 3 4 5 6 ... 54 55 56 57 58 59</div><input id='attrs-610045b0-fa51-414f-9ae8-5d974fcfd886' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-610045b0-fa51-414f-9ae8-5d974fcfd886' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-4f116e69-e487-473f-b9fd-bf24f487ea39' class='xr-var-data-in' type='checkbox'><label for='data-4f116e69-e487-473f-b9fd-bf24f487ea39' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><pre class='xr-var-data'>array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
       "       54, 55, 56, 57, 58, 59])</pre></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>lat</span></div><div class='xr-var-dims'>(lat)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>60.0 58.0 56.0 ... -58.0 -60.0</div><input id='attrs-2732f9c2-20d5-4edd-a453-40f9831fd16e' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-2732f9c2-20d5-4edd-a453-40f9831fd16e' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-f94081d7-5e33-4948-ae04-e15184c4ea62' class='xr-var-data-in' type='checkbox'><label for='data-f94081d7-5e33-4948-ae04-e15184c4ea62' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>standard_name :</span></dt><dd>latitude</dd><dt><span>long_name :</span></dt><dd>latitude</dd><dt><span>units :</span></dt><dd>degrees_north</dd><dt><span>axis :</span></dt><dd>Y</dd></dl></div><pre class='xr-var-data'>array([ 60.,  58.,  56.,  54.,  52.,  50.,  48.,  46.,  44.,  42.,  40.,  38.,\n",
       "        36.,  34.,  32.,  30.,  28.,  26.,  24.,  22.,  20.,  18.,  16.,  14.,\n",
       "        12.,  10.,   8.,   6.,   4.,   2.,   0.,  -2.,  -4.,  -6.,  -8., -10.,\n",
       "       -12., -14., -16., -18., -20., -22., -24., -26., -28., -30., -32., -34.,\n",
       "       -36., -38., -40., -42., -44., -46., -48., -50., -52., -54., -56., -58.,\n",
       "       -60.], dtype=float32)</pre></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>lon</span></div><div class='xr-var-dims'>(lon)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>-180.0 -178.0 ... 176.0 178.0</div><input id='attrs-76dbf4bc-4bc7-494b-923f-c8f1afee1416' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-76dbf4bc-4bc7-494b-923f-c8f1afee1416' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-85ca2ec9-f641-4fa2-b782-fb1f1d6e988d' class='xr-var-data-in' type='checkbox'><label for='data-85ca2ec9-f641-4fa2-b782-fb1f1d6e988d' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>standard_name :</span></dt><dd>longitude</dd><dt><span>long_name :</span></dt><dd>longitude</dd><dt><span>units :</span></dt><dd>degrees_east</dd><dt><span>axis :</span></dt><dd>X</dd></dl></div><pre class='xr-var-data'>array([-180., -178., -176., -174., -172., -170., -168., -166., -164., -162.,\n",
       "       -160., -158., -156., -154., -152., -150., -148., -146., -144., -142.,\n",
       "       -140., -138., -136., -134., -132., -130., -128., -126., -124., -122.,\n",
       "       -120., -118., -116., -114., -112., -110., -108., -106., -104., -102.,\n",
       "       -100.,  -98.,  -96.,  -94.,  -92.,  -90.,  -88.,  -86.,  -84.,  -82.,\n",
       "        -80.,  -78.,  -76.,  -74.,  -72.,  -70.,  -68.,  -66.,  -64.,  -62.,\n",
       "        -60.,  -58.,  -56.,  -54.,  -52.,  -50.,  -48.,  -46.,  -44.,  -42.,\n",
       "        -40.,  -38.,  -36.,  -34.,  -32.,  -30.,  -28.,  -26.,  -24.,  -22.,\n",
       "        -20.,  -18.,  -16.,  -14.,  -12.,  -10.,   -8.,   -6.,   -4.,   -2.,\n",
       "          0.,    2.,    4.,    6.,    8.,   10.,   12.,   14.,   16.,   18.,\n",
       "         20.,   22.,   24.,   26.,   28.,   30.,   32.,   34.,   36.,   38.,\n",
       "         40.,   42.,   44.,   46.,   48.,   50.,   52.,   54.,   56.,   58.,\n",
       "         60.,   62.,   64.,   66.,   68.,   70.,   72.,   74.,   76.,   78.,\n",
       "         80.,   82.,   84.,   86.,   88.,   90.,   92.,   94.,   96.,   98.,\n",
       "        100.,  102.,  104.,  106.,  108.,  110.,  112.,  114.,  116.,  118.,\n",
       "        120.,  122.,  124.,  126.,  128.,  130.,  132.,  134.,  136.,  138.,\n",
       "        140.,  142.,  144.,  146.,  148.,  150.,  152.,  154.,  156.,  158.,\n",
       "        160.,  162.,  164.,  166.,  168.,  170.,  172.,  174.,  176.,  178.],\n",
       "      dtype=float32)</pre></li><li class='xr-var-item'><div class='xr-var-name'><span>dayofyear</span></div><div class='xr-var-dims'>(time, lag)</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>198 199 200 201 ... 340 341 342 343</div><input id='attrs-f1c0fa87-d74b-4d42-a287-dca5325f0081' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-f1c0fa87-d74b-4d42-a287-dca5325f0081' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-a183a888-0c87-40c1-b798-4cb0c2fff883' class='xr-var-data-in' type='checkbox'><label for='data-a183a888-0c87-40c1-b798-4cb0c2fff883' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><pre class='xr-var-data'>array([[198, 199, 200, ..., 255, 256, 257],\n",
       "       [199, 200, 201, ..., 256, 257, 258],\n",
       "       [200, 201, 202, ..., 257, 258, 259],\n",
       "       ...,\n",
       "       [282, 283, 284, ..., 339, 340, 341],\n",
       "       [283, 284, 285, ..., 340, 341, 342],\n",
       "       [284, 285, 286, ..., 341, 342, 343]])</pre></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>time</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>1981-10-04 ... 2020-12-28</div><input id='attrs-ab984c9e-b143-4534-ba19-382f799ab365' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-ab984c9e-b143-4534-ba19-382f799ab365' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-cad018ef-0c5b-482d-bad4-c31390a3f44f' class='xr-var-data-in' type='checkbox'><label for='data-cad018ef-0c5b-482d-bad4-c31390a3f44f' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><pre class='xr-var-data'>array([&#x27;1981-10-04T00:00:00.000000000&#x27;, &#x27;1981-10-05T00:00:00.000000000&#x27;,\n",
       "       &#x27;1981-10-06T00:00:00.000000000&#x27;, ..., &#x27;2020-12-26T00:00:00.000000000&#x27;,\n",
       "       &#x27;2020-12-27T00:00:00.000000000&#x27;, &#x27;2020-12-28T00:00:00.000000000&#x27;],\n",
       "      dtype=&#x27;datetime64[ns]&#x27;)</pre></li></ul></div></li><li class='xr-section-item'><input id='section-1cf2964d-d620-48e4-ab99-9fb7a1967515' class='xr-section-summary-in' type='checkbox'  checked><label for='section-1cf2964d-d620-48e4-ab99-9fb7a1967515' class='xr-section-summary' >Data variables: <span>(1)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>sst</span></div><div class='xr-var-dims'>(time, lag, lat, lon)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>-0.36218587 ... 0.24122316</div><input id='attrs-1ca0eb8e-4a43-4807-be8c-2385e59eee0b' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-1ca0eb8e-4a43-4807-be8c-2385e59eee0b' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-b3cd08ac-1496-48f1-a58b-8d37da2849eb' class='xr-var-data-in' type='checkbox'><label for='data-b3cd08ac-1496-48f1-a58b-8d37da2849eb' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><pre class='xr-var-data'>array([[[[-3.62185866e-01,  3.10576499e-01,  5.07928073e-01, ...,\n",
       "           3.67879808e-01,  3.25133681e-01, -3.39804977e-01],\n",
       "         [ 4.65958118e-02,  4.00977582e-01,  7.80841947e-01, ...,\n",
       "           6.71721101e-01,  6.03961468e-01,  5.84028900e-01],\n",
       "         [ 6.76504612e-01,  8.65939558e-01,  7.75304794e-01, ...,\n",
       "           7.52957523e-01,  6.04648471e-01,  6.45179451e-01],\n",
       "         ...,\n",
       "         [ 1.31917195e-02,  6.20493054e-01,  4.20646071e-01, ...,\n",
       "           2.08507404e-01,  3.40076119e-01,  5.22748530e-01],\n",
       "         [ 7.42579639e-01, -1.36416182e-01, -2.30123685e-03, ...,\n",
       "           1.64106369e+00,  1.28742111e+00,  1.18877852e+00],\n",
       "         [ 8.97619605e-01,  1.49698973e+00,  1.96731377e+00, ...,\n",
       "           1.83101034e+00,  1.78363490e+00,  1.56304681e+00]],\n",
       "\n",
       "        [[-3.54809254e-01,  3.49349380e-01,  5.40288150e-01, ...,\n",
       "           4.48003441e-01,  3.88327032e-01, -2.97131151e-01],\n",
       "         [ 1.12070717e-01,  4.23052967e-01,  7.83311009e-01, ...,\n",
       "           6.91575646e-01,  6.13563299e-01,  6.29878521e-01],\n",
       "         [ 6.93953097e-01,  8.70202363e-01,  7.59072006e-01, ...,\n",
       "           7.67725289e-01,  6.23503089e-01,  7.11687446e-01],\n",
       "         ...,\n",
       "         [ 1.20163308e-02,  6.30487025e-01,  3.02919269e-01, ...,\n",
       "           2.67718613e-01,  4.01345760e-01,  6.17449880e-01],\n",
       "         [ 8.62130046e-01, -1.11007534e-01,  1.92449056e-02, ...,\n",
       "           1.84401584e+00,  1.44534183e+00,  1.35463166e+00],\n",
       "         [ 1.04553270e+00,  1.76997089e+00,  2.24499035e+00, ...,\n",
       "           2.14317679e+00,  2.01471210e+00,  1.88421190e+00]],\n",
       "\n",
       "        [[-3.42830747e-01,  3.73506010e-01,  5.46182752e-01, ...,\n",
       "           5.41157544e-01,  4.60779071e-01, -2.58643657e-01],\n",
       "         [ 2.18399197e-01,  4.72794622e-01,  8.00716877e-01, ...,\n",
       "           7.39316642e-01,  6.69109285e-01,  7.03879774e-01],\n",
       "         [ 7.57432997e-01,  8.98841143e-01,  7.73531616e-01, ...,\n",
       "           7.94056714e-01,  6.58193588e-01,  8.05383742e-01],\n",
       "         ...,\n",
       "         [ 5.83060049e-02,  7.13439226e-01,  2.30205163e-01, ...,\n",
       "           4.42897409e-01,  5.26245713e-01,  7.75482595e-01],\n",
       "         [ 1.01246369e+00, -4.89043035e-02,  1.08723655e-01, ...,\n",
       "           2.04081988e+00,  1.61541104e+00,  1.56025541e+00],\n",
       "         [ 1.21782935e+00,  2.03469133e+00,  2.52169538e+00, ...,\n",
       "           2.40121460e+00,  2.16749620e+00,  2.22528529e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.89238697e-01, -2.24710032e-01,  4.23177838e-01, ...,\n",
       "          -2.35135213e-01, -1.98847502e-01, -3.14200193e-01],\n",
       "         [ 3.35881382e-01,  4.55341727e-01,  4.81544733e-01, ...,\n",
       "           8.99040818e-01,  6.78904057e-01,  6.03037179e-01],\n",
       "         [ 1.41179430e+00,  1.43971336e+00,  1.41443598e+00, ...,\n",
       "           8.59741509e-01,  1.05343568e+00,  1.29795957e+00],\n",
       "         ...,\n",
       "         [ 1.27684617e+00,  1.69404328e+00,  1.67089903e+00, ...,\n",
       "           5.05574465e-01,  4.94341224e-01,  1.59012341e+00],\n",
       "         [ 2.06658125e+00,  2.64937115e+00,  1.55695510e+00, ...,\n",
       "           8.03708851e-01,  1.64456725e+00,  2.15517974e+00],\n",
       "         [ 2.53595042e+00,  2.53145838e+00,  3.37910271e+00, ...,\n",
       "           1.87308347e+00,  2.45421982e+00,  2.49602628e+00]],\n",
       "\n",
       "        [[-1.16541468e-01, -1.73735186e-01,  4.50839847e-01, ...,\n",
       "          -8.77307653e-02, -9.97462720e-02, -2.07597807e-01],\n",
       "         [ 4.29405212e-01,  5.50040126e-01,  5.59651911e-01, ...,\n",
       "           1.00286484e+00,  7.37543464e-01,  6.60762370e-01],\n",
       "         [ 1.45638251e+00,  1.49776351e+00,  1.49852157e+00, ...,\n",
       "           8.79310310e-01,  1.03144550e+00,  1.30704069e+00],\n",
       "         ...,\n",
       "         [ 1.36080384e+00,  1.86275506e+00,  1.77320147e+00, ...,\n",
       "           6.41663551e-01,  6.36572957e-01,  1.77160847e+00],\n",
       "         [ 2.22900081e+00,  2.79405284e+00,  1.68586099e+00, ...,\n",
       "           1.01761854e+00,  1.75572109e+00,  2.24060678e+00],\n",
       "         [ 2.58279824e+00,  2.63683200e+00,  3.49761844e+00, ...,\n",
       "           1.98000157e+00,  2.48671365e+00,  2.54368401e+00]],\n",
       "\n",
       "        [[-3.26002799e-02, -8.74749795e-02,  4.84744817e-01, ...,\n",
       "           4.02493887e-02, -9.68386699e-03, -1.01645648e-01],\n",
       "         [ 5.36003292e-01,  6.56846881e-01,  6.14730954e-01, ...,\n",
       "           1.09287715e+00,  7.96037853e-01,  7.29901910e-01],\n",
       "         [ 1.52206004e+00,  1.55389607e+00,  1.54789031e+00, ...,\n",
       "           8.96352530e-01,  1.01808619e+00,  1.33967614e+00],\n",
       "         ...,\n",
       "         [ 1.46165609e+00,  2.04387426e+00,  1.88087571e+00, ...,\n",
       "           7.96604455e-01,  8.02872181e-01,  2.04422998e+00],\n",
       "         [ 2.41753268e+00,  2.90234756e+00,  1.78626764e+00, ...,\n",
       "           1.23611224e+00,  1.93281913e+00,  2.38242960e+00],\n",
       "         [ 2.69791126e+00,  2.82957363e+00,  3.56921649e+00, ...,\n",
       "           2.11037993e+00,  2.56703997e+00,  2.65462637e+00]]],\n",
       "\n",
       "\n",
       "       [[[-3.54809254e-01,  3.49349380e-01,  5.40288150e-01, ...,\n",
       "           4.48003441e-01,  3.88327032e-01, -2.97131151e-01],\n",
       "         [ 1.12070717e-01,  4.23052967e-01,  7.83311009e-01, ...,\n",
       "           6.91575646e-01,  6.13563299e-01,  6.29878521e-01],\n",
       "         [ 6.93953097e-01,  8.70202363e-01,  7.59072006e-01, ...,\n",
       "           7.67725289e-01,  6.23503089e-01,  7.11687446e-01],\n",
       "         ...,\n",
       "         [ 1.20163308e-02,  6.30487025e-01,  3.02919269e-01, ...,\n",
       "           2.67718613e-01,  4.01345760e-01,  6.17449880e-01],\n",
       "         [ 8.62130046e-01, -1.11007534e-01,  1.92449056e-02, ...,\n",
       "           1.84401584e+00,  1.44534183e+00,  1.35463166e+00],\n",
       "         [ 1.04553270e+00,  1.76997089e+00,  2.24499035e+00, ...,\n",
       "           2.14317679e+00,  2.01471210e+00,  1.88421190e+00]],\n",
       "\n",
       "        [[-3.42830747e-01,  3.73506010e-01,  5.46182752e-01, ...,\n",
       "           5.41157544e-01,  4.60779071e-01, -2.58643657e-01],\n",
       "         [ 2.18399197e-01,  4.72794622e-01,  8.00716877e-01, ...,\n",
       "           7.39316642e-01,  6.69109285e-01,  7.03879774e-01],\n",
       "         [ 7.57432997e-01,  8.98841143e-01,  7.73531616e-01, ...,\n",
       "           7.94056714e-01,  6.58193588e-01,  8.05383742e-01],\n",
       "         ...,\n",
       "         [ 5.83060049e-02,  7.13439226e-01,  2.30205163e-01, ...,\n",
       "           4.42897409e-01,  5.26245713e-01,  7.75482595e-01],\n",
       "         [ 1.01246369e+00, -4.89043035e-02,  1.08723655e-01, ...,\n",
       "           2.04081988e+00,  1.61541104e+00,  1.56025541e+00],\n",
       "         [ 1.21782935e+00,  2.03469133e+00,  2.52169538e+00, ...,\n",
       "           2.40121460e+00,  2.16749620e+00,  2.22528529e+00]],\n",
       "\n",
       "        [[-3.26204777e-01,  3.98652762e-01,  5.40608764e-01, ...,\n",
       "           6.32930338e-01,  5.33978820e-01, -2.21612617e-01],\n",
       "         [ 3.65119278e-01,  5.50025880e-01,  8.39799523e-01, ...,\n",
       "           8.31931174e-01,  7.65980840e-01,  8.07559252e-01],\n",
       "         [ 8.80454957e-01,  9.60588992e-01,  8.33143771e-01, ...,\n",
       "           8.42785537e-01,  7.09306180e-01,  9.37528551e-01],\n",
       "         ...,\n",
       "         [ 1.19364597e-01,  8.23661089e-01,  1.78819329e-01, ...,\n",
       "           6.24326468e-01,  6.55635536e-01,  9.09860432e-01],\n",
       "         [ 1.14875698e+00,  7.80493859e-03,  2.17258990e-01, ...,\n",
       "           2.08862042e+00,  1.72013092e+00,  1.77909744e+00],\n",
       "         [ 1.38127649e+00,  2.26480556e+00,  2.75266623e+00, ...,\n",
       "           2.54446363e+00,  2.24002504e+00,  2.46532941e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.16541468e-01, -1.73735186e-01,  4.50839847e-01, ...,\n",
       "          -8.77307653e-02, -9.97462720e-02, -2.07597807e-01],\n",
       "         [ 4.29405212e-01,  5.50040126e-01,  5.59651911e-01, ...,\n",
       "           1.00286484e+00,  7.37543464e-01,  6.60762370e-01],\n",
       "         [ 1.45638251e+00,  1.49776351e+00,  1.49852157e+00, ...,\n",
       "           8.79310310e-01,  1.03144550e+00,  1.30704069e+00],\n",
       "         ...,\n",
       "         [ 1.36080384e+00,  1.86275506e+00,  1.77320147e+00, ...,\n",
       "           6.41663551e-01,  6.36572957e-01,  1.77160847e+00],\n",
       "         [ 2.22900081e+00,  2.79405284e+00,  1.68586099e+00, ...,\n",
       "           1.01761854e+00,  1.75572109e+00,  2.24060678e+00],\n",
       "         [ 2.58279824e+00,  2.63683200e+00,  3.49761844e+00, ...,\n",
       "           1.98000157e+00,  2.48671365e+00,  2.54368401e+00]],\n",
       "\n",
       "        [[-3.26002799e-02, -8.74749795e-02,  4.84744817e-01, ...,\n",
       "           4.02493887e-02, -9.68386699e-03, -1.01645648e-01],\n",
       "         [ 5.36003292e-01,  6.56846881e-01,  6.14730954e-01, ...,\n",
       "           1.09287715e+00,  7.96037853e-01,  7.29901910e-01],\n",
       "         [ 1.52206004e+00,  1.55389607e+00,  1.54789031e+00, ...,\n",
       "           8.96352530e-01,  1.01808619e+00,  1.33967614e+00],\n",
       "         ...,\n",
       "         [ 1.46165609e+00,  2.04387426e+00,  1.88087571e+00, ...,\n",
       "           7.96604455e-01,  8.02872181e-01,  2.04422998e+00],\n",
       "         [ 2.41753268e+00,  2.90234756e+00,  1.78626764e+00, ...,\n",
       "           1.23611224e+00,  1.93281913e+00,  2.38242960e+00],\n",
       "         [ 2.69791126e+00,  2.82957363e+00,  3.56921649e+00, ...,\n",
       "           2.11037993e+00,  2.56703997e+00,  2.65462637e+00]],\n",
       "\n",
       "        [[ 4.49403934e-02,  1.81330070e-02,  5.23392737e-01, ...,\n",
       "           1.25443235e-01,  5.04921451e-02, -1.90902688e-02],\n",
       "         [ 6.35598183e-01,  7.46625066e-01,  6.27017081e-01, ...,\n",
       "           1.13776195e+00,  8.42199326e-01,  8.01237345e-01],\n",
       "         [ 1.59554482e+00,  1.58711433e+00,  1.54427624e+00, ...,\n",
       "           9.21095014e-01,  1.02062726e+00,  1.39626312e+00],\n",
       "         ...,\n",
       "         [ 1.56243324e+00,  2.16065836e+00,  1.95397115e+00, ...,\n",
       "           9.62666512e-01,  1.00572073e+00,  2.25374556e+00],\n",
       "         [ 2.55276108e+00,  2.96053052e+00,  1.84011018e+00, ...,\n",
       "           1.42001033e+00,  2.04855180e+00,  2.50614238e+00],\n",
       "         [ 2.77681518e+00,  2.97497487e+00,  3.61599922e+00, ...,\n",
       "           2.20910692e+00,  2.63238931e+00,  2.78463411e+00]]],\n",
       "\n",
       "\n",
       "       [[[-3.42830747e-01,  3.73506010e-01,  5.46182752e-01, ...,\n",
       "           5.41157544e-01,  4.60779071e-01, -2.58643657e-01],\n",
       "         [ 2.18399197e-01,  4.72794622e-01,  8.00716877e-01, ...,\n",
       "           7.39316642e-01,  6.69109285e-01,  7.03879774e-01],\n",
       "         [ 7.57432997e-01,  8.98841143e-01,  7.73531616e-01, ...,\n",
       "           7.94056714e-01,  6.58193588e-01,  8.05383742e-01],\n",
       "         ...,\n",
       "         [ 5.83060049e-02,  7.13439226e-01,  2.30205163e-01, ...,\n",
       "           4.42897409e-01,  5.26245713e-01,  7.75482595e-01],\n",
       "         [ 1.01246369e+00, -4.89043035e-02,  1.08723655e-01, ...,\n",
       "           2.04081988e+00,  1.61541104e+00,  1.56025541e+00],\n",
       "         [ 1.21782935e+00,  2.03469133e+00,  2.52169538e+00, ...,\n",
       "           2.40121460e+00,  2.16749620e+00,  2.22528529e+00]],\n",
       "\n",
       "        [[-3.26204777e-01,  3.98652762e-01,  5.40608764e-01, ...,\n",
       "           6.32930338e-01,  5.33978820e-01, -2.21612617e-01],\n",
       "         [ 3.65119278e-01,  5.50025880e-01,  8.39799523e-01, ...,\n",
       "           8.31931174e-01,  7.65980840e-01,  8.07559252e-01],\n",
       "         [ 8.80454957e-01,  9.60588992e-01,  8.33143771e-01, ...,\n",
       "           8.42785537e-01,  7.09306180e-01,  9.37528551e-01],\n",
       "         ...,\n",
       "         [ 1.19364597e-01,  8.23661089e-01,  1.78819329e-01, ...,\n",
       "           6.24326468e-01,  6.55635536e-01,  9.09860432e-01],\n",
       "         [ 1.14875698e+00,  7.80493859e-03,  2.17258990e-01, ...,\n",
       "           2.08862042e+00,  1.72013092e+00,  1.77909744e+00],\n",
       "         [ 1.38127649e+00,  2.26480556e+00,  2.75266623e+00, ...,\n",
       "           2.54446363e+00,  2.24002504e+00,  2.46532941e+00]],\n",
       "\n",
       "        [[-3.08468640e-01,  4.05654550e-01,  5.19501269e-01, ...,\n",
       "           7.07557023e-01,  5.88403106e-01, -2.03750372e-01],\n",
       "         [ 4.98608351e-01,  6.14696443e-01,  8.63741398e-01, ...,\n",
       "           9.28709745e-01,  8.61032724e-01,  8.76570404e-01],\n",
       "         [ 9.85637367e-01,  9.87466514e-01,  8.80683005e-01, ...,\n",
       "           8.93809915e-01,  7.52661824e-01,  1.04538929e+00],\n",
       "         ...,\n",
       "         [ 1.71227351e-01,  9.16177809e-01,  1.26424253e-01, ...,\n",
       "           7.76435375e-01,  7.79946446e-01,  9.92677808e-01],\n",
       "         [ 1.21839845e+00,  4.08345237e-02,  3.07571560e-01, ...,\n",
       "           2.04105210e+00,  1.81836522e+00,  1.91364408e+00],\n",
       "         [ 1.47358215e+00,  2.38321900e+00,  2.88763857e+00, ...,\n",
       "           2.53974175e+00,  2.20182514e+00,  2.53175974e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-3.26002799e-02, -8.74749795e-02,  4.84744817e-01, ...,\n",
       "           4.02493887e-02, -9.68386699e-03, -1.01645648e-01],\n",
       "         [ 5.36003292e-01,  6.56846881e-01,  6.14730954e-01, ...,\n",
       "           1.09287715e+00,  7.96037853e-01,  7.29901910e-01],\n",
       "         [ 1.52206004e+00,  1.55389607e+00,  1.54789031e+00, ...,\n",
       "           8.96352530e-01,  1.01808619e+00,  1.33967614e+00],\n",
       "         ...,\n",
       "         [ 1.46165609e+00,  2.04387426e+00,  1.88087571e+00, ...,\n",
       "           7.96604455e-01,  8.02872181e-01,  2.04422998e+00],\n",
       "         [ 2.41753268e+00,  2.90234756e+00,  1.78626764e+00, ...,\n",
       "           1.23611224e+00,  1.93281913e+00,  2.38242960e+00],\n",
       "         [ 2.69791126e+00,  2.82957363e+00,  3.56921649e+00, ...,\n",
       "           2.11037993e+00,  2.56703997e+00,  2.65462637e+00]],\n",
       "\n",
       "        [[ 4.49403934e-02,  1.81330070e-02,  5.23392737e-01, ...,\n",
       "           1.25443235e-01,  5.04921451e-02, -1.90902688e-02],\n",
       "         [ 6.35598183e-01,  7.46625066e-01,  6.27017081e-01, ...,\n",
       "           1.13776195e+00,  8.42199326e-01,  8.01237345e-01],\n",
       "         [ 1.59554482e+00,  1.58711433e+00,  1.54427624e+00, ...,\n",
       "           9.21095014e-01,  1.02062726e+00,  1.39626312e+00],\n",
       "         ...,\n",
       "         [ 1.56243324e+00,  2.16065836e+00,  1.95397115e+00, ...,\n",
       "           9.62666512e-01,  1.00572073e+00,  2.25374556e+00],\n",
       "         [ 2.55276108e+00,  2.96053052e+00,  1.84011018e+00, ...,\n",
       "           1.42001033e+00,  2.04855180e+00,  2.50614238e+00],\n",
       "         [ 2.77681518e+00,  2.97497487e+00,  3.61599922e+00, ...,\n",
       "           2.20910692e+00,  2.63238931e+00,  2.78463411e+00]],\n",
       "\n",
       "        [[ 1.17082015e-01,  1.43522307e-01,  5.89896500e-01, ...,\n",
       "           1.69830620e-01,  8.04324448e-02,  3.56611572e-02],\n",
       "         [ 7.34560013e-01,  8.29773724e-01,  6.28765166e-01, ...,\n",
       "           1.16204154e+00,  9.01340246e-01,  8.89847398e-01],\n",
       "         [ 1.71286476e+00,  1.62981319e+00,  1.53560567e+00, ...,\n",
       "           9.72728968e-01,  1.07526243e+00,  1.51131082e+00],\n",
       "         ...,\n",
       "         [ 1.59942853e+00,  2.23330235e+00,  1.92820919e+00, ...,\n",
       "           1.16506243e+00,  1.20026207e+00,  2.42853570e+00],\n",
       "         [ 2.60840893e+00,  2.97143364e+00,  1.82785559e+00, ...,\n",
       "           1.57493508e+00,  2.10826993e+00,  2.56952000e+00],\n",
       "         [ 2.83913946e+00,  3.04170346e+00,  3.59176111e+00, ...,\n",
       "           2.31427431e+00,  2.73937464e+00,  2.92112494e+00]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ 1.03308702e+00,  1.69784164e+00,  1.68024600e+00, ...,\n",
       "           6.35935068e-01,  8.57552111e-01,  1.30080700e+00],\n",
       "         [ 1.29060650e+00,  1.43619072e+00,  1.40601134e+00, ...,\n",
       "           8.65531027e-01,  8.76721501e-01,  1.23686123e+00],\n",
       "         [ 6.98989391e-01,  1.00895357e+00,  1.08209515e+00, ...,\n",
       "          -1.16583623e-01,  2.43505985e-01,  5.70319533e-01],\n",
       "         ...,\n",
       "         [-1.14689499e-01, -1.88133463e-01, -9.32067409e-02, ...,\n",
       "           8.15775543e-02,  7.64952600e-02, -2.72771567e-01],\n",
       "         [-1.45107293e+00, -1.73756886e+00, -4.24777597e-01, ...,\n",
       "           4.15101320e-01, -3.84063482e-01, -6.70521140e-01],\n",
       "         [ 7.11864293e-01, -1.77370000e+00, -1.29621482e+00, ...,\n",
       "           3.88957143e-01, -4.04194921e-01,  5.50590813e-01]],\n",
       "\n",
       "        [[ 1.01933658e+00,  1.69587147e+00,  1.70706797e+00, ...,\n",
       "           6.84034586e-01,  9.30006027e-01,  1.37139559e+00],\n",
       "         [ 1.28319097e+00,  1.42629254e+00,  1.42786157e+00, ...,\n",
       "           8.87850165e-01,  9.03963208e-01,  1.23649764e+00],\n",
       "         [ 5.90400338e-01,  8.81069243e-01,  1.00919712e+00, ...,\n",
       "          -1.79806709e-01,  1.65868983e-01,  5.32285929e-01],\n",
       "         ...,\n",
       "         [-3.78004042e-03, -1.53568268e-01,  9.46630090e-02, ...,\n",
       "           4.11532335e-02, -5.01118042e-02, -2.38398820e-01],\n",
       "         [-1.28618920e+00, -1.71408999e+00, -3.60433400e-01, ...,\n",
       "           3.44249457e-01, -6.56080723e-01, -6.22785509e-01],\n",
       "         [ 7.28772700e-01, -1.78303683e+00, -1.38569200e+00, ...,\n",
       "           3.66918474e-01, -5.06005168e-01,  4.92732584e-01]],\n",
       "\n",
       "        [[ 1.00387836e+00,  1.68604946e+00,  1.72188222e+00, ...,\n",
       "           7.26276517e-01,  9.32465494e-01,  1.42219031e+00],\n",
       "         [ 1.32123303e+00,  1.43914521e+00,  1.39888322e+00, ...,\n",
       "           8.96897554e-01,  9.20771301e-01,  1.26074481e+00],\n",
       "         [ 5.65136909e-01,  7.83142865e-01,  9.42872107e-01, ...,\n",
       "          -2.61655658e-01,  9.83102769e-02,  4.74030286e-01],\n",
       "         ...,\n",
       "         [ 3.68845165e-02, -2.34260172e-01,  1.46705762e-01, ...,\n",
       "           2.00828519e-02, -1.59957796e-01, -2.45466873e-01],\n",
       "         [-1.19658053e+00, -1.69308519e+00, -3.87873232e-01, ...,\n",
       "           2.23403528e-01, -8.86636198e-01, -5.75449407e-01],\n",
       "         [ 7.08105505e-01, -1.81255817e+00, -1.48121953e+00, ...,\n",
       "           3.76806200e-01, -5.42918205e-01,  4.41376567e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.12776792e+00,  1.46306956e+00,  1.48767996e+00, ...,\n",
       "           1.62803185e+00,  1.46827650e+00,  1.58148670e+00],\n",
       "         [ 1.46356225e+00,  1.23436880e+00,  1.02596915e+00, ...,\n",
       "           1.11573827e+00,  1.37246263e+00,  1.41418564e+00],\n",
       "         [ 9.68146980e-01,  1.30618238e+00,  1.04609156e+00, ...,\n",
       "           4.44402456e-01,  7.58437216e-01,  6.08541787e-01],\n",
       "         ...,\n",
       "         [ 3.19015563e-01,  9.85141039e-01, -2.48603120e-01, ...,\n",
       "           2.63715446e-01,  1.05289483e+00,  1.56243145e+00],\n",
       "         [-4.84912544e-01,  2.99735755e-01,  6.49971843e-01, ...,\n",
       "           1.26385808e+00,  1.48721111e+00, -7.69702435e-01],\n",
       "         [ 5.59716761e-01,  1.43127963e-01, -2.02869803e-01, ...,\n",
       "           9.34976637e-01,  8.76965463e-01,  1.68173164e-01]],\n",
       "\n",
       "        [[ 1.05417287e+00,  1.40661156e+00,  1.42573321e+00, ...,\n",
       "           1.73294747e+00,  1.50425363e+00,  1.52578843e+00],\n",
       "         [ 1.40035176e+00,  1.14405310e+00,  9.71972525e-01, ...,\n",
       "           1.21504438e+00,  1.42232680e+00,  1.37450683e+00],\n",
       "         [ 9.96286869e-01,  1.25535703e+00,  9.92358804e-01, ...,\n",
       "           4.65553015e-01,  8.08831573e-01,  6.76446855e-01],\n",
       "         ...,\n",
       "         [ 3.90573740e-01,  9.78541136e-01, -2.05351770e-01, ...,\n",
       "           2.11067826e-01,  9.68537629e-01,  1.43002021e+00],\n",
       "         [-3.61989111e-01,  3.57380569e-01,  7.39674449e-01, ...,\n",
       "           1.20333457e+00,  1.27605832e+00, -5.52498937e-01],\n",
       "         [ 6.97079420e-01,  2.63943672e-01, -4.88851480e-02, ...,\n",
       "           1.08215559e+00,  8.70064735e-01,  1.80758521e-01]],\n",
       "\n",
       "        [[ 1.05650604e+00,  1.36264014e+00,  1.34554887e+00, ...,\n",
       "           1.74142349e+00,  1.51377082e+00,  1.57523370e+00],\n",
       "         [ 1.34015131e+00,  1.09261858e+00,  9.20372546e-01, ...,\n",
       "           1.34204280e+00,  1.49625623e+00,  1.37692010e+00],\n",
       "         [ 1.00881195e+00,  1.18061197e+00,  9.32613790e-01, ...,\n",
       "           5.74113429e-01,  9.43301082e-01,  7.84881473e-01],\n",
       "         ...,\n",
       "         [ 4.41790104e-01,  8.83235514e-01, -2.20139697e-01, ...,\n",
       "           2.10817009e-01,  9.21731591e-01,  1.39006925e+00],\n",
       "         [-2.74293870e-01,  3.96349132e-01,  7.88462937e-01, ...,\n",
       "           1.19147193e+00,  1.09640014e+00, -4.41391051e-01],\n",
       "         [ 8.03167760e-01,  3.22496265e-01,  5.22812046e-02, ...,\n",
       "           1.22210550e+00,  8.73679459e-01,  1.67143539e-01]]],\n",
       "\n",
       "\n",
       "       [[[ 1.01933658e+00,  1.69587147e+00,  1.70706797e+00, ...,\n",
       "           6.84034586e-01,  9.30006027e-01,  1.37139559e+00],\n",
       "         [ 1.28319097e+00,  1.42629254e+00,  1.42786157e+00, ...,\n",
       "           8.87850165e-01,  9.03963208e-01,  1.23649764e+00],\n",
       "         [ 5.90400338e-01,  8.81069243e-01,  1.00919712e+00, ...,\n",
       "          -1.79806709e-01,  1.65868983e-01,  5.32285929e-01],\n",
       "         ...,\n",
       "         [-3.78004042e-03, -1.53568268e-01,  9.46630090e-02, ...,\n",
       "           4.11532335e-02, -5.01118042e-02, -2.38398820e-01],\n",
       "         [-1.28618920e+00, -1.71408999e+00, -3.60433400e-01, ...,\n",
       "           3.44249457e-01, -6.56080723e-01, -6.22785509e-01],\n",
       "         [ 7.28772700e-01, -1.78303683e+00, -1.38569200e+00, ...,\n",
       "           3.66918474e-01, -5.06005168e-01,  4.92732584e-01]],\n",
       "\n",
       "        [[ 1.00387836e+00,  1.68604946e+00,  1.72188222e+00, ...,\n",
       "           7.26276517e-01,  9.32465494e-01,  1.42219031e+00],\n",
       "         [ 1.32123303e+00,  1.43914521e+00,  1.39888322e+00, ...,\n",
       "           8.96897554e-01,  9.20771301e-01,  1.26074481e+00],\n",
       "         [ 5.65136909e-01,  7.83142865e-01,  9.42872107e-01, ...,\n",
       "          -2.61655658e-01,  9.83102769e-02,  4.74030286e-01],\n",
       "         ...,\n",
       "         [ 3.68845165e-02, -2.34260172e-01,  1.46705762e-01, ...,\n",
       "           2.00828519e-02, -1.59957796e-01, -2.45466873e-01],\n",
       "         [-1.19658053e+00, -1.69308519e+00, -3.87873232e-01, ...,\n",
       "           2.23403528e-01, -8.86636198e-01, -5.75449407e-01],\n",
       "         [ 7.08105505e-01, -1.81255817e+00, -1.48121953e+00, ...,\n",
       "           3.76806200e-01, -5.42918205e-01,  4.41376567e-01]],\n",
       "\n",
       "        [[ 1.03278148e+00,  1.71551669e+00,  1.79083228e+00, ...,\n",
       "           7.72730947e-01,  9.34396684e-01,  1.51204646e+00],\n",
       "         [ 1.37950313e+00,  1.47791374e+00,  1.37511265e+00, ...,\n",
       "           9.63298142e-01,  9.72806811e-01,  1.31849670e+00],\n",
       "         [ 5.66580474e-01,  7.37329125e-01,  9.06634450e-01, ...,\n",
       "          -2.58229762e-01,  9.68143344e-02,  4.63079482e-01],\n",
       "         ...,\n",
       "         [ 2.06299096e-01, -2.37500936e-01,  2.25534946e-01, ...,\n",
       "          -6.12203404e-02, -2.51177311e-01, -1.40642911e-01],\n",
       "         [-1.08322549e+00, -1.72485042e+00, -3.78283978e-01, ...,\n",
       "           9.36858431e-02, -1.12265623e+00, -6.07779026e-01],\n",
       "         [ 7.15143144e-01, -1.88257873e+00, -1.63362455e+00, ...,\n",
       "           3.28968883e-01, -6.43231690e-01,  3.52597058e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.05417287e+00,  1.40661156e+00,  1.42573321e+00, ...,\n",
       "           1.73294747e+00,  1.50425363e+00,  1.52578843e+00],\n",
       "         [ 1.40035176e+00,  1.14405310e+00,  9.71972525e-01, ...,\n",
       "           1.21504438e+00,  1.42232680e+00,  1.37450683e+00],\n",
       "         [ 9.96286869e-01,  1.25535703e+00,  9.92358804e-01, ...,\n",
       "           4.65553015e-01,  8.08831573e-01,  6.76446855e-01],\n",
       "         ...,\n",
       "         [ 3.90573740e-01,  9.78541136e-01, -2.05351770e-01, ...,\n",
       "           2.11067826e-01,  9.68537629e-01,  1.43002021e+00],\n",
       "         [-3.61989111e-01,  3.57380569e-01,  7.39674449e-01, ...,\n",
       "           1.20333457e+00,  1.27605832e+00, -5.52498937e-01],\n",
       "         [ 6.97079420e-01,  2.63943672e-01, -4.88851480e-02, ...,\n",
       "           1.08215559e+00,  8.70064735e-01,  1.80758521e-01]],\n",
       "\n",
       "        [[ 1.05650604e+00,  1.36264014e+00,  1.34554887e+00, ...,\n",
       "           1.74142349e+00,  1.51377082e+00,  1.57523370e+00],\n",
       "         [ 1.34015131e+00,  1.09261858e+00,  9.20372546e-01, ...,\n",
       "           1.34204280e+00,  1.49625623e+00,  1.37692010e+00],\n",
       "         [ 1.00881195e+00,  1.18061197e+00,  9.32613790e-01, ...,\n",
       "           5.74113429e-01,  9.43301082e-01,  7.84881473e-01],\n",
       "         ...,\n",
       "         [ 4.41790104e-01,  8.83235514e-01, -2.20139697e-01, ...,\n",
       "           2.10817009e-01,  9.21731591e-01,  1.39006925e+00],\n",
       "         [-2.74293870e-01,  3.96349132e-01,  7.88462937e-01, ...,\n",
       "           1.19147193e+00,  1.09640014e+00, -4.41391051e-01],\n",
       "         [ 8.03167760e-01,  3.22496265e-01,  5.22812046e-02, ...,\n",
       "           1.22210550e+00,  8.73679459e-01,  1.67143539e-01]],\n",
       "\n",
       "        [[ 1.10257900e+00,  1.40027845e+00,  1.35083449e+00, ...,\n",
       "           1.89066970e+00,  1.68356597e+00,  1.72060966e+00],\n",
       "         [ 1.28571832e+00,  1.06180978e+00,  9.03383911e-01, ...,\n",
       "           1.46462691e+00,  1.54794002e+00,  1.37113214e+00],\n",
       "         [ 9.69825208e-01,  1.09793878e+00,  8.88440430e-01, ...,\n",
       "           5.86842775e-01,  9.77596521e-01,  7.69184649e-01],\n",
       "         ...,\n",
       "         [ 5.72660208e-01,  8.72038662e-01, -1.47511303e-01, ...,\n",
       "           2.52896816e-01,  8.90182555e-01,  1.44093525e+00],\n",
       "         [-2.07899272e-01,  3.79277438e-01,  8.15327942e-01, ...,\n",
       "           1.17658854e+00,  1.03979814e+00, -3.05826843e-01],\n",
       "         [ 8.79944444e-01,  3.39401633e-01,  8.96904618e-02, ...,\n",
       "           1.31369293e+00,  8.91908348e-01,  1.85550138e-01]]],\n",
       "\n",
       "\n",
       "       [[[ 1.00387836e+00,  1.68604946e+00,  1.72188222e+00, ...,\n",
       "           7.26276517e-01,  9.32465494e-01,  1.42219031e+00],\n",
       "         [ 1.32123303e+00,  1.43914521e+00,  1.39888322e+00, ...,\n",
       "           8.96897554e-01,  9.20771301e-01,  1.26074481e+00],\n",
       "         [ 5.65136909e-01,  7.83142865e-01,  9.42872107e-01, ...,\n",
       "          -2.61655658e-01,  9.83102769e-02,  4.74030286e-01],\n",
       "         ...,\n",
       "         [ 3.68845165e-02, -2.34260172e-01,  1.46705762e-01, ...,\n",
       "           2.00828519e-02, -1.59957796e-01, -2.45466873e-01],\n",
       "         [-1.19658053e+00, -1.69308519e+00, -3.87873232e-01, ...,\n",
       "           2.23403528e-01, -8.86636198e-01, -5.75449407e-01],\n",
       "         [ 7.08105505e-01, -1.81255817e+00, -1.48121953e+00, ...,\n",
       "           3.76806200e-01, -5.42918205e-01,  4.41376567e-01]],\n",
       "\n",
       "        [[ 1.03278148e+00,  1.71551669e+00,  1.79083228e+00, ...,\n",
       "           7.72730947e-01,  9.34396684e-01,  1.51204646e+00],\n",
       "         [ 1.37950313e+00,  1.47791374e+00,  1.37511265e+00, ...,\n",
       "           9.63298142e-01,  9.72806811e-01,  1.31849670e+00],\n",
       "         [ 5.66580474e-01,  7.37329125e-01,  9.06634450e-01, ...,\n",
       "          -2.58229762e-01,  9.68143344e-02,  4.63079482e-01],\n",
       "         ...,\n",
       "         [ 2.06299096e-01, -2.37500936e-01,  2.25534946e-01, ...,\n",
       "          -6.12203404e-02, -2.51177311e-01, -1.40642911e-01],\n",
       "         [-1.08322549e+00, -1.72485042e+00, -3.78283978e-01, ...,\n",
       "           9.36858431e-02, -1.12265623e+00, -6.07779026e-01],\n",
       "         [ 7.15143144e-01, -1.88257873e+00, -1.63362455e+00, ...,\n",
       "           3.28968883e-01, -6.43231690e-01,  3.52597058e-01]],\n",
       "\n",
       "        [[ 1.01747394e+00,  1.75199354e+00,  1.88086724e+00, ...,\n",
       "           7.64736056e-01,  8.79015386e-01,  1.48260689e+00],\n",
       "         [ 1.41790557e+00,  1.55994701e+00,  1.45534444e+00, ...,\n",
       "           9.44264352e-01,  9.50580418e-01,  1.30947375e+00],\n",
       "         [ 6.60528541e-01,  8.55886579e-01,  1.04852355e+00, ...,\n",
       "          -3.15560132e-01,  2.32891478e-02,  4.68549639e-01],\n",
       "         ...,\n",
       "         [ 4.73285794e-01, -1.55688807e-01,  3.91176611e-01, ...,\n",
       "          -1.92234024e-01, -3.26620191e-01, -2.29428038e-02],\n",
       "         [-8.59293401e-01, -1.61295569e+00, -2.61210710e-01, ...,\n",
       "           9.90962461e-02, -1.20373106e+00, -5.23873568e-01],\n",
       "         [ 7.26767242e-01, -1.84673488e+00, -1.75555873e+00, ...,\n",
       "           3.70759934e-01, -6.50319457e-01,  3.74336809e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.05650604e+00,  1.36264014e+00,  1.34554887e+00, ...,\n",
       "           1.74142349e+00,  1.51377082e+00,  1.57523370e+00],\n",
       "         [ 1.34015131e+00,  1.09261858e+00,  9.20372546e-01, ...,\n",
       "           1.34204280e+00,  1.49625623e+00,  1.37692010e+00],\n",
       "         [ 1.00881195e+00,  1.18061197e+00,  9.32613790e-01, ...,\n",
       "           5.74113429e-01,  9.43301082e-01,  7.84881473e-01],\n",
       "         ...,\n",
       "         [ 4.41790104e-01,  8.83235514e-01, -2.20139697e-01, ...,\n",
       "           2.10817009e-01,  9.21731591e-01,  1.39006925e+00],\n",
       "         [-2.74293870e-01,  3.96349132e-01,  7.88462937e-01, ...,\n",
       "           1.19147193e+00,  1.09640014e+00, -4.41391051e-01],\n",
       "         [ 8.03167760e-01,  3.22496265e-01,  5.22812046e-02, ...,\n",
       "           1.22210550e+00,  8.73679459e-01,  1.67143539e-01]],\n",
       "\n",
       "        [[ 1.10257900e+00,  1.40027845e+00,  1.35083449e+00, ...,\n",
       "           1.89066970e+00,  1.68356597e+00,  1.72060966e+00],\n",
       "         [ 1.28571832e+00,  1.06180978e+00,  9.03383911e-01, ...,\n",
       "           1.46462691e+00,  1.54794002e+00,  1.37113214e+00],\n",
       "         [ 9.69825208e-01,  1.09793878e+00,  8.88440430e-01, ...,\n",
       "           5.86842775e-01,  9.77596521e-01,  7.69184649e-01],\n",
       "         ...,\n",
       "         [ 5.72660208e-01,  8.72038662e-01, -1.47511303e-01, ...,\n",
       "           2.52896816e-01,  8.90182555e-01,  1.44093525e+00],\n",
       "         [-2.07899272e-01,  3.79277438e-01,  8.15327942e-01, ...,\n",
       "           1.17658854e+00,  1.03979814e+00, -3.05826843e-01],\n",
       "         [ 8.79944444e-01,  3.39401633e-01,  8.96904618e-02, ...,\n",
       "           1.31369293e+00,  8.91908348e-01,  1.85550138e-01]],\n",
       "\n",
       "        [[ 1.08343196e+00,  1.40071011e+00,  1.26597083e+00, ...,\n",
       "           1.91848624e+00,  1.70034504e+00,  1.72931206e+00],\n",
       "         [ 1.22668839e+00,  1.00202823e+00,  8.20403337e-01, ...,\n",
       "           1.46821892e+00,  1.56984556e+00,  1.32960367e+00],\n",
       "         [ 9.50440466e-01,  1.02767503e+00,  8.38067353e-01, ...,\n",
       "           5.77073514e-01,  9.79359865e-01,  7.76819885e-01],\n",
       "         ...,\n",
       "         [ 7.31231928e-01,  9.20140266e-01, -7.74875283e-02, ...,\n",
       "           2.91301161e-01,  9.10074115e-01,  1.55428052e+00],\n",
       "         [-1.03045069e-01,  4.12901521e-01,  8.12982798e-01, ...,\n",
       "           1.21738899e+00,  1.13054645e+00, -1.94724575e-01],\n",
       "         [ 1.00549436e+00,  4.23224896e-01,  1.41115010e-01, ...,\n",
       "           1.50357926e+00,  9.27153587e-01,  2.41223156e-01]]]],\n",
       "      dtype=float32)</pre></li></ul></div></li><li class='xr-section-item'><input id='section-c0bf1a68-e11c-480a-8a9f-73710a56bfdf' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-c0bf1a68-e11c-480a-8a9f-73710a56bfdf' class='xr-section-summary'  title='Expand/collapse section'>Attributes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:    (lag: 60, lat: 61, lon: 180, time: 3440)\n",
       "Coordinates:\n",
       "  * lag        (lag) int64 0 1 2 3 4 5 6 7 8 9 ... 50 51 52 53 54 55 56 57 58 59\n",
       "  * lat        (lat) float32 60.0 58.0 56.0 54.0 ... -54.0 -56.0 -58.0 -60.0\n",
       "  * lon        (lon) float32 -180.0 -178.0 -176.0 -174.0 ... 174.0 176.0 178.0\n",
       "    dayofyear  (time, lag) int64 198 199 200 201 202 203 ... 339 340 341 342 343\n",
       "  * time       (time) datetime64[ns] 1981-10-04 1981-10-05 ... 2020-12-28\n",
       "Data variables:\n",
       "    sst        (time, lag, lat, lon) float32 -0.36218587 ... 0.24122316"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_vars[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3440, 60, 61, 180, 1)\n",
      "(3440, 60, 61, 180, 1) (3440, 4)\n"
     ]
    }
   ],
   "source": [
    "from post import score\n",
    "# Feature array\n",
    "tmp = predictor_array.expand_dims(dim={\"channel\": 1},axis=4)\n",
    "tmp = tmp.fillna(100)\n",
    "x = tmp.sst\n",
    "print(x.shape)\n",
    "\n",
    "# Model params\n",
    "ntimestep = x.shape[1]    # number of time step used in the predictors\n",
    "nx = x.shape[2] \n",
    "ny = x.shape[3]\n",
    "\n",
    "# define input and output data for LSTM\n",
    "n_cat=3\n",
    "y_all_tmp = keras.utils.to_categorical(tp_target['precip']) # 4 values, but it creates max(y) + 1\n",
    "time_dim = tp_target.coords['time']\n",
    "nclass = np.arange(n_cat+1)\n",
    "\n",
    "y_all_n = xr.DataArray(y_all_tmp, coords={'time': time_dim, 'category': nclass}, dims=[\"time\",\"category\"])\n",
    "X_all_n = x\n",
    "print(X_all_n.shape,y_all_n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<title>Show/Hide data repr</title>\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<title>Show/Hide attributes</title>\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt, dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2 {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><div class='xr-wrap'><div class='xr-header'><div class='xr-obj-type'>xarray.DataArray</div><div class='xr-array-name'>'sst'</div><ul class='xr-dim-list'><li><span class='xr-has-index'>time</span>: 3440</li><li><span class='xr-has-index'>lag</span>: 60</li><li><span class='xr-has-index'>lat</span>: 61</li><li><span class='xr-has-index'>lon</span>: 180</li><li><span>channel</span>: 1</li></ul></div><ul class='xr-sections'><li class='xr-section-item'><div class='xr-array-wrap'><input id='section-620eba99-800e-4739-af22-07e083341917' class='xr-array-in' type='checkbox' ><label for='section-620eba99-800e-4739-af22-07e083341917' title='Show/hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-array-preview xr-preview'><span>-0.36218587 0.3105765 0.5079281 ... 1.5035793 0.9271536 0.24122316</span></div><pre class='xr-array-data'>array([[[[[-3.62185866e-01],\n",
       "          [ 3.10576499e-01],\n",
       "          [ 5.07928073e-01],\n",
       "          ...,\n",
       "          [ 3.67879808e-01],\n",
       "          [ 3.25133681e-01],\n",
       "          [-3.39804977e-01]],\n",
       "\n",
       "         [[ 4.65958118e-02],\n",
       "          [ 4.00977582e-01],\n",
       "          [ 7.80841947e-01],\n",
       "          ...,\n",
       "          [ 6.71721101e-01],\n",
       "          [ 6.03961468e-01],\n",
       "          [ 5.84028900e-01]],\n",
       "\n",
       "         [[ 6.76504612e-01],\n",
       "          [ 8.65939558e-01],\n",
       "          [ 7.75304794e-01],\n",
       "          ...,\n",
       "          [ 7.52957523e-01],\n",
       "          [ 6.04648471e-01],\n",
       "          [ 6.45179451e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.31917195e-02],\n",
       "          [ 6.20493054e-01],\n",
       "          [ 4.20646071e-01],\n",
       "          ...,\n",
       "          [ 2.08507404e-01],\n",
       "          [ 3.40076119e-01],\n",
       "          [ 5.22748530e-01]],\n",
       "\n",
       "         [[ 7.42579639e-01],\n",
       "          [-1.36416182e-01],\n",
       "          [-2.30123685e-03],\n",
       "          ...,\n",
       "          [ 1.64106369e+00],\n",
       "          [ 1.28742111e+00],\n",
       "          [ 1.18877852e+00]],\n",
       "\n",
       "         [[ 8.97619605e-01],\n",
       "          [ 1.49698973e+00],\n",
       "          [ 1.96731377e+00],\n",
       "          ...,\n",
       "          [ 1.83101034e+00],\n",
       "          [ 1.78363490e+00],\n",
       "          [ 1.56304681e+00]]],\n",
       "\n",
       "\n",
       "        [[[-3.54809254e-01],\n",
       "          [ 3.49349380e-01],\n",
       "          [ 5.40288150e-01],\n",
       "          ...,\n",
       "          [ 4.48003441e-01],\n",
       "          [ 3.88327032e-01],\n",
       "          [-2.97131151e-01]],\n",
       "\n",
       "         [[ 1.12070717e-01],\n",
       "          [ 4.23052967e-01],\n",
       "          [ 7.83311009e-01],\n",
       "          ...,\n",
       "          [ 6.91575646e-01],\n",
       "          [ 6.13563299e-01],\n",
       "          [ 6.29878521e-01]],\n",
       "\n",
       "         [[ 6.93953097e-01],\n",
       "          [ 8.70202363e-01],\n",
       "          [ 7.59072006e-01],\n",
       "          ...,\n",
       "          [ 7.67725289e-01],\n",
       "          [ 6.23503089e-01],\n",
       "          [ 7.11687446e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.20163308e-02],\n",
       "          [ 6.30487025e-01],\n",
       "          [ 3.02919269e-01],\n",
       "          ...,\n",
       "          [ 2.67718613e-01],\n",
       "          [ 4.01345760e-01],\n",
       "          [ 6.17449880e-01]],\n",
       "\n",
       "         [[ 8.62130046e-01],\n",
       "          [-1.11007534e-01],\n",
       "          [ 1.92449056e-02],\n",
       "          ...,\n",
       "          [ 1.84401584e+00],\n",
       "          [ 1.44534183e+00],\n",
       "          [ 1.35463166e+00]],\n",
       "\n",
       "         [[ 1.04553270e+00],\n",
       "          [ 1.76997089e+00],\n",
       "          [ 2.24499035e+00],\n",
       "          ...,\n",
       "          [ 2.14317679e+00],\n",
       "          [ 2.01471210e+00],\n",
       "          [ 1.88421190e+00]]],\n",
       "\n",
       "\n",
       "        [[[-3.42830747e-01],\n",
       "          [ 3.73506010e-01],\n",
       "          [ 5.46182752e-01],\n",
       "          ...,\n",
       "          [ 5.41157544e-01],\n",
       "          [ 4.60779071e-01],\n",
       "          [-2.58643657e-01]],\n",
       "\n",
       "         [[ 2.18399197e-01],\n",
       "          [ 4.72794622e-01],\n",
       "          [ 8.00716877e-01],\n",
       "          ...,\n",
       "          [ 7.39316642e-01],\n",
       "          [ 6.69109285e-01],\n",
       "          [ 7.03879774e-01]],\n",
       "\n",
       "         [[ 7.57432997e-01],\n",
       "          [ 8.98841143e-01],\n",
       "          [ 7.73531616e-01],\n",
       "          ...,\n",
       "          [ 7.94056714e-01],\n",
       "          [ 6.58193588e-01],\n",
       "          [ 8.05383742e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 5.83060049e-02],\n",
       "          [ 7.13439226e-01],\n",
       "          [ 2.30205163e-01],\n",
       "          ...,\n",
       "          [ 4.42897409e-01],\n",
       "          [ 5.26245713e-01],\n",
       "          [ 7.75482595e-01]],\n",
       "\n",
       "         [[ 1.01246369e+00],\n",
       "          [-4.89043035e-02],\n",
       "          [ 1.08723655e-01],\n",
       "          ...,\n",
       "          [ 2.04081988e+00],\n",
       "          [ 1.61541104e+00],\n",
       "          [ 1.56025541e+00]],\n",
       "\n",
       "         [[ 1.21782935e+00],\n",
       "          [ 2.03469133e+00],\n",
       "          [ 2.52169538e+00],\n",
       "          ...,\n",
       "          [ 2.40121460e+00],\n",
       "          [ 2.16749620e+00],\n",
       "          [ 2.22528529e+00]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-1.89238697e-01],\n",
       "          [-2.24710032e-01],\n",
       "          [ 4.23177838e-01],\n",
       "          ...,\n",
       "          [-2.35135213e-01],\n",
       "          [-1.98847502e-01],\n",
       "          [-3.14200193e-01]],\n",
       "\n",
       "         [[ 3.35881382e-01],\n",
       "          [ 4.55341727e-01],\n",
       "          [ 4.81544733e-01],\n",
       "          ...,\n",
       "          [ 8.99040818e-01],\n",
       "          [ 6.78904057e-01],\n",
       "          [ 6.03037179e-01]],\n",
       "\n",
       "         [[ 1.41179430e+00],\n",
       "          [ 1.43971336e+00],\n",
       "          [ 1.41443598e+00],\n",
       "          ...,\n",
       "          [ 8.59741509e-01],\n",
       "          [ 1.05343568e+00],\n",
       "          [ 1.29795957e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.27684617e+00],\n",
       "          [ 1.69404328e+00],\n",
       "          [ 1.67089903e+00],\n",
       "          ...,\n",
       "          [ 5.05574465e-01],\n",
       "          [ 4.94341224e-01],\n",
       "          [ 1.59012341e+00]],\n",
       "\n",
       "         [[ 2.06658125e+00],\n",
       "          [ 2.64937115e+00],\n",
       "          [ 1.55695510e+00],\n",
       "          ...,\n",
       "          [ 8.03708851e-01],\n",
       "          [ 1.64456725e+00],\n",
       "          [ 2.15517974e+00]],\n",
       "\n",
       "         [[ 2.53595042e+00],\n",
       "          [ 2.53145838e+00],\n",
       "          [ 3.37910271e+00],\n",
       "          ...,\n",
       "          [ 1.87308347e+00],\n",
       "          [ 2.45421982e+00],\n",
       "          [ 2.49602628e+00]]],\n",
       "\n",
       "\n",
       "        [[[-1.16541468e-01],\n",
       "          [-1.73735186e-01],\n",
       "          [ 4.50839847e-01],\n",
       "          ...,\n",
       "          [-8.77307653e-02],\n",
       "          [-9.97462720e-02],\n",
       "          [-2.07597807e-01]],\n",
       "\n",
       "         [[ 4.29405212e-01],\n",
       "          [ 5.50040126e-01],\n",
       "          [ 5.59651911e-01],\n",
       "          ...,\n",
       "          [ 1.00286484e+00],\n",
       "          [ 7.37543464e-01],\n",
       "          [ 6.60762370e-01]],\n",
       "\n",
       "         [[ 1.45638251e+00],\n",
       "          [ 1.49776351e+00],\n",
       "          [ 1.49852157e+00],\n",
       "          ...,\n",
       "          [ 8.79310310e-01],\n",
       "          [ 1.03144550e+00],\n",
       "          [ 1.30704069e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.36080384e+00],\n",
       "          [ 1.86275506e+00],\n",
       "          [ 1.77320147e+00],\n",
       "          ...,\n",
       "          [ 6.41663551e-01],\n",
       "          [ 6.36572957e-01],\n",
       "          [ 1.77160847e+00]],\n",
       "\n",
       "         [[ 2.22900081e+00],\n",
       "          [ 2.79405284e+00],\n",
       "          [ 1.68586099e+00],\n",
       "          ...,\n",
       "          [ 1.01761854e+00],\n",
       "          [ 1.75572109e+00],\n",
       "          [ 2.24060678e+00]],\n",
       "\n",
       "         [[ 2.58279824e+00],\n",
       "          [ 2.63683200e+00],\n",
       "          [ 3.49761844e+00],\n",
       "          ...,\n",
       "          [ 1.98000157e+00],\n",
       "          [ 2.48671365e+00],\n",
       "          [ 2.54368401e+00]]],\n",
       "\n",
       "\n",
       "        [[[-3.26002799e-02],\n",
       "          [-8.74749795e-02],\n",
       "          [ 4.84744817e-01],\n",
       "          ...,\n",
       "          [ 4.02493887e-02],\n",
       "          [-9.68386699e-03],\n",
       "          [-1.01645648e-01]],\n",
       "\n",
       "         [[ 5.36003292e-01],\n",
       "          [ 6.56846881e-01],\n",
       "          [ 6.14730954e-01],\n",
       "          ...,\n",
       "          [ 1.09287715e+00],\n",
       "          [ 7.96037853e-01],\n",
       "          [ 7.29901910e-01]],\n",
       "\n",
       "         [[ 1.52206004e+00],\n",
       "          [ 1.55389607e+00],\n",
       "          [ 1.54789031e+00],\n",
       "          ...,\n",
       "          [ 8.96352530e-01],\n",
       "          [ 1.01808619e+00],\n",
       "          [ 1.33967614e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.46165609e+00],\n",
       "          [ 2.04387426e+00],\n",
       "          [ 1.88087571e+00],\n",
       "          ...,\n",
       "          [ 7.96604455e-01],\n",
       "          [ 8.02872181e-01],\n",
       "          [ 2.04422998e+00]],\n",
       "\n",
       "         [[ 2.41753268e+00],\n",
       "          [ 2.90234756e+00],\n",
       "          [ 1.78626764e+00],\n",
       "          ...,\n",
       "          [ 1.23611224e+00],\n",
       "          [ 1.93281913e+00],\n",
       "          [ 2.38242960e+00]],\n",
       "\n",
       "         [[ 2.69791126e+00],\n",
       "          [ 2.82957363e+00],\n",
       "          [ 3.56921649e+00],\n",
       "          ...,\n",
       "          [ 2.11037993e+00],\n",
       "          [ 2.56703997e+00],\n",
       "          [ 2.65462637e+00]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[-3.54809254e-01],\n",
       "          [ 3.49349380e-01],\n",
       "          [ 5.40288150e-01],\n",
       "          ...,\n",
       "          [ 4.48003441e-01],\n",
       "          [ 3.88327032e-01],\n",
       "          [-2.97131151e-01]],\n",
       "\n",
       "         [[ 1.12070717e-01],\n",
       "          [ 4.23052967e-01],\n",
       "          [ 7.83311009e-01],\n",
       "          ...,\n",
       "          [ 6.91575646e-01],\n",
       "          [ 6.13563299e-01],\n",
       "          [ 6.29878521e-01]],\n",
       "\n",
       "         [[ 6.93953097e-01],\n",
       "          [ 8.70202363e-01],\n",
       "          [ 7.59072006e-01],\n",
       "          ...,\n",
       "          [ 7.67725289e-01],\n",
       "          [ 6.23503089e-01],\n",
       "          [ 7.11687446e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.20163308e-02],\n",
       "          [ 6.30487025e-01],\n",
       "          [ 3.02919269e-01],\n",
       "          ...,\n",
       "          [ 2.67718613e-01],\n",
       "          [ 4.01345760e-01],\n",
       "          [ 6.17449880e-01]],\n",
       "\n",
       "         [[ 8.62130046e-01],\n",
       "          [-1.11007534e-01],\n",
       "          [ 1.92449056e-02],\n",
       "          ...,\n",
       "          [ 1.84401584e+00],\n",
       "          [ 1.44534183e+00],\n",
       "          [ 1.35463166e+00]],\n",
       "\n",
       "         [[ 1.04553270e+00],\n",
       "          [ 1.76997089e+00],\n",
       "          [ 2.24499035e+00],\n",
       "          ...,\n",
       "          [ 2.14317679e+00],\n",
       "          [ 2.01471210e+00],\n",
       "          [ 1.88421190e+00]]],\n",
       "\n",
       "\n",
       "        [[[-3.42830747e-01],\n",
       "          [ 3.73506010e-01],\n",
       "          [ 5.46182752e-01],\n",
       "          ...,\n",
       "          [ 5.41157544e-01],\n",
       "          [ 4.60779071e-01],\n",
       "          [-2.58643657e-01]],\n",
       "\n",
       "         [[ 2.18399197e-01],\n",
       "          [ 4.72794622e-01],\n",
       "          [ 8.00716877e-01],\n",
       "          ...,\n",
       "          [ 7.39316642e-01],\n",
       "          [ 6.69109285e-01],\n",
       "          [ 7.03879774e-01]],\n",
       "\n",
       "         [[ 7.57432997e-01],\n",
       "          [ 8.98841143e-01],\n",
       "          [ 7.73531616e-01],\n",
       "          ...,\n",
       "          [ 7.94056714e-01],\n",
       "          [ 6.58193588e-01],\n",
       "          [ 8.05383742e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 5.83060049e-02],\n",
       "          [ 7.13439226e-01],\n",
       "          [ 2.30205163e-01],\n",
       "          ...,\n",
       "          [ 4.42897409e-01],\n",
       "          [ 5.26245713e-01],\n",
       "          [ 7.75482595e-01]],\n",
       "\n",
       "         [[ 1.01246369e+00],\n",
       "          [-4.89043035e-02],\n",
       "          [ 1.08723655e-01],\n",
       "          ...,\n",
       "          [ 2.04081988e+00],\n",
       "          [ 1.61541104e+00],\n",
       "          [ 1.56025541e+00]],\n",
       "\n",
       "         [[ 1.21782935e+00],\n",
       "          [ 2.03469133e+00],\n",
       "          [ 2.52169538e+00],\n",
       "          ...,\n",
       "          [ 2.40121460e+00],\n",
       "          [ 2.16749620e+00],\n",
       "          [ 2.22528529e+00]]],\n",
       "\n",
       "\n",
       "        [[[-3.26204777e-01],\n",
       "          [ 3.98652762e-01],\n",
       "          [ 5.40608764e-01],\n",
       "          ...,\n",
       "          [ 6.32930338e-01],\n",
       "          [ 5.33978820e-01],\n",
       "          [-2.21612617e-01]],\n",
       "\n",
       "         [[ 3.65119278e-01],\n",
       "          [ 5.50025880e-01],\n",
       "          [ 8.39799523e-01],\n",
       "          ...,\n",
       "          [ 8.31931174e-01],\n",
       "          [ 7.65980840e-01],\n",
       "          [ 8.07559252e-01]],\n",
       "\n",
       "         [[ 8.80454957e-01],\n",
       "          [ 9.60588992e-01],\n",
       "          [ 8.33143771e-01],\n",
       "          ...,\n",
       "          [ 8.42785537e-01],\n",
       "          [ 7.09306180e-01],\n",
       "          [ 9.37528551e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.19364597e-01],\n",
       "          [ 8.23661089e-01],\n",
       "          [ 1.78819329e-01],\n",
       "          ...,\n",
       "          [ 6.24326468e-01],\n",
       "          [ 6.55635536e-01],\n",
       "          [ 9.09860432e-01]],\n",
       "\n",
       "         [[ 1.14875698e+00],\n",
       "          [ 7.80493859e-03],\n",
       "          [ 2.17258990e-01],\n",
       "          ...,\n",
       "          [ 2.08862042e+00],\n",
       "          [ 1.72013092e+00],\n",
       "          [ 1.77909744e+00]],\n",
       "\n",
       "         [[ 1.38127649e+00],\n",
       "          [ 2.26480556e+00],\n",
       "          [ 2.75266623e+00],\n",
       "          ...,\n",
       "          [ 2.54446363e+00],\n",
       "          [ 2.24002504e+00],\n",
       "          [ 2.46532941e+00]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-1.16541468e-01],\n",
       "          [-1.73735186e-01],\n",
       "          [ 4.50839847e-01],\n",
       "          ...,\n",
       "          [-8.77307653e-02],\n",
       "          [-9.97462720e-02],\n",
       "          [-2.07597807e-01]],\n",
       "\n",
       "         [[ 4.29405212e-01],\n",
       "          [ 5.50040126e-01],\n",
       "          [ 5.59651911e-01],\n",
       "          ...,\n",
       "          [ 1.00286484e+00],\n",
       "          [ 7.37543464e-01],\n",
       "          [ 6.60762370e-01]],\n",
       "\n",
       "         [[ 1.45638251e+00],\n",
       "          [ 1.49776351e+00],\n",
       "          [ 1.49852157e+00],\n",
       "          ...,\n",
       "          [ 8.79310310e-01],\n",
       "          [ 1.03144550e+00],\n",
       "          [ 1.30704069e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.36080384e+00],\n",
       "          [ 1.86275506e+00],\n",
       "          [ 1.77320147e+00],\n",
       "          ...,\n",
       "          [ 6.41663551e-01],\n",
       "          [ 6.36572957e-01],\n",
       "          [ 1.77160847e+00]],\n",
       "\n",
       "         [[ 2.22900081e+00],\n",
       "          [ 2.79405284e+00],\n",
       "          [ 1.68586099e+00],\n",
       "          ...,\n",
       "          [ 1.01761854e+00],\n",
       "          [ 1.75572109e+00],\n",
       "          [ 2.24060678e+00]],\n",
       "\n",
       "         [[ 2.58279824e+00],\n",
       "          [ 2.63683200e+00],\n",
       "          [ 3.49761844e+00],\n",
       "          ...,\n",
       "          [ 1.98000157e+00],\n",
       "          [ 2.48671365e+00],\n",
       "          [ 2.54368401e+00]]],\n",
       "\n",
       "\n",
       "        [[[-3.26002799e-02],\n",
       "          [-8.74749795e-02],\n",
       "          [ 4.84744817e-01],\n",
       "          ...,\n",
       "          [ 4.02493887e-02],\n",
       "          [-9.68386699e-03],\n",
       "          [-1.01645648e-01]],\n",
       "\n",
       "         [[ 5.36003292e-01],\n",
       "          [ 6.56846881e-01],\n",
       "          [ 6.14730954e-01],\n",
       "          ...,\n",
       "          [ 1.09287715e+00],\n",
       "          [ 7.96037853e-01],\n",
       "          [ 7.29901910e-01]],\n",
       "\n",
       "         [[ 1.52206004e+00],\n",
       "          [ 1.55389607e+00],\n",
       "          [ 1.54789031e+00],\n",
       "          ...,\n",
       "          [ 8.96352530e-01],\n",
       "          [ 1.01808619e+00],\n",
       "          [ 1.33967614e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.46165609e+00],\n",
       "          [ 2.04387426e+00],\n",
       "          [ 1.88087571e+00],\n",
       "          ...,\n",
       "          [ 7.96604455e-01],\n",
       "          [ 8.02872181e-01],\n",
       "          [ 2.04422998e+00]],\n",
       "\n",
       "         [[ 2.41753268e+00],\n",
       "          [ 2.90234756e+00],\n",
       "          [ 1.78626764e+00],\n",
       "          ...,\n",
       "          [ 1.23611224e+00],\n",
       "          [ 1.93281913e+00],\n",
       "          [ 2.38242960e+00]],\n",
       "\n",
       "         [[ 2.69791126e+00],\n",
       "          [ 2.82957363e+00],\n",
       "          [ 3.56921649e+00],\n",
       "          ...,\n",
       "          [ 2.11037993e+00],\n",
       "          [ 2.56703997e+00],\n",
       "          [ 2.65462637e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 4.49403934e-02],\n",
       "          [ 1.81330070e-02],\n",
       "          [ 5.23392737e-01],\n",
       "          ...,\n",
       "          [ 1.25443235e-01],\n",
       "          [ 5.04921451e-02],\n",
       "          [-1.90902688e-02]],\n",
       "\n",
       "         [[ 6.35598183e-01],\n",
       "          [ 7.46625066e-01],\n",
       "          [ 6.27017081e-01],\n",
       "          ...,\n",
       "          [ 1.13776195e+00],\n",
       "          [ 8.42199326e-01],\n",
       "          [ 8.01237345e-01]],\n",
       "\n",
       "         [[ 1.59554482e+00],\n",
       "          [ 1.58711433e+00],\n",
       "          [ 1.54427624e+00],\n",
       "          ...,\n",
       "          [ 9.21095014e-01],\n",
       "          [ 1.02062726e+00],\n",
       "          [ 1.39626312e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.56243324e+00],\n",
       "          [ 2.16065836e+00],\n",
       "          [ 1.95397115e+00],\n",
       "          ...,\n",
       "          [ 9.62666512e-01],\n",
       "          [ 1.00572073e+00],\n",
       "          [ 2.25374556e+00]],\n",
       "\n",
       "         [[ 2.55276108e+00],\n",
       "          [ 2.96053052e+00],\n",
       "          [ 1.84011018e+00],\n",
       "          ...,\n",
       "          [ 1.42001033e+00],\n",
       "          [ 2.04855180e+00],\n",
       "          [ 2.50614238e+00]],\n",
       "\n",
       "         [[ 2.77681518e+00],\n",
       "          [ 2.97497487e+00],\n",
       "          [ 3.61599922e+00],\n",
       "          ...,\n",
       "          [ 2.20910692e+00],\n",
       "          [ 2.63238931e+00],\n",
       "          [ 2.78463411e+00]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[-3.42830747e-01],\n",
       "          [ 3.73506010e-01],\n",
       "          [ 5.46182752e-01],\n",
       "          ...,\n",
       "          [ 5.41157544e-01],\n",
       "          [ 4.60779071e-01],\n",
       "          [-2.58643657e-01]],\n",
       "\n",
       "         [[ 2.18399197e-01],\n",
       "          [ 4.72794622e-01],\n",
       "          [ 8.00716877e-01],\n",
       "          ...,\n",
       "          [ 7.39316642e-01],\n",
       "          [ 6.69109285e-01],\n",
       "          [ 7.03879774e-01]],\n",
       "\n",
       "         [[ 7.57432997e-01],\n",
       "          [ 8.98841143e-01],\n",
       "          [ 7.73531616e-01],\n",
       "          ...,\n",
       "          [ 7.94056714e-01],\n",
       "          [ 6.58193588e-01],\n",
       "          [ 8.05383742e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 5.83060049e-02],\n",
       "          [ 7.13439226e-01],\n",
       "          [ 2.30205163e-01],\n",
       "          ...,\n",
       "          [ 4.42897409e-01],\n",
       "          [ 5.26245713e-01],\n",
       "          [ 7.75482595e-01]],\n",
       "\n",
       "         [[ 1.01246369e+00],\n",
       "          [-4.89043035e-02],\n",
       "          [ 1.08723655e-01],\n",
       "          ...,\n",
       "          [ 2.04081988e+00],\n",
       "          [ 1.61541104e+00],\n",
       "          [ 1.56025541e+00]],\n",
       "\n",
       "         [[ 1.21782935e+00],\n",
       "          [ 2.03469133e+00],\n",
       "          [ 2.52169538e+00],\n",
       "          ...,\n",
       "          [ 2.40121460e+00],\n",
       "          [ 2.16749620e+00],\n",
       "          [ 2.22528529e+00]]],\n",
       "\n",
       "\n",
       "        [[[-3.26204777e-01],\n",
       "          [ 3.98652762e-01],\n",
       "          [ 5.40608764e-01],\n",
       "          ...,\n",
       "          [ 6.32930338e-01],\n",
       "          [ 5.33978820e-01],\n",
       "          [-2.21612617e-01]],\n",
       "\n",
       "         [[ 3.65119278e-01],\n",
       "          [ 5.50025880e-01],\n",
       "          [ 8.39799523e-01],\n",
       "          ...,\n",
       "          [ 8.31931174e-01],\n",
       "          [ 7.65980840e-01],\n",
       "          [ 8.07559252e-01]],\n",
       "\n",
       "         [[ 8.80454957e-01],\n",
       "          [ 9.60588992e-01],\n",
       "          [ 8.33143771e-01],\n",
       "          ...,\n",
       "          [ 8.42785537e-01],\n",
       "          [ 7.09306180e-01],\n",
       "          [ 9.37528551e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.19364597e-01],\n",
       "          [ 8.23661089e-01],\n",
       "          [ 1.78819329e-01],\n",
       "          ...,\n",
       "          [ 6.24326468e-01],\n",
       "          [ 6.55635536e-01],\n",
       "          [ 9.09860432e-01]],\n",
       "\n",
       "         [[ 1.14875698e+00],\n",
       "          [ 7.80493859e-03],\n",
       "          [ 2.17258990e-01],\n",
       "          ...,\n",
       "          [ 2.08862042e+00],\n",
       "          [ 1.72013092e+00],\n",
       "          [ 1.77909744e+00]],\n",
       "\n",
       "         [[ 1.38127649e+00],\n",
       "          [ 2.26480556e+00],\n",
       "          [ 2.75266623e+00],\n",
       "          ...,\n",
       "          [ 2.54446363e+00],\n",
       "          [ 2.24002504e+00],\n",
       "          [ 2.46532941e+00]]],\n",
       "\n",
       "\n",
       "        [[[-3.08468640e-01],\n",
       "          [ 4.05654550e-01],\n",
       "          [ 5.19501269e-01],\n",
       "          ...,\n",
       "          [ 7.07557023e-01],\n",
       "          [ 5.88403106e-01],\n",
       "          [-2.03750372e-01]],\n",
       "\n",
       "         [[ 4.98608351e-01],\n",
       "          [ 6.14696443e-01],\n",
       "          [ 8.63741398e-01],\n",
       "          ...,\n",
       "          [ 9.28709745e-01],\n",
       "          [ 8.61032724e-01],\n",
       "          [ 8.76570404e-01]],\n",
       "\n",
       "         [[ 9.85637367e-01],\n",
       "          [ 9.87466514e-01],\n",
       "          [ 8.80683005e-01],\n",
       "          ...,\n",
       "          [ 8.93809915e-01],\n",
       "          [ 7.52661824e-01],\n",
       "          [ 1.04538929e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.71227351e-01],\n",
       "          [ 9.16177809e-01],\n",
       "          [ 1.26424253e-01],\n",
       "          ...,\n",
       "          [ 7.76435375e-01],\n",
       "          [ 7.79946446e-01],\n",
       "          [ 9.92677808e-01]],\n",
       "\n",
       "         [[ 1.21839845e+00],\n",
       "          [ 4.08345237e-02],\n",
       "          [ 3.07571560e-01],\n",
       "          ...,\n",
       "          [ 2.04105210e+00],\n",
       "          [ 1.81836522e+00],\n",
       "          [ 1.91364408e+00]],\n",
       "\n",
       "         [[ 1.47358215e+00],\n",
       "          [ 2.38321900e+00],\n",
       "          [ 2.88763857e+00],\n",
       "          ...,\n",
       "          [ 2.53974175e+00],\n",
       "          [ 2.20182514e+00],\n",
       "          [ 2.53175974e+00]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-3.26002799e-02],\n",
       "          [-8.74749795e-02],\n",
       "          [ 4.84744817e-01],\n",
       "          ...,\n",
       "          [ 4.02493887e-02],\n",
       "          [-9.68386699e-03],\n",
       "          [-1.01645648e-01]],\n",
       "\n",
       "         [[ 5.36003292e-01],\n",
       "          [ 6.56846881e-01],\n",
       "          [ 6.14730954e-01],\n",
       "          ...,\n",
       "          [ 1.09287715e+00],\n",
       "          [ 7.96037853e-01],\n",
       "          [ 7.29901910e-01]],\n",
       "\n",
       "         [[ 1.52206004e+00],\n",
       "          [ 1.55389607e+00],\n",
       "          [ 1.54789031e+00],\n",
       "          ...,\n",
       "          [ 8.96352530e-01],\n",
       "          [ 1.01808619e+00],\n",
       "          [ 1.33967614e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.46165609e+00],\n",
       "          [ 2.04387426e+00],\n",
       "          [ 1.88087571e+00],\n",
       "          ...,\n",
       "          [ 7.96604455e-01],\n",
       "          [ 8.02872181e-01],\n",
       "          [ 2.04422998e+00]],\n",
       "\n",
       "         [[ 2.41753268e+00],\n",
       "          [ 2.90234756e+00],\n",
       "          [ 1.78626764e+00],\n",
       "          ...,\n",
       "          [ 1.23611224e+00],\n",
       "          [ 1.93281913e+00],\n",
       "          [ 2.38242960e+00]],\n",
       "\n",
       "         [[ 2.69791126e+00],\n",
       "          [ 2.82957363e+00],\n",
       "          [ 3.56921649e+00],\n",
       "          ...,\n",
       "          [ 2.11037993e+00],\n",
       "          [ 2.56703997e+00],\n",
       "          [ 2.65462637e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 4.49403934e-02],\n",
       "          [ 1.81330070e-02],\n",
       "          [ 5.23392737e-01],\n",
       "          ...,\n",
       "          [ 1.25443235e-01],\n",
       "          [ 5.04921451e-02],\n",
       "          [-1.90902688e-02]],\n",
       "\n",
       "         [[ 6.35598183e-01],\n",
       "          [ 7.46625066e-01],\n",
       "          [ 6.27017081e-01],\n",
       "          ...,\n",
       "          [ 1.13776195e+00],\n",
       "          [ 8.42199326e-01],\n",
       "          [ 8.01237345e-01]],\n",
       "\n",
       "         [[ 1.59554482e+00],\n",
       "          [ 1.58711433e+00],\n",
       "          [ 1.54427624e+00],\n",
       "          ...,\n",
       "          [ 9.21095014e-01],\n",
       "          [ 1.02062726e+00],\n",
       "          [ 1.39626312e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.56243324e+00],\n",
       "          [ 2.16065836e+00],\n",
       "          [ 1.95397115e+00],\n",
       "          ...,\n",
       "          [ 9.62666512e-01],\n",
       "          [ 1.00572073e+00],\n",
       "          [ 2.25374556e+00]],\n",
       "\n",
       "         [[ 2.55276108e+00],\n",
       "          [ 2.96053052e+00],\n",
       "          [ 1.84011018e+00],\n",
       "          ...,\n",
       "          [ 1.42001033e+00],\n",
       "          [ 2.04855180e+00],\n",
       "          [ 2.50614238e+00]],\n",
       "\n",
       "         [[ 2.77681518e+00],\n",
       "          [ 2.97497487e+00],\n",
       "          [ 3.61599922e+00],\n",
       "          ...,\n",
       "          [ 2.20910692e+00],\n",
       "          [ 2.63238931e+00],\n",
       "          [ 2.78463411e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 1.17082015e-01],\n",
       "          [ 1.43522307e-01],\n",
       "          [ 5.89896500e-01],\n",
       "          ...,\n",
       "          [ 1.69830620e-01],\n",
       "          [ 8.04324448e-02],\n",
       "          [ 3.56611572e-02]],\n",
       "\n",
       "         [[ 7.34560013e-01],\n",
       "          [ 8.29773724e-01],\n",
       "          [ 6.28765166e-01],\n",
       "          ...,\n",
       "          [ 1.16204154e+00],\n",
       "          [ 9.01340246e-01],\n",
       "          [ 8.89847398e-01]],\n",
       "\n",
       "         [[ 1.71286476e+00],\n",
       "          [ 1.62981319e+00],\n",
       "          [ 1.53560567e+00],\n",
       "          ...,\n",
       "          [ 9.72728968e-01],\n",
       "          [ 1.07526243e+00],\n",
       "          [ 1.51131082e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.59942853e+00],\n",
       "          [ 2.23330235e+00],\n",
       "          [ 1.92820919e+00],\n",
       "          ...,\n",
       "          [ 1.16506243e+00],\n",
       "          [ 1.20026207e+00],\n",
       "          [ 2.42853570e+00]],\n",
       "\n",
       "         [[ 2.60840893e+00],\n",
       "          [ 2.97143364e+00],\n",
       "          [ 1.82785559e+00],\n",
       "          ...,\n",
       "          [ 1.57493508e+00],\n",
       "          [ 2.10826993e+00],\n",
       "          [ 2.56952000e+00]],\n",
       "\n",
       "         [[ 2.83913946e+00],\n",
       "          [ 3.04170346e+00],\n",
       "          [ 3.59176111e+00],\n",
       "          ...,\n",
       "          [ 2.31427431e+00],\n",
       "          [ 2.73937464e+00],\n",
       "          [ 2.92112494e+00]]]],\n",
       "\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "\n",
       "       [[[[ 1.03308702e+00],\n",
       "          [ 1.69784164e+00],\n",
       "          [ 1.68024600e+00],\n",
       "          ...,\n",
       "          [ 6.35935068e-01],\n",
       "          [ 8.57552111e-01],\n",
       "          [ 1.30080700e+00]],\n",
       "\n",
       "         [[ 1.29060650e+00],\n",
       "          [ 1.43619072e+00],\n",
       "          [ 1.40601134e+00],\n",
       "          ...,\n",
       "          [ 8.65531027e-01],\n",
       "          [ 8.76721501e-01],\n",
       "          [ 1.23686123e+00]],\n",
       "\n",
       "         [[ 6.98989391e-01],\n",
       "          [ 1.00895357e+00],\n",
       "          [ 1.08209515e+00],\n",
       "          ...,\n",
       "          [-1.16583623e-01],\n",
       "          [ 2.43505985e-01],\n",
       "          [ 5.70319533e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.14689499e-01],\n",
       "          [-1.88133463e-01],\n",
       "          [-9.32067409e-02],\n",
       "          ...,\n",
       "          [ 8.15775543e-02],\n",
       "          [ 7.64952600e-02],\n",
       "          [-2.72771567e-01]],\n",
       "\n",
       "         [[-1.45107293e+00],\n",
       "          [-1.73756886e+00],\n",
       "          [-4.24777597e-01],\n",
       "          ...,\n",
       "          [ 4.15101320e-01],\n",
       "          [-3.84063482e-01],\n",
       "          [-6.70521140e-01]],\n",
       "\n",
       "         [[ 7.11864293e-01],\n",
       "          [-1.77370000e+00],\n",
       "          [-1.29621482e+00],\n",
       "          ...,\n",
       "          [ 3.88957143e-01],\n",
       "          [-4.04194921e-01],\n",
       "          [ 5.50590813e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.01933658e+00],\n",
       "          [ 1.69587147e+00],\n",
       "          [ 1.70706797e+00],\n",
       "          ...,\n",
       "          [ 6.84034586e-01],\n",
       "          [ 9.30006027e-01],\n",
       "          [ 1.37139559e+00]],\n",
       "\n",
       "         [[ 1.28319097e+00],\n",
       "          [ 1.42629254e+00],\n",
       "          [ 1.42786157e+00],\n",
       "          ...,\n",
       "          [ 8.87850165e-01],\n",
       "          [ 9.03963208e-01],\n",
       "          [ 1.23649764e+00]],\n",
       "\n",
       "         [[ 5.90400338e-01],\n",
       "          [ 8.81069243e-01],\n",
       "          [ 1.00919712e+00],\n",
       "          ...,\n",
       "          [-1.79806709e-01],\n",
       "          [ 1.65868983e-01],\n",
       "          [ 5.32285929e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-3.78004042e-03],\n",
       "          [-1.53568268e-01],\n",
       "          [ 9.46630090e-02],\n",
       "          ...,\n",
       "          [ 4.11532335e-02],\n",
       "          [-5.01118042e-02],\n",
       "          [-2.38398820e-01]],\n",
       "\n",
       "         [[-1.28618920e+00],\n",
       "          [-1.71408999e+00],\n",
       "          [-3.60433400e-01],\n",
       "          ...,\n",
       "          [ 3.44249457e-01],\n",
       "          [-6.56080723e-01],\n",
       "          [-6.22785509e-01]],\n",
       "\n",
       "         [[ 7.28772700e-01],\n",
       "          [-1.78303683e+00],\n",
       "          [-1.38569200e+00],\n",
       "          ...,\n",
       "          [ 3.66918474e-01],\n",
       "          [-5.06005168e-01],\n",
       "          [ 4.92732584e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.00387836e+00],\n",
       "          [ 1.68604946e+00],\n",
       "          [ 1.72188222e+00],\n",
       "          ...,\n",
       "          [ 7.26276517e-01],\n",
       "          [ 9.32465494e-01],\n",
       "          [ 1.42219031e+00]],\n",
       "\n",
       "         [[ 1.32123303e+00],\n",
       "          [ 1.43914521e+00],\n",
       "          [ 1.39888322e+00],\n",
       "          ...,\n",
       "          [ 8.96897554e-01],\n",
       "          [ 9.20771301e-01],\n",
       "          [ 1.26074481e+00]],\n",
       "\n",
       "         [[ 5.65136909e-01],\n",
       "          [ 7.83142865e-01],\n",
       "          [ 9.42872107e-01],\n",
       "          ...,\n",
       "          [-2.61655658e-01],\n",
       "          [ 9.83102769e-02],\n",
       "          [ 4.74030286e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.68845165e-02],\n",
       "          [-2.34260172e-01],\n",
       "          [ 1.46705762e-01],\n",
       "          ...,\n",
       "          [ 2.00828519e-02],\n",
       "          [-1.59957796e-01],\n",
       "          [-2.45466873e-01]],\n",
       "\n",
       "         [[-1.19658053e+00],\n",
       "          [-1.69308519e+00],\n",
       "          [-3.87873232e-01],\n",
       "          ...,\n",
       "          [ 2.23403528e-01],\n",
       "          [-8.86636198e-01],\n",
       "          [-5.75449407e-01]],\n",
       "\n",
       "         [[ 7.08105505e-01],\n",
       "          [-1.81255817e+00],\n",
       "          [-1.48121953e+00],\n",
       "          ...,\n",
       "          [ 3.76806200e-01],\n",
       "          [-5.42918205e-01],\n",
       "          [ 4.41376567e-01]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 1.12776792e+00],\n",
       "          [ 1.46306956e+00],\n",
       "          [ 1.48767996e+00],\n",
       "          ...,\n",
       "          [ 1.62803185e+00],\n",
       "          [ 1.46827650e+00],\n",
       "          [ 1.58148670e+00]],\n",
       "\n",
       "         [[ 1.46356225e+00],\n",
       "          [ 1.23436880e+00],\n",
       "          [ 1.02596915e+00],\n",
       "          ...,\n",
       "          [ 1.11573827e+00],\n",
       "          [ 1.37246263e+00],\n",
       "          [ 1.41418564e+00]],\n",
       "\n",
       "         [[ 9.68146980e-01],\n",
       "          [ 1.30618238e+00],\n",
       "          [ 1.04609156e+00],\n",
       "          ...,\n",
       "          [ 4.44402456e-01],\n",
       "          [ 7.58437216e-01],\n",
       "          [ 6.08541787e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.19015563e-01],\n",
       "          [ 9.85141039e-01],\n",
       "          [-2.48603120e-01],\n",
       "          ...,\n",
       "          [ 2.63715446e-01],\n",
       "          [ 1.05289483e+00],\n",
       "          [ 1.56243145e+00]],\n",
       "\n",
       "         [[-4.84912544e-01],\n",
       "          [ 2.99735755e-01],\n",
       "          [ 6.49971843e-01],\n",
       "          ...,\n",
       "          [ 1.26385808e+00],\n",
       "          [ 1.48721111e+00],\n",
       "          [-7.69702435e-01]],\n",
       "\n",
       "         [[ 5.59716761e-01],\n",
       "          [ 1.43127963e-01],\n",
       "          [-2.02869803e-01],\n",
       "          ...,\n",
       "          [ 9.34976637e-01],\n",
       "          [ 8.76965463e-01],\n",
       "          [ 1.68173164e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.05417287e+00],\n",
       "          [ 1.40661156e+00],\n",
       "          [ 1.42573321e+00],\n",
       "          ...,\n",
       "          [ 1.73294747e+00],\n",
       "          [ 1.50425363e+00],\n",
       "          [ 1.52578843e+00]],\n",
       "\n",
       "         [[ 1.40035176e+00],\n",
       "          [ 1.14405310e+00],\n",
       "          [ 9.71972525e-01],\n",
       "          ...,\n",
       "          [ 1.21504438e+00],\n",
       "          [ 1.42232680e+00],\n",
       "          [ 1.37450683e+00]],\n",
       "\n",
       "         [[ 9.96286869e-01],\n",
       "          [ 1.25535703e+00],\n",
       "          [ 9.92358804e-01],\n",
       "          ...,\n",
       "          [ 4.65553015e-01],\n",
       "          [ 8.08831573e-01],\n",
       "          [ 6.76446855e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.90573740e-01],\n",
       "          [ 9.78541136e-01],\n",
       "          [-2.05351770e-01],\n",
       "          ...,\n",
       "          [ 2.11067826e-01],\n",
       "          [ 9.68537629e-01],\n",
       "          [ 1.43002021e+00]],\n",
       "\n",
       "         [[-3.61989111e-01],\n",
       "          [ 3.57380569e-01],\n",
       "          [ 7.39674449e-01],\n",
       "          ...,\n",
       "          [ 1.20333457e+00],\n",
       "          [ 1.27605832e+00],\n",
       "          [-5.52498937e-01]],\n",
       "\n",
       "         [[ 6.97079420e-01],\n",
       "          [ 2.63943672e-01],\n",
       "          [-4.88851480e-02],\n",
       "          ...,\n",
       "          [ 1.08215559e+00],\n",
       "          [ 8.70064735e-01],\n",
       "          [ 1.80758521e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.05650604e+00],\n",
       "          [ 1.36264014e+00],\n",
       "          [ 1.34554887e+00],\n",
       "          ...,\n",
       "          [ 1.74142349e+00],\n",
       "          [ 1.51377082e+00],\n",
       "          [ 1.57523370e+00]],\n",
       "\n",
       "         [[ 1.34015131e+00],\n",
       "          [ 1.09261858e+00],\n",
       "          [ 9.20372546e-01],\n",
       "          ...,\n",
       "          [ 1.34204280e+00],\n",
       "          [ 1.49625623e+00],\n",
       "          [ 1.37692010e+00]],\n",
       "\n",
       "         [[ 1.00881195e+00],\n",
       "          [ 1.18061197e+00],\n",
       "          [ 9.32613790e-01],\n",
       "          ...,\n",
       "          [ 5.74113429e-01],\n",
       "          [ 9.43301082e-01],\n",
       "          [ 7.84881473e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 4.41790104e-01],\n",
       "          [ 8.83235514e-01],\n",
       "          [-2.20139697e-01],\n",
       "          ...,\n",
       "          [ 2.10817009e-01],\n",
       "          [ 9.21731591e-01],\n",
       "          [ 1.39006925e+00]],\n",
       "\n",
       "         [[-2.74293870e-01],\n",
       "          [ 3.96349132e-01],\n",
       "          [ 7.88462937e-01],\n",
       "          ...,\n",
       "          [ 1.19147193e+00],\n",
       "          [ 1.09640014e+00],\n",
       "          [-4.41391051e-01]],\n",
       "\n",
       "         [[ 8.03167760e-01],\n",
       "          [ 3.22496265e-01],\n",
       "          [ 5.22812046e-02],\n",
       "          ...,\n",
       "          [ 1.22210550e+00],\n",
       "          [ 8.73679459e-01],\n",
       "          [ 1.67143539e-01]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[ 1.01933658e+00],\n",
       "          [ 1.69587147e+00],\n",
       "          [ 1.70706797e+00],\n",
       "          ...,\n",
       "          [ 6.84034586e-01],\n",
       "          [ 9.30006027e-01],\n",
       "          [ 1.37139559e+00]],\n",
       "\n",
       "         [[ 1.28319097e+00],\n",
       "          [ 1.42629254e+00],\n",
       "          [ 1.42786157e+00],\n",
       "          ...,\n",
       "          [ 8.87850165e-01],\n",
       "          [ 9.03963208e-01],\n",
       "          [ 1.23649764e+00]],\n",
       "\n",
       "         [[ 5.90400338e-01],\n",
       "          [ 8.81069243e-01],\n",
       "          [ 1.00919712e+00],\n",
       "          ...,\n",
       "          [-1.79806709e-01],\n",
       "          [ 1.65868983e-01],\n",
       "          [ 5.32285929e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-3.78004042e-03],\n",
       "          [-1.53568268e-01],\n",
       "          [ 9.46630090e-02],\n",
       "          ...,\n",
       "          [ 4.11532335e-02],\n",
       "          [-5.01118042e-02],\n",
       "          [-2.38398820e-01]],\n",
       "\n",
       "         [[-1.28618920e+00],\n",
       "          [-1.71408999e+00],\n",
       "          [-3.60433400e-01],\n",
       "          ...,\n",
       "          [ 3.44249457e-01],\n",
       "          [-6.56080723e-01],\n",
       "          [-6.22785509e-01]],\n",
       "\n",
       "         [[ 7.28772700e-01],\n",
       "          [-1.78303683e+00],\n",
       "          [-1.38569200e+00],\n",
       "          ...,\n",
       "          [ 3.66918474e-01],\n",
       "          [-5.06005168e-01],\n",
       "          [ 4.92732584e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.00387836e+00],\n",
       "          [ 1.68604946e+00],\n",
       "          [ 1.72188222e+00],\n",
       "          ...,\n",
       "          [ 7.26276517e-01],\n",
       "          [ 9.32465494e-01],\n",
       "          [ 1.42219031e+00]],\n",
       "\n",
       "         [[ 1.32123303e+00],\n",
       "          [ 1.43914521e+00],\n",
       "          [ 1.39888322e+00],\n",
       "          ...,\n",
       "          [ 8.96897554e-01],\n",
       "          [ 9.20771301e-01],\n",
       "          [ 1.26074481e+00]],\n",
       "\n",
       "         [[ 5.65136909e-01],\n",
       "          [ 7.83142865e-01],\n",
       "          [ 9.42872107e-01],\n",
       "          ...,\n",
       "          [-2.61655658e-01],\n",
       "          [ 9.83102769e-02],\n",
       "          [ 4.74030286e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.68845165e-02],\n",
       "          [-2.34260172e-01],\n",
       "          [ 1.46705762e-01],\n",
       "          ...,\n",
       "          [ 2.00828519e-02],\n",
       "          [-1.59957796e-01],\n",
       "          [-2.45466873e-01]],\n",
       "\n",
       "         [[-1.19658053e+00],\n",
       "          [-1.69308519e+00],\n",
       "          [-3.87873232e-01],\n",
       "          ...,\n",
       "          [ 2.23403528e-01],\n",
       "          [-8.86636198e-01],\n",
       "          [-5.75449407e-01]],\n",
       "\n",
       "         [[ 7.08105505e-01],\n",
       "          [-1.81255817e+00],\n",
       "          [-1.48121953e+00],\n",
       "          ...,\n",
       "          [ 3.76806200e-01],\n",
       "          [-5.42918205e-01],\n",
       "          [ 4.41376567e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.03278148e+00],\n",
       "          [ 1.71551669e+00],\n",
       "          [ 1.79083228e+00],\n",
       "          ...,\n",
       "          [ 7.72730947e-01],\n",
       "          [ 9.34396684e-01],\n",
       "          [ 1.51204646e+00]],\n",
       "\n",
       "         [[ 1.37950313e+00],\n",
       "          [ 1.47791374e+00],\n",
       "          [ 1.37511265e+00],\n",
       "          ...,\n",
       "          [ 9.63298142e-01],\n",
       "          [ 9.72806811e-01],\n",
       "          [ 1.31849670e+00]],\n",
       "\n",
       "         [[ 5.66580474e-01],\n",
       "          [ 7.37329125e-01],\n",
       "          [ 9.06634450e-01],\n",
       "          ...,\n",
       "          [-2.58229762e-01],\n",
       "          [ 9.68143344e-02],\n",
       "          [ 4.63079482e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.06299096e-01],\n",
       "          [-2.37500936e-01],\n",
       "          [ 2.25534946e-01],\n",
       "          ...,\n",
       "          [-6.12203404e-02],\n",
       "          [-2.51177311e-01],\n",
       "          [-1.40642911e-01]],\n",
       "\n",
       "         [[-1.08322549e+00],\n",
       "          [-1.72485042e+00],\n",
       "          [-3.78283978e-01],\n",
       "          ...,\n",
       "          [ 9.36858431e-02],\n",
       "          [-1.12265623e+00],\n",
       "          [-6.07779026e-01]],\n",
       "\n",
       "         [[ 7.15143144e-01],\n",
       "          [-1.88257873e+00],\n",
       "          [-1.63362455e+00],\n",
       "          ...,\n",
       "          [ 3.28968883e-01],\n",
       "          [-6.43231690e-01],\n",
       "          [ 3.52597058e-01]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 1.05417287e+00],\n",
       "          [ 1.40661156e+00],\n",
       "          [ 1.42573321e+00],\n",
       "          ...,\n",
       "          [ 1.73294747e+00],\n",
       "          [ 1.50425363e+00],\n",
       "          [ 1.52578843e+00]],\n",
       "\n",
       "         [[ 1.40035176e+00],\n",
       "          [ 1.14405310e+00],\n",
       "          [ 9.71972525e-01],\n",
       "          ...,\n",
       "          [ 1.21504438e+00],\n",
       "          [ 1.42232680e+00],\n",
       "          [ 1.37450683e+00]],\n",
       "\n",
       "         [[ 9.96286869e-01],\n",
       "          [ 1.25535703e+00],\n",
       "          [ 9.92358804e-01],\n",
       "          ...,\n",
       "          [ 4.65553015e-01],\n",
       "          [ 8.08831573e-01],\n",
       "          [ 6.76446855e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.90573740e-01],\n",
       "          [ 9.78541136e-01],\n",
       "          [-2.05351770e-01],\n",
       "          ...,\n",
       "          [ 2.11067826e-01],\n",
       "          [ 9.68537629e-01],\n",
       "          [ 1.43002021e+00]],\n",
       "\n",
       "         [[-3.61989111e-01],\n",
       "          [ 3.57380569e-01],\n",
       "          [ 7.39674449e-01],\n",
       "          ...,\n",
       "          [ 1.20333457e+00],\n",
       "          [ 1.27605832e+00],\n",
       "          [-5.52498937e-01]],\n",
       "\n",
       "         [[ 6.97079420e-01],\n",
       "          [ 2.63943672e-01],\n",
       "          [-4.88851480e-02],\n",
       "          ...,\n",
       "          [ 1.08215559e+00],\n",
       "          [ 8.70064735e-01],\n",
       "          [ 1.80758521e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.05650604e+00],\n",
       "          [ 1.36264014e+00],\n",
       "          [ 1.34554887e+00],\n",
       "          ...,\n",
       "          [ 1.74142349e+00],\n",
       "          [ 1.51377082e+00],\n",
       "          [ 1.57523370e+00]],\n",
       "\n",
       "         [[ 1.34015131e+00],\n",
       "          [ 1.09261858e+00],\n",
       "          [ 9.20372546e-01],\n",
       "          ...,\n",
       "          [ 1.34204280e+00],\n",
       "          [ 1.49625623e+00],\n",
       "          [ 1.37692010e+00]],\n",
       "\n",
       "         [[ 1.00881195e+00],\n",
       "          [ 1.18061197e+00],\n",
       "          [ 9.32613790e-01],\n",
       "          ...,\n",
       "          [ 5.74113429e-01],\n",
       "          [ 9.43301082e-01],\n",
       "          [ 7.84881473e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 4.41790104e-01],\n",
       "          [ 8.83235514e-01],\n",
       "          [-2.20139697e-01],\n",
       "          ...,\n",
       "          [ 2.10817009e-01],\n",
       "          [ 9.21731591e-01],\n",
       "          [ 1.39006925e+00]],\n",
       "\n",
       "         [[-2.74293870e-01],\n",
       "          [ 3.96349132e-01],\n",
       "          [ 7.88462937e-01],\n",
       "          ...,\n",
       "          [ 1.19147193e+00],\n",
       "          [ 1.09640014e+00],\n",
       "          [-4.41391051e-01]],\n",
       "\n",
       "         [[ 8.03167760e-01],\n",
       "          [ 3.22496265e-01],\n",
       "          [ 5.22812046e-02],\n",
       "          ...,\n",
       "          [ 1.22210550e+00],\n",
       "          [ 8.73679459e-01],\n",
       "          [ 1.67143539e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.10257900e+00],\n",
       "          [ 1.40027845e+00],\n",
       "          [ 1.35083449e+00],\n",
       "          ...,\n",
       "          [ 1.89066970e+00],\n",
       "          [ 1.68356597e+00],\n",
       "          [ 1.72060966e+00]],\n",
       "\n",
       "         [[ 1.28571832e+00],\n",
       "          [ 1.06180978e+00],\n",
       "          [ 9.03383911e-01],\n",
       "          ...,\n",
       "          [ 1.46462691e+00],\n",
       "          [ 1.54794002e+00],\n",
       "          [ 1.37113214e+00]],\n",
       "\n",
       "         [[ 9.69825208e-01],\n",
       "          [ 1.09793878e+00],\n",
       "          [ 8.88440430e-01],\n",
       "          ...,\n",
       "          [ 5.86842775e-01],\n",
       "          [ 9.77596521e-01],\n",
       "          [ 7.69184649e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 5.72660208e-01],\n",
       "          [ 8.72038662e-01],\n",
       "          [-1.47511303e-01],\n",
       "          ...,\n",
       "          [ 2.52896816e-01],\n",
       "          [ 8.90182555e-01],\n",
       "          [ 1.44093525e+00]],\n",
       "\n",
       "         [[-2.07899272e-01],\n",
       "          [ 3.79277438e-01],\n",
       "          [ 8.15327942e-01],\n",
       "          ...,\n",
       "          [ 1.17658854e+00],\n",
       "          [ 1.03979814e+00],\n",
       "          [-3.05826843e-01]],\n",
       "\n",
       "         [[ 8.79944444e-01],\n",
       "          [ 3.39401633e-01],\n",
       "          [ 8.96904618e-02],\n",
       "          ...,\n",
       "          [ 1.31369293e+00],\n",
       "          [ 8.91908348e-01],\n",
       "          [ 1.85550138e-01]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[ 1.00387836e+00],\n",
       "          [ 1.68604946e+00],\n",
       "          [ 1.72188222e+00],\n",
       "          ...,\n",
       "          [ 7.26276517e-01],\n",
       "          [ 9.32465494e-01],\n",
       "          [ 1.42219031e+00]],\n",
       "\n",
       "         [[ 1.32123303e+00],\n",
       "          [ 1.43914521e+00],\n",
       "          [ 1.39888322e+00],\n",
       "          ...,\n",
       "          [ 8.96897554e-01],\n",
       "          [ 9.20771301e-01],\n",
       "          [ 1.26074481e+00]],\n",
       "\n",
       "         [[ 5.65136909e-01],\n",
       "          [ 7.83142865e-01],\n",
       "          [ 9.42872107e-01],\n",
       "          ...,\n",
       "          [-2.61655658e-01],\n",
       "          [ 9.83102769e-02],\n",
       "          [ 4.74030286e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.68845165e-02],\n",
       "          [-2.34260172e-01],\n",
       "          [ 1.46705762e-01],\n",
       "          ...,\n",
       "          [ 2.00828519e-02],\n",
       "          [-1.59957796e-01],\n",
       "          [-2.45466873e-01]],\n",
       "\n",
       "         [[-1.19658053e+00],\n",
       "          [-1.69308519e+00],\n",
       "          [-3.87873232e-01],\n",
       "          ...,\n",
       "          [ 2.23403528e-01],\n",
       "          [-8.86636198e-01],\n",
       "          [-5.75449407e-01]],\n",
       "\n",
       "         [[ 7.08105505e-01],\n",
       "          [-1.81255817e+00],\n",
       "          [-1.48121953e+00],\n",
       "          ...,\n",
       "          [ 3.76806200e-01],\n",
       "          [-5.42918205e-01],\n",
       "          [ 4.41376567e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.03278148e+00],\n",
       "          [ 1.71551669e+00],\n",
       "          [ 1.79083228e+00],\n",
       "          ...,\n",
       "          [ 7.72730947e-01],\n",
       "          [ 9.34396684e-01],\n",
       "          [ 1.51204646e+00]],\n",
       "\n",
       "         [[ 1.37950313e+00],\n",
       "          [ 1.47791374e+00],\n",
       "          [ 1.37511265e+00],\n",
       "          ...,\n",
       "          [ 9.63298142e-01],\n",
       "          [ 9.72806811e-01],\n",
       "          [ 1.31849670e+00]],\n",
       "\n",
       "         [[ 5.66580474e-01],\n",
       "          [ 7.37329125e-01],\n",
       "          [ 9.06634450e-01],\n",
       "          ...,\n",
       "          [-2.58229762e-01],\n",
       "          [ 9.68143344e-02],\n",
       "          [ 4.63079482e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.06299096e-01],\n",
       "          [-2.37500936e-01],\n",
       "          [ 2.25534946e-01],\n",
       "          ...,\n",
       "          [-6.12203404e-02],\n",
       "          [-2.51177311e-01],\n",
       "          [-1.40642911e-01]],\n",
       "\n",
       "         [[-1.08322549e+00],\n",
       "          [-1.72485042e+00],\n",
       "          [-3.78283978e-01],\n",
       "          ...,\n",
       "          [ 9.36858431e-02],\n",
       "          [-1.12265623e+00],\n",
       "          [-6.07779026e-01]],\n",
       "\n",
       "         [[ 7.15143144e-01],\n",
       "          [-1.88257873e+00],\n",
       "          [-1.63362455e+00],\n",
       "          ...,\n",
       "          [ 3.28968883e-01],\n",
       "          [-6.43231690e-01],\n",
       "          [ 3.52597058e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.01747394e+00],\n",
       "          [ 1.75199354e+00],\n",
       "          [ 1.88086724e+00],\n",
       "          ...,\n",
       "          [ 7.64736056e-01],\n",
       "          [ 8.79015386e-01],\n",
       "          [ 1.48260689e+00]],\n",
       "\n",
       "         [[ 1.41790557e+00],\n",
       "          [ 1.55994701e+00],\n",
       "          [ 1.45534444e+00],\n",
       "          ...,\n",
       "          [ 9.44264352e-01],\n",
       "          [ 9.50580418e-01],\n",
       "          [ 1.30947375e+00]],\n",
       "\n",
       "         [[ 6.60528541e-01],\n",
       "          [ 8.55886579e-01],\n",
       "          [ 1.04852355e+00],\n",
       "          ...,\n",
       "          [-3.15560132e-01],\n",
       "          [ 2.32891478e-02],\n",
       "          [ 4.68549639e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 4.73285794e-01],\n",
       "          [-1.55688807e-01],\n",
       "          [ 3.91176611e-01],\n",
       "          ...,\n",
       "          [-1.92234024e-01],\n",
       "          [-3.26620191e-01],\n",
       "          [-2.29428038e-02]],\n",
       "\n",
       "         [[-8.59293401e-01],\n",
       "          [-1.61295569e+00],\n",
       "          [-2.61210710e-01],\n",
       "          ...,\n",
       "          [ 9.90962461e-02],\n",
       "          [-1.20373106e+00],\n",
       "          [-5.23873568e-01]],\n",
       "\n",
       "         [[ 7.26767242e-01],\n",
       "          [-1.84673488e+00],\n",
       "          [-1.75555873e+00],\n",
       "          ...,\n",
       "          [ 3.70759934e-01],\n",
       "          [-6.50319457e-01],\n",
       "          [ 3.74336809e-01]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 1.05650604e+00],\n",
       "          [ 1.36264014e+00],\n",
       "          [ 1.34554887e+00],\n",
       "          ...,\n",
       "          [ 1.74142349e+00],\n",
       "          [ 1.51377082e+00],\n",
       "          [ 1.57523370e+00]],\n",
       "\n",
       "         [[ 1.34015131e+00],\n",
       "          [ 1.09261858e+00],\n",
       "          [ 9.20372546e-01],\n",
       "          ...,\n",
       "          [ 1.34204280e+00],\n",
       "          [ 1.49625623e+00],\n",
       "          [ 1.37692010e+00]],\n",
       "\n",
       "         [[ 1.00881195e+00],\n",
       "          [ 1.18061197e+00],\n",
       "          [ 9.32613790e-01],\n",
       "          ...,\n",
       "          [ 5.74113429e-01],\n",
       "          [ 9.43301082e-01],\n",
       "          [ 7.84881473e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 4.41790104e-01],\n",
       "          [ 8.83235514e-01],\n",
       "          [-2.20139697e-01],\n",
       "          ...,\n",
       "          [ 2.10817009e-01],\n",
       "          [ 9.21731591e-01],\n",
       "          [ 1.39006925e+00]],\n",
       "\n",
       "         [[-2.74293870e-01],\n",
       "          [ 3.96349132e-01],\n",
       "          [ 7.88462937e-01],\n",
       "          ...,\n",
       "          [ 1.19147193e+00],\n",
       "          [ 1.09640014e+00],\n",
       "          [-4.41391051e-01]],\n",
       "\n",
       "         [[ 8.03167760e-01],\n",
       "          [ 3.22496265e-01],\n",
       "          [ 5.22812046e-02],\n",
       "          ...,\n",
       "          [ 1.22210550e+00],\n",
       "          [ 8.73679459e-01],\n",
       "          [ 1.67143539e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.10257900e+00],\n",
       "          [ 1.40027845e+00],\n",
       "          [ 1.35083449e+00],\n",
       "          ...,\n",
       "          [ 1.89066970e+00],\n",
       "          [ 1.68356597e+00],\n",
       "          [ 1.72060966e+00]],\n",
       "\n",
       "         [[ 1.28571832e+00],\n",
       "          [ 1.06180978e+00],\n",
       "          [ 9.03383911e-01],\n",
       "          ...,\n",
       "          [ 1.46462691e+00],\n",
       "          [ 1.54794002e+00],\n",
       "          [ 1.37113214e+00]],\n",
       "\n",
       "         [[ 9.69825208e-01],\n",
       "          [ 1.09793878e+00],\n",
       "          [ 8.88440430e-01],\n",
       "          ...,\n",
       "          [ 5.86842775e-01],\n",
       "          [ 9.77596521e-01],\n",
       "          [ 7.69184649e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 5.72660208e-01],\n",
       "          [ 8.72038662e-01],\n",
       "          [-1.47511303e-01],\n",
       "          ...,\n",
       "          [ 2.52896816e-01],\n",
       "          [ 8.90182555e-01],\n",
       "          [ 1.44093525e+00]],\n",
       "\n",
       "         [[-2.07899272e-01],\n",
       "          [ 3.79277438e-01],\n",
       "          [ 8.15327942e-01],\n",
       "          ...,\n",
       "          [ 1.17658854e+00],\n",
       "          [ 1.03979814e+00],\n",
       "          [-3.05826843e-01]],\n",
       "\n",
       "         [[ 8.79944444e-01],\n",
       "          [ 3.39401633e-01],\n",
       "          [ 8.96904618e-02],\n",
       "          ...,\n",
       "          [ 1.31369293e+00],\n",
       "          [ 8.91908348e-01],\n",
       "          [ 1.85550138e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.08343196e+00],\n",
       "          [ 1.40071011e+00],\n",
       "          [ 1.26597083e+00],\n",
       "          ...,\n",
       "          [ 1.91848624e+00],\n",
       "          [ 1.70034504e+00],\n",
       "          [ 1.72931206e+00]],\n",
       "\n",
       "         [[ 1.22668839e+00],\n",
       "          [ 1.00202823e+00],\n",
       "          [ 8.20403337e-01],\n",
       "          ...,\n",
       "          [ 1.46821892e+00],\n",
       "          [ 1.56984556e+00],\n",
       "          [ 1.32960367e+00]],\n",
       "\n",
       "         [[ 9.50440466e-01],\n",
       "          [ 1.02767503e+00],\n",
       "          [ 8.38067353e-01],\n",
       "          ...,\n",
       "          [ 5.77073514e-01],\n",
       "          [ 9.79359865e-01],\n",
       "          [ 7.76819885e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 7.31231928e-01],\n",
       "          [ 9.20140266e-01],\n",
       "          [-7.74875283e-02],\n",
       "          ...,\n",
       "          [ 2.91301161e-01],\n",
       "          [ 9.10074115e-01],\n",
       "          [ 1.55428052e+00]],\n",
       "\n",
       "         [[-1.03045069e-01],\n",
       "          [ 4.12901521e-01],\n",
       "          [ 8.12982798e-01],\n",
       "          ...,\n",
       "          [ 1.21738899e+00],\n",
       "          [ 1.13054645e+00],\n",
       "          [-1.94724575e-01]],\n",
       "\n",
       "         [[ 1.00549436e+00],\n",
       "          [ 4.23224896e-01],\n",
       "          [ 1.41115010e-01],\n",
       "          ...,\n",
       "          [ 1.50357926e+00],\n",
       "          [ 9.27153587e-01],\n",
       "          [ 2.41223156e-01]]]]], dtype=float32)</pre></div></li><li class='xr-section-item'><input id='section-dc4af30d-6728-4102-ab85-04e36be98ac9' class='xr-section-summary-in' type='checkbox'  checked><label for='section-dc4af30d-6728-4102-ab85-04e36be98ac9' class='xr-section-summary' >Coordinates: <span>(5)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>lag</span></div><div class='xr-var-dims'>(lag)</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>0 1 2 3 4 5 6 ... 54 55 56 57 58 59</div><input id='attrs-2587fe6c-d1bd-4239-b445-7e59a94ea3c5' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-2587fe6c-d1bd-4239-b445-7e59a94ea3c5' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-1529c49d-8a80-4ffb-be07-535fbc3670f6' class='xr-var-data-in' type='checkbox'><label for='data-1529c49d-8a80-4ffb-be07-535fbc3670f6' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><pre class='xr-var-data'>array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
       "       54, 55, 56, 57, 58, 59])</pre></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>lat</span></div><div class='xr-var-dims'>(lat)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>60.0 58.0 56.0 ... -58.0 -60.0</div><input id='attrs-371e7d37-baf6-4c94-9777-b8838056dc84' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-371e7d37-baf6-4c94-9777-b8838056dc84' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-e2c7dcd0-322e-4fbc-b9bd-6f0bafd1cb3e' class='xr-var-data-in' type='checkbox'><label for='data-e2c7dcd0-322e-4fbc-b9bd-6f0bafd1cb3e' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>standard_name :</span></dt><dd>latitude</dd><dt><span>long_name :</span></dt><dd>latitude</dd><dt><span>units :</span></dt><dd>degrees_north</dd><dt><span>axis :</span></dt><dd>Y</dd></dl></div><pre class='xr-var-data'>array([ 60.,  58.,  56.,  54.,  52.,  50.,  48.,  46.,  44.,  42.,  40.,  38.,\n",
       "        36.,  34.,  32.,  30.,  28.,  26.,  24.,  22.,  20.,  18.,  16.,  14.,\n",
       "        12.,  10.,   8.,   6.,   4.,   2.,   0.,  -2.,  -4.,  -6.,  -8., -10.,\n",
       "       -12., -14., -16., -18., -20., -22., -24., -26., -28., -30., -32., -34.,\n",
       "       -36., -38., -40., -42., -44., -46., -48., -50., -52., -54., -56., -58.,\n",
       "       -60.], dtype=float32)</pre></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>lon</span></div><div class='xr-var-dims'>(lon)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>-180.0 -178.0 ... 176.0 178.0</div><input id='attrs-5ff08c44-31de-4be9-8ea4-7873678ae509' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-5ff08c44-31de-4be9-8ea4-7873678ae509' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-d2acfd63-47ae-42b9-8740-786c3e4f76d6' class='xr-var-data-in' type='checkbox'><label for='data-d2acfd63-47ae-42b9-8740-786c3e4f76d6' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>standard_name :</span></dt><dd>longitude</dd><dt><span>long_name :</span></dt><dd>longitude</dd><dt><span>units :</span></dt><dd>degrees_east</dd><dt><span>axis :</span></dt><dd>X</dd></dl></div><pre class='xr-var-data'>array([-180., -178., -176., -174., -172., -170., -168., -166., -164., -162.,\n",
       "       -160., -158., -156., -154., -152., -150., -148., -146., -144., -142.,\n",
       "       -140., -138., -136., -134., -132., -130., -128., -126., -124., -122.,\n",
       "       -120., -118., -116., -114., -112., -110., -108., -106., -104., -102.,\n",
       "       -100.,  -98.,  -96.,  -94.,  -92.,  -90.,  -88.,  -86.,  -84.,  -82.,\n",
       "        -80.,  -78.,  -76.,  -74.,  -72.,  -70.,  -68.,  -66.,  -64.,  -62.,\n",
       "        -60.,  -58.,  -56.,  -54.,  -52.,  -50.,  -48.,  -46.,  -44.,  -42.,\n",
       "        -40.,  -38.,  -36.,  -34.,  -32.,  -30.,  -28.,  -26.,  -24.,  -22.,\n",
       "        -20.,  -18.,  -16.,  -14.,  -12.,  -10.,   -8.,   -6.,   -4.,   -2.,\n",
       "          0.,    2.,    4.,    6.,    8.,   10.,   12.,   14.,   16.,   18.,\n",
       "         20.,   22.,   24.,   26.,   28.,   30.,   32.,   34.,   36.,   38.,\n",
       "         40.,   42.,   44.,   46.,   48.,   50.,   52.,   54.,   56.,   58.,\n",
       "         60.,   62.,   64.,   66.,   68.,   70.,   72.,   74.,   76.,   78.,\n",
       "         80.,   82.,   84.,   86.,   88.,   90.,   92.,   94.,   96.,   98.,\n",
       "        100.,  102.,  104.,  106.,  108.,  110.,  112.,  114.,  116.,  118.,\n",
       "        120.,  122.,  124.,  126.,  128.,  130.,  132.,  134.,  136.,  138.,\n",
       "        140.,  142.,  144.,  146.,  148.,  150.,  152.,  154.,  156.,  158.,\n",
       "        160.,  162.,  164.,  166.,  168.,  170.,  172.,  174.,  176.,  178.],\n",
       "      dtype=float32)</pre></li><li class='xr-var-item'><div class='xr-var-name'><span>dayofyear</span></div><div class='xr-var-dims'>(time, lag)</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>198 199 200 201 ... 340 341 342 343</div><input id='attrs-067be806-49c6-4329-a2ba-6a640969c7dd' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-067be806-49c6-4329-a2ba-6a640969c7dd' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-b4c47826-a1ad-4365-b4d0-669cbc9a322a' class='xr-var-data-in' type='checkbox'><label for='data-b4c47826-a1ad-4365-b4d0-669cbc9a322a' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><pre class='xr-var-data'>array([[198, 199, 200, ..., 255, 256, 257],\n",
       "       [199, 200, 201, ..., 256, 257, 258],\n",
       "       [200, 201, 202, ..., 257, 258, 259],\n",
       "       ...,\n",
       "       [282, 283, 284, ..., 339, 340, 341],\n",
       "       [283, 284, 285, ..., 340, 341, 342],\n",
       "       [284, 285, 286, ..., 341, 342, 343]])</pre></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>time</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>1981-10-04 ... 2020-12-28</div><input id='attrs-1ef39f7d-958c-4bb7-a60f-3dd752767574' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-1ef39f7d-958c-4bb7-a60f-3dd752767574' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-7bc5920d-3127-4c95-955c-62ba17bb169a' class='xr-var-data-in' type='checkbox'><label for='data-7bc5920d-3127-4c95-955c-62ba17bb169a' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><pre class='xr-var-data'>array([&#x27;1981-10-04T00:00:00.000000000&#x27;, &#x27;1981-10-05T00:00:00.000000000&#x27;,\n",
       "       &#x27;1981-10-06T00:00:00.000000000&#x27;, ..., &#x27;2020-12-26T00:00:00.000000000&#x27;,\n",
       "       &#x27;2020-12-27T00:00:00.000000000&#x27;, &#x27;2020-12-28T00:00:00.000000000&#x27;],\n",
       "      dtype=&#x27;datetime64[ns]&#x27;)</pre></li></ul></div></li><li class='xr-section-item'><input id='section-8ab320fb-481c-4d6a-948d-c2d4a984c78c' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-8ab320fb-481c-4d6a-948d-c2d4a984c78c' class='xr-section-summary'  title='Expand/collapse section'>Attributes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.DataArray 'sst' (time: 3440, lag: 60, lat: 61, lon: 180, channel: 1)>\n",
       "array([[[[[-3.62185866e-01],\n",
       "          [ 3.10576499e-01],\n",
       "          [ 5.07928073e-01],\n",
       "          ...,\n",
       "          [ 3.67879808e-01],\n",
       "          [ 3.25133681e-01],\n",
       "          [-3.39804977e-01]],\n",
       "\n",
       "         [[ 4.65958118e-02],\n",
       "          [ 4.00977582e-01],\n",
       "          [ 7.80841947e-01],\n",
       "          ...,\n",
       "          [ 6.71721101e-01],\n",
       "          [ 6.03961468e-01],\n",
       "          [ 5.84028900e-01]],\n",
       "\n",
       "         [[ 6.76504612e-01],\n",
       "          [ 8.65939558e-01],\n",
       "          [ 7.75304794e-01],\n",
       "          ...,\n",
       "          [ 7.52957523e-01],\n",
       "          [ 6.04648471e-01],\n",
       "          [ 6.45179451e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.31917195e-02],\n",
       "          [ 6.20493054e-01],\n",
       "          [ 4.20646071e-01],\n",
       "          ...,\n",
       "          [ 2.08507404e-01],\n",
       "          [ 3.40076119e-01],\n",
       "          [ 5.22748530e-01]],\n",
       "\n",
       "         [[ 7.42579639e-01],\n",
       "          [-1.36416182e-01],\n",
       "          [-2.30123685e-03],\n",
       "          ...,\n",
       "          [ 1.64106369e+00],\n",
       "          [ 1.28742111e+00],\n",
       "          [ 1.18877852e+00]],\n",
       "\n",
       "         [[ 8.97619605e-01],\n",
       "          [ 1.49698973e+00],\n",
       "          [ 1.96731377e+00],\n",
       "          ...,\n",
       "          [ 1.83101034e+00],\n",
       "          [ 1.78363490e+00],\n",
       "          [ 1.56304681e+00]]],\n",
       "\n",
       "\n",
       "        [[[-3.54809254e-01],\n",
       "          [ 3.49349380e-01],\n",
       "          [ 5.40288150e-01],\n",
       "          ...,\n",
       "          [ 4.48003441e-01],\n",
       "          [ 3.88327032e-01],\n",
       "          [-2.97131151e-01]],\n",
       "\n",
       "         [[ 1.12070717e-01],\n",
       "          [ 4.23052967e-01],\n",
       "          [ 7.83311009e-01],\n",
       "          ...,\n",
       "          [ 6.91575646e-01],\n",
       "          [ 6.13563299e-01],\n",
       "          [ 6.29878521e-01]],\n",
       "\n",
       "         [[ 6.93953097e-01],\n",
       "          [ 8.70202363e-01],\n",
       "          [ 7.59072006e-01],\n",
       "          ...,\n",
       "          [ 7.67725289e-01],\n",
       "          [ 6.23503089e-01],\n",
       "          [ 7.11687446e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.20163308e-02],\n",
       "          [ 6.30487025e-01],\n",
       "          [ 3.02919269e-01],\n",
       "          ...,\n",
       "          [ 2.67718613e-01],\n",
       "          [ 4.01345760e-01],\n",
       "          [ 6.17449880e-01]],\n",
       "\n",
       "         [[ 8.62130046e-01],\n",
       "          [-1.11007534e-01],\n",
       "          [ 1.92449056e-02],\n",
       "          ...,\n",
       "          [ 1.84401584e+00],\n",
       "          [ 1.44534183e+00],\n",
       "          [ 1.35463166e+00]],\n",
       "\n",
       "         [[ 1.04553270e+00],\n",
       "          [ 1.76997089e+00],\n",
       "          [ 2.24499035e+00],\n",
       "          ...,\n",
       "          [ 2.14317679e+00],\n",
       "          [ 2.01471210e+00],\n",
       "          [ 1.88421190e+00]]],\n",
       "\n",
       "\n",
       "        [[[-3.42830747e-01],\n",
       "          [ 3.73506010e-01],\n",
       "          [ 5.46182752e-01],\n",
       "          ...,\n",
       "          [ 5.41157544e-01],\n",
       "          [ 4.60779071e-01],\n",
       "          [-2.58643657e-01]],\n",
       "\n",
       "         [[ 2.18399197e-01],\n",
       "          [ 4.72794622e-01],\n",
       "          [ 8.00716877e-01],\n",
       "          ...,\n",
       "          [ 7.39316642e-01],\n",
       "          [ 6.69109285e-01],\n",
       "          [ 7.03879774e-01]],\n",
       "\n",
       "         [[ 7.57432997e-01],\n",
       "          [ 8.98841143e-01],\n",
       "          [ 7.73531616e-01],\n",
       "          ...,\n",
       "          [ 7.94056714e-01],\n",
       "          [ 6.58193588e-01],\n",
       "          [ 8.05383742e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 5.83060049e-02],\n",
       "          [ 7.13439226e-01],\n",
       "          [ 2.30205163e-01],\n",
       "          ...,\n",
       "          [ 4.42897409e-01],\n",
       "          [ 5.26245713e-01],\n",
       "          [ 7.75482595e-01]],\n",
       "\n",
       "         [[ 1.01246369e+00],\n",
       "          [-4.89043035e-02],\n",
       "          [ 1.08723655e-01],\n",
       "          ...,\n",
       "          [ 2.04081988e+00],\n",
       "          [ 1.61541104e+00],\n",
       "          [ 1.56025541e+00]],\n",
       "\n",
       "         [[ 1.21782935e+00],\n",
       "          [ 2.03469133e+00],\n",
       "          [ 2.52169538e+00],\n",
       "          ...,\n",
       "          [ 2.40121460e+00],\n",
       "          [ 2.16749620e+00],\n",
       "          [ 2.22528529e+00]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-1.89238697e-01],\n",
       "          [-2.24710032e-01],\n",
       "          [ 4.23177838e-01],\n",
       "          ...,\n",
       "          [-2.35135213e-01],\n",
       "          [-1.98847502e-01],\n",
       "          [-3.14200193e-01]],\n",
       "\n",
       "         [[ 3.35881382e-01],\n",
       "          [ 4.55341727e-01],\n",
       "          [ 4.81544733e-01],\n",
       "          ...,\n",
       "          [ 8.99040818e-01],\n",
       "          [ 6.78904057e-01],\n",
       "          [ 6.03037179e-01]],\n",
       "\n",
       "         [[ 1.41179430e+00],\n",
       "          [ 1.43971336e+00],\n",
       "          [ 1.41443598e+00],\n",
       "          ...,\n",
       "          [ 8.59741509e-01],\n",
       "          [ 1.05343568e+00],\n",
       "          [ 1.29795957e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.27684617e+00],\n",
       "          [ 1.69404328e+00],\n",
       "          [ 1.67089903e+00],\n",
       "          ...,\n",
       "          [ 5.05574465e-01],\n",
       "          [ 4.94341224e-01],\n",
       "          [ 1.59012341e+00]],\n",
       "\n",
       "         [[ 2.06658125e+00],\n",
       "          [ 2.64937115e+00],\n",
       "          [ 1.55695510e+00],\n",
       "          ...,\n",
       "          [ 8.03708851e-01],\n",
       "          [ 1.64456725e+00],\n",
       "          [ 2.15517974e+00]],\n",
       "\n",
       "         [[ 2.53595042e+00],\n",
       "          [ 2.53145838e+00],\n",
       "          [ 3.37910271e+00],\n",
       "          ...,\n",
       "          [ 1.87308347e+00],\n",
       "          [ 2.45421982e+00],\n",
       "          [ 2.49602628e+00]]],\n",
       "\n",
       "\n",
       "        [[[-1.16541468e-01],\n",
       "          [-1.73735186e-01],\n",
       "          [ 4.50839847e-01],\n",
       "          ...,\n",
       "          [-8.77307653e-02],\n",
       "          [-9.97462720e-02],\n",
       "          [-2.07597807e-01]],\n",
       "\n",
       "         [[ 4.29405212e-01],\n",
       "          [ 5.50040126e-01],\n",
       "          [ 5.59651911e-01],\n",
       "          ...,\n",
       "          [ 1.00286484e+00],\n",
       "          [ 7.37543464e-01],\n",
       "          [ 6.60762370e-01]],\n",
       "\n",
       "         [[ 1.45638251e+00],\n",
       "          [ 1.49776351e+00],\n",
       "          [ 1.49852157e+00],\n",
       "          ...,\n",
       "          [ 8.79310310e-01],\n",
       "          [ 1.03144550e+00],\n",
       "          [ 1.30704069e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.36080384e+00],\n",
       "          [ 1.86275506e+00],\n",
       "          [ 1.77320147e+00],\n",
       "          ...,\n",
       "          [ 6.41663551e-01],\n",
       "          [ 6.36572957e-01],\n",
       "          [ 1.77160847e+00]],\n",
       "\n",
       "         [[ 2.22900081e+00],\n",
       "          [ 2.79405284e+00],\n",
       "          [ 1.68586099e+00],\n",
       "          ...,\n",
       "          [ 1.01761854e+00],\n",
       "          [ 1.75572109e+00],\n",
       "          [ 2.24060678e+00]],\n",
       "\n",
       "         [[ 2.58279824e+00],\n",
       "          [ 2.63683200e+00],\n",
       "          [ 3.49761844e+00],\n",
       "          ...,\n",
       "          [ 1.98000157e+00],\n",
       "          [ 2.48671365e+00],\n",
       "          [ 2.54368401e+00]]],\n",
       "\n",
       "\n",
       "        [[[-3.26002799e-02],\n",
       "          [-8.74749795e-02],\n",
       "          [ 4.84744817e-01],\n",
       "          ...,\n",
       "          [ 4.02493887e-02],\n",
       "          [-9.68386699e-03],\n",
       "          [-1.01645648e-01]],\n",
       "\n",
       "         [[ 5.36003292e-01],\n",
       "          [ 6.56846881e-01],\n",
       "          [ 6.14730954e-01],\n",
       "          ...,\n",
       "          [ 1.09287715e+00],\n",
       "          [ 7.96037853e-01],\n",
       "          [ 7.29901910e-01]],\n",
       "\n",
       "         [[ 1.52206004e+00],\n",
       "          [ 1.55389607e+00],\n",
       "          [ 1.54789031e+00],\n",
       "          ...,\n",
       "          [ 8.96352530e-01],\n",
       "          [ 1.01808619e+00],\n",
       "          [ 1.33967614e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.46165609e+00],\n",
       "          [ 2.04387426e+00],\n",
       "          [ 1.88087571e+00],\n",
       "          ...,\n",
       "          [ 7.96604455e-01],\n",
       "          [ 8.02872181e-01],\n",
       "          [ 2.04422998e+00]],\n",
       "\n",
       "         [[ 2.41753268e+00],\n",
       "          [ 2.90234756e+00],\n",
       "          [ 1.78626764e+00],\n",
       "          ...,\n",
       "          [ 1.23611224e+00],\n",
       "          [ 1.93281913e+00],\n",
       "          [ 2.38242960e+00]],\n",
       "\n",
       "         [[ 2.69791126e+00],\n",
       "          [ 2.82957363e+00],\n",
       "          [ 3.56921649e+00],\n",
       "          ...,\n",
       "          [ 2.11037993e+00],\n",
       "          [ 2.56703997e+00],\n",
       "          [ 2.65462637e+00]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[-3.54809254e-01],\n",
       "          [ 3.49349380e-01],\n",
       "          [ 5.40288150e-01],\n",
       "          ...,\n",
       "          [ 4.48003441e-01],\n",
       "          [ 3.88327032e-01],\n",
       "          [-2.97131151e-01]],\n",
       "\n",
       "         [[ 1.12070717e-01],\n",
       "          [ 4.23052967e-01],\n",
       "          [ 7.83311009e-01],\n",
       "          ...,\n",
       "          [ 6.91575646e-01],\n",
       "          [ 6.13563299e-01],\n",
       "          [ 6.29878521e-01]],\n",
       "\n",
       "         [[ 6.93953097e-01],\n",
       "          [ 8.70202363e-01],\n",
       "          [ 7.59072006e-01],\n",
       "          ...,\n",
       "          [ 7.67725289e-01],\n",
       "          [ 6.23503089e-01],\n",
       "          [ 7.11687446e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.20163308e-02],\n",
       "          [ 6.30487025e-01],\n",
       "          [ 3.02919269e-01],\n",
       "          ...,\n",
       "          [ 2.67718613e-01],\n",
       "          [ 4.01345760e-01],\n",
       "          [ 6.17449880e-01]],\n",
       "\n",
       "         [[ 8.62130046e-01],\n",
       "          [-1.11007534e-01],\n",
       "          [ 1.92449056e-02],\n",
       "          ...,\n",
       "          [ 1.84401584e+00],\n",
       "          [ 1.44534183e+00],\n",
       "          [ 1.35463166e+00]],\n",
       "\n",
       "         [[ 1.04553270e+00],\n",
       "          [ 1.76997089e+00],\n",
       "          [ 2.24499035e+00],\n",
       "          ...,\n",
       "          [ 2.14317679e+00],\n",
       "          [ 2.01471210e+00],\n",
       "          [ 1.88421190e+00]]],\n",
       "\n",
       "\n",
       "        [[[-3.42830747e-01],\n",
       "          [ 3.73506010e-01],\n",
       "          [ 5.46182752e-01],\n",
       "          ...,\n",
       "          [ 5.41157544e-01],\n",
       "          [ 4.60779071e-01],\n",
       "          [-2.58643657e-01]],\n",
       "\n",
       "         [[ 2.18399197e-01],\n",
       "          [ 4.72794622e-01],\n",
       "          [ 8.00716877e-01],\n",
       "          ...,\n",
       "          [ 7.39316642e-01],\n",
       "          [ 6.69109285e-01],\n",
       "          [ 7.03879774e-01]],\n",
       "\n",
       "         [[ 7.57432997e-01],\n",
       "          [ 8.98841143e-01],\n",
       "          [ 7.73531616e-01],\n",
       "          ...,\n",
       "          [ 7.94056714e-01],\n",
       "          [ 6.58193588e-01],\n",
       "          [ 8.05383742e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 5.83060049e-02],\n",
       "          [ 7.13439226e-01],\n",
       "          [ 2.30205163e-01],\n",
       "          ...,\n",
       "          [ 4.42897409e-01],\n",
       "          [ 5.26245713e-01],\n",
       "          [ 7.75482595e-01]],\n",
       "\n",
       "         [[ 1.01246369e+00],\n",
       "          [-4.89043035e-02],\n",
       "          [ 1.08723655e-01],\n",
       "          ...,\n",
       "          [ 2.04081988e+00],\n",
       "          [ 1.61541104e+00],\n",
       "          [ 1.56025541e+00]],\n",
       "\n",
       "         [[ 1.21782935e+00],\n",
       "          [ 2.03469133e+00],\n",
       "          [ 2.52169538e+00],\n",
       "          ...,\n",
       "          [ 2.40121460e+00],\n",
       "          [ 2.16749620e+00],\n",
       "          [ 2.22528529e+00]]],\n",
       "\n",
       "\n",
       "        [[[-3.26204777e-01],\n",
       "          [ 3.98652762e-01],\n",
       "          [ 5.40608764e-01],\n",
       "          ...,\n",
       "          [ 6.32930338e-01],\n",
       "          [ 5.33978820e-01],\n",
       "          [-2.21612617e-01]],\n",
       "\n",
       "         [[ 3.65119278e-01],\n",
       "          [ 5.50025880e-01],\n",
       "          [ 8.39799523e-01],\n",
       "          ...,\n",
       "          [ 8.31931174e-01],\n",
       "          [ 7.65980840e-01],\n",
       "          [ 8.07559252e-01]],\n",
       "\n",
       "         [[ 8.80454957e-01],\n",
       "          [ 9.60588992e-01],\n",
       "          [ 8.33143771e-01],\n",
       "          ...,\n",
       "          [ 8.42785537e-01],\n",
       "          [ 7.09306180e-01],\n",
       "          [ 9.37528551e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.19364597e-01],\n",
       "          [ 8.23661089e-01],\n",
       "          [ 1.78819329e-01],\n",
       "          ...,\n",
       "          [ 6.24326468e-01],\n",
       "          [ 6.55635536e-01],\n",
       "          [ 9.09860432e-01]],\n",
       "\n",
       "         [[ 1.14875698e+00],\n",
       "          [ 7.80493859e-03],\n",
       "          [ 2.17258990e-01],\n",
       "          ...,\n",
       "          [ 2.08862042e+00],\n",
       "          [ 1.72013092e+00],\n",
       "          [ 1.77909744e+00]],\n",
       "\n",
       "         [[ 1.38127649e+00],\n",
       "          [ 2.26480556e+00],\n",
       "          [ 2.75266623e+00],\n",
       "          ...,\n",
       "          [ 2.54446363e+00],\n",
       "          [ 2.24002504e+00],\n",
       "          [ 2.46532941e+00]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-1.16541468e-01],\n",
       "          [-1.73735186e-01],\n",
       "          [ 4.50839847e-01],\n",
       "          ...,\n",
       "          [-8.77307653e-02],\n",
       "          [-9.97462720e-02],\n",
       "          [-2.07597807e-01]],\n",
       "\n",
       "         [[ 4.29405212e-01],\n",
       "          [ 5.50040126e-01],\n",
       "          [ 5.59651911e-01],\n",
       "          ...,\n",
       "          [ 1.00286484e+00],\n",
       "          [ 7.37543464e-01],\n",
       "          [ 6.60762370e-01]],\n",
       "\n",
       "         [[ 1.45638251e+00],\n",
       "          [ 1.49776351e+00],\n",
       "          [ 1.49852157e+00],\n",
       "          ...,\n",
       "          [ 8.79310310e-01],\n",
       "          [ 1.03144550e+00],\n",
       "          [ 1.30704069e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.36080384e+00],\n",
       "          [ 1.86275506e+00],\n",
       "          [ 1.77320147e+00],\n",
       "          ...,\n",
       "          [ 6.41663551e-01],\n",
       "          [ 6.36572957e-01],\n",
       "          [ 1.77160847e+00]],\n",
       "\n",
       "         [[ 2.22900081e+00],\n",
       "          [ 2.79405284e+00],\n",
       "          [ 1.68586099e+00],\n",
       "          ...,\n",
       "          [ 1.01761854e+00],\n",
       "          [ 1.75572109e+00],\n",
       "          [ 2.24060678e+00]],\n",
       "\n",
       "         [[ 2.58279824e+00],\n",
       "          [ 2.63683200e+00],\n",
       "          [ 3.49761844e+00],\n",
       "          ...,\n",
       "          [ 1.98000157e+00],\n",
       "          [ 2.48671365e+00],\n",
       "          [ 2.54368401e+00]]],\n",
       "\n",
       "\n",
       "        [[[-3.26002799e-02],\n",
       "          [-8.74749795e-02],\n",
       "          [ 4.84744817e-01],\n",
       "          ...,\n",
       "          [ 4.02493887e-02],\n",
       "          [-9.68386699e-03],\n",
       "          [-1.01645648e-01]],\n",
       "\n",
       "         [[ 5.36003292e-01],\n",
       "          [ 6.56846881e-01],\n",
       "          [ 6.14730954e-01],\n",
       "          ...,\n",
       "          [ 1.09287715e+00],\n",
       "          [ 7.96037853e-01],\n",
       "          [ 7.29901910e-01]],\n",
       "\n",
       "         [[ 1.52206004e+00],\n",
       "          [ 1.55389607e+00],\n",
       "          [ 1.54789031e+00],\n",
       "          ...,\n",
       "          [ 8.96352530e-01],\n",
       "          [ 1.01808619e+00],\n",
       "          [ 1.33967614e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.46165609e+00],\n",
       "          [ 2.04387426e+00],\n",
       "          [ 1.88087571e+00],\n",
       "          ...,\n",
       "          [ 7.96604455e-01],\n",
       "          [ 8.02872181e-01],\n",
       "          [ 2.04422998e+00]],\n",
       "\n",
       "         [[ 2.41753268e+00],\n",
       "          [ 2.90234756e+00],\n",
       "          [ 1.78626764e+00],\n",
       "          ...,\n",
       "          [ 1.23611224e+00],\n",
       "          [ 1.93281913e+00],\n",
       "          [ 2.38242960e+00]],\n",
       "\n",
       "         [[ 2.69791126e+00],\n",
       "          [ 2.82957363e+00],\n",
       "          [ 3.56921649e+00],\n",
       "          ...,\n",
       "          [ 2.11037993e+00],\n",
       "          [ 2.56703997e+00],\n",
       "          [ 2.65462637e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 4.49403934e-02],\n",
       "          [ 1.81330070e-02],\n",
       "          [ 5.23392737e-01],\n",
       "          ...,\n",
       "          [ 1.25443235e-01],\n",
       "          [ 5.04921451e-02],\n",
       "          [-1.90902688e-02]],\n",
       "\n",
       "         [[ 6.35598183e-01],\n",
       "          [ 7.46625066e-01],\n",
       "          [ 6.27017081e-01],\n",
       "          ...,\n",
       "          [ 1.13776195e+00],\n",
       "          [ 8.42199326e-01],\n",
       "          [ 8.01237345e-01]],\n",
       "\n",
       "         [[ 1.59554482e+00],\n",
       "          [ 1.58711433e+00],\n",
       "          [ 1.54427624e+00],\n",
       "          ...,\n",
       "          [ 9.21095014e-01],\n",
       "          [ 1.02062726e+00],\n",
       "          [ 1.39626312e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.56243324e+00],\n",
       "          [ 2.16065836e+00],\n",
       "          [ 1.95397115e+00],\n",
       "          ...,\n",
       "          [ 9.62666512e-01],\n",
       "          [ 1.00572073e+00],\n",
       "          [ 2.25374556e+00]],\n",
       "\n",
       "         [[ 2.55276108e+00],\n",
       "          [ 2.96053052e+00],\n",
       "          [ 1.84011018e+00],\n",
       "          ...,\n",
       "          [ 1.42001033e+00],\n",
       "          [ 2.04855180e+00],\n",
       "          [ 2.50614238e+00]],\n",
       "\n",
       "         [[ 2.77681518e+00],\n",
       "          [ 2.97497487e+00],\n",
       "          [ 3.61599922e+00],\n",
       "          ...,\n",
       "          [ 2.20910692e+00],\n",
       "          [ 2.63238931e+00],\n",
       "          [ 2.78463411e+00]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[-3.42830747e-01],\n",
       "          [ 3.73506010e-01],\n",
       "          [ 5.46182752e-01],\n",
       "          ...,\n",
       "          [ 5.41157544e-01],\n",
       "          [ 4.60779071e-01],\n",
       "          [-2.58643657e-01]],\n",
       "\n",
       "         [[ 2.18399197e-01],\n",
       "          [ 4.72794622e-01],\n",
       "          [ 8.00716877e-01],\n",
       "          ...,\n",
       "          [ 7.39316642e-01],\n",
       "          [ 6.69109285e-01],\n",
       "          [ 7.03879774e-01]],\n",
       "\n",
       "         [[ 7.57432997e-01],\n",
       "          [ 8.98841143e-01],\n",
       "          [ 7.73531616e-01],\n",
       "          ...,\n",
       "          [ 7.94056714e-01],\n",
       "          [ 6.58193588e-01],\n",
       "          [ 8.05383742e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 5.83060049e-02],\n",
       "          [ 7.13439226e-01],\n",
       "          [ 2.30205163e-01],\n",
       "          ...,\n",
       "          [ 4.42897409e-01],\n",
       "          [ 5.26245713e-01],\n",
       "          [ 7.75482595e-01]],\n",
       "\n",
       "         [[ 1.01246369e+00],\n",
       "          [-4.89043035e-02],\n",
       "          [ 1.08723655e-01],\n",
       "          ...,\n",
       "          [ 2.04081988e+00],\n",
       "          [ 1.61541104e+00],\n",
       "          [ 1.56025541e+00]],\n",
       "\n",
       "         [[ 1.21782935e+00],\n",
       "          [ 2.03469133e+00],\n",
       "          [ 2.52169538e+00],\n",
       "          ...,\n",
       "          [ 2.40121460e+00],\n",
       "          [ 2.16749620e+00],\n",
       "          [ 2.22528529e+00]]],\n",
       "\n",
       "\n",
       "        [[[-3.26204777e-01],\n",
       "          [ 3.98652762e-01],\n",
       "          [ 5.40608764e-01],\n",
       "          ...,\n",
       "          [ 6.32930338e-01],\n",
       "          [ 5.33978820e-01],\n",
       "          [-2.21612617e-01]],\n",
       "\n",
       "         [[ 3.65119278e-01],\n",
       "          [ 5.50025880e-01],\n",
       "          [ 8.39799523e-01],\n",
       "          ...,\n",
       "          [ 8.31931174e-01],\n",
       "          [ 7.65980840e-01],\n",
       "          [ 8.07559252e-01]],\n",
       "\n",
       "         [[ 8.80454957e-01],\n",
       "          [ 9.60588992e-01],\n",
       "          [ 8.33143771e-01],\n",
       "          ...,\n",
       "          [ 8.42785537e-01],\n",
       "          [ 7.09306180e-01],\n",
       "          [ 9.37528551e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.19364597e-01],\n",
       "          [ 8.23661089e-01],\n",
       "          [ 1.78819329e-01],\n",
       "          ...,\n",
       "          [ 6.24326468e-01],\n",
       "          [ 6.55635536e-01],\n",
       "          [ 9.09860432e-01]],\n",
       "\n",
       "         [[ 1.14875698e+00],\n",
       "          [ 7.80493859e-03],\n",
       "          [ 2.17258990e-01],\n",
       "          ...,\n",
       "          [ 2.08862042e+00],\n",
       "          [ 1.72013092e+00],\n",
       "          [ 1.77909744e+00]],\n",
       "\n",
       "         [[ 1.38127649e+00],\n",
       "          [ 2.26480556e+00],\n",
       "          [ 2.75266623e+00],\n",
       "          ...,\n",
       "          [ 2.54446363e+00],\n",
       "          [ 2.24002504e+00],\n",
       "          [ 2.46532941e+00]]],\n",
       "\n",
       "\n",
       "        [[[-3.08468640e-01],\n",
       "          [ 4.05654550e-01],\n",
       "          [ 5.19501269e-01],\n",
       "          ...,\n",
       "          [ 7.07557023e-01],\n",
       "          [ 5.88403106e-01],\n",
       "          [-2.03750372e-01]],\n",
       "\n",
       "         [[ 4.98608351e-01],\n",
       "          [ 6.14696443e-01],\n",
       "          [ 8.63741398e-01],\n",
       "          ...,\n",
       "          [ 9.28709745e-01],\n",
       "          [ 8.61032724e-01],\n",
       "          [ 8.76570404e-01]],\n",
       "\n",
       "         [[ 9.85637367e-01],\n",
       "          [ 9.87466514e-01],\n",
       "          [ 8.80683005e-01],\n",
       "          ...,\n",
       "          [ 8.93809915e-01],\n",
       "          [ 7.52661824e-01],\n",
       "          [ 1.04538929e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.71227351e-01],\n",
       "          [ 9.16177809e-01],\n",
       "          [ 1.26424253e-01],\n",
       "          ...,\n",
       "          [ 7.76435375e-01],\n",
       "          [ 7.79946446e-01],\n",
       "          [ 9.92677808e-01]],\n",
       "\n",
       "         [[ 1.21839845e+00],\n",
       "          [ 4.08345237e-02],\n",
       "          [ 3.07571560e-01],\n",
       "          ...,\n",
       "          [ 2.04105210e+00],\n",
       "          [ 1.81836522e+00],\n",
       "          [ 1.91364408e+00]],\n",
       "\n",
       "         [[ 1.47358215e+00],\n",
       "          [ 2.38321900e+00],\n",
       "          [ 2.88763857e+00],\n",
       "          ...,\n",
       "          [ 2.53974175e+00],\n",
       "          [ 2.20182514e+00],\n",
       "          [ 2.53175974e+00]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-3.26002799e-02],\n",
       "          [-8.74749795e-02],\n",
       "          [ 4.84744817e-01],\n",
       "          ...,\n",
       "          [ 4.02493887e-02],\n",
       "          [-9.68386699e-03],\n",
       "          [-1.01645648e-01]],\n",
       "\n",
       "         [[ 5.36003292e-01],\n",
       "          [ 6.56846881e-01],\n",
       "          [ 6.14730954e-01],\n",
       "          ...,\n",
       "          [ 1.09287715e+00],\n",
       "          [ 7.96037853e-01],\n",
       "          [ 7.29901910e-01]],\n",
       "\n",
       "         [[ 1.52206004e+00],\n",
       "          [ 1.55389607e+00],\n",
       "          [ 1.54789031e+00],\n",
       "          ...,\n",
       "          [ 8.96352530e-01],\n",
       "          [ 1.01808619e+00],\n",
       "          [ 1.33967614e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.46165609e+00],\n",
       "          [ 2.04387426e+00],\n",
       "          [ 1.88087571e+00],\n",
       "          ...,\n",
       "          [ 7.96604455e-01],\n",
       "          [ 8.02872181e-01],\n",
       "          [ 2.04422998e+00]],\n",
       "\n",
       "         [[ 2.41753268e+00],\n",
       "          [ 2.90234756e+00],\n",
       "          [ 1.78626764e+00],\n",
       "          ...,\n",
       "          [ 1.23611224e+00],\n",
       "          [ 1.93281913e+00],\n",
       "          [ 2.38242960e+00]],\n",
       "\n",
       "         [[ 2.69791126e+00],\n",
       "          [ 2.82957363e+00],\n",
       "          [ 3.56921649e+00],\n",
       "          ...,\n",
       "          [ 2.11037993e+00],\n",
       "          [ 2.56703997e+00],\n",
       "          [ 2.65462637e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 4.49403934e-02],\n",
       "          [ 1.81330070e-02],\n",
       "          [ 5.23392737e-01],\n",
       "          ...,\n",
       "          [ 1.25443235e-01],\n",
       "          [ 5.04921451e-02],\n",
       "          [-1.90902688e-02]],\n",
       "\n",
       "         [[ 6.35598183e-01],\n",
       "          [ 7.46625066e-01],\n",
       "          [ 6.27017081e-01],\n",
       "          ...,\n",
       "          [ 1.13776195e+00],\n",
       "          [ 8.42199326e-01],\n",
       "          [ 8.01237345e-01]],\n",
       "\n",
       "         [[ 1.59554482e+00],\n",
       "          [ 1.58711433e+00],\n",
       "          [ 1.54427624e+00],\n",
       "          ...,\n",
       "          [ 9.21095014e-01],\n",
       "          [ 1.02062726e+00],\n",
       "          [ 1.39626312e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.56243324e+00],\n",
       "          [ 2.16065836e+00],\n",
       "          [ 1.95397115e+00],\n",
       "          ...,\n",
       "          [ 9.62666512e-01],\n",
       "          [ 1.00572073e+00],\n",
       "          [ 2.25374556e+00]],\n",
       "\n",
       "         [[ 2.55276108e+00],\n",
       "          [ 2.96053052e+00],\n",
       "          [ 1.84011018e+00],\n",
       "          ...,\n",
       "          [ 1.42001033e+00],\n",
       "          [ 2.04855180e+00],\n",
       "          [ 2.50614238e+00]],\n",
       "\n",
       "         [[ 2.77681518e+00],\n",
       "          [ 2.97497487e+00],\n",
       "          [ 3.61599922e+00],\n",
       "          ...,\n",
       "          [ 2.20910692e+00],\n",
       "          [ 2.63238931e+00],\n",
       "          [ 2.78463411e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 1.17082015e-01],\n",
       "          [ 1.43522307e-01],\n",
       "          [ 5.89896500e-01],\n",
       "          ...,\n",
       "          [ 1.69830620e-01],\n",
       "          [ 8.04324448e-02],\n",
       "          [ 3.56611572e-02]],\n",
       "\n",
       "         [[ 7.34560013e-01],\n",
       "          [ 8.29773724e-01],\n",
       "          [ 6.28765166e-01],\n",
       "          ...,\n",
       "          [ 1.16204154e+00],\n",
       "          [ 9.01340246e-01],\n",
       "          [ 8.89847398e-01]],\n",
       "\n",
       "         [[ 1.71286476e+00],\n",
       "          [ 1.62981319e+00],\n",
       "          [ 1.53560567e+00],\n",
       "          ...,\n",
       "          [ 9.72728968e-01],\n",
       "          [ 1.07526243e+00],\n",
       "          [ 1.51131082e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.59942853e+00],\n",
       "          [ 2.23330235e+00],\n",
       "          [ 1.92820919e+00],\n",
       "          ...,\n",
       "          [ 1.16506243e+00],\n",
       "          [ 1.20026207e+00],\n",
       "          [ 2.42853570e+00]],\n",
       "\n",
       "         [[ 2.60840893e+00],\n",
       "          [ 2.97143364e+00],\n",
       "          [ 1.82785559e+00],\n",
       "          ...,\n",
       "          [ 1.57493508e+00],\n",
       "          [ 2.10826993e+00],\n",
       "          [ 2.56952000e+00]],\n",
       "\n",
       "         [[ 2.83913946e+00],\n",
       "          [ 3.04170346e+00],\n",
       "          [ 3.59176111e+00],\n",
       "          ...,\n",
       "          [ 2.31427431e+00],\n",
       "          [ 2.73937464e+00],\n",
       "          [ 2.92112494e+00]]]],\n",
       "\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "\n",
       "       [[[[ 1.03308702e+00],\n",
       "          [ 1.69784164e+00],\n",
       "          [ 1.68024600e+00],\n",
       "          ...,\n",
       "          [ 6.35935068e-01],\n",
       "          [ 8.57552111e-01],\n",
       "          [ 1.30080700e+00]],\n",
       "\n",
       "         [[ 1.29060650e+00],\n",
       "          [ 1.43619072e+00],\n",
       "          [ 1.40601134e+00],\n",
       "          ...,\n",
       "          [ 8.65531027e-01],\n",
       "          [ 8.76721501e-01],\n",
       "          [ 1.23686123e+00]],\n",
       "\n",
       "         [[ 6.98989391e-01],\n",
       "          [ 1.00895357e+00],\n",
       "          [ 1.08209515e+00],\n",
       "          ...,\n",
       "          [-1.16583623e-01],\n",
       "          [ 2.43505985e-01],\n",
       "          [ 5.70319533e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.14689499e-01],\n",
       "          [-1.88133463e-01],\n",
       "          [-9.32067409e-02],\n",
       "          ...,\n",
       "          [ 8.15775543e-02],\n",
       "          [ 7.64952600e-02],\n",
       "          [-2.72771567e-01]],\n",
       "\n",
       "         [[-1.45107293e+00],\n",
       "          [-1.73756886e+00],\n",
       "          [-4.24777597e-01],\n",
       "          ...,\n",
       "          [ 4.15101320e-01],\n",
       "          [-3.84063482e-01],\n",
       "          [-6.70521140e-01]],\n",
       "\n",
       "         [[ 7.11864293e-01],\n",
       "          [-1.77370000e+00],\n",
       "          [-1.29621482e+00],\n",
       "          ...,\n",
       "          [ 3.88957143e-01],\n",
       "          [-4.04194921e-01],\n",
       "          [ 5.50590813e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.01933658e+00],\n",
       "          [ 1.69587147e+00],\n",
       "          [ 1.70706797e+00],\n",
       "          ...,\n",
       "          [ 6.84034586e-01],\n",
       "          [ 9.30006027e-01],\n",
       "          [ 1.37139559e+00]],\n",
       "\n",
       "         [[ 1.28319097e+00],\n",
       "          [ 1.42629254e+00],\n",
       "          [ 1.42786157e+00],\n",
       "          ...,\n",
       "          [ 8.87850165e-01],\n",
       "          [ 9.03963208e-01],\n",
       "          [ 1.23649764e+00]],\n",
       "\n",
       "         [[ 5.90400338e-01],\n",
       "          [ 8.81069243e-01],\n",
       "          [ 1.00919712e+00],\n",
       "          ...,\n",
       "          [-1.79806709e-01],\n",
       "          [ 1.65868983e-01],\n",
       "          [ 5.32285929e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-3.78004042e-03],\n",
       "          [-1.53568268e-01],\n",
       "          [ 9.46630090e-02],\n",
       "          ...,\n",
       "          [ 4.11532335e-02],\n",
       "          [-5.01118042e-02],\n",
       "          [-2.38398820e-01]],\n",
       "\n",
       "         [[-1.28618920e+00],\n",
       "          [-1.71408999e+00],\n",
       "          [-3.60433400e-01],\n",
       "          ...,\n",
       "          [ 3.44249457e-01],\n",
       "          [-6.56080723e-01],\n",
       "          [-6.22785509e-01]],\n",
       "\n",
       "         [[ 7.28772700e-01],\n",
       "          [-1.78303683e+00],\n",
       "          [-1.38569200e+00],\n",
       "          ...,\n",
       "          [ 3.66918474e-01],\n",
       "          [-5.06005168e-01],\n",
       "          [ 4.92732584e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.00387836e+00],\n",
       "          [ 1.68604946e+00],\n",
       "          [ 1.72188222e+00],\n",
       "          ...,\n",
       "          [ 7.26276517e-01],\n",
       "          [ 9.32465494e-01],\n",
       "          [ 1.42219031e+00]],\n",
       "\n",
       "         [[ 1.32123303e+00],\n",
       "          [ 1.43914521e+00],\n",
       "          [ 1.39888322e+00],\n",
       "          ...,\n",
       "          [ 8.96897554e-01],\n",
       "          [ 9.20771301e-01],\n",
       "          [ 1.26074481e+00]],\n",
       "\n",
       "         [[ 5.65136909e-01],\n",
       "          [ 7.83142865e-01],\n",
       "          [ 9.42872107e-01],\n",
       "          ...,\n",
       "          [-2.61655658e-01],\n",
       "          [ 9.83102769e-02],\n",
       "          [ 4.74030286e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.68845165e-02],\n",
       "          [-2.34260172e-01],\n",
       "          [ 1.46705762e-01],\n",
       "          ...,\n",
       "          [ 2.00828519e-02],\n",
       "          [-1.59957796e-01],\n",
       "          [-2.45466873e-01]],\n",
       "\n",
       "         [[-1.19658053e+00],\n",
       "          [-1.69308519e+00],\n",
       "          [-3.87873232e-01],\n",
       "          ...,\n",
       "          [ 2.23403528e-01],\n",
       "          [-8.86636198e-01],\n",
       "          [-5.75449407e-01]],\n",
       "\n",
       "         [[ 7.08105505e-01],\n",
       "          [-1.81255817e+00],\n",
       "          [-1.48121953e+00],\n",
       "          ...,\n",
       "          [ 3.76806200e-01],\n",
       "          [-5.42918205e-01],\n",
       "          [ 4.41376567e-01]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 1.12776792e+00],\n",
       "          [ 1.46306956e+00],\n",
       "          [ 1.48767996e+00],\n",
       "          ...,\n",
       "          [ 1.62803185e+00],\n",
       "          [ 1.46827650e+00],\n",
       "          [ 1.58148670e+00]],\n",
       "\n",
       "         [[ 1.46356225e+00],\n",
       "          [ 1.23436880e+00],\n",
       "          [ 1.02596915e+00],\n",
       "          ...,\n",
       "          [ 1.11573827e+00],\n",
       "          [ 1.37246263e+00],\n",
       "          [ 1.41418564e+00]],\n",
       "\n",
       "         [[ 9.68146980e-01],\n",
       "          [ 1.30618238e+00],\n",
       "          [ 1.04609156e+00],\n",
       "          ...,\n",
       "          [ 4.44402456e-01],\n",
       "          [ 7.58437216e-01],\n",
       "          [ 6.08541787e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.19015563e-01],\n",
       "          [ 9.85141039e-01],\n",
       "          [-2.48603120e-01],\n",
       "          ...,\n",
       "          [ 2.63715446e-01],\n",
       "          [ 1.05289483e+00],\n",
       "          [ 1.56243145e+00]],\n",
       "\n",
       "         [[-4.84912544e-01],\n",
       "          [ 2.99735755e-01],\n",
       "          [ 6.49971843e-01],\n",
       "          ...,\n",
       "          [ 1.26385808e+00],\n",
       "          [ 1.48721111e+00],\n",
       "          [-7.69702435e-01]],\n",
       "\n",
       "         [[ 5.59716761e-01],\n",
       "          [ 1.43127963e-01],\n",
       "          [-2.02869803e-01],\n",
       "          ...,\n",
       "          [ 9.34976637e-01],\n",
       "          [ 8.76965463e-01],\n",
       "          [ 1.68173164e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.05417287e+00],\n",
       "          [ 1.40661156e+00],\n",
       "          [ 1.42573321e+00],\n",
       "          ...,\n",
       "          [ 1.73294747e+00],\n",
       "          [ 1.50425363e+00],\n",
       "          [ 1.52578843e+00]],\n",
       "\n",
       "         [[ 1.40035176e+00],\n",
       "          [ 1.14405310e+00],\n",
       "          [ 9.71972525e-01],\n",
       "          ...,\n",
       "          [ 1.21504438e+00],\n",
       "          [ 1.42232680e+00],\n",
       "          [ 1.37450683e+00]],\n",
       "\n",
       "         [[ 9.96286869e-01],\n",
       "          [ 1.25535703e+00],\n",
       "          [ 9.92358804e-01],\n",
       "          ...,\n",
       "          [ 4.65553015e-01],\n",
       "          [ 8.08831573e-01],\n",
       "          [ 6.76446855e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.90573740e-01],\n",
       "          [ 9.78541136e-01],\n",
       "          [-2.05351770e-01],\n",
       "          ...,\n",
       "          [ 2.11067826e-01],\n",
       "          [ 9.68537629e-01],\n",
       "          [ 1.43002021e+00]],\n",
       "\n",
       "         [[-3.61989111e-01],\n",
       "          [ 3.57380569e-01],\n",
       "          [ 7.39674449e-01],\n",
       "          ...,\n",
       "          [ 1.20333457e+00],\n",
       "          [ 1.27605832e+00],\n",
       "          [-5.52498937e-01]],\n",
       "\n",
       "         [[ 6.97079420e-01],\n",
       "          [ 2.63943672e-01],\n",
       "          [-4.88851480e-02],\n",
       "          ...,\n",
       "          [ 1.08215559e+00],\n",
       "          [ 8.70064735e-01],\n",
       "          [ 1.80758521e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.05650604e+00],\n",
       "          [ 1.36264014e+00],\n",
       "          [ 1.34554887e+00],\n",
       "          ...,\n",
       "          [ 1.74142349e+00],\n",
       "          [ 1.51377082e+00],\n",
       "          [ 1.57523370e+00]],\n",
       "\n",
       "         [[ 1.34015131e+00],\n",
       "          [ 1.09261858e+00],\n",
       "          [ 9.20372546e-01],\n",
       "          ...,\n",
       "          [ 1.34204280e+00],\n",
       "          [ 1.49625623e+00],\n",
       "          [ 1.37692010e+00]],\n",
       "\n",
       "         [[ 1.00881195e+00],\n",
       "          [ 1.18061197e+00],\n",
       "          [ 9.32613790e-01],\n",
       "          ...,\n",
       "          [ 5.74113429e-01],\n",
       "          [ 9.43301082e-01],\n",
       "          [ 7.84881473e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 4.41790104e-01],\n",
       "          [ 8.83235514e-01],\n",
       "          [-2.20139697e-01],\n",
       "          ...,\n",
       "          [ 2.10817009e-01],\n",
       "          [ 9.21731591e-01],\n",
       "          [ 1.39006925e+00]],\n",
       "\n",
       "         [[-2.74293870e-01],\n",
       "          [ 3.96349132e-01],\n",
       "          [ 7.88462937e-01],\n",
       "          ...,\n",
       "          [ 1.19147193e+00],\n",
       "          [ 1.09640014e+00],\n",
       "          [-4.41391051e-01]],\n",
       "\n",
       "         [[ 8.03167760e-01],\n",
       "          [ 3.22496265e-01],\n",
       "          [ 5.22812046e-02],\n",
       "          ...,\n",
       "          [ 1.22210550e+00],\n",
       "          [ 8.73679459e-01],\n",
       "          [ 1.67143539e-01]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[ 1.01933658e+00],\n",
       "          [ 1.69587147e+00],\n",
       "          [ 1.70706797e+00],\n",
       "          ...,\n",
       "          [ 6.84034586e-01],\n",
       "          [ 9.30006027e-01],\n",
       "          [ 1.37139559e+00]],\n",
       "\n",
       "         [[ 1.28319097e+00],\n",
       "          [ 1.42629254e+00],\n",
       "          [ 1.42786157e+00],\n",
       "          ...,\n",
       "          [ 8.87850165e-01],\n",
       "          [ 9.03963208e-01],\n",
       "          [ 1.23649764e+00]],\n",
       "\n",
       "         [[ 5.90400338e-01],\n",
       "          [ 8.81069243e-01],\n",
       "          [ 1.00919712e+00],\n",
       "          ...,\n",
       "          [-1.79806709e-01],\n",
       "          [ 1.65868983e-01],\n",
       "          [ 5.32285929e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-3.78004042e-03],\n",
       "          [-1.53568268e-01],\n",
       "          [ 9.46630090e-02],\n",
       "          ...,\n",
       "          [ 4.11532335e-02],\n",
       "          [-5.01118042e-02],\n",
       "          [-2.38398820e-01]],\n",
       "\n",
       "         [[-1.28618920e+00],\n",
       "          [-1.71408999e+00],\n",
       "          [-3.60433400e-01],\n",
       "          ...,\n",
       "          [ 3.44249457e-01],\n",
       "          [-6.56080723e-01],\n",
       "          [-6.22785509e-01]],\n",
       "\n",
       "         [[ 7.28772700e-01],\n",
       "          [-1.78303683e+00],\n",
       "          [-1.38569200e+00],\n",
       "          ...,\n",
       "          [ 3.66918474e-01],\n",
       "          [-5.06005168e-01],\n",
       "          [ 4.92732584e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.00387836e+00],\n",
       "          [ 1.68604946e+00],\n",
       "          [ 1.72188222e+00],\n",
       "          ...,\n",
       "          [ 7.26276517e-01],\n",
       "          [ 9.32465494e-01],\n",
       "          [ 1.42219031e+00]],\n",
       "\n",
       "         [[ 1.32123303e+00],\n",
       "          [ 1.43914521e+00],\n",
       "          [ 1.39888322e+00],\n",
       "          ...,\n",
       "          [ 8.96897554e-01],\n",
       "          [ 9.20771301e-01],\n",
       "          [ 1.26074481e+00]],\n",
       "\n",
       "         [[ 5.65136909e-01],\n",
       "          [ 7.83142865e-01],\n",
       "          [ 9.42872107e-01],\n",
       "          ...,\n",
       "          [-2.61655658e-01],\n",
       "          [ 9.83102769e-02],\n",
       "          [ 4.74030286e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.68845165e-02],\n",
       "          [-2.34260172e-01],\n",
       "          [ 1.46705762e-01],\n",
       "          ...,\n",
       "          [ 2.00828519e-02],\n",
       "          [-1.59957796e-01],\n",
       "          [-2.45466873e-01]],\n",
       "\n",
       "         [[-1.19658053e+00],\n",
       "          [-1.69308519e+00],\n",
       "          [-3.87873232e-01],\n",
       "          ...,\n",
       "          [ 2.23403528e-01],\n",
       "          [-8.86636198e-01],\n",
       "          [-5.75449407e-01]],\n",
       "\n",
       "         [[ 7.08105505e-01],\n",
       "          [-1.81255817e+00],\n",
       "          [-1.48121953e+00],\n",
       "          ...,\n",
       "          [ 3.76806200e-01],\n",
       "          [-5.42918205e-01],\n",
       "          [ 4.41376567e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.03278148e+00],\n",
       "          [ 1.71551669e+00],\n",
       "          [ 1.79083228e+00],\n",
       "          ...,\n",
       "          [ 7.72730947e-01],\n",
       "          [ 9.34396684e-01],\n",
       "          [ 1.51204646e+00]],\n",
       "\n",
       "         [[ 1.37950313e+00],\n",
       "          [ 1.47791374e+00],\n",
       "          [ 1.37511265e+00],\n",
       "          ...,\n",
       "          [ 9.63298142e-01],\n",
       "          [ 9.72806811e-01],\n",
       "          [ 1.31849670e+00]],\n",
       "\n",
       "         [[ 5.66580474e-01],\n",
       "          [ 7.37329125e-01],\n",
       "          [ 9.06634450e-01],\n",
       "          ...,\n",
       "          [-2.58229762e-01],\n",
       "          [ 9.68143344e-02],\n",
       "          [ 4.63079482e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.06299096e-01],\n",
       "          [-2.37500936e-01],\n",
       "          [ 2.25534946e-01],\n",
       "          ...,\n",
       "          [-6.12203404e-02],\n",
       "          [-2.51177311e-01],\n",
       "          [-1.40642911e-01]],\n",
       "\n",
       "         [[-1.08322549e+00],\n",
       "          [-1.72485042e+00],\n",
       "          [-3.78283978e-01],\n",
       "          ...,\n",
       "          [ 9.36858431e-02],\n",
       "          [-1.12265623e+00],\n",
       "          [-6.07779026e-01]],\n",
       "\n",
       "         [[ 7.15143144e-01],\n",
       "          [-1.88257873e+00],\n",
       "          [-1.63362455e+00],\n",
       "          ...,\n",
       "          [ 3.28968883e-01],\n",
       "          [-6.43231690e-01],\n",
       "          [ 3.52597058e-01]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 1.05417287e+00],\n",
       "          [ 1.40661156e+00],\n",
       "          [ 1.42573321e+00],\n",
       "          ...,\n",
       "          [ 1.73294747e+00],\n",
       "          [ 1.50425363e+00],\n",
       "          [ 1.52578843e+00]],\n",
       "\n",
       "         [[ 1.40035176e+00],\n",
       "          [ 1.14405310e+00],\n",
       "          [ 9.71972525e-01],\n",
       "          ...,\n",
       "          [ 1.21504438e+00],\n",
       "          [ 1.42232680e+00],\n",
       "          [ 1.37450683e+00]],\n",
       "\n",
       "         [[ 9.96286869e-01],\n",
       "          [ 1.25535703e+00],\n",
       "          [ 9.92358804e-01],\n",
       "          ...,\n",
       "          [ 4.65553015e-01],\n",
       "          [ 8.08831573e-01],\n",
       "          [ 6.76446855e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.90573740e-01],\n",
       "          [ 9.78541136e-01],\n",
       "          [-2.05351770e-01],\n",
       "          ...,\n",
       "          [ 2.11067826e-01],\n",
       "          [ 9.68537629e-01],\n",
       "          [ 1.43002021e+00]],\n",
       "\n",
       "         [[-3.61989111e-01],\n",
       "          [ 3.57380569e-01],\n",
       "          [ 7.39674449e-01],\n",
       "          ...,\n",
       "          [ 1.20333457e+00],\n",
       "          [ 1.27605832e+00],\n",
       "          [-5.52498937e-01]],\n",
       "\n",
       "         [[ 6.97079420e-01],\n",
       "          [ 2.63943672e-01],\n",
       "          [-4.88851480e-02],\n",
       "          ...,\n",
       "          [ 1.08215559e+00],\n",
       "          [ 8.70064735e-01],\n",
       "          [ 1.80758521e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.05650604e+00],\n",
       "          [ 1.36264014e+00],\n",
       "          [ 1.34554887e+00],\n",
       "          ...,\n",
       "          [ 1.74142349e+00],\n",
       "          [ 1.51377082e+00],\n",
       "          [ 1.57523370e+00]],\n",
       "\n",
       "         [[ 1.34015131e+00],\n",
       "          [ 1.09261858e+00],\n",
       "          [ 9.20372546e-01],\n",
       "          ...,\n",
       "          [ 1.34204280e+00],\n",
       "          [ 1.49625623e+00],\n",
       "          [ 1.37692010e+00]],\n",
       "\n",
       "         [[ 1.00881195e+00],\n",
       "          [ 1.18061197e+00],\n",
       "          [ 9.32613790e-01],\n",
       "          ...,\n",
       "          [ 5.74113429e-01],\n",
       "          [ 9.43301082e-01],\n",
       "          [ 7.84881473e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 4.41790104e-01],\n",
       "          [ 8.83235514e-01],\n",
       "          [-2.20139697e-01],\n",
       "          ...,\n",
       "          [ 2.10817009e-01],\n",
       "          [ 9.21731591e-01],\n",
       "          [ 1.39006925e+00]],\n",
       "\n",
       "         [[-2.74293870e-01],\n",
       "          [ 3.96349132e-01],\n",
       "          [ 7.88462937e-01],\n",
       "          ...,\n",
       "          [ 1.19147193e+00],\n",
       "          [ 1.09640014e+00],\n",
       "          [-4.41391051e-01]],\n",
       "\n",
       "         [[ 8.03167760e-01],\n",
       "          [ 3.22496265e-01],\n",
       "          [ 5.22812046e-02],\n",
       "          ...,\n",
       "          [ 1.22210550e+00],\n",
       "          [ 8.73679459e-01],\n",
       "          [ 1.67143539e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.10257900e+00],\n",
       "          [ 1.40027845e+00],\n",
       "          [ 1.35083449e+00],\n",
       "          ...,\n",
       "          [ 1.89066970e+00],\n",
       "          [ 1.68356597e+00],\n",
       "          [ 1.72060966e+00]],\n",
       "\n",
       "         [[ 1.28571832e+00],\n",
       "          [ 1.06180978e+00],\n",
       "          [ 9.03383911e-01],\n",
       "          ...,\n",
       "          [ 1.46462691e+00],\n",
       "          [ 1.54794002e+00],\n",
       "          [ 1.37113214e+00]],\n",
       "\n",
       "         [[ 9.69825208e-01],\n",
       "          [ 1.09793878e+00],\n",
       "          [ 8.88440430e-01],\n",
       "          ...,\n",
       "          [ 5.86842775e-01],\n",
       "          [ 9.77596521e-01],\n",
       "          [ 7.69184649e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 5.72660208e-01],\n",
       "          [ 8.72038662e-01],\n",
       "          [-1.47511303e-01],\n",
       "          ...,\n",
       "          [ 2.52896816e-01],\n",
       "          [ 8.90182555e-01],\n",
       "          [ 1.44093525e+00]],\n",
       "\n",
       "         [[-2.07899272e-01],\n",
       "          [ 3.79277438e-01],\n",
       "          [ 8.15327942e-01],\n",
       "          ...,\n",
       "          [ 1.17658854e+00],\n",
       "          [ 1.03979814e+00],\n",
       "          [-3.05826843e-01]],\n",
       "\n",
       "         [[ 8.79944444e-01],\n",
       "          [ 3.39401633e-01],\n",
       "          [ 8.96904618e-02],\n",
       "          ...,\n",
       "          [ 1.31369293e+00],\n",
       "          [ 8.91908348e-01],\n",
       "          [ 1.85550138e-01]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[ 1.00387836e+00],\n",
       "          [ 1.68604946e+00],\n",
       "          [ 1.72188222e+00],\n",
       "          ...,\n",
       "          [ 7.26276517e-01],\n",
       "          [ 9.32465494e-01],\n",
       "          [ 1.42219031e+00]],\n",
       "\n",
       "         [[ 1.32123303e+00],\n",
       "          [ 1.43914521e+00],\n",
       "          [ 1.39888322e+00],\n",
       "          ...,\n",
       "          [ 8.96897554e-01],\n",
       "          [ 9.20771301e-01],\n",
       "          [ 1.26074481e+00]],\n",
       "\n",
       "         [[ 5.65136909e-01],\n",
       "          [ 7.83142865e-01],\n",
       "          [ 9.42872107e-01],\n",
       "          ...,\n",
       "          [-2.61655658e-01],\n",
       "          [ 9.83102769e-02],\n",
       "          [ 4.74030286e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.68845165e-02],\n",
       "          [-2.34260172e-01],\n",
       "          [ 1.46705762e-01],\n",
       "          ...,\n",
       "          [ 2.00828519e-02],\n",
       "          [-1.59957796e-01],\n",
       "          [-2.45466873e-01]],\n",
       "\n",
       "         [[-1.19658053e+00],\n",
       "          [-1.69308519e+00],\n",
       "          [-3.87873232e-01],\n",
       "          ...,\n",
       "          [ 2.23403528e-01],\n",
       "          [-8.86636198e-01],\n",
       "          [-5.75449407e-01]],\n",
       "\n",
       "         [[ 7.08105505e-01],\n",
       "          [-1.81255817e+00],\n",
       "          [-1.48121953e+00],\n",
       "          ...,\n",
       "          [ 3.76806200e-01],\n",
       "          [-5.42918205e-01],\n",
       "          [ 4.41376567e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.03278148e+00],\n",
       "          [ 1.71551669e+00],\n",
       "          [ 1.79083228e+00],\n",
       "          ...,\n",
       "          [ 7.72730947e-01],\n",
       "          [ 9.34396684e-01],\n",
       "          [ 1.51204646e+00]],\n",
       "\n",
       "         [[ 1.37950313e+00],\n",
       "          [ 1.47791374e+00],\n",
       "          [ 1.37511265e+00],\n",
       "          ...,\n",
       "          [ 9.63298142e-01],\n",
       "          [ 9.72806811e-01],\n",
       "          [ 1.31849670e+00]],\n",
       "\n",
       "         [[ 5.66580474e-01],\n",
       "          [ 7.37329125e-01],\n",
       "          [ 9.06634450e-01],\n",
       "          ...,\n",
       "          [-2.58229762e-01],\n",
       "          [ 9.68143344e-02],\n",
       "          [ 4.63079482e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.06299096e-01],\n",
       "          [-2.37500936e-01],\n",
       "          [ 2.25534946e-01],\n",
       "          ...,\n",
       "          [-6.12203404e-02],\n",
       "          [-2.51177311e-01],\n",
       "          [-1.40642911e-01]],\n",
       "\n",
       "         [[-1.08322549e+00],\n",
       "          [-1.72485042e+00],\n",
       "          [-3.78283978e-01],\n",
       "          ...,\n",
       "          [ 9.36858431e-02],\n",
       "          [-1.12265623e+00],\n",
       "          [-6.07779026e-01]],\n",
       "\n",
       "         [[ 7.15143144e-01],\n",
       "          [-1.88257873e+00],\n",
       "          [-1.63362455e+00],\n",
       "          ...,\n",
       "          [ 3.28968883e-01],\n",
       "          [-6.43231690e-01],\n",
       "          [ 3.52597058e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.01747394e+00],\n",
       "          [ 1.75199354e+00],\n",
       "          [ 1.88086724e+00],\n",
       "          ...,\n",
       "          [ 7.64736056e-01],\n",
       "          [ 8.79015386e-01],\n",
       "          [ 1.48260689e+00]],\n",
       "\n",
       "         [[ 1.41790557e+00],\n",
       "          [ 1.55994701e+00],\n",
       "          [ 1.45534444e+00],\n",
       "          ...,\n",
       "          [ 9.44264352e-01],\n",
       "          [ 9.50580418e-01],\n",
       "          [ 1.30947375e+00]],\n",
       "\n",
       "         [[ 6.60528541e-01],\n",
       "          [ 8.55886579e-01],\n",
       "          [ 1.04852355e+00],\n",
       "          ...,\n",
       "          [-3.15560132e-01],\n",
       "          [ 2.32891478e-02],\n",
       "          [ 4.68549639e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 4.73285794e-01],\n",
       "          [-1.55688807e-01],\n",
       "          [ 3.91176611e-01],\n",
       "          ...,\n",
       "          [-1.92234024e-01],\n",
       "          [-3.26620191e-01],\n",
       "          [-2.29428038e-02]],\n",
       "\n",
       "         [[-8.59293401e-01],\n",
       "          [-1.61295569e+00],\n",
       "          [-2.61210710e-01],\n",
       "          ...,\n",
       "          [ 9.90962461e-02],\n",
       "          [-1.20373106e+00],\n",
       "          [-5.23873568e-01]],\n",
       "\n",
       "         [[ 7.26767242e-01],\n",
       "          [-1.84673488e+00],\n",
       "          [-1.75555873e+00],\n",
       "          ...,\n",
       "          [ 3.70759934e-01],\n",
       "          [-6.50319457e-01],\n",
       "          [ 3.74336809e-01]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 1.05650604e+00],\n",
       "          [ 1.36264014e+00],\n",
       "          [ 1.34554887e+00],\n",
       "          ...,\n",
       "          [ 1.74142349e+00],\n",
       "          [ 1.51377082e+00],\n",
       "          [ 1.57523370e+00]],\n",
       "\n",
       "         [[ 1.34015131e+00],\n",
       "          [ 1.09261858e+00],\n",
       "          [ 9.20372546e-01],\n",
       "          ...,\n",
       "          [ 1.34204280e+00],\n",
       "          [ 1.49625623e+00],\n",
       "          [ 1.37692010e+00]],\n",
       "\n",
       "         [[ 1.00881195e+00],\n",
       "          [ 1.18061197e+00],\n",
       "          [ 9.32613790e-01],\n",
       "          ...,\n",
       "          [ 5.74113429e-01],\n",
       "          [ 9.43301082e-01],\n",
       "          [ 7.84881473e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 4.41790104e-01],\n",
       "          [ 8.83235514e-01],\n",
       "          [-2.20139697e-01],\n",
       "          ...,\n",
       "          [ 2.10817009e-01],\n",
       "          [ 9.21731591e-01],\n",
       "          [ 1.39006925e+00]],\n",
       "\n",
       "         [[-2.74293870e-01],\n",
       "          [ 3.96349132e-01],\n",
       "          [ 7.88462937e-01],\n",
       "          ...,\n",
       "          [ 1.19147193e+00],\n",
       "          [ 1.09640014e+00],\n",
       "          [-4.41391051e-01]],\n",
       "\n",
       "         [[ 8.03167760e-01],\n",
       "          [ 3.22496265e-01],\n",
       "          [ 5.22812046e-02],\n",
       "          ...,\n",
       "          [ 1.22210550e+00],\n",
       "          [ 8.73679459e-01],\n",
       "          [ 1.67143539e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.10257900e+00],\n",
       "          [ 1.40027845e+00],\n",
       "          [ 1.35083449e+00],\n",
       "          ...,\n",
       "          [ 1.89066970e+00],\n",
       "          [ 1.68356597e+00],\n",
       "          [ 1.72060966e+00]],\n",
       "\n",
       "         [[ 1.28571832e+00],\n",
       "          [ 1.06180978e+00],\n",
       "          [ 9.03383911e-01],\n",
       "          ...,\n",
       "          [ 1.46462691e+00],\n",
       "          [ 1.54794002e+00],\n",
       "          [ 1.37113214e+00]],\n",
       "\n",
       "         [[ 9.69825208e-01],\n",
       "          [ 1.09793878e+00],\n",
       "          [ 8.88440430e-01],\n",
       "          ...,\n",
       "          [ 5.86842775e-01],\n",
       "          [ 9.77596521e-01],\n",
       "          [ 7.69184649e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 5.72660208e-01],\n",
       "          [ 8.72038662e-01],\n",
       "          [-1.47511303e-01],\n",
       "          ...,\n",
       "          [ 2.52896816e-01],\n",
       "          [ 8.90182555e-01],\n",
       "          [ 1.44093525e+00]],\n",
       "\n",
       "         [[-2.07899272e-01],\n",
       "          [ 3.79277438e-01],\n",
       "          [ 8.15327942e-01],\n",
       "          ...,\n",
       "          [ 1.17658854e+00],\n",
       "          [ 1.03979814e+00],\n",
       "          [-3.05826843e-01]],\n",
       "\n",
       "         [[ 8.79944444e-01],\n",
       "          [ 3.39401633e-01],\n",
       "          [ 8.96904618e-02],\n",
       "          ...,\n",
       "          [ 1.31369293e+00],\n",
       "          [ 8.91908348e-01],\n",
       "          [ 1.85550138e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.08343196e+00],\n",
       "          [ 1.40071011e+00],\n",
       "          [ 1.26597083e+00],\n",
       "          ...,\n",
       "          [ 1.91848624e+00],\n",
       "          [ 1.70034504e+00],\n",
       "          [ 1.72931206e+00]],\n",
       "\n",
       "         [[ 1.22668839e+00],\n",
       "          [ 1.00202823e+00],\n",
       "          [ 8.20403337e-01],\n",
       "          ...,\n",
       "          [ 1.46821892e+00],\n",
       "          [ 1.56984556e+00],\n",
       "          [ 1.32960367e+00]],\n",
       "\n",
       "         [[ 9.50440466e-01],\n",
       "          [ 1.02767503e+00],\n",
       "          [ 8.38067353e-01],\n",
       "          ...,\n",
       "          [ 5.77073514e-01],\n",
       "          [ 9.79359865e-01],\n",
       "          [ 7.76819885e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 7.31231928e-01],\n",
       "          [ 9.20140266e-01],\n",
       "          [-7.74875283e-02],\n",
       "          ...,\n",
       "          [ 2.91301161e-01],\n",
       "          [ 9.10074115e-01],\n",
       "          [ 1.55428052e+00]],\n",
       "\n",
       "         [[-1.03045069e-01],\n",
       "          [ 4.12901521e-01],\n",
       "          [ 8.12982798e-01],\n",
       "          ...,\n",
       "          [ 1.21738899e+00],\n",
       "          [ 1.13054645e+00],\n",
       "          [-1.94724575e-01]],\n",
       "\n",
       "         [[ 1.00549436e+00],\n",
       "          [ 4.23224896e-01],\n",
       "          [ 1.41115010e-01],\n",
       "          ...,\n",
       "          [ 1.50357926e+00],\n",
       "          [ 9.27153587e-01],\n",
       "          [ 2.41223156e-01]]]]], dtype=float32)\n",
       "Coordinates:\n",
       "  * lag        (lag) int64 0 1 2 3 4 5 6 7 8 9 ... 50 51 52 53 54 55 56 57 58 59\n",
       "  * lat        (lat) float32 60.0 58.0 56.0 54.0 ... -54.0 -56.0 -58.0 -60.0\n",
       "  * lon        (lon) float32 -180.0 -178.0 -176.0 -174.0 ... 174.0 176.0 178.0\n",
       "    dayofyear  (time, lag) int64 198 199 200 201 202 203 ... 339 340 341 342 343\n",
       "  * time       (time) datetime64[ns] 1981-10-04 1981-10-05 ... 2020-12-28\n",
       "Dimensions without coordinates: channel"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3440, 60, 23, 23, 1)\n",
      "tf.Tensor(\n",
      "[[[0.2253395  0.         0.00381813 0.         0.15441748]\n",
      "  [0.26483253 0.         0.02860689 0.         0.1603842 ]\n",
      "  [0.2679529  0.         0.01727467 0.         0.16836566]\n",
      "  ...\n",
      "  [0.06504946 0.1917446  0.1370848  0.         0.42671403]\n",
      "  [0.05352365 0.23758274 0.12607719 0.         0.42049035]\n",
      "  [0.08251506 0.26748037 0.10798278 0.         0.4008337 ]]\n",
      "\n",
      " [[0.26483253 0.         0.02860689 0.         0.1603842 ]\n",
      "  [0.2679529  0.         0.01727467 0.         0.16836566]\n",
      "  [0.23541021 0.00464223 0.00923259 0.         0.19475445]\n",
      "  ...\n",
      "  [0.05352365 0.23758274 0.12607719 0.         0.42049035]\n",
      "  [0.08251506 0.26748037 0.10798278 0.         0.4008337 ]\n",
      "  [0.1262748  0.2688949  0.11820648 0.         0.37279347]]\n",
      "\n",
      " [[0.2679529  0.         0.01727467 0.         0.16836566]\n",
      "  [0.23541021 0.00464223 0.00923259 0.         0.19475445]\n",
      "  [0.18735442 0.05565199 0.00530745 0.         0.20910195]\n",
      "  ...\n",
      "  [0.08251506 0.26748037 0.10798278 0.         0.4008337 ]\n",
      "  [0.1262748  0.2688949  0.11820648 0.         0.37279347]\n",
      "  [0.13842966 0.2587969  0.12727636 0.         0.3377121 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.06351158 0.         0.         0.01839108 0.        ]\n",
      "  [0.05795249 0.         0.         0.0080426  0.        ]\n",
      "  [0.07298182 0.         0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.15603033 0.         0.         0.         0.0307194 ]\n",
      "  [0.11733059 0.         0.         0.         0.        ]\n",
      "  [0.11694374 0.         0.         0.         0.        ]]\n",
      "\n",
      " [[0.05795249 0.         0.         0.0080426  0.        ]\n",
      "  [0.07298182 0.         0.         0.         0.        ]\n",
      "  [0.08314194 0.         0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.11733059 0.         0.         0.         0.        ]\n",
      "  [0.11694374 0.         0.         0.         0.        ]\n",
      "  [0.08268698 0.         0.         0.         0.        ]]\n",
      "\n",
      " [[0.07298182 0.         0.         0.         0.        ]\n",
      "  [0.08314194 0.         0.         0.         0.        ]\n",
      "  [0.08191998 0.         0.02004792 0.         0.        ]\n",
      "  ...\n",
      "  [0.11694374 0.         0.         0.         0.        ]\n",
      "  [0.08268698 0.         0.         0.         0.        ]\n",
      "  [0.08703648 0.         0.         0.         0.        ]]], shape=(3440, 60, 5), dtype=float32)\n",
      "(3440, 60, 61, 180, 1)\n",
      "tf.Tensor(\n",
      "[[[ 0.       38.34895  12.372612 46.414955 41.94499 ]\n",
      "  [ 0.       38.38639  12.320982 46.38968  41.942314]\n",
      "  [ 0.       38.46425  12.282899 46.410027 41.897266]\n",
      "  ...\n",
      "  [ 0.       38.94644  12.211814 46.247715 41.271145]\n",
      "  [ 0.       38.856163 12.208162 46.248966 41.35366 ]\n",
      "  [ 0.       38.776463 12.208376 46.252987 41.434734]]\n",
      "\n",
      " [[ 0.       38.38639  12.320982 46.38968  41.942314]\n",
      "  [ 0.       38.46425  12.282899 46.410027 41.897266]\n",
      "  [ 0.       38.578033 12.28278  46.47213  41.834015]\n",
      "  ...\n",
      "  [ 0.       38.856163 12.208162 46.248966 41.35366 ]\n",
      "  [ 0.       38.776463 12.208376 46.252987 41.434734]\n",
      "  [ 0.       38.72192  12.215406 46.247658 41.50347 ]]\n",
      "\n",
      " [[ 0.       38.46425  12.282899 46.410027 41.897266]\n",
      "  [ 0.       38.578033 12.28278  46.47213  41.834015]\n",
      "  [ 0.       38.697536 12.318755 46.541874 41.769173]\n",
      "  ...\n",
      "  [ 0.       38.776463 12.208376 46.252987 41.434734]\n",
      "  [ 0.       38.72192  12.215406 46.247658 41.50347 ]\n",
      "  [ 0.       38.679356 12.24421  46.24102  41.54614 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.       38.201954 12.575233 46.150326 41.19035 ]\n",
      "  [ 0.       38.15298  12.544256 46.170933 41.135014]\n",
      "  [ 0.       38.142704 12.555965 46.201275 41.110794]\n",
      "  ...\n",
      "  [ 0.       39.091938 12.931275 46.656258 40.318123]\n",
      "  [ 0.       39.173874 12.944014 46.659084 40.24232 ]\n",
      "  [ 0.       39.244774 13.013059 46.653355 40.201813]]\n",
      "\n",
      " [[ 0.       38.15298  12.544256 46.170933 41.135014]\n",
      "  [ 0.       38.142704 12.555965 46.201275 41.110794]\n",
      "  [ 0.       38.120995 12.527163 46.234806 41.078796]\n",
      "  ...\n",
      "  [ 0.       39.173874 12.944014 46.659084 40.24232 ]\n",
      "  [ 0.       39.244774 13.013059 46.653355 40.201813]\n",
      "  [ 0.       39.306973 13.099513 46.625233 40.177998]]\n",
      "\n",
      " [[ 0.       38.142704 12.555965 46.201275 41.110794]\n",
      "  [ 0.       38.120995 12.527163 46.234806 41.078796]\n",
      "  [ 0.       38.108635 12.498724 46.26613  41.036427]\n",
      "  ...\n",
      "  [ 0.       39.244774 13.013059 46.653355 40.201813]\n",
      "  [ 0.       39.306973 13.099513 46.625233 40.177998]\n",
      "  [ 0.       39.326424 13.19249  46.643536 40.168816]]], shape=(3440, 60, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Feature array\n",
    "tmp = output_vars[0].expand_dims(dim={\"channel\": 1},axis=4)\n",
    "tmp = tmp.fillna(100)\n",
    "x = tmp.t2m\n",
    "print(x.shape)\n",
    "\n",
    "cnn_op1 = build_CNN(x)\n",
    "print(cnn_op1)\n",
    "\n",
    "del x\n",
    "\n",
    "# Feature array\n",
    "tmp = output_vars[1].expand_dims(dim={\"channel\": 1},axis=4)\n",
    "tmp = tmp.fillna(100)\n",
    "x = tmp.sst\n",
    "print(x.shape)\n",
    "\n",
    "cnn_op2 = build_CNN(x)\n",
    "print(cnn_op2)\n",
    "\n",
    "del x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3440, 60, 10), dtype=float32, numpy=\n",
       "array([[[2.25339502e-01, 0.00000000e+00, 3.81813012e-03, ...,\n",
       "         1.23726120e+01, 4.64149551e+01, 4.19449883e+01],\n",
       "        [2.64832526e-01, 0.00000000e+00, 2.86068898e-02, ...,\n",
       "         1.23209820e+01, 4.63896790e+01, 4.19423141e+01],\n",
       "        [2.67952889e-01, 0.00000000e+00, 1.72746666e-02, ...,\n",
       "         1.22828989e+01, 4.64100266e+01, 4.18972664e+01],\n",
       "        ...,\n",
       "        [6.50494620e-02, 1.91744596e-01, 1.37084797e-01, ...,\n",
       "         1.22118139e+01, 4.62477150e+01, 4.12711449e+01],\n",
       "        [5.35236523e-02, 2.37582743e-01, 1.26077190e-01, ...,\n",
       "         1.22081623e+01, 4.62489662e+01, 4.13536606e+01],\n",
       "        [8.25150609e-02, 2.67480373e-01, 1.07982785e-01, ...,\n",
       "         1.22083759e+01, 4.62529869e+01, 4.14347343e+01]],\n",
       "\n",
       "       [[2.64832526e-01, 0.00000000e+00, 2.86068898e-02, ...,\n",
       "         1.23209820e+01, 4.63896790e+01, 4.19423141e+01],\n",
       "        [2.67952889e-01, 0.00000000e+00, 1.72746666e-02, ...,\n",
       "         1.22828989e+01, 4.64100266e+01, 4.18972664e+01],\n",
       "        [2.35410213e-01, 4.64223139e-03, 9.23259370e-03, ...,\n",
       "         1.22827797e+01, 4.64721298e+01, 4.18340149e+01],\n",
       "        ...,\n",
       "        [5.35236523e-02, 2.37582743e-01, 1.26077190e-01, ...,\n",
       "         1.22081623e+01, 4.62489662e+01, 4.13536606e+01],\n",
       "        [8.25150609e-02, 2.67480373e-01, 1.07982785e-01, ...,\n",
       "         1.22083759e+01, 4.62529869e+01, 4.14347343e+01],\n",
       "        [1.26274794e-01, 2.68894911e-01, 1.18206479e-01, ...,\n",
       "         1.22154064e+01, 4.62476578e+01, 4.15034714e+01]],\n",
       "\n",
       "       [[2.67952889e-01, 0.00000000e+00, 1.72746666e-02, ...,\n",
       "         1.22828989e+01, 4.64100266e+01, 4.18972664e+01],\n",
       "        [2.35410213e-01, 4.64223139e-03, 9.23259370e-03, ...,\n",
       "         1.22827797e+01, 4.64721298e+01, 4.18340149e+01],\n",
       "        [1.87354416e-01, 5.56519926e-02, 5.30744717e-03, ...,\n",
       "         1.23187551e+01, 4.65418739e+01, 4.17691727e+01],\n",
       "        ...,\n",
       "        [8.25150609e-02, 2.67480373e-01, 1.07982785e-01, ...,\n",
       "         1.22083759e+01, 4.62529869e+01, 4.14347343e+01],\n",
       "        [1.26274794e-01, 2.68894911e-01, 1.18206479e-01, ...,\n",
       "         1.22154064e+01, 4.62476578e+01, 4.15034714e+01],\n",
       "        [1.38429657e-01, 2.58796901e-01, 1.27276361e-01, ...,\n",
       "         1.22442102e+01, 4.62410202e+01, 4.15461388e+01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[6.35115802e-02, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.25752335e+01, 4.61503258e+01, 4.11903496e+01],\n",
       "        [5.79524934e-02, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.25442562e+01, 4.61709328e+01, 4.11350136e+01],\n",
       "        [7.29818195e-02, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.25559654e+01, 4.62012749e+01, 4.11107941e+01],\n",
       "        ...,\n",
       "        [1.56030327e-01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.29312754e+01, 4.66562576e+01, 4.03181229e+01],\n",
       "        [1.17330588e-01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.29440136e+01, 4.66590843e+01, 4.02423210e+01],\n",
       "        [1.16943739e-01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.30130587e+01, 4.66533546e+01, 4.02018127e+01]],\n",
       "\n",
       "       [[5.79524934e-02, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.25442562e+01, 4.61709328e+01, 4.11350136e+01],\n",
       "        [7.29818195e-02, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.25559654e+01, 4.62012749e+01, 4.11107941e+01],\n",
       "        [8.31419379e-02, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.25271626e+01, 4.62348061e+01, 4.10787964e+01],\n",
       "        ...,\n",
       "        [1.17330588e-01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.29440136e+01, 4.66590843e+01, 4.02423210e+01],\n",
       "        [1.16943739e-01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.30130587e+01, 4.66533546e+01, 4.02018127e+01],\n",
       "        [8.26869830e-02, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.30995131e+01, 4.66252327e+01, 4.01779976e+01]],\n",
       "\n",
       "       [[7.29818195e-02, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.25559654e+01, 4.62012749e+01, 4.11107941e+01],\n",
       "        [8.31419379e-02, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.25271626e+01, 4.62348061e+01, 4.10787964e+01],\n",
       "        [8.19199830e-02, 0.00000000e+00, 2.00479198e-02, ...,\n",
       "         1.24987240e+01, 4.62661285e+01, 4.10364265e+01],\n",
       "        ...,\n",
       "        [1.16943739e-01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.30130587e+01, 4.66533546e+01, 4.02018127e+01],\n",
       "        [8.26869830e-02, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.30995131e+01, 4.66252327e+01, 4.01779976e+01],\n",
       "        [8.70364755e-02, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.31924896e+01, 4.66435356e+01, 4.01688156e+01]]], dtype=float32)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = Concatenate(axis=-1)([cnn_op1,cnn_op2])\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 2\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 60, 10)]          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 60, 66)            20328     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 60, 35)            14280     \n",
      "_________________________________________________________________\n",
      "attention_with_context_1 (At ((None, 60, 35), (None, 6 1295      \n",
      "_________________________________________________________________\n",
      "addition_1 (Addition)        (None, 35)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 144       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 20        \n",
      "=================================================================\n",
      "Total params: 36,067\n",
      "Trainable params: 36,067\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "Epoch 1/100\n",
      "53/54 [============================>.] - ETA: 0s - loss: 7.8914 - accuracy: 0.2567WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "54/54 [==============================] - 9s 89ms/step - loss: 7.8850 - accuracy: 0.2580 - val_loss: 7.2539 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "54/54 [==============================] - 4s 67ms/step - loss: 7.2864 - accuracy: 0.2962 - val_loss: 6.7948 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n",
      "54/54 [==============================] - 3s 64ms/step - loss: 6.8564 - accuracy: 0.3025 - val_loss: 6.4079 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "54/54 [==============================] - 3s 63ms/step - loss: 6.4752 - accuracy: 0.2958 - val_loss: 6.0474 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "54/54 [==============================] - 3s 65ms/step - loss: 6.1229 - accuracy: 0.2982 - val_loss: 5.7174 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "54/54 [==============================] - 3s 61ms/step - loss: 5.7966 - accuracy: 0.3022 - val_loss: 5.4102 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100\n",
      "54/54 [==============================] - 3s 58ms/step - loss: 5.4991 - accuracy: 0.3170 - val_loss: 5.1285 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100\n",
      "54/54 [==============================] - 3s 58ms/step - loss: 5.2247 - accuracy: 0.3065 - val_loss: 4.8679 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100\n",
      "54/54 [==============================] - 3s 57ms/step - loss: 4.9691 - accuracy: 0.3019 - val_loss: 4.6276 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100\n",
      "54/54 [==============================] - 3s 60ms/step - loss: 4.7337 - accuracy: 0.3059 - val_loss: 4.4048 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100\n",
      "54/54 [==============================] - 3s 59ms/step - loss: 4.5174 - accuracy: 0.3114 - val_loss: 4.1978 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100\n",
      "54/54 [==============================] - 3s 59ms/step - loss: 4.3150 - accuracy: 0.3113 - val_loss: 4.0058 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100\n",
      "54/54 [==============================] - 3s 56ms/step - loss: 4.1277 - accuracy: 0.3153 - val_loss: 3.8267 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100\n",
      "54/54 [==============================] - 3s 58ms/step - loss: 3.9498 - accuracy: 0.3160 - val_loss: 3.6610 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100\n",
      "54/54 [==============================] - 3s 59ms/step - loss: 3.7875 - accuracy: 0.3149 - val_loss: 3.5057 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100\n",
      "54/54 [==============================] - 4s 68ms/step - loss: 3.6353 - accuracy: 0.3044 - val_loss: 3.3602 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100\n",
      "54/54 [==============================] - 3s 57ms/step - loss: 3.4906 - accuracy: 0.3067 - val_loss: 3.2262 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100\n",
      "54/54 [==============================] - 3s 58ms/step - loss: 3.3607 - accuracy: 0.3025 - val_loss: 3.1000 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100\n",
      "54/54 [==============================] - 4s 67ms/step - loss: 3.2342 - accuracy: 0.3023 - val_loss: 2.9817 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100\n",
      "54/54 [==============================] - 3s 59ms/step - loss: 3.1214 - accuracy: 0.3192 - val_loss: 2.8711 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100\n",
      "54/54 [==============================] - 3s 60ms/step - loss: 3.0092 - accuracy: 0.3137 - val_loss: 2.7685 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100\n",
      "54/54 [==============================] - 4s 66ms/step - loss: 2.9088 - accuracy: 0.3218 - val_loss: 2.6716 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100\n",
      "54/54 [==============================] - 3s 56ms/step - loss: 2.8143 - accuracy: 0.3082 - val_loss: 2.5814 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100\n",
      "54/54 [==============================] - 3s 59ms/step - loss: 2.7250 - accuracy: 0.3084 - val_loss: 2.4967 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100\n",
      "54/54 [==============================] - 4s 68ms/step - loss: 2.6400 - accuracy: 0.3329 - val_loss: 2.4169 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100\n",
      "54/54 [==============================] - 3s 57ms/step - loss: 2.5643 - accuracy: 0.3059 - val_loss: 2.3423 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100\n",
      "54/54 [==============================] - 3s 64ms/step - loss: 2.4883 - accuracy: 0.3407 - val_loss: 2.2724 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100\n",
      "54/54 [==============================] - 4s 68ms/step - loss: 2.4211 - accuracy: 0.2976 - val_loss: 2.2069 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100\n",
      "54/54 [==============================] - 3s 58ms/step - loss: 2.3553 - accuracy: 0.3133 - val_loss: 2.1452 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100\n",
      "54/54 [==============================] - 3s 56ms/step - loss: 2.2913 - accuracy: 0.3353 - val_loss: 2.0875 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100\n",
      "54/54 [==============================] - 3s 64ms/step - loss: 2.2385 - accuracy: 0.3013 - val_loss: 2.0328 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100\n",
      "54/54 [==============================] - 3s 57ms/step - loss: 2.1839 - accuracy: 0.3038 - val_loss: 1.9812 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100\n",
      "54/54 [==============================] - 3s 57ms/step - loss: 2.1335 - accuracy: 0.3188 - val_loss: 1.9333 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100\n",
      "54/54 [==============================] - 4s 65ms/step - loss: 2.0845 - accuracy: 0.3165 - val_loss: 1.8876 - val_accuracy: 0.3105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100\n",
      "54/54 [==============================] - 3s 56ms/step - loss: 2.0378 - accuracy: 0.3268 - val_loss: 1.8451 - val_accuracy: 0.3105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/100\n",
      "54/54 [==============================] - 3s 55ms/step - loss: 1.9990 - accuracy: 0.3189 - val_loss: 1.8045 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/100\n",
      "54/54 [==============================] - 3s 55ms/step - loss: 1.9580 - accuracy: 0.2835 - val_loss: 1.7666 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/100\n",
      "54/54 [==============================] - 3s 56ms/step - loss: 1.9243 - accuracy: 0.3101 - val_loss: 1.7302 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/100\n",
      "54/54 [==============================] - 3s 55ms/step - loss: 1.8857 - accuracy: 0.3566 - val_loss: 1.6967 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100\n",
      "54/54 [==============================] - 3s 56ms/step - loss: 1.8555 - accuracy: 0.3111 - val_loss: 1.6645 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/100\n",
      "54/54 [==============================] - 3s 56ms/step - loss: 1.8250 - accuracy: 0.3168 - val_loss: 1.6344 - val_accuracy: 0.3026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/100\n",
      "54/54 [==============================] - 3s 58ms/step - loss: 1.7933 - accuracy: 0.3407 - val_loss: 1.6058 - val_accuracy: 0.3831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100\n",
      "54/54 [==============================] - 3s 57ms/step - loss: 1.7643 - accuracy: 0.3407 - val_loss: 1.5790 - val_accuracy: 0.3831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/100\n",
      "54/54 [==============================] - 3s 55ms/step - loss: 1.7433 - accuracy: 0.3174 - val_loss: 1.5537 - val_accuracy: 0.3831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100\n",
      "54/54 [==============================] - 3s 56ms/step - loss: 1.7151 - accuracy: 0.3364 - val_loss: 1.5301 - val_accuracy: 0.3105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/100\n",
      "54/54 [==============================] - 3s 59ms/step - loss: 1.6898 - accuracy: 0.3239 - val_loss: 1.5075 - val_accuracy: 0.3721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100\n",
      "54/54 [==============================] - 4s 66ms/step - loss: 1.6692 - accuracy: 0.3184 - val_loss: 1.4861 - val_accuracy: 0.3547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/100\n",
      "54/54 [==============================] - 3s 59ms/step - loss: 1.6512 - accuracy: 0.3001 - val_loss: 1.4656 - val_accuracy: 0.3831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100\n",
      "54/54 [==============================] - 3s 59ms/step - loss: 1.6259 - accuracy: 0.3656 - val_loss: 1.4469 - val_accuracy: 0.3831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100\n",
      "54/54 [==============================] - 3s 63ms/step - loss: 1.6080 - accuracy: 0.3575 - val_loss: 1.4290 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100\n",
      "54/54 [==============================] - 3s 56ms/step - loss: 1.5885 - accuracy: 0.3266 - val_loss: 1.4123 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/100\n",
      "54/54 [==============================] - 3s 59ms/step - loss: 1.5689 - accuracy: 0.3430 - val_loss: 1.3964 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100\n",
      "54/54 [==============================] - 3s 63ms/step - loss: 1.5562 - accuracy: 0.2876 - val_loss: 1.3813 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/100\n",
      "54/54 [==============================] - 3s 58ms/step - loss: 1.5432 - accuracy: 0.2951 - val_loss: 1.3666 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100\n",
      "54/54 [==============================] - 3s 59ms/step - loss: 1.5262 - accuracy: 0.3335 - val_loss: 1.3530 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100\n",
      "54/54 [==============================] - 4s 68ms/step - loss: 1.5136 - accuracy: 0.3176 - val_loss: 1.3403 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "54/54 [==============================] - 3s 59ms/step - loss: 1.5017 - accuracy: 0.3234 - val_loss: 1.3281 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "54/54 [==============================] - 3s 58ms/step - loss: 1.4872 - accuracy: 0.3635 - val_loss: 1.3164 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "54/54 [==============================] - 4s 67ms/step - loss: 1.4773 - accuracy: 0.3308 - val_loss: 1.3056 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100\n",
      "54/54 [==============================] - 3s 58ms/step - loss: 1.4706 - accuracy: 0.3094 - val_loss: 1.2951 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100\n",
      "54/54 [==============================] - 3s 59ms/step - loss: 1.4563 - accuracy: 0.3363 - val_loss: 1.2851 - val_accuracy: 0.3105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100\n",
      "54/54 [==============================] - 3s 65ms/step - loss: 1.4468 - accuracy: 0.3229 - val_loss: 1.2762 - val_accuracy: 0.3105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/100\n",
      "54/54 [==============================] - 3s 56ms/step - loss: 1.4361 - accuracy: 0.3223 - val_loss: 1.2676 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/100\n",
      "54/54 [==============================] - 3s 57ms/step - loss: 1.4304 - accuracy: 0.3032 - val_loss: 1.2591 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/100\n",
      "54/54 [==============================] - 3s 56ms/step - loss: 1.4243 - accuracy: 0.3219 - val_loss: 1.2507 - val_accuracy: 0.3831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/100\n",
      "54/54 [==============================] - 3s 55ms/step - loss: 1.4133 - accuracy: 0.3760 - val_loss: 1.2436 - val_accuracy: 0.3105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/100\n",
      "54/54 [==============================] - 3s 60ms/step - loss: 1.4089 - accuracy: 0.3085 - val_loss: 1.2362 - val_accuracy: 0.3105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/100\n",
      "54/54 [==============================] - 3s 59ms/step - loss: 1.3984 - accuracy: 0.3708 - val_loss: 1.2295 - val_accuracy: 0.3360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/100\n",
      "54/54 [==============================] - 3s 58ms/step - loss: 1.3944 - accuracy: 0.3096 - val_loss: 1.2233 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/100\n",
      "54/54 [==============================] - 3s 59ms/step - loss: 1.3833 - accuracy: 0.3758 - val_loss: 1.2173 - val_accuracy: 0.2721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/100\n",
      "54/54 [==============================] - 3s 63ms/step - loss: 1.3788 - accuracy: 0.3464 - val_loss: 1.2118 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/100\n",
      "54/54 [==============================] - 3s 58ms/step - loss: 1.3741 - accuracy: 0.3104 - val_loss: 1.2061 - val_accuracy: 0.3105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100\n",
      "54/54 [==============================] - 3s 58ms/step - loss: 1.3668 - accuracy: 0.3213 - val_loss: 1.2014 - val_accuracy: 0.3105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/100\n",
      "54/54 [==============================] - 3s 56ms/step - loss: 1.3655 - accuracy: 0.3005 - val_loss: 1.1965 - val_accuracy: 0.3105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/100\n",
      "54/54 [==============================] - 3s 57ms/step - loss: 1.3580 - accuracy: 0.3097 - val_loss: 1.1915 - val_accuracy: 0.3105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100\n",
      "54/54 [==============================] - 3s 57ms/step - loss: 1.3535 - accuracy: 0.3129 - val_loss: 1.1876 - val_accuracy: 0.3105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100\n",
      "54/54 [==============================] - 3s 57ms/step - loss: 1.3483 - accuracy: 0.3191 - val_loss: 1.1833 - val_accuracy: 0.3105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100\n",
      "54/54 [==============================] - 3s 56ms/step - loss: 1.3469 - accuracy: 0.3108 - val_loss: 1.1794 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/100\n",
      "54/54 [==============================] - 3s 57ms/step - loss: 1.3445 - accuracy: 0.3088 - val_loss: 1.1752 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "54/54 [==============================] - 3s 57ms/step - loss: 1.3414 - accuracy: 0.2974 - val_loss: 1.1715 - val_accuracy: 0.3759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/100\n",
      "54/54 [==============================] - 3s 56ms/step - loss: 1.3369 - accuracy: 0.3286 - val_loss: 1.1681 - val_accuracy: 0.3709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100\n",
      "54/54 [==============================] - 3s 63ms/step - loss: 1.3327 - accuracy: 0.3346 - val_loss: 1.1650 - val_accuracy: 0.3105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100\n",
      "54/54 [==============================] - 3s 64ms/step - loss: 1.3256 - accuracy: 0.3767 - val_loss: 1.1619 - val_accuracy: 0.3831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/100\n",
      "54/54 [==============================] - 3s 61ms/step - loss: 1.3282 - accuracy: 0.3009 - val_loss: 1.1595 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/100\n",
      "54/54 [==============================] - 3s 61ms/step - loss: 1.3227 - accuracy: 0.3223 - val_loss: 1.1563 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/100\n",
      "54/54 [==============================] - 3s 63ms/step - loss: 1.3187 - accuracy: 0.3444 - val_loss: 1.1537 - val_accuracy: 0.3265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/100\n",
      "54/54 [==============================] - 3s 62ms/step - loss: 1.3193 - accuracy: 0.3198 - val_loss: 1.1513 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/100\n",
      "54/54 [==============================] - 3s 56ms/step - loss: 1.3144 - accuracy: 0.3358 - val_loss: 1.1492 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/100\n",
      "54/54 [==============================] - 3s 59ms/step - loss: 1.3164 - accuracy: 0.3122 - val_loss: 1.1463 - val_accuracy: 0.3831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/100\n",
      "54/54 [==============================] - 4s 67ms/step - loss: 1.3085 - accuracy: 0.3783 - val_loss: 1.1445 - val_accuracy: 0.3105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/100\n",
      "54/54 [==============================] - 3s 57ms/step - loss: 1.3111 - accuracy: 0.3262 - val_loss: 1.1431 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/100\n",
      "54/54 [==============================] - 3s 56ms/step - loss: 1.3042 - accuracy: 0.3533 - val_loss: 1.1406 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/100\n",
      "54/54 [==============================] - 4s 66ms/step - loss: 1.3041 - accuracy: 0.3207 - val_loss: 1.1393 - val_accuracy: 0.3105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/100\n",
      "54/54 [==============================] - 3s 61ms/step - loss: 1.3037 - accuracy: 0.2896 - val_loss: 1.1371 - val_accuracy: 0.3105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/100\n",
      "54/54 [==============================] - 3s 58ms/step - loss: 1.3006 - accuracy: 0.3249 - val_loss: 1.1354 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/100\n",
      "54/54 [==============================] - 4s 67ms/step - loss: 1.2972 - accuracy: 0.3294 - val_loss: 1.1343 - val_accuracy: 0.3105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/100\n",
      "54/54 [==============================] - 3s 58ms/step - loss: 1.2981 - accuracy: 0.3046 - val_loss: 1.1326 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/100\n",
      "54/54 [==============================] - 3s 61ms/step - loss: 1.3004 - accuracy: 0.3171 - val_loss: 1.1311 - val_accuracy: 0.3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/100\n",
      "54/54 [==============================] - 3s 65ms/step - loss: 1.2963 - accuracy: 0.3108 - val_loss: 1.1296 - val_accuracy: 0.3128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100\n",
      "54/54 [==============================] - 3s 55ms/step - loss: 1.2944 - accuracy: 0.3589 - val_loss: 1.1281 - val_accuracy: 0.3831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    }
   ],
   "source": [
    "model = build_lstm(60, 10, layers=2, neurons=[66,35], regval=[0.1,0.05], out_neurons=n_cat+1)\n",
    "train_y_tmp = y_all_n.values\n",
    "class_weight = class_weight_creator(train_y_tmp)\n",
    "callbacks_path = root_results+'tmp/Run20/weight_cnn_1'\n",
    "history_path = root_results+'tmp/Run20/history_cnn_1'\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "val_X = inputs\n",
    "val_y = y_all_n\n",
    "model_history = train_model(model, inputs, y_all_n, val_X, val_y, callbacks_path, epochs, batch_size, class_weight)\n",
    "loss_tr, loss_va = [],[]\n",
    "metri_tr, metri_va = [],[]\n",
    "loss_tr.append(model_history.history['loss'])\n",
    "loss_va.append(model_history.history['val_loss'])\n",
    "metri_tr.append(model_history.history['accuracy'])\n",
    "metri_va.append(model_history.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def built_cnn_lstm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf    \n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "#tf.compat.v1.disable_v2_behavior() # <-- HERE !\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import (Dropout, Activation, Reshape, Flatten, \n",
    "                                     Conv2D, MaxPooling2D, LSTM, Dense, Dropout, Embedding, Bidirectional, GRU,\n",
    "                                     TimeDistributed, Concatenate)\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import initializers, regularizers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import constraints\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(ntimestep,nx,ny,1))\n",
    "layer1 = TimeDistributed(Conv2D(8, [3,3], padding='same', strides=(2, 2)))(inputs)\n",
    "layer1 = TimeDistributed(MaxPooling2D((2, 2), strides=2))(layer1)\n",
    "layer1 = TimeDistributed(Conv2D(8*np.power(2,1), [3, 3], padding='same', strides=(2, 2)))(layer1)\n",
    "layer1 = TimeDistributed(MaxPooling2D((2, 2), strides=2))(layer1)\n",
    "flat1 = TimeDistributed(Flatten())(layer1)\n",
    "cnn_op1 = TimeDistributed(Dense(100, activation=\"relu\", use_bias=True,\n",
    "                        kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01),\n",
    "                        bias_initializer=initializers.RandomNormal(seed=None),\n",
    "                        kernel_initializer=initializers.RandomNormal(seed=None), name='dense_out'))(flat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 60, 61, 180, 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 60, 31, 90, 8 80          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 60, 15, 45, 8 0           time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 60, 8, 23, 16 1168        time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 60, 4, 11, 16 0           time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 60, 704)      0           time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, 60, 100)      70500       time_distributed_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 60, 200)      0           time_distributed_5[0][0]         \n",
      "                                                                 time_distributed_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 60, 20)       17680       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 10)           1240        lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 6)            66          lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 4)            28          dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 90,762\n",
      "Trainable params: 90,762\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_tensor = Concatenate(axis=-1)([cnn_op1,cnn_op1])\n",
    "layer2 = LSTM(20, return_sequences=True, dropout=0.2, kernel_regularizer=regularizers.l2(0.01))(input_tensor)\n",
    "layer2 = LSTM(10, return_sequences=False, dropout=0.1, kernel_regularizer=regularizers.l2(0.01))(layer2)\n",
    "layer2 = Dense(6, activation=\"relu\")(layer2)\n",
    "outputs = Dense(n_cat+1, activation='softmax')(layer2)\n",
    "model = Model(inputs, outputs)\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation years [1981, 1982]\n",
      "train years {1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020}\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/cfc/s2s/zhengwu/code/models.py:271: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  weights = np.max(np.sum(Y, axis=0)) / np.sum(Y, axis=0)\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "52/52 [==============================] - ETA: 0s - loss: 32.0405 - accuracy: 0.3125WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "52/52 [==============================] - 29s 281ms/step - loss: 32.0313 - accuracy: 0.3126 - val_loss: 30.3241 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "52/52 [==============================] - 12s 222ms/step - loss: 29.9411 - accuracy: 0.3114 - val_loss: 28.2890 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n",
      "52/52 [==============================] - 12s 226ms/step - loss: 27.9311 - accuracy: 0.3096 - val_loss: 26.3075 - val_accuracy: 0.4535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "52/52 [==============================] - 13s 247ms/step - loss: 26.0099 - accuracy: 0.3049 - val_loss: 24.4667 - val_accuracy: 0.4535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "52/52 [==============================] - 13s 244ms/step - loss: 24.1927 - accuracy: 0.3083 - val_loss: 22.7348 - val_accuracy: 0.4535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "52/52 [==============================] - 13s 242ms/step - loss: 22.4792 - accuracy: 0.3145 - val_loss: 21.0908 - val_accuracy: 0.4535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100\n",
      "52/52 [==============================] - 13s 254ms/step - loss: 20.8616 - accuracy: 0.3491 - val_loss: 19.5362 - val_accuracy: 0.4535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100\n",
      "52/52 [==============================] - 12s 227ms/step - loss: 19.3344 - accuracy: 0.3726 - val_loss: 18.0940 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100\n",
      "52/52 [==============================] - 11s 218ms/step - loss: 17.9078 - accuracy: 0.3440 - val_loss: 16.7346 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100\n",
      "52/52 [==============================] - 12s 230ms/step - loss: 16.5666 - accuracy: 0.3250 - val_loss: 15.4570 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100\n",
      "52/52 [==============================] - 13s 243ms/step - loss: 15.3073 - accuracy: 0.3124 - val_loss: 14.2623 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100\n",
      "52/52 [==============================] - 12s 230ms/step - loss: 14.1334 - accuracy: 0.3171 - val_loss: 13.1440 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100\n",
      "52/52 [==============================] - 12s 228ms/step - loss: 13.0323 - accuracy: 0.3389 - val_loss: 12.1093 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100\n",
      "52/52 [==============================] - 12s 231ms/step - loss: 12.0138 - accuracy: 0.3150 - val_loss: 11.1421 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100\n",
      "52/52 [==============================] - 12s 233ms/step - loss: 11.0661 - accuracy: 0.3076 - val_loss: 10.2442 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100\n",
      "52/52 [==============================] - 12s 240ms/step - loss: 10.1843 - accuracy: 0.3224 - val_loss: 9.4192 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100\n",
      "52/52 [==============================] - 12s 224ms/step - loss: 9.3733 - accuracy: 0.3218 - val_loss: 8.6540 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100\n",
      "52/52 [==============================] - 12s 229ms/step - loss: 8.6269 - accuracy: 0.3239 - val_loss: 7.9466 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100\n",
      "52/52 [==============================] - 12s 227ms/step - loss: 7.9392 - accuracy: 0.3454 - val_loss: 7.3034 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100\n",
      "52/52 [==============================] - 12s 229ms/step - loss: 7.3097 - accuracy: 0.3583 - val_loss: 6.7158 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100\n",
      "52/52 [==============================] - 12s 232ms/step - loss: 6.7304 - accuracy: 0.3566 - val_loss: 6.1823 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100\n",
      "52/52 [==============================] - 12s 223ms/step - loss: 6.2096 - accuracy: 0.3429 - val_loss: 5.6961 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100\n",
      "52/52 [==============================] - 12s 233ms/step - loss: 5.7385 - accuracy: 0.3146 - val_loss: 5.2535 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100\n",
      "52/52 [==============================] - 12s 225ms/step - loss: 5.3034 - accuracy: 0.3253 - val_loss: 4.8543 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100\n",
      "52/52 [==============================] - 12s 228ms/step - loss: 4.9156 - accuracy: 0.3307 - val_loss: 4.4958 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100\n",
      "52/52 [==============================] - 12s 233ms/step - loss: 4.5666 - accuracy: 0.3237 - val_loss: 4.1783 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100\n",
      "52/52 [==============================] - 12s 234ms/step - loss: 4.2553 - accuracy: 0.2951 - val_loss: 3.8850 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100\n",
      "52/52 [==============================] - 13s 243ms/step - loss: 3.9696 - accuracy: 0.3535 - val_loss: 3.6264 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100\n",
      "52/52 [==============================] - 12s 231ms/step - loss: 3.7198 - accuracy: 0.3146 - val_loss: 3.3969 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100\n",
      "52/52 [==============================] - 12s 222ms/step - loss: 3.4916 - accuracy: 0.3369 - val_loss: 3.1903 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100\n",
      "52/52 [==============================] - 12s 223ms/step - loss: 3.2957 - accuracy: 0.3533 - val_loss: 3.0100 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100\n",
      "52/52 [==============================] - 12s 240ms/step - loss: 3.1222 - accuracy: 0.3168 - val_loss: 2.8501 - val_accuracy: 0.2267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100\n",
      "52/52 [==============================] - 12s 231ms/step - loss: 2.9675 - accuracy: 0.3299 - val_loss: 2.7101 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100\n",
      "52/52 [==============================] - 13s 250ms/step - loss: 2.8294 - accuracy: 0.3309 - val_loss: 2.5880 - val_accuracy: 0.2267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100\n",
      "52/52 [==============================] - 12s 224ms/step - loss: 2.7105 - accuracy: 0.3333 - val_loss: 2.4764 - val_accuracy: 0.2267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/100\n",
      "52/52 [==============================] - 15s 281ms/step - loss: 2.6049 - accuracy: 0.3487 - val_loss: 2.3818 - val_accuracy: 0.4535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/100\n",
      "52/52 [==============================] - 13s 257ms/step - loss: 2.5163 - accuracy: 0.3218 - val_loss: 2.2979 - val_accuracy: 0.4535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/100\n",
      "52/52 [==============================] - 12s 230ms/step - loss: 2.4396 - accuracy: 0.3475 - val_loss: 2.2370 - val_accuracy: 0.4419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/100\n",
      "52/52 [==============================] - 12s 230ms/step - loss: 2.3733 - accuracy: 0.3584 - val_loss: 2.1650 - val_accuracy: 0.4535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100\n",
      "52/52 [==============================] - 18s 353ms/step - loss: 2.3059 - accuracy: 0.3539 - val_loss: 2.1980 - val_accuracy: 0.4535\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 17s 335ms/step - loss: 2.5671 - accuracy: 0.3808 - val_loss: 2.2827 - val_accuracy: 0.3198\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 15s 285ms/step - loss: 2.3948 - accuracy: 0.3297 - val_loss: 2.1282 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100\n",
      "52/52 [==============================] - 12s 228ms/step - loss: 2.2583 - accuracy: 0.3388 - val_loss: 2.0490 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/100\n",
      "52/52 [==============================] - 14s 267ms/step - loss: 2.1839 - accuracy: 0.3441 - val_loss: 1.9962 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100\n",
      "52/52 [==============================] - 16s 316ms/step - loss: 2.1331 - accuracy: 0.3052 - val_loss: 1.9551 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/100\n",
      "52/52 [==============================] - 24s 457ms/step - loss: 2.0969 - accuracy: 0.3463 - val_loss: 1.9289 - val_accuracy: 0.2267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100\n",
      "52/52 [==============================] - 17s 333ms/step - loss: 2.0697 - accuracy: 0.3155 - val_loss: 1.9045 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/100\n",
      "52/52 [==============================] - 15s 289ms/step - loss: 2.0526 - accuracy: 0.3030 - val_loss: 1.9047 - val_accuracy: 0.2267\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 20s 386ms/step - loss: 2.0369 - accuracy: 0.3293 - val_loss: 1.9243 - val_accuracy: 0.3198\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 20s 375ms/step - loss: 2.0299 - accuracy: 0.3065 - val_loss: 1.8598 - val_accuracy: 0.2267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100\n",
      "52/52 [==============================] - 45s 871ms/step - loss: 2.0027 - accuracy: 0.3289 - val_loss: 1.8331 - val_accuracy: 0.4535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/100\n",
      "52/52 [==============================] - 22s 421ms/step - loss: 1.9849 - accuracy: 0.3262 - val_loss: 1.8321 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100\n",
      "52/52 [==============================] - 13s 253ms/step - loss: 1.9836 - accuracy: 0.3223 - val_loss: 1.8139 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/100\n",
      "52/52 [==============================] - 12s 229ms/step - loss: 1.9681 - accuracy: 0.3148 - val_loss: 1.8140 - val_accuracy: 0.2267\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 14s 261ms/step - loss: 1.9571 - accuracy: 0.3267 - val_loss: 1.8082 - val_accuracy: 0.2267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100\n",
      "52/52 [==============================] - 12s 233ms/step - loss: 1.9527 - accuracy: 0.3275 - val_loss: 1.7841 - val_accuracy: 0.4535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "52/52 [==============================] - 13s 243ms/step - loss: 1.9507 - accuracy: 0.3540 - val_loss: 1.7769 - val_accuracy: 0.4535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "52/52 [==============================] - 12s 225ms/step - loss: 1.9356 - accuracy: 0.3312 - val_loss: 1.7604 - val_accuracy: 0.4535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "52/52 [==============================] - 12s 224ms/step - loss: 1.9379 - accuracy: 0.3223 - val_loss: 1.7669 - val_accuracy: 0.2267\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 12s 237ms/step - loss: 1.9178 - accuracy: 0.3276 - val_loss: 1.7549 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100\n",
      "52/52 [==============================] - 12s 235ms/step - loss: 1.9141 - accuracy: 0.3208 - val_loss: 1.7431 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100\n",
      "52/52 [==============================] - 12s 240ms/step - loss: 1.9021 - accuracy: 0.3565 - val_loss: 1.7554 - val_accuracy: 0.3198\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 13s 243ms/step - loss: 1.8997 - accuracy: 0.3337 - val_loss: 1.7438 - val_accuracy: 0.2267\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 12s 234ms/step - loss: 1.8888 - accuracy: 0.3361 - val_loss: 1.7428 - val_accuracy: 0.2267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/100\n",
      "52/52 [==============================] - 12s 226ms/step - loss: 1.8846 - accuracy: 0.3419 - val_loss: 1.7257 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/100\n",
      "52/52 [==============================] - 12s 231ms/step - loss: 1.8774 - accuracy: 0.3236 - val_loss: 1.7325 - val_accuracy: 0.2267\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 12s 227ms/step - loss: 1.8735 - accuracy: 0.3308 - val_loss: 1.7246 - val_accuracy: 0.2267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/100\n",
      "52/52 [==============================] - 12s 237ms/step - loss: 1.8711 - accuracy: 0.3068 - val_loss: 1.7749 - val_accuracy: 0.3198\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 12s 240ms/step - loss: 1.8786 - accuracy: 0.3159 - val_loss: 1.7369 - val_accuracy: 0.2267\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 12s 235ms/step - loss: 1.8573 - accuracy: 0.3127 - val_loss: 1.6870 - val_accuracy: 0.4535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/100\n",
      "52/52 [==============================] - 12s 239ms/step - loss: 1.8492 - accuracy: 0.3612 - val_loss: 1.6907 - val_accuracy: 0.4535\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 12s 240ms/step - loss: 1.8508 - accuracy: 0.3642 - val_loss: 1.6775 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100\n",
      "52/52 [==============================] - 13s 241ms/step - loss: 1.8399 - accuracy: 0.3320 - val_loss: 1.6849 - val_accuracy: 0.2267\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 12s 228ms/step - loss: 1.8262 - accuracy: 0.3274 - val_loss: 1.6721 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/100\n",
      "52/52 [==============================] - 12s 236ms/step - loss: 1.8233 - accuracy: 0.3100 - val_loss: 1.6604 - val_accuracy: 0.4535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100\n",
      "52/52 [==============================] - 12s 238ms/step - loss: 1.8149 - accuracy: 0.3743 - val_loss: 1.7141 - val_accuracy: 0.3198\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 12s 226ms/step - loss: 1.8310 - accuracy: 0.3318 - val_loss: 1.6544 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100\n",
      "52/52 [==============================] - 12s 237ms/step - loss: 1.8040 - accuracy: 0.3183 - val_loss: 1.6478 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/100\n",
      "52/52 [==============================] - 12s 225ms/step - loss: 1.8018 - accuracy: 0.3372 - val_loss: 1.6458 - val_accuracy: 0.2267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "52/52 [==============================] - 12s 233ms/step - loss: 1.7924 - accuracy: 0.3185 - val_loss: 1.6277 - val_accuracy: 0.4535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/100\n",
      "52/52 [==============================] - 15s 290ms/step - loss: 1.7944 - accuracy: 0.3473 - val_loss: 1.6407 - val_accuracy: 0.2267\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 18s 354ms/step - loss: 1.7883 - accuracy: 0.3108 - val_loss: 1.6063 - val_accuracy: 0.4535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100\n",
      "52/52 [==============================] - 18s 343ms/step - loss: 1.7840 - accuracy: 0.3646 - val_loss: 1.6369 - val_accuracy: 0.2267\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 17s 336ms/step - loss: 1.7793 - accuracy: 0.3334 - val_loss: 1.6242 - val_accuracy: 0.3198\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 14s 274ms/step - loss: 1.7663 - accuracy: 0.3400 - val_loss: 1.6241 - val_accuracy: 0.2267\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 14s 273ms/step - loss: 1.7617 - accuracy: 0.3288 - val_loss: 1.6119 - val_accuracy: 0.2267\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 17s 332ms/step - loss: 1.7721 - accuracy: 0.3445 - val_loss: 1.6061 - val_accuracy: 0.2267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/100\n",
      "52/52 [==============================] - 18s 339ms/step - loss: 1.7516 - accuracy: 0.3271 - val_loss: 1.5873 - val_accuracy: 0.4535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/100\n",
      "52/52 [==============================] - 12s 223ms/step - loss: 1.7522 - accuracy: 0.3444 - val_loss: 1.5854 - val_accuracy: 0.4535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/100\n",
      "52/52 [==============================] - 19s 366ms/step - loss: 1.7450 - accuracy: 0.3689 - val_loss: 1.5885 - val_accuracy: 0.3198\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 18s 354ms/step - loss: 1.7371 - accuracy: 0.3252 - val_loss: 1.5917 - val_accuracy: 0.3198\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 19s 361ms/step - loss: 1.7339 - accuracy: 0.2994 - val_loss: 1.5911 - val_accuracy: 0.3198\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 15s 297ms/step - loss: 1.7349 - accuracy: 0.3473 - val_loss: 1.5812 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/100\n",
      "52/52 [==============================] - 17s 333ms/step - loss: 1.7252 - accuracy: 0.3299 - val_loss: 1.5807 - val_accuracy: 0.2267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/100\n",
      "52/52 [==============================] - 19s 369ms/step - loss: 1.7199 - accuracy: 0.3059 - val_loss: 1.5606 - val_accuracy: 0.4535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/100\n",
      "52/52 [==============================] - 14s 263ms/step - loss: 1.7170 - accuracy: 0.3653 - val_loss: 1.5980 - val_accuracy: 0.3198\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 18s 346ms/step - loss: 1.7164 - accuracy: 0.3496 - val_loss: 1.5530 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/100\n",
      "52/52 [==============================] - 23s 445ms/step - loss: 1.7077 - accuracy: 0.3245 - val_loss: 1.5502 - val_accuracy: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/100\n",
      "52/52 [==============================] - 20s 384ms/step - loss: 1.7001 - accuracy: 0.3220 - val_loss: 1.5470 - val_accuracy: 0.2267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /s2s_nobackup/zhengwu/workshop/Results/tmp/Run20/weight_cnn_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100\n",
      "52/52 [==============================] - 12s 237ms/step - loss: 1.7005 - accuracy: 0.3327 - val_loss: 1.5551 - val_accuracy: 0.2267\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.31\n",
      "Precision: 0.31\n",
      "F1-score: 0.31\n",
      "Accuracy: 0.31\n",
      "Brier score:0.21677347665469593\n",
      "Brier climat:0.21157053243574053\n",
      "Brier skill score:-0.024592007965644536\n",
      "Recall: 0.31\n",
      "Precision: 0.31\n",
      "F1-score: 0.31\n",
      "Accuracy: 0.31\n",
      "Brier score:0.2382071558606374\n",
      "Brier climat:0.22915391676866584\n",
      "Brier skill score:-0.03950724133208228\n",
      "Recall: 0.31\n",
      "Precision: 0.31\n",
      "F1-score: 0.31\n",
      "Accuracy: 0.31\n",
      "Brier score:0.21258491725121398\n",
      "Brier climat:0.210234088127295\n",
      "Brier skill score:-0.011181959808989506\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.23\n",
      "Precision: 0.23\n",
      "F1-score: 0.23\n",
      "Accuracy: 0.23\n",
      "Brier score:0.1898481315140249\n",
      "Brier climat:0.18510174418604652\n",
      "Brier skill score:-0.02564204539967907\n",
      "Recall: 0.23\n",
      "Precision: 0.23\n",
      "F1-score: 0.23\n",
      "Accuracy: 0.23\n",
      "Brier score:0.26381872226416936\n",
      "Brier climat:0.25363372093023256\n",
      "Brier skill score:-0.04015633763752735\n",
      "Recall: 0.23\n",
      "Precision: 0.23\n",
      "F1-score: 0.23\n",
      "Accuracy: 0.23\n",
      "Brier score:0.21753311312463675\n",
      "Brier climat:0.21427325581395348\n",
      "Brier skill score:-0.015213551958690008\n",
      "bss_va1 print [-0.02564204539967907]\n",
      "bss_va3 print [-0.04015633763752735]\n",
      "bss_va4 print [-0.015213551958690008]\n"
     ]
    }
   ],
   "source": [
    "# train the lstm\n",
    "#del model_history\n",
    "from random import sample as random_s\n",
    "NY_train = 30 # its not used\n",
    "recall_tr1, precision_tr1, f1_tr1, acc_tr1, bss_tr1, bs_tr1, calib_y_tr1, calib_x_tr1 = [],[],[],[],[],[],[],[]\n",
    "recall_tr2, precision_tr2, f1_tr2, acc_tr2, bss_tr2, bs_tr2, calib_y_tr2, calib_x_tr2 = [],[],[],[],[],[],[],[]\n",
    "recall_tr3, precision_tr3, f1_tr3, acc_tr3, bss_tr3, bs_tr3, calib_y_tr3, calib_x_tr3 = [],[],[],[],[],[],[],[]\n",
    "recall_tr4, precision_tr4, f1_tr4, acc_tr4, bss_tr4, bs_tr4, calib_y_tr4, calib_x_tr4 = [],[],[],[],[],[],[],[]\n",
    "\n",
    "recall_va1, precision_va1, f1_va1, acc_va1, bss_va1, bs_va1, calib_y_va1, calib_x_va1 = [],[],[],[],[],[],[],[]\n",
    "recall_va2, precision_va2, f1_va2, acc_va2, bss_va2, bs_va2, calib_y_va2, calib_x_va2 = [],[],[],[],[],[],[],[]\n",
    "recall_va3, precision_va3, f1_va3, acc_va3, bss_va3, bs_va3, calib_y_va3, calib_x_va3 = [],[],[],[],[],[],[],[]\n",
    "recall_va4, precision_va4, f1_va4, acc_va4, bss_va4, bs_va4, calib_y_va4, calib_x_va4 = [],[],[],[],[],[],[],[]\n",
    "\n",
    "loss_tr, loss_va = [],[]\n",
    "metri_tr, metri_va = [],[]\n",
    "\n",
    "import random\n",
    "NS = 1\n",
    "#select_ind = random.sample(list(np.arange(EYY-SYY+1-5)), NS) #it selects NS indices our of 35\n",
    "for ii in range(NS):\n",
    "    #model = build_lstm(ntimestep, nfeature, layers=2, neurons=[66,35], regval=[0.1,0.05], out_neurons=n_cat+1)\n",
    "    #train_X, train_y, val_X, val_y = cross_valid_one_out(X_all, y_all, all_years, val_year_ind=ii)\n",
    "    val_year = [SYY+ii,SYY+ii+1]\n",
    "    train_X, train_y, val_X, val_y = cross_valid_n_out(X_all_n, y_all_n, all_years, val_year=val_year)\n",
    "    \n",
    "    train_y_tmp = train_y.values\n",
    "    class_weight = class_weight_creator(train_y_tmp)\n",
    "    callbacks_path = root_results+'tmp/Run20/weight_cnn_'+str(ii+1)\n",
    "    history_path = root_results+'tmp/Run20/history_cnn_'+str(ii+1)\n",
    "    batch_size = 64\n",
    "    epochs = 100\n",
    "    model_history = train_model(model, train_X, train_y, val_X, val_y, callbacks_path, epochs, batch_size, class_weight)\n",
    "    loss_tr.append(model_history.history['loss'])\n",
    "    loss_va.append(model_history.history['val_loss'])\n",
    "    metri_tr.append(model_history.history['accuracy'])\n",
    "    metri_va.append(model_history.history['val_accuracy'])\n",
    "    \n",
    "    # train score\n",
    "    #ref_prob = clim_pr_y[0:int(train_y[:,1].shape[0])]\n",
    "    #print(ref_prob.shape)\n",
    "    y_prob = model.predict(train_X) #y_pred = np.argmax(y_prob,axis=1)\n",
    "    #print('y prediction shape', y_pred.shape)\n",
    "    nbins = 10\n",
    "    \n",
    "    recall, precision, f1, acc, bs, bss, calib_y, calib_x = score(np.argmax(train_y,axis=1), train_y[:,1], np.argmax(y_prob,axis=1), \n",
    "                                                    y_prob[:,1], nbins, clim_pr_y_cat1[0:int(train_y[:,1].shape[0])])\n",
    "    recall_tr1.append(recall); precision_tr1.append(precision); f1_tr1.append(f1)\n",
    "    acc_tr1.append(acc); bs_tr1.append(bs); bss_tr1.append(bss)\n",
    "    calib_y_tr1.append(calib_y); calib_x_tr1.append(calib_x)\n",
    "    \n",
    "    recall, precision, f1, acc, bs, bss, calib_y, calib_x = score(np.argmax(train_y,axis=1), train_y[:,2], np.argmax(y_prob,axis=1), y_prob[:,2], nbins, \n",
    "                                                                  clim_pr_y_cat3[0:int(train_y[:,2].shape[0])])\n",
    "    recall_tr3.append(recall); precision_tr3.append(precision); f1_tr3.append(f1)\n",
    "    acc_tr3.append(acc); bs_tr3.append(bs); bss_tr3.append(bss)\n",
    "    calib_y_tr3.append(calib_y); calib_x_tr3.append(calib_x)\n",
    "    \n",
    "    recall, precision, f1, acc, bs, bss, calib_y, calib_x = score(np.argmax(train_y,axis=1), train_y[:,3], np.argmax(y_prob,axis=1), y_prob[:,3], nbins, \n",
    "                                                                  clim_pr_y_cat4[0:int(train_y[:,3].shape[0])])\n",
    "    recall_tr4.append(recall); precision_tr4.append(precision); f1_tr4.append(f1)\n",
    "    acc_tr4.append(acc); bs_tr4.append(bs); bss_tr4.append(bss)\n",
    "    calib_y_tr4.append(calib_y); calib_x_tr4.append(calib_x)\n",
    "    \n",
    "    # validation score\n",
    "    y_prob = model.predict(val_X); #y_pred = np.argmax(y_prob,axis=1)\n",
    "    nbins = 10\n",
    "    \n",
    "    recall, precision, f1, acc, bs, bss, calib_y, calib_x = score(np.argmax(val_y,axis=1), val_y[:,1], np.argmax(y_prob,axis=1),\n",
    "                                                    y_prob[:,1], nbins, clim_pr_y_cat1[0:int(val_y[:,1].shape[0])])\n",
    "    recall_va1.append(recall); precision_va1.append(precision); f1_va1.append(f1)\n",
    "    acc_va1.append(acc); bs_va1.append(bs); bss_va1.append(bss)\n",
    "    calib_y_va1.append(calib_y); calib_x_va1.append(calib_x)\n",
    "    \n",
    "    recall, precision, f1, acc, bs, bss, calib_y, calib_x = score(np.argmax(val_y,axis=1), val_y[:,2], np.argmax(y_prob,axis=1),\n",
    "                                                    y_prob[:,2], nbins, clim_pr_y_cat3[0:int(val_y[:,2].shape[0])])\n",
    "    recall_va3.append(recall); precision_va3.append(precision); f1_va3.append(f1)\n",
    "    acc_va3.append(acc); bs_va3.append(bs); bss_va3.append(bss)\n",
    "    calib_y_va3.append(calib_y); calib_x_va3.append(calib_x)\n",
    "    \n",
    "    recall, precision, f1, acc, bs, bss, calib_y, calib_x = score(np.argmax(val_y,axis=1), val_y[:,3], np.argmax(y_prob,axis=1),\n",
    "                                                    y_prob[:,3], nbins, clim_pr_y_cat4[0:int(val_y[:,3].shape[0])])\n",
    "    recall_va4.append(recall); precision_va4.append(precision); f1_va4.append(f1)\n",
    "    acc_va4.append(acc); bs_va4.append(bs); bss_va4.append(bss)\n",
    "    calib_y_va4.append(calib_y); calib_x_va4.append(calib_x)\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    print('bss_va1 print',bss_va1)\n",
    "    print('bss_va3 print',bss_va3)\n",
    "    print('bss_va4 print',bss_va4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fddfc13cb90>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We can also look at the optimisation history\n",
    "train_loss = np.array(loss_tr)\n",
    "valid_loss = np.array(loss_va)\n",
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=plt.figaspect(0.25))\n",
    "ax1.plot(np.nanmean(train_loss,axis=0), label='Training Loss')\n",
    "ax1.plot(np.nanmean(valid_loss,axis=0), label='Validation Loss')\n",
    "ax1.set_title('loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(np.nanmean(metri_tr,axis=0), label='Training ACC')\n",
    "ax2.plot(np.nanmean(metri_va,axis=0), label='Validation ACC')\n",
    "ax2.set_title('ACC')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('acc')\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing a 3-class dataset from sklearn's toy dataset\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "dataset = load_wine()\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "svc = SVC(kernel='rbf', C=1).fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "print(X_train.shape,y_train.shape,y_pred.shape)\n",
    "\n",
    "#importing confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "#importing accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_test, y_pred, target_names=['Class 1', 'Class 2', 'Class 3']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:conda-ai_2022] *",
   "language": "python",
   "name": "conda-env-conda-ai_2022-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
