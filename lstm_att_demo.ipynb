{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from netCDF4 import Dataset as ncread\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams #For changing text properties\n",
    "import cmocean #A package with beautiful colormaps\n",
    "from cartopy import crs as ccrs #Useful for plotting maps\n",
    "import cartopy.util #Requires separate import\n",
    "from cartopy.util import add_cyclic_point\n",
    "import cartopy.feature as cf\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
    "import matplotlib.path as mpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import regularizers\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Reshape, Flatten, LSTM, Dense, Dropout, Embedding, Bidirectional, GRU\n",
    "from keras.optimizers import Adam\n",
    "from keras import initializers, regularizers\n",
    "from keras import optimizers\n",
    "from keras.engine.topology import Layer\n",
    "from keras import constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM with attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://github.com/gentaiscool/lstm-attention/blob/58adc7e345b5b3a79638483049704802a66aa1f4/layers.py#L50 def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    def dot_product(x, kernel):\n",
    "        if K.backend() == 'tensorflow':\n",
    "            return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "        else:\n",
    "            return K.dot(x, kernel)\n",
    "        \n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    follows these equations:\n",
    "    \n",
    "    (1) u_t = tanh(W h_t + b)\n",
    "    (2) \\alpha_t = \\frac{exp(u^T u)}{\\sum_t(exp(u_t^T u))}, this is the attention weight\n",
    "    (3) v_t = \\alpha_t * h_t, v in time t\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                initializer=self.init,\n",
    "                                name='{}_W'.format(self.name),\n",
    "                                regularizer=self.W_regularizer,\n",
    "                                constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                    initializer='zero',\n",
    "                                    name='{}_b'.format(self.name),\n",
    "                                    regularizer=self.b_regularizer,\n",
    "                                    constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                initializer=self.init,\n",
    "                                name='{}_u'.format(self.name),\n",
    "                                regularizer=self.u_regularizer,\n",
    "                                constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero and this results in NaN's. \n",
    "        # Should add a small epsilon as the workaround\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        \n",
    "        return weighted_input\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[1], input_shape[2]\n",
    "    \n",
    "class Addition(Layer):\n",
    "    \"\"\"\n",
    "    This layer is supposed to add of all activation weight.\n",
    "    We split this from AttentionWithContext to help us getting the activation weights\n",
    "    follows this equation:\n",
    "    (1) v = \\sum_t(\\alpha_t * h_t)\n",
    "    \n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Addition, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.output_dim = input_shape[-1]\n",
    "        super(Addition, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        return K.sum(x, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 40, 2000)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 40, 10)            80440     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 40, 10)            840       \n",
      "_________________________________________________________________\n",
      "attention_with_context_2 (At (None, 40, 10)            120       \n",
      "_________________________________________________________________\n",
      "addition_2 (Addition)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 81,532\n",
      "Trainable params: 81,532\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# LSTM with attention layer\n",
    "mlen = 40\n",
    "mdim = 2000\n",
    "input_tensor = Input(shape=(mlen,mdim))\n",
    "layer1 = layers.LSTM(10, return_sequences=True, kernel_regularizer=regularizers.l2(25))(input_tensor)\n",
    "layer1 = layers.LSTM(10, return_sequences=True, kernel_regularizer=regularizers.l2(0.01))(layer1)\n",
    "layer1 = AttentionWithContext()(layer1)\n",
    "layer1 = Addition()(layer1)\n",
    "layer1 = layers.Dense(10, activation=\"relu\")(layer1)\n",
    "output_tensor = layers.Dense(2,activation='softmax')(layer1)\n",
    "\n",
    "callbacks_path = '/net/cfc/s2s/zhengwu/code/tmp/checkpoint_test'\n",
    "callbacks_list = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=callbacks_path,\n",
    "        monitor='val_acc',\n",
    "        save_best_only=True,\n",
    "    )\n",
    "]\n",
    "model = Model(input_tensor, output_tensor)\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, callbacks=callbacks_list, validation_data=(X_validation, Y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output attention weights \n",
    "layer_weights = model.layers[3].get_weights()\n",
    "\n",
    "# extract layer output before the attention layer\n",
    "new_model = Model(inputs=model.input, outputs=model.layers[2].output)\n",
    "output_before_att = new_model.predict(X_validation)\n",
    "print(type(output_before_att),np.shape(output_before_att))\n",
    "\n",
    "# weights from Yang et al. (2016) HAN\n",
    "uit = np.tanh(np.dot(output_before_att, layer_weights[0]) + layer_weights[1]) \n",
    "eij = np.dot(uit, layer_weights[2])\n",
    "print(uit.shape,eij.shape)\n",
    "eij = eij.reshape((eij.shape[0], eij.shape[1]))\n",
    "print(eij.shape)\n",
    "ai = np.exp(eij)\n",
    "weights = np.ndarray((len(ai),mlen))+np.nan\n",
    "for ii in range(len(ai)):\n",
    "    weights[ii,:] = ai[ii,:] / np.sum(ai[ii,:])\n",
    "print(ai.shape,weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_norm = weights/np.nanmax(weights, axis=(-1))[:,np.newaxis]\n",
    "fig, axs = plt.subplots(1, figsize=plt.figaspect(0.15))\n",
    "for ii in range(len(weights)):\n",
    "    plt.plot(weights[ii,:]*30)\n",
    "\n",
    "fig, axs = plt.subplots(1, figsize=plt.figaspect(0.15))\n",
    "plt.plot(np.mean(weights,axis=0))\n",
    "\n",
    "fig, axs = plt.subplots(1, figsize=plt.figaspect(0.15))\n",
    "plt.plot(np.mean(weights,axis=0)/np.nanmax(np.mean(weights,axis=0)),'k',linewidth=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
