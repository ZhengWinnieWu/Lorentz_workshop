{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from netCDF4 import Dataset as ncread\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import math\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "from cftime import DatetimeNoLeap\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import get_principle_components_and_EOFs\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams #For changing text properties\n",
    "import cmocean #A package with beautiful colormaps\n",
    "import matplotlib.path as mpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime \n",
    "from datetime import timedelta\n",
    "from datetime import date\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in true_divide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf    \n",
    "#tf.compat.v1.disable_v2_behavior() # <-- HERE !\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dropout, Activation, Reshape, Flatten, LSTM, Dense, Dropout, Embedding, Bidirectional, GRU\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import initializers, regularizers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import constraints\n",
    "from tensorflow.keras.layers import Layer, InputSpec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import regional mask\n",
    "root_data = '/data/volume_2/observational/raw/'\n",
    "\n",
    "import pathlib\n",
    "root_results = str(pathlib.Path.home() / 'Results')\n",
    "\n",
    "file = '/data/volume_2/observational/era5_hoa_dry_mask_2deg.nc' #0.25\n",
    "\n",
    "mask=xr.open_mfdataset(file,combine='by_coords',parallel=True)\n",
    "mask_nan=mask.where(mask==1) #keep the values==1 and mask the rest\n",
    "mask_nan.tp.plot()\n",
    "mask_nan.sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare target \n",
    "#### define drought events when 31-day running mean rainfall in OND from 1980-2021 is below the 33rd percentile, the tercile is calculated based on the 2000-2020 period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the spatial mean of the tp file after applying the spatial mask\n",
    "\n",
    "file=xr.open_mfdataset(root_data+f'/era5_tp_1959-2021_1_12_daily_2.0deg.nc',\n",
    "                          combine='by_coords',parallel=True)\n",
    "tp_dim=file.sel(longitude=slice(10,70),latitude=slice(24,-30))\n",
    "tp_series=np.multiply(mask_nan,tp_dim).mean(dim='latitude',skipna=True).mean(dim='longitude',skipna=True)\n",
    "\n",
    "# Calculate 33 percentile\n",
    "# Create daily values equal to a 31 day rolling. Select OND 2000-2020 data to decide the quantile threshold\n",
    "tp_rol=tp_series.rolling(time=28, center=False).mean().sel(time=tp_series.time.dt.year.isin([np.arange(2000,2021)]))\n",
    "tp_quantile=tp_rol.sel(time=tp_rol.time.dt.month.isin([10,11,12])).quantile(0.33)\n",
    "print('value of the 33 percentile',tp_quantile.tp.values)\n",
    "\n",
    "# Create index time series\n",
    "# Replace the values bellow the 33 percentile with 1 and the rest with zeros\n",
    "SYY = 1980   # start year, could be changed\n",
    "EYY = 2021   # end year, could be changed\n",
    "tp_rol = tp_series.rolling(time=28, center=False).mean().sel(time=tp_series.time.dt.month.isin([10,11,12]))\n",
    "tp_rol_sel = tp_rol.sel(time = slice(str(SYY),str(EYY)))\n",
    "tp_index = tp_rol_sel < tp_quantile\n",
    "tp_index = tp_index.astype(int)\n",
    "print('tp index',tp_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select from the index time series the period for the target values (predictant) \n",
    "# Oct 16 to Dec 16 for the period 1980-2020 (each day corresponds to a 31-day rolling mean)\n",
    "# The chosen time period could be changed to any time period you want\n",
    "for iyr in range(SYY,EYY+1):\n",
    "    if iyr == SYY:\n",
    "        tp_target = tp_index.sel(time = slice(str(iyr)+'-10-01',str(iyr)+'-12-01'))\n",
    "    else:\n",
    "        tp_target = xr.concat([tp_target,tp_index.sel(time = slice(str(iyr)+'-10-01',str(iyr)+'-12-01'))], dim='time')\n",
    "print('number of 0 and 1: ',np.unique(tp_target['tp'],return_counts=True))\n",
    "print(tp_target)\n",
    "plt.plot(tp_target.tp)\n",
    "\n",
    "# Make it into a numpy array\n",
    "target = tp_target['tp'].values\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare predictors: 30 days time series with the last day two weeks before the target day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor data preprocessing\n",
    "# can select the values and region you want by changing the parameters\n",
    "\n",
    "file_vars = ['ERA5_t2m', 'era5_t_850hpa', 'era5_z_200hpa', 'era5_z_500hpa', 'sst', 'era5_olr']\n",
    "#file_vars = ['sst']\n",
    "header_vars = ['t2m', 't', 'z', 'z', 'sst', 'olr-mean']\n",
    "#header_vars = ['sst']\n",
    "\n",
    "# select regions for the individual predictor\n",
    "lon_slices = [[-16,54],[-30,90],[-30,90],[-30,90],[-180,180],[40,180]]\n",
    "lat_slices = [[16,0],[30,-20],[-20,30],[-20,30],[40,-20],[-20,20]]\n",
    "\n",
    "nmode = 5 # for eofs\n",
    "\n",
    "for file_var, header_var, lon_slice, lat_slice in zip (file_vars, header_vars, lon_slices, lat_slices):\n",
    "    # use existing\n",
    "    path = root_results+'PC_series_n_'+str(nmode)+'_var_'+file_var+'_.nc'\n",
    "    if os.path.exists(path):\n",
    "        continue\n",
    "    if file_var == 'era5_olr':\n",
    "        file = xr.open_dataset(root_data+file_var+'_1950_2021_daily_1deg_tropics.nc')\n",
    "        print('olr')\n",
    "    else:\n",
    "        file = xr.open_dataset(root_data+file_var+'_1959-2021_1_12_daily_2.0deg.nc')\n",
    "        print(header_var)\n",
    "\n",
    "    if \"longitude\" in file.coords:\n",
    "        file = file.rename({\"longitude\": \"lon\",\"latitude\": \"lat\"})\n",
    "\n",
    "    assert \"lat\" in file.coords\n",
    "    assert \"lon\" in file.coords\n",
    "    \n",
    "    # select region\n",
    "    var_dim = file.sel(lon=slice(lon_slice[0],lon_slice[1]),lat=slice(lat_slice[0],lat_slice[1]))\n",
    "    \n",
    "    # take years 1980 - 2021 daily and only and 7 day rolling mean\n",
    "    var_series = var_dim.sel(time=var_dim.time.dt.year.isin([np.arange(SYY,EYY+1)])).rolling(time=7, center=False).mean(skipna=True)\n",
    "    \n",
    "    # remove climatology\n",
    "    var_anom_series = var_series.groupby(\"time.dayofyear\") - var_series.groupby(\"time.dayofyear\").mean(\"time\",skipna=True)\n",
    "    \n",
    "    # use the months you want (base on how long the time series used as predictors)\n",
    "    var_anom_sel = var_anom_series.sel(time=var_anom_series.time.dt.month.isin([7,8,9,10,11]))[header_var]\n",
    "    \n",
    "    # Apply EOF\n",
    "    pc_xr, EOF = get_principle_components_and_EOFs(var_anom_sel, nmode=nmode)\n",
    "    # save to disk\n",
    "    pc_xr.to_netcdf(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick viz\n",
    "# pc_xr.plot(hue=\"mode\", figsize=(20,3))\n",
    "# var_anom_series[header_var].isel(time=122).plot(size=5, aspect=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exec(open('/home/mpyrina/Notebooks/Lorentz_workshop/L_functions.py').read())\n",
    "\n",
    "from L_functions import sel_train_data_lead\n",
    "\n",
    "#Create predictor multi-file\n",
    "nc_in_file='PC_serie*.nc'\n",
    "dim_to_stack='mode'\n",
    "pc_xr = xr.open_mfdataset(root_results+nc_in_file,concat_dim=dim_to_stack,\n",
    "                          combine=\"nested\")# \n",
    "\n",
    "# Run the function\n",
    "s_target_date='01-10-1980'\n",
    "e_target_date='01-12-2021'\n",
    "rw_1=7\n",
    "lead_time=15\n",
    "rw=0 # because the data are not centered\n",
    "ntimestep=60\n",
    "target_len=len(tp_target['tp'])\n",
    "\n",
    "predictor_array=sel_train_data_lead(pc_xr, target_len, s_target_date, e_target_date,\n",
    "                rw_1, lead_time, rw, ntimestep)\n",
    "\n",
    "np_out_name='Predictor_array_crosscor_number.nc'\n",
    "\n",
    "#np.save(root_results+np_out_name,predictor_array)\n",
    "predictor_array.to_netcdf(root_results+np_out_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot EOF modes\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cf\n",
    "ii = 1\n",
    "color = 'RdBu_r' \n",
    "colorbarMin = -1\n",
    "colorbarMax = 1\n",
    "colorspace = 0.1\n",
    "level = np.arange(colorbarMin,colorbarMax+colorspace,colorspace)\n",
    "ax = plt.axes(projection=ccrs.cartopy.crs.PlateCarree(central_longitude=180))\n",
    "h = ax.contourf(lon, lat, EOF[ii,:,:], level, transform=ccrs.PlateCarree(), cmap=color,extend='both')\n",
    "cbar = plt.colorbar(h, orientation='horizontal', shrink=1,fraction=0.1,pad=0.1,aspect=40)\n",
    "cbar.ax.tick_params(labelsize=10) \n",
    "colorLabel='SST EOF [K]'\n",
    "cbar.set_label(label=colorLabel,fontsize=10)\n",
    "#Add in the coordinate system:\n",
    "long = np.arange(-180, 180, 45)#spacing of 45 degrees\n",
    "latg = np.arange(-20, 40, 10)#spacing of 15 degrees\n",
    "ax.set_xticks(long, crs=ccrs.PlateCarree());\n",
    "ax.set_yticks(latg, crs=ccrs.PlateCarree());\n",
    "ax.set_xticklabels(long,fontsize=8)\n",
    "ax.set_yticklabels(latg,fontsize=8)\n",
    "ax.set_ylabel('lat',fontsize=10);\n",
    "ax.set_xlabel('lon',fontsize=10);\n",
    "\n",
    "#Add in the continents\n",
    "#define the coastlines, the color (#000000) and the resolution (110m) \n",
    "feature1 = cf.NaturalEarthFeature(\n",
    "    name='coastline', category='physical',\n",
    "    scale='110m',\n",
    "    edgecolor='#000000', facecolor='none')\n",
    "#define the land, the color (#AAAAAA) and the resolution (110m), mask the land, use for SST\n",
    "feature2 = cf.NaturalEarthFeature(\n",
    "    name='land', category='physical',\n",
    "    scale='110m',\n",
    "    facecolor='#AAAAAA')\n",
    "\n",
    "ax.add_feature(feature2)\n",
    "\n",
    "#Set a title for your map:\n",
    "title = 'SST JAS EOF'+str(ii+1)\n",
    "plt.title(title,fontsize=10, y=1.03)\n",
    "\n",
    "fig, axs = plt.subplots(1, figsize=plt.figaspect(0.15))\n",
    "plt.plot(PC[:,ii])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_val(data_predictor, data_target, test_frac, val_frac):\n",
    "    \"\"\"Splits data across periods into train, test, and validation\"\"\"\n",
    "    # assign the last int(-test_frac*len(tp_predictor)) rows to test data\n",
    "    test_predictor = data_predictor[int(-test_frac*len(data_target)):]\n",
    "    test_target = data_target[int(-test_frac*len(data_target)):]\n",
    "    \n",
    "    # assign the last int(-test_frac*len(tp_predictor)) from the remaining rows to validation data\n",
    "    remain_predictor = data_predictor[0:int(-test_frac*len(data_target))]\n",
    "    remain_target = data_target[0:int(-test_frac*len(data_target))]\n",
    "    val_predictor = remain_predictor[int(-val_frac*len(remain_predictor)):]\n",
    "    val_target = remain_target[int(-val_frac*len(remain_predictor)):]\n",
    "    \n",
    "    # the remaining rows are assigned to train data\n",
    "    train_predictor = remain_predictor[:int(-val_frac*len(remain_predictor))]\n",
    "    train_target = remain_target[:int(-val_frac*len(remain_predictor))]\n",
    "    return train_predictor, train_target, test_predictor, test_target, val_predictor, val_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input and output data for LSTM\n",
    "y_all = keras.utils.to_categorical(target)\n",
    "X_all = pc_predictor\n",
    "print(X_all.shape,y_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y, test_X, test_y, val_X, val_y = get_train_test_val(X_all, y_all, test_frac=0.2, val_frac=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = [0, 1, 2]\n",
    "names = [\"train\", \"val\", \"test\"]\n",
    "width = 0.75\n",
    "event_cnts = [np.unique(train_y[:,1],return_counts=True)[1][1],np.unique(val_y[:,1],return_counts=True)[1][1],np.unique(test_y[:,1],return_counts=True)[1][1]]\n",
    "nonevent_cnts = [np.unique(train_y[:,1],return_counts=True)[1][0],np.unique(val_y[:,1],return_counts=True)[1][0],np.unique(test_y[:,1],return_counts=True)[1][0]]\n",
    "\n",
    "p1 = plt.barh(ind, event_cnts, width)\n",
    "p2 = plt.barh(ind, nonevent_cnts, width, left=event_cnts)\n",
    "\n",
    "plt.yticks(ind, names)\n",
    "plt.ylabel(\"data set\")\n",
    "plt.xlabel(\"samples\")\n",
    "plt.title(\"Train/Validation/Test Splits\", fontsize=16)\n",
    "plt.legend([\"Event\", \"Non-event\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM with attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/gentaiscool/lstm-attention/blob/58adc7e345b5b3a79638483049704802a66aa1f4/layers.py#L50\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    follows these equations:\n",
    "    \n",
    "    (1) u_t = tanh(W h_t + b)\n",
    "    (2) \\alpha_t = \\frac{exp(u^T u)}{\\sum_t(exp(u_t^T u))}, this is the attention weight\n",
    "    (3) v_t = \\alpha_t * h_t, v in time t\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "                'W_regularizer': self.W_regularizer,\n",
    "                'u_regularizer': self.u_regularizer,\n",
    "                'b_regularizer': self.b_regularizer,\n",
    "                'W_constraint': self.W_constraint,\n",
    "                'u_constraint': self.u_constraint,\n",
    "                'b_constraint': self.b_constraint,\n",
    "                'bias': self.bias,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1],),\n",
    "                                initializer=self.init,\n",
    "                                name='{}_W'.format(self.name),\n",
    "                                regularizer=self.W_regularizer,\n",
    "                                constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[-1],),\n",
    "                                    initializer='zero',\n",
    "                                    name='{}_b'.format(self.name),\n",
    "                                    regularizer=self.b_regularizer,\n",
    "                                    constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight(shape=(input_shape[-1],),\n",
    "                                initializer=self.init,\n",
    "                                name='{}_u'.format(self.name),\n",
    "                                regularizer=self.u_regularizer,\n",
    "                                constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero and this results in NaN's. \n",
    "        # Should add a small epsilon as the workaround\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        \n",
    "        return weighted_input, a\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[1], input_shape[2]\n",
    "    \n",
    "class Addition(Layer):\n",
    "    \"\"\"\n",
    "    This layer is supposed to add of all activation weight.\n",
    "    We split this from AttentionWithContext to help us getting the activation weights\n",
    "    follows this equation:\n",
    "    (1) v = \\sum_t(\\alpha_t * h_t)\n",
    "    \n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Addition, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.output_dim = input_shape[-1]\n",
    "        super(Addition, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        return K.sum(x, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a class weight dictionary to help if the classes are unbalanced\n",
    "def class_weight_creator(Y):\n",
    "    class_dict = {}\n",
    "    weights = np.max(np.sum(Y, axis=0)) / np.sum(Y, axis=0)\n",
    "    for i in range( Y.shape[-1] ):\n",
    "        class_dict[i] = weights[i]\n",
    "        \n",
    "    return class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = class_weight_creator(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 30\n",
    "shuffle = True \n",
    "verbose = 2 #Set whether the model will output information when trained (0 = no output; 2 = output accuracy every epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_path = '/home/zwu/Lorentz_workshop/test/checkpoint_test'\n",
    "callbacks_list = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=callbacks_path,\n",
    "        monitor='val_acc',   # tf.keras.metrics.AUC(from_logits=True)\n",
    "        save_best_only=True,\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM with attention layer\n",
    "ntimestep = 60    # number of time step used in the predictors\n",
    "nfeature = 30   # number of features\n",
    "input_tensor = Input(shape=(ntimestep,nfeature))\n",
    "layer1 = layers.LSTM(100, return_sequences=True, kernel_regularizer=regularizers.l2(1))(input_tensor)\n",
    "layer1 = layers.LSTM(20, return_sequences=True, kernel_regularizer=regularizers.l2(0.01))(layer1)\n",
    "layer1, alfa = AttentionWithContext()(layer1)\n",
    "layer1 = Addition()(layer1)\n",
    "layer1 = layers.Dense(5, activation=\"relu\")(layer1)\n",
    "output_tensor = layers.Dense(2,activation='softmax')(layer1)\n",
    "\n",
    "model = Model(input_tensor, output_tensor)\n",
    "opt = optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "history = model.fit(train_X, train_y, epochs=epochs, batch_size=batch_size, validation_data=(val_X, val_y), shuffle = shuffle, verbose=verbose, class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curve\n",
    "train_acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=plt.figaspect(0.25))\n",
    "ax1.plot(train_acc, label='Training Accuracy')\n",
    "ax1.plot(val_acc, label='Validation Accuracy')\n",
    "ax1.set_title('Accuracy')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_ylim(0,1.1)\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(train_loss, label='Training loss')\n",
    "ax2.plot(val_loss, label='Validation loss')\n",
    "ax2.set_title('Loss')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curve\n",
    "train_acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=plt.figaspect(0.25))\n",
    "ax1.plot(train_acc, label='Training Accuracy')\n",
    "ax1.plot(val_acc, label='Validation Accuracy')\n",
    "ax1.set_title('Accuracy')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_ylim(0,1.1)\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(train_loss, label='Training loss')\n",
    "ax2.plot(val_loss, label='Validation loss')\n",
    "ax2.set_title('Loss')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.calibration import calibration_curve\n",
    "test_predict = model.predict(test_X)\n",
    "y_pred = np.argmax(model.predict(test_X),axis=1)\n",
    "print('Recall: '+str(round(recall_score(test_y[:,1],y_pred),2)))\n",
    "print('Precision: '+str(round(precision_score(test_y[:,1],y_pred),2)))\n",
    "print('F1-score: '+str(round(f1_score(test_y[:,1],y_pred),2)))\n",
    "print('Accuracy: '+str(round(accuracy_score(test_y[:,1],y_pred),2)))\n",
    "print('Brier score:' +str(brier_score_loss(test_y[0:-20,1], test_predict[0:-20,1])))\n",
    "\n",
    "calib_y, calib_x = calibration_curve(test_y[:,1],test_predict[:,1],n_bins=10)\n",
    "plt.plot(calib_y, calib_x, marker='o', color=\"darkorange\", label='LSTMatt')\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", linestyle=\"--\", label='Best score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "yprob = model.predict(test_X)\n",
    "yprob = yprob[0:-20,1]\n",
    "testy = test_y[0:-20,1]\n",
    "lr_auc = roc_auc_score(testy, yprob)\n",
    "print(lr_auc)\n",
    "lr_fpr, lr_tpr, thredhs = roc_curve(testy, yprob)\n",
    "print(testy.shape,lr_fpr.shape, lr_tpr.shape,thredhs.shape)\n",
    "# plot the roc curve for the model\n",
    "plt.plot(lr_fpr, lr_tpr, marker='.', color=\"darkorange\", label='LSTMatt')\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", linestyle=\"--\", label='No Skill')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()\n",
    "\n",
    "Y_predict = model.predict(test_X)\n",
    "yhat = np.argmax(Y_predict,axis=1)\n",
    "yhat = yhat[0:-20]\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(testy, yprob)\n",
    "lr_f1, lr_auc = f1_score(testy, yhat), auc(lr_recall, lr_precision)\n",
    "print(testy.shape,lr_recall.shape, lr_precision.shape)\n",
    "# summarize scores\n",
    "print('f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n",
    "# plot the precision-recall curves\n",
    "no_skill = len(testy[testy==1]) / len(testy)\n",
    "plt.plot(lr_recall, lr_precision, marker='.', color=\"darkorange\", label='LSTMatt')\n",
    "plt.plot([0, 1], [no_skill, no_skill], color=\"navy\", linestyle='--', label='No Skill')\n",
    "# axis labels\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get weightings of each time step and each sample\n",
    "intermediate_layer_model2 = Model(inputs=model.input,\n",
    "                                 outputs=model.layers[3].output)\n",
    "\n",
    "intermediate_layer_model1 = Model(inputs=model.input,\n",
    "                                 outputs=model.layers[2].output)\n",
    "\n",
    "intermediate_layer_model3 = Model(inputs=model.input,\n",
    "                                 outputs=model.layers[4].output)\n",
    "\n",
    "intermediate_output2, alfa_output = intermediate_layer_model2.predict(test_X, verbose=0)\n",
    "intermediate_output1 = intermediate_layer_model1.predict(test_X, verbose=0)\n",
    "intermediate_output3 = intermediate_layer_model3.predict(test_X, verbose=0)\n",
    "\n",
    "weights = intermediate_output2 / intermediate_output1\n",
    "print(np.shape(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the weights\n",
    "val_weights = np.ndarray((len(test_X),ntimestep))+np.nan\n",
    "for ii in range(len(test_X)):\n",
    "    for j in range(ntimestep):\n",
    "        val_weights[ii,j] = weights[ii][j][0]\n",
    "print(np.shape(val_weights))\n",
    "\n",
    "fig, axs = plt.subplots(1, figsize=plt.figaspect(0.15))\n",
    "for ii in range(len(test_X)):\n",
    "    plt.plot(val_weights[ii,:])\n",
    "\n",
    "fig, axs = plt.subplots(1, figsize=plt.figaspect(0.15))\n",
    "plt.plot(np.nanmean(val_weights,axis=0),'k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SST predictor\n",
    "root_data = '/s2s_nobackup/zhengwu/workshop/'\n",
    "file = xr.open_mfdataset(root_data+f'/era5_sst_1959-2021_1_12_daily_2.0deg.nc',\n",
    "                          combine='by_coords',parallel=True)\n",
    "#sst_dim = file.sel(longitude=slice(50,180),latitude=slice(40,-20))\n",
    "sst_dim = file.sel(latitude=slice(40,-20))\n",
    "lon = sst_dim.coords['longitude'].values\n",
    "lat = sst_dim.coords['latitude'].values\n",
    "nlon = len(lon)\n",
    "nlat = len(lat)\n",
    "\n",
    "sst_series = sst_dim.sel(time=sst_dim.time.dt.year.isin([np.arange(SYY,EYY+1)])).rolling(time=7, center=False).mean(skipna=True)\n",
    "print(sst_series)\n",
    "\n",
    "sst_anom_series = sst_series.groupby(\"time.dayofyear\") - sst_series.groupby(\"time.dayofyear\").mean(\"time\",skipna=True)\n",
    "\n",
    "# EOF focus on July to Sep\n",
    "sst_anom_sel = sst_anom_series.sel(time=sst_anom_series.time.dt.month.isin([7,8,9,10,11]))['sst'].values\n",
    "# first, make a grid out of the longitude and latitude vectors so they have the same dimensions \n",
    "lon2d,lat2d = np.meshgrid(lon,lat)\n",
    "wgts = np.cos(lat2d/180*np.pi)**0.5\n",
    "print(wgts.shape)\n",
    "solver = Eof(sst_anom_sel,weights=wgts)\n",
    "\n",
    "# EOFs are multiplied by the square-root of their eigenvalues (then the EOF patterns will carry the units)\n",
    "nmode = 4\n",
    "EOF = solver.eofs(neofs=nmode,eofscaling=2) # get the first four eofs\n",
    "print(np.shape(EOF))\n",
    "eigenv = solver.eigenvalues(neigs=nmode)\n",
    "print(eigenv)\n",
    "VarEx = solver.varianceFraction(neigs=nmode)*100\n",
    "print(sum(VarEx))\n",
    "PC = solver.pcs(npcs=nmode,pcscaling=1)\n",
    "print(np.shape(PC))\n",
    "\n",
    "mode = np.arange(nmode)\n",
    "sst_anom_dim = sst_anom_series.sel(time=sst_anom_series.time.dt.month.isin([7,8,9,10,11]))\n",
    "time_dim = sst_anom_dim.coords['time']\n",
    "print(sst_anom_dim.coords['time'])\n",
    "pc_xr = xr.DataArray(PC, coords={'time': time_dim, 'mode': mode}, dims=[\"time\",\"mode\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (iacpy3_2022)",
   "language": "python",
   "name": "iacpy3_2022"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
