{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7c21248-6d56-4b4e-b370-70494eedef04",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from netCDF4 import Dataset as ncread\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import math\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "from cftime import DatetimeNoLeap\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa455640-4a33-4614-810a-04475de1081e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (models.py, line 231)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py:3398\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  Input \u001b[0;32mIn [9]\u001b[0;36m in \u001b[0;35m<cell line: 4>\u001b[0;36m\u001b[0m\n\u001b[0;31m    import models\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/Lorentz_workshop/models.py:231\u001b[0;36m\u001b[0m\n\u001b[0;31m    def create_multi_Inp(data: Dict)\u001b[0m\n\u001b[0m                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from preprocess import get_principle_components_and_EOFs\n",
    "import os\n",
    "import visualization\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e562f05-6350-4434-827e-99129199533d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28c32e97-1e82-483d-9e9a-7c86102db157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams #For changing text properties\n",
    "import cmocean #A package with beautiful colormaps\n",
    "import matplotlib.path as mpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a393346-9a75-4f68-b5ec-ce4e31820b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime \n",
    "from datetime import timedelta\n",
    "from datetime import date\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25045afd-a276-49ca-b10f-0e9d6eec30f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in true_divide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b57c18c-c6dc-4a2a-a8be-8627c7868bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d015a488-7308-4d94-a9fe-b264e053d1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-08 18:19:37.769195: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-08 18:19:37.769230: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf    \n",
    "#tf.compat.v1.disable_v2_behavior() # <-- HERE !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8acdcdf-9d1e-4ef3-a8bc-6d70a1c774c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Frozen(SortedKeysDict({'latitude': 28, 'longitude': 31}))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import regional mask\n",
    "root_data = '/data/volume_2/observational/raw/'\n",
    "\n",
    "import pathlib\n",
    "root_results = str(pathlib.Path.home() / 'Results')\n",
    "\n",
    "file = '/data/volume_2/observational/era5_hoa_dry_mask_2deg.nc' #0.25\n",
    "\n",
    "mask=xr.open_mfdataset(file,combine='by_coords',parallel=True)\n",
    "mask_nan=mask.where(mask==1) #keep the values==1 and mask the rest\n",
    "mask_nan.tp.plot()\n",
    "mask_nan.sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea326d2-1b6d-4ce1-aabf-1fbcc115efa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYY = 1980   # start year, could be changed\n",
    "EYY = 2021   # end year, could be changed\n",
    "\n",
    "\n",
    "drop_OND_years = [2005,2007,2018,2004,2006]\n",
    "take_OND_years = list(np.arange(SYY,EYY+1))\n",
    "take_OND_years = [y for y in take_OND_years if y not in drop_OND_years]\n",
    "# drop_MAM_years = [2009,2001,2002,2005,2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb5dc0f-6f59-4371-8206-ddf787e8d0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the spatial mean of the tp file after applying the spatial mask\n",
    "\n",
    "file=xr.open_mfdataset(root_data+f'/era5_tp_1959-2021_1_12_daily_2.0deg.nc',\n",
    "                          combine='by_coords',parallel=True)\n",
    "tp_dim=file.sel(longitude=slice(10,70),latitude=slice(24,-30))\n",
    "tp_series=np.multiply(mask_nan,tp_dim).mean(dim='latitude',skipna=True).mean(dim='longitude',skipna=True)\n",
    "\n",
    "# Calculate 33 percentile\n",
    "# Create daily values equal to a 31 day rolling. Select OND 2000-2020 data to decide the quantile threshold\n",
    "tp_rol=tp_series.rolling(time=28, center=False).mean().sel(time=tp_series.time.dt.year.isin([np.arange(2000,2021)]))\n",
    "tp_quantile=tp_rol.sel(time=tp_rol.time.dt.month.isin([10,11,12])).quantile(0.33)\n",
    "print('value of the 33 percentile',tp_quantile.tp.values)\n",
    "\n",
    "\n",
    "# Create index time series\n",
    "# Replace the values bellow the 33 percentile with 1 and the rest with zeros\n",
    "tp_rol = tp_series.rolling(time=28, center=False).mean().sel(time=tp_series.time.dt.month.isin([10,11,12]))\n",
    "tp_rol_sel = tp_rol.sel(time = slice(str(SYY),str(EYY)))\n",
    "tp_index = tp_rol_sel < tp_quantile\n",
    "tp_index = tp_index.sel(time=tp_index.time.dt.year.isin(take_OND_years))\n",
    "tp_index = tp_index.astype(int)\n",
    "print('tp index',tp_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c55ef96-b529-4fa5-9797-037c2c2a1f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select from the index time series the period for the target values (predictant) \n",
    "# Oct 16 to Dec 16 for the period 1980-2020 (each day corresponds to a 31-day rolling mean)\n",
    "# The chosen time period could be changed to any time period you want\n",
    "for iyr in range(SYY,EYY+1):\n",
    "    if iyr == SYY:\n",
    "        tp_target = tp_index.sel(time = slice(str(iyr)+'-10-01',str(iyr)+'-12-01'))\n",
    "    else:\n",
    "        tp_target = xr.concat([tp_target,tp_index.sel(time = slice(str(iyr)+'-10-01',str(iyr)+'-12-01'))], dim='time')\n",
    "print('number of 0 and 1: ',np.unique(tp_target['tp'],return_counts=True))\n",
    "print(tp_target)\n",
    "plt.plot(tp_target.tp)\n",
    "\n",
    "# Make it into a numpy array\n",
    "target2 = tp_target['tp']\n",
    "target = tp_target['tp'].values\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b24ef30-7082-4698-8eaf-4ff563e03ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor data preprocessing\n",
    "# can select the values and region you want by changing the parameters\n",
    "\n",
    "file_vars = ['ERA5_t2m', 'era5_t_850hpa', 'era5_z_200hpa', 'era5_z_500hpa', 'sst', 'era5_olr']\n",
    "#file_vars = ['sst']\n",
    "header_vars = ['t2m', 't', 'z', 'z', 'sst', 'olr-mean']\n",
    "#header_vars = ['sst']\n",
    "\n",
    "# select regions for the individual predictor\n",
    "lon_slices = [[-16,54],[-30,90],[-30,90],[-30,90],[-180,180],[40,180]]\n",
    "lat_slices = [[16,0],[30,-20],[-20,30],[-20,30],[40,-20],[-20,20]]\n",
    "\n",
    "nmode = 5 # for eofs\n",
    "\n",
    "for file_var, header_var, lon_slice, lat_slice in zip (file_vars, header_vars, lon_slices, lat_slices):\n",
    "    # use existing\n",
    "    path = root_results+'/PC_series_n_'+str(nmode)+'_var_'+file_var+'_.nc'\n",
    "    path_eof = root_results+'/EOF_maps_n_'+str(nmode)+'_var_'+file_var+'_.nc'\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        continue\n",
    "    if file_var == 'era5_olr':\n",
    "        file = xr.open_dataset(root_data+file_var+'_1950_2021_daily_1deg_tropics.nc')\n",
    "        print('olr')\n",
    "    else:\n",
    "        file = xr.open_dataset(root_data+file_var+'_1959-2021_1_12_daily_2.0deg.nc')\n",
    "        print(header_var)\n",
    "\n",
    "    if \"longitude\" in file.coords:\n",
    "        file = file.rename({\"longitude\": \"lon\",\"latitude\": \"lat\"})\n",
    "\n",
    "    assert \"lat\" in file.coords\n",
    "    assert \"lon\" in file.coords\n",
    "    \n",
    "    # select region\n",
    "    var_dim = file.sel(lon=slice(lon_slice[0],lon_slice[1]),lat=slice(lat_slice[0],lat_slice[1]))\n",
    "    \n",
    "    # todo: train_valid_test_split: exclude test    \n",
    "    # take years 1980 - 2021 daily and only and 7 day rolling mean\n",
    "    var_series = var_dim.sel(time=var_dim.time.dt.year.isin([np.arange(SYY,EYY+1)])).rolling(time=7, center=False).mean(skipna=True)\n",
    "    var_series = var_series.sel(time=var_series.time.dt.year.isin(take_OND_years))\n",
    "\n",
    "    # remove climatology\n",
    "    var_anom_series = var_series.groupby(\"time.dayofyear\") - var_series.groupby(\"time.dayofyear\").mean(\"time\",skipna=True)\n",
    "    \n",
    "    # use the months you want (base on how long the time series used as predictors)\n",
    "    var_anom_sel = var_anom_series.sel(time=var_anom_series.time.dt.month.isin([7,8,9,10,11]))[header_var]\n",
    "    \n",
    "    # Apply EOF\n",
    "    if file_var == \"era5_z_500hpa\":\n",
    "        header_var = \"z500\"\n",
    "    pc_xr, EOF = get_principle_components_and_EOFs(var_anom_sel, nmode=nmode)\n",
    "    \n",
    "    pc_xr = pc_xr.assign_coords(mode=[str(header_var)+'_'+str(int(m)) for m in pc_xr.mode])\n",
    "    EOF = EOF.assign_coords(mode=[str(header_var)+'_'+str(int(m)) for m in EOF.mode])\n",
    "    # save to disk\n",
    "    pc_xr.to_netcdf(path)\n",
    "    EOF.to_netcdf(path_eof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8939ff-0f79-465d-9b1b-a72ac2eff32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from L_functions import sel_train_data_lead\n",
    "\n",
    "#Create predictor multi-file\n",
    "nc_in_file='PC_serie*.nc'\n",
    "dim_to_stack='mode'\n",
    "pc_xr = xr.open_mfdataset(root_results+\"/\"+nc_in_file,concat_dim=dim_to_stack,\n",
    "                          combine=\"nested\")# \n",
    "\n",
    "# Run the function\n",
    "s_target_date='01-10-1980'\n",
    "e_target_date='01-12-2021'\n",
    "rw_1=7\n",
    "lead_time=15\n",
    "rw=0 # because the data are not centered\n",
    "ntimestep=60\n",
    "target_len=len(tp_target['tp'])\n",
    "\n",
    "predictor_array=sel_train_data_lead(pc_xr, target_len, s_target_date, e_target_date,\n",
    "                rw_1, lead_time, rw, ntimestep)\n",
    "\n",
    "np_out_name='Predictor_array_crosscor_number.nc'\n",
    "\n",
    "#np.save(root_results+np_out_name,predictor_array)\n",
    "predictor_array.to_netcdf(root_results+\"/\"+np_out_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7d2348-30b9-4268-a793-72b9ef86af3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot EOF modes\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cf\n",
    "ii = 1\n",
    "color = 'RdBu_r' \n",
    "colorbarMin = -1\n",
    "colorbarMax = 1\n",
    "colorspace = 0.1\n",
    "level = np.arange(colorbarMin,colorbarMax+colorspace,colorspace)\n",
    "ax = plt.axes(projection=ccrs.cartopy.crs.PlateCarree(central_longitude=180))\n",
    "h = ax.contourf(lon, lat, EOF[ii,:,:], level, transform=ccrs.PlateCarree(), cmap=color,extend='both')\n",
    "cbar = plt.colorbar(h, orientation='horizontal', shrink=1,fraction=0.1,pad=0.1,aspect=40)\n",
    "cbar.ax.tick_params(labelsize=10) \n",
    "colorLabel='SST EOF [K]'\n",
    "cbar.set_label(label=colorLabel,fontsize=10)\n",
    "#Add in the coordinate system:\n",
    "long = np.arange(-180, 180, 45)#spacing of 45 degrees\n",
    "latg = np.arange(-20, 40, 10)#spacing of 15 degrees\n",
    "ax.set_xticks(long, crs=ccrs.PlateCarree());\n",
    "ax.set_yticks(latg, crs=ccrs.PlateCarree());\n",
    "ax.set_xticklabels(long,fontsize=8)\n",
    "ax.set_yticklabels(latg,fontsize=8)\n",
    "ax.set_ylabel('lat',fontsize=10);\n",
    "ax.set_xlabel('lon',fontsize=10);\n",
    "\n",
    "#Add in the continents\n",
    "#define the coastlines, the color (#000000) and the resolution (110m) \n",
    "feature1 = cf.NaturalEarthFeature(\n",
    "    name='coastline', category='physical',\n",
    "    scale='110m',\n",
    "    edgecolor='#000000', facecolor='none')\n",
    "#define the land, the color (#AAAAAA) and the resolution (110m), mask the land, use for SST\n",
    "feature2 = cf.NaturalEarthFeature(\n",
    "    name='land', category='physical',\n",
    "    scale='110m',\n",
    "    facecolor='#AAAAAA')\n",
    "\n",
    "ax.add_feature(feature2)\n",
    "\n",
    "#Set a title for your map:\n",
    "title = 'SST JAS EOF'+str(ii+1)\n",
    "plt.title(title,fontsize=10, y=1.03)\n",
    "\n",
    "fig, axs = plt.subplots(1, figsize=plt.figaspect(0.15))\n",
    "plt.plot(PC[:,ii])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c3b5de64-bc72-4073-9b26-20d4fcffa959",
   "metadata": {},
   "source": [
    "### Causal Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cd16f0-5052-4a3f-9fd7-3653acfecdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hard fix dates intersection\n",
    "dates = tp_index.tp.time.to_index().intersection(pc_xr.time.to_index()).intersection(predictor_array.time.to_index())\n",
    "tp_index = tp_index.sel(time=dates).compute()\n",
    "target2 = target2.sel(time=dates).compute()\n",
    "pc_xr = pc_xr.sel(time=dates).compute()\n",
    "predictor_array = predictor_array.sel(time=dates).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e5da53-a78c-43ba-88e5-c9e21ee6331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tofix: pc_xr.where(pc_xr.isnull().any(\"mode\"),drop=True) shows 36 NaNs towards the end\n",
    "pc_xr = pc_xr.sel(time=slice(\"1980\",\"2021-10-25\"))\n",
    "tp_index = tp_index.sel(time=slice(\"1980\",\"2021-10-25\"))\n",
    "target2 = target2.sel(time=slice(\"1980\",\"2021-10-25\"))\n",
    "predictor_array = predictor_array.sel(time=slice(\"1980\",\"2021-10-25\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa70e15-2258-42ca-bef6-728a7c2e7ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# install with pip install tigramite\n",
    "import tigramite\n",
    "from tigramite import data_processing as pp\n",
    "from tigramite.pcmci import PCMCI\n",
    "from tigramite.independence_tests import ParCorr, CMIknn, GPDC\n",
    "\n",
    "# Example data\n",
    "features_train = pc_xr.pcs\n",
    "sample_size = features_train.time.size\n",
    "N_features = features_train.mode.size\n",
    "allX = features_train.values # np.random.randn(sample_size, N_features)\n",
    "# true_causal_features = [0, 5, 8] # unknown\n",
    "\n",
    "# Target\n",
    "y = tp_index.tp.values # np.random.randn(sample_size) + 3*allX[:, true_causal_features].mean(axis=1)\n",
    "\n",
    "# Construct array needed for tigramite, we need to lag X behind y here just for computation reasons\n",
    "data = np.hstack((y[:-1].reshape(sample_size-1, 1), allX[1:]))\n",
    "\n",
    "# Initialize class with ParCorr test, can be changed to nonlinear CI tests, eg CMIknn, but these use more computation time\n",
    "dataframe = pp.DataFrame(data, var_names = ['target',] + list(range(N_features)))\n",
    "pcmci = PCMCI(\n",
    "    dataframe=dataframe, \n",
    "    cond_ind_test=ParCorr(),  # or CMIknn()  GPDC()\n",
    "    verbosity=0)\n",
    "\n",
    "# Set alpha_level for selecting causal features, the smaller the stricter\n",
    "pc_alpha = 0.01\n",
    "\n",
    "# Only run on target variable\n",
    "selected_links = [(i, -1) for i in range(1, N_features + 1)]\n",
    "causal_predictors = pcmci._run_pc_stable_single(j=0,\n",
    "                              selected_links=selected_links,\n",
    "                              tau_min=1,\n",
    "                              tau_max=1,\n",
    "                              pc_alpha=pc_alpha)\n",
    "\n",
    "# Indices of causal features from X\n",
    "causal_features = [varlag[0] - 1 for varlag in causal_predictors['parents']]\n",
    "print(causal_features)\n",
    "\n",
    "eofs = xr.open_mfdataset(root_results+\"/EOF_*\",concat_dim=dim_to_stack,\n",
    "                          combine=\"nested\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f24d257-e16b-4262-95aa-3ce47474d893",
   "metadata": {},
   "source": [
    "## Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f70108-401b-41a9-ab21-4b2d0ed34bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_OND_years = [2005,2007,2018,2004,2006]\n",
    "drop_MAM_years = [2009,2001,2002,2005,2020]\n",
    "\n",
    "def get_train_test_val(data_predictor, data_target, test_frac, val_frac):\n",
    "    \"\"\"Splits data across periods into train, test, and validation\"\"\"\n",
    "    # assign the last int(-test_frac*len(tp_predictor)) rows to test data\n",
    "    test_predictor = data_predictor[int(-test_frac*len(data_target)):]\n",
    "    test_target = data_target[int(-test_frac*len(data_target)):]\n",
    "    \n",
    "    # assign the last int(-test_frac*len(tp_predictor)) from the remaining rows to validation data\n",
    "    remain_predictor = data_predictor[0:int(-test_frac*len(data_target))]\n",
    "    remain_target = data_target[0:int(-test_frac*len(data_target))]\n",
    "    val_predictor = remain_predictor[int(-val_frac*len(remain_predictor)):]\n",
    "    val_target = remain_target[int(-val_frac*len(remain_predictor)):]\n",
    "    \n",
    "    # the remaining rows are assigned to train data\n",
    "    train_predictor = remain_predictor[:int(-val_frac*len(remain_predictor))]\n",
    "    train_target = remain_target[:int(-val_frac*len(remain_predictor))]\n",
    "    return train_predictor, train_target, test_predictor, test_target, val_predictor, val_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31c5c67-d249-42c8-840b-29c3540a4528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input and output data for LSTM\n",
    "y_all = keras.utils.to_categorical(target2)\n",
    "X_all = predictor_array.pcs\n",
    "print(X_all.shape,y_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bb2440-59f5-43ac-ae0e-36be83fdc7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y, test_X, test_y, val_X, val_y = get_train_test_val(X_all, y_all, test_frac=0.2, val_frac=0.2)\n",
    "visualization.plot_split_counts(train_y, val_y, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8d3b62-4fb1-4cab-b900-a3a5c00afd98",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d766164b-5208-4249-b2b0-96ddd1a1a111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import build_lstm, class_weight_creator\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "shuffle = True \n",
    "verbose = 2 #Set whether the model will output information when trained (0 = no output; 2 = output accuracy every epoch)\n",
    "class_weight = class_weight_creator(train_y)\n",
    "\n",
    "callbacks_path = '/home/zwu/Lorentz_workshop/test/checkpoint_test'\n",
    "callbacks_list = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=callbacks_path,\n",
    "        monitor='val_acc',   # tf.keras.metrics.AUC(from_logits=True)\n",
    "        save_best_only=True,\n",
    "    )\n",
    "]\n",
    "\n",
    "ntimestep = 60    # number of time step used in the predictors\n",
    "nfeature = 30   # number of features\n",
    "## SET kwargs in order to expand lstm ('layers') or change layer size ('neurons' - list of ints with neurons per layer)\n",
    "model = build_lstm(ntimestep, nfeature)\n",
    "\n",
    "opt = optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['acc'])\n",
    "\n",
    "model.summary()\n",
    "# train the model\n",
    "history = model.fit(train_X, train_y, epochs=epochs, batch_size=batch_size, validation_data=(val_X, val_y), shuffle = shuffle, verbose=verbose, class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc08460-98cc-400a-ae53-c5d19e9bac1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
